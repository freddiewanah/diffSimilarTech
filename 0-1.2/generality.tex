\section{The Generality of Approach}
\chen{Will have a look at it later.}
In order to show the generality of our method, we select other two websites from the Stack Exchange Networks that have different domains compare with StackOverflow, which is for professional and enthusiast programmers. The two sites are Super User, which is focus on computer enthusiasts and power users, and Unix \& Linux, which is an Q\&A site for users of Linux, FreeBSD and other Unix-like operating systems. We successfully adopt our approach to both of the websites, and collect 858 comparative sentences out of 431k questions from Superuser as well as 611 comparative sentences out of 171k questions from Unix \& Linux. We also cluster the comparative sentences based on technology pairs, and summarize the overall opinions.

Based on the low number of comparative sentences we have for the two sites, we simplify the experiment to justify the accuracy of our method. For the accuracy of extracting comparative sentences, we randomly select 50 sentences from each site. For Super User, the acucracy is 88\%, and for Unix \& Linux the accuracy is 84\%. In regards of the accuracy of clustering, our model also out performs the other baselines. Finally, similar to what we did in Section~\ref{sec:summarization}, we decrease the test samples from 150 to 50 for each site. The accuracy for Super User is 0.78. For Unix \& Linux, the accuracy is 0.76. More detailed data can be found in our website\footnote{\url{https:///}}.

There are some reasons that we think the performance of our method on the two sites is not as good as it is on StackOverflow. Firstly, the mistake could come from the very first step while we find similar technology pairs. For example, the pair \textit{Linux} and \textit{Ubuntu} are not used for comparing, \textit{Ubuntu} is a subset of \textit{Linux}, but they are often mentioned in the same sentence, which happens a lot in Unix \& Linux. In addition, it seems that users in the two sites are more likely to user the technology pairs side by side to compare with a third party, i.e. \textit{Make sure with shortcuts etc that \textbf{opera} \textbf{firefox} is easier to start find than internet explorer} and \textit{It s could not be as stable as \textbf{rhel} or \textbf{slackware}}. These also explain the decrease of accuracy in summarize the overall opinion, for there are more neutral sentences which are harder to predict. Despite of all the decrease in accuracy, our model still has a relatively high accuracy and performs the best out of all the baselines. These show that
given a corpus of domain-specific data, our approach can be
used to distill similar technologies. 