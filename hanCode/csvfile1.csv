"3des","aes"," is no longer overall, is even lower; overall,faster in slower faster,more overall,much slower in slower faster,ek plaintext with  overall,better overall, is harder overall,","48780283,16836589,3938726,149409,23658313,51387491,25982745,9998385,","this seems like a step backwards as aes is the current standard while 3des is no longer recommended,note that the security margin of 3des is even lower;aes on the other hand can be used with 128 192 and 256 bits all of which are used,should be as fast as 3des aes turned out to be much faster than 3des in software typically 5 to 10 times faster,3des is more expensive than aes for example,furthermore 3des is much slower than aes,ek3 dk2 ek1 plaintext that is the definition of triple des 3des not des that is encrypt plaintext with des as with key 1 decrypt that result with key 2 encrypt that result with key 3 3des or triple des encrypts three time with des;so aes would be ek plaintext with aes,des is the least secure 3des is better but i d go for the aes,3des implementations may be less vulnerable to side channel attacks although it certainly is not immune from it;aes is harder to get right - as i ve come to understand - because of it s internal design table accesses,"
"flot","jqplot","better documentation in free opinion chart,better overall,better chart in free opinion chart, does a better overall,","21896130,6759961,9848654,17604670,","i found jqplot easier to use with more options and better documentation than flot or graphael,by looking at the google groups for qjplot and flot the support for jqplot is better lot s of unanswered questions at the flot group,jqplot is free in my opinion has better chart than flot it doesnâ t have events,i tried to do the same thing with jqplot which has some advantages but doesn t work with bubbles and other kind of series on the same graph;also flot does a better job to synchronise common axis scale with many series,"
"directory","folder","more problem with  in final projéčť characters, there were still older in war dates tomcat, or not in file root files, name not overall, called it api in permissions information hidden, i couldn overall, is not in file root files, which contains much more overall, acts more in normal google application.startuppath,general overall, doesn in context dockerignore aware,","34484169,16374534,10668256,52714644,54009254,38486581,14515513,16583769,48479810,48658614,28222696,","one more problem with folder name projéčť contains the non ascii characters éčť and therefore one more problem with folder name projéčť is important that in text editor used for editing the batch file the correct code page is set as otherwise directory projéčť would not be found,he key is to check the file dates in the tomcat directory of where you deployed the war;it may happen even though you are deploying an entirely new cleaned war and deleting all folder there were still older cached files in there probably because eclipse keeps older cached files to save compile time thinking older cached files don t have any changes,if you didnot copied you plist from resource folder to document directory then first copy it there and then try to access it;first check whether your plist file exist in document directory or not,by the looks of it you want to get a list of directory containing a single _ and then every .jpg file inside those folder appending it to a text file;this is pretty simple there are a few issues in your code the usage of -name here returns only the folder name not the full path,i needed to know which folder is more suitable to host my rest api in yii2 template advanced backend or frontend . i also saw some examples where developers created a new directory called it api and programmed the api from there but i don t find much information by developing from this method. if you know the most correct way to do it i would apreciate if you could explain me,resizing the same image regardless of how many other files are in the same folder now takes the same time;looking at the release notes of 1.3.22 this may have fixed it since mentions many files in the directory i couldn t find the exact commit,in godaddy web hosting you should mark set application root to the folder sub-folder containing your application;making it a virtual directory is not enough,as for how to use this folder even though there is no detailed documentation on developer.android.com there is a documentation.html file in the ndk directory which contains much more information,while sync is paused your google drive folder acts more like a normal directory,if you have rename installed a.k.a perl rename you can do it very simply with a one-liner including making the target directory sample output other_sh would be renamed to folder other_priv sample1_sh would be renamed to folder sample1_priv when you become more familiar with the switches that will become even shorter if you are not that familiar with perl there are two substitutions going on in the middle there separated by a comma the first substitution replaces which means start of line with the prefix folder,git will ignore the context of a folder if the folder doesn t already contain tracked paths;you know by the presence of test_fold that there are files in there git will never show you an empty directory as being untracked and you know that those files are not tracked,"
"directory","folder"," or enter the information in permissions information hidden, permissions is a bit overall, do not overall, but not in file root files, opening a file in project drawable clean, can not overall, to remove downloaded nuget overall, and not overall, to project eclipse overall,more overall,much more overall,","40986554,3972208,3587158,34576363,15050488,13534900,44171563,5118859,49659080,31870629,44990287,","the new location of the folder does not have proper permissions;you will have to change the permissions of that directory or enter the information that wordpress is asking you for,well creating the folder is very easy;setting the directory permissions is a bit more difficult,the temporary upload folder is specified in php settings as upload_tmp_dir;make sure your file destination and tmp upload directory do not,if it is exist in sub folder then write as below classpath subfolder idm.properties;make sure your properties file exist in classpath directory but not in sub folder of your classpath directory,when you run the application inside visual studio the current directory points to your project s root folder not the application s executalbe path;also some operations can change the current directory opening a file using openfiledialog,note if the folder was already tracked you can untrack it with;i have struggled with the same problem and learned that the the command must be run in the parent folder and that the ignored directory can not be a sub directory,if you wish you can also remove bin folder not mandatory though as they should already be empty after step 1;delete packages folder from your solution directory to remove downloaded nuget packages,if you are getting compile errors not linker errors make sure libav folder reside in usr local include and you include ffmpeg headers with their folder names;you have to define usr local include as an include directory and not libav folder inside,that looks very primitive but has one advantage - manually added source folder is not removed on right click - maven - update project;although your plugin adds extra source directory to project eclipse cannot recognize that,i have a directory which has more than 10 folder,the first option is easy with one directory but becomes much more tedious with multiple folder not to mention that windows will likely create an object in the target directory before it has finished copying and i am compressing files potentially over 10 gb so that solution won t work,"
"directory","folder"," is more in documents term appropriate, so i overall, is not in repo local git, not overall, that is the root in file root files, which contains user in files file faster, that mac linux in file root files, that is closer just overall, and not in current success container,working link as the  in project drawable clean, does not overall,","5078792,35459067,9093506,56281698,17963477,51130059,52911315,16270842,46454263,35494994,57720702,","if one is referring to a container of documents the term folder is more appropriate;the term directory refers to the way a structured list of document files and folder is stored on the computer,btw microsoft azure storage explorer in my case showed only some subset of folder which is much worse than displaying duplicated directory so i switched to azure explorer mentioned above and it s worth recommending,you pass in the root folder and backup destination and it finds all folder tests if it is a mercurial repo looking for a .hg directory and clones the repo to the backup destination;if the folder is not a mercurial repo then it does the recursion itself,so when doing -v c data c app you hidden from your container all your application files inside app and left with only the files that exists in the host s c data directory possible solution create entrypoint script then mount the host folder to temp docker run -v c data c temp test;when you are mounting volume with docker you are overriding the directory not adding to it,however here the target directory is not the site-packages folder but the;prefix folder that is the root of the venv,dr i want the installer to either overwrite or delete all files in the application directory except for one folder which contains user data,let s create that folder and i ll name it bin after the directory that mac linux hold its scripts in,if you are using a linux mac unix machine most of time i have had to either start from the root folder such as users .....to the source folder or when using a directory that is closer just use in front of the computer,this will make sure that your current directory also is mounted in container and you start in the same folder inside the container also;the issue is most probably that your mail.conf is in your current folder and not inside the container,you have to create a folder to store the images in your project and then call to it your folder name in front of the image name;otherwise you can find the image by going through the entire directory which isn t a good way as it takes longer and when you move your project it won t be a working link as the directory will be different,in your file directory that contains your ipynb file there is a folder called .ipynb_checkpoints;this folder does not show in the jupyter application so find it through windows explorer. inside there s will be a file called urfilenamehere-checkpoint.ipynb copy paste it to your file directory and open through the jupyter application it should probably work,"
"directory","folder"," is master then overall,less than 30  overall,faster ls in files file faster,less in files file faster, may not overall,less overall, not in msp debu reading,bigger in file root files, it seems less in array files program,more in file root files, makes it easier in file root files,","52722802,56696905,32341733,15032819,37603728,4225772,56971061,27509536,52136614,24197390,24016276,","imagine you have the following folder structure master | +-- folder1 | +-- file1.csv +-- file2.csv +-- folder2 | +-- file1.csv +-- file2.csv and your working directory is master then you can do the following,conditions the script will delete the files 45 days old and less than 30 folder in directory it will not delete any,if i run ls folder | head in a directory with a lot of files the execution time is about 50 times faster than ls folder | tail,i have tested on mp3 files in different directory it takes less than one second with over 6000 files stored in different folder,as it says here you can make asset bundle in common folder and set property sourcepath using an alias or absolute path to directory with your scripts and styles;the scripts directory may not be web-accessible,test code updated so you can see that both it works identically whether using varargs or not will fail if your home directory has less than 5 sub folder,done sleeping for 15 seconds to wait for fabric to complete start up 2019-07-10 10 56 11.397 utc viperutil getkeysrecursively - debu 001 found map string interface value for peer.bccsp2019-07-10 10 56 11.399 utc viperutil getkeysrecursively - debu 002 found map string interface value for peer.bccsp.pkcs11 2019-07-10 10 56 11.399 utc viperutil unmarshaljson - debu 003 unmarshal json value is not a string 2019-07-10 10 56 11.400 utc viperutil getkeysrecursively - debu 004 found real value for peer.bccsp.pkcs11.security setting to 2019-07-10 10 56 11.400 utc viperutil getkeysrecursively - debu 005 found map string interface value for peer.bccsp.pkcs11.filekeystore 2019-07-10 10 56 11.401 utc viperutil unmarshaljson - debu 006 unmarshal json value is not a string 2019-07-10 10 56 11.402 utc viperutil getkeysrecursively - debu 007 found real value for peer.bccsp.pkcs11.filekeystore.keystore setting to 2019-07-10 10 56 11.402 utc viperutil unmarshaljson - debu 008 unmarshal json value is not a string 2019-07-10 10 56 11.403 utc viperutil getkeysrecursively - debu 009 found real value for peer.bccsp.pkcs11.library setting to 2019-07-10 10 56 11.403 utc viperutil unmarshaljson - debu 00a unmarshal json value is not a string 2019-07-10 10 56 11.403 utc viperutil getkeysrecursively - debu 00b found real value for peer.bccsp.pkcs11.label setting to 2019-07-10 10 56 11.404 utc viperutil unmarshaljson - debu 00c unmarshal json value is not a string 2019-07-10 10 56 11.404 utc viperutil getkeysrecursively - debu 00d found real value for peer.bccsp.pkcs11.pin setting to 2019-07-10 10 56 11.404 utc viperutil unmarshaljson - debu 00e unmarshal json value is not a string 2019-07-10 10 56 11.405 utc viperutil getkeysrecursively - debu 00f found real value for peer.bccsp.pkcs11.hash setting to 2019-07-10 10 56 11.405 utc viperutil unmarshaljson - debu 010 unmarshal json value cannot be unmarshalled invalid chars s looking for beginning of value 2019-07-10 10 56 11.405 utc viperutil getkeysrecursively - debu 011 found real value for peer.bccsp.default setting to string sw 2019-07-10 10 56 11.405 utc viperutil getkeysrecursively - debu 012 found map string interface value for peer.bccsp.sw 2019-07-10 10 56 11.406 utc viperutil unmarshaljson - debu 013 unmarshal json value cannot be unmarshalled invalid chars s looking for beginning of value 2019-07-10 10 56 11.406 utc viperutil getkeysrecursively - debu 014 found real value for peer.bccsp.sw.hash setting to string sha2 2019-07-10 10 56 11.406 utc viperutil unmarshaljson - debu 015 unmarshal json value is not a string 256 2019-07-10 10 56 11.406 utc viperutil getkeysrecursively - debu 016 found real value for peer.bccsp.sw.security setting to int 256 2019-07-10 10 56 11.407 utc viperutil getkeysrecursively - debu 017 found map string interface value for peer.bccsp.sw.filekeystore 2019-07-10 10 56 11.407 utc viperutil unmarshaljson - debu 018 unmarshal json value cannot be unmarshalled unexpected end of json input 2019-07-10 10 56 11.407 utc viperutil getkeysrecursively - debu 019 found real value for peer.bccsp.sw.filekeystore.keystore setting to string 2019-07-10 10 56 11.407 utc viperutil enhancedexactunmarshalkey - debu 01a map peer.bccsp map pkcs11 map pin hash security filekeystore map keystore library label default sw sw map hash sha2 security 256 filekeystore map keystore 2019-07-10 10 56 11.417 utc bccsp_sw openkeystore - debu 01b keystore opened at etc hyperledger peer msp keystore ...done 2019-07-10 10 56 11.418 utc bccsp initbccsp - debu 01c initialize bccsp sw 2019-07-10 10 56 11.418 utc msp getpemmaterialfromdir - debu 01d reading directory etc hyperledger peer msp signcerts 2019-07-10 10 56 11.431 utc msp getpemmaterialfromdir - debu 01e inspecting file etc hyperledger peer msp signcerts peer0.org1.example.com-cert.pem 2019-07-10 10 56 11.439 utc msp getpemmaterialfromdir - debu 01f reading directory etc hyperledger peer msp cacerts 2019-07-10 10 56 11.452 utc msp getpemmaterialfromdir - debu 020 inspecting file etc hyperledger peer msp cacerts ca.org1.example.com-cert.pem 2019-07-10 10 56 11.457 utc msp getpemmaterialfromdir - debu 021 reading directory etc hyperledger peer msp admincerts 2019-07-10 10 56 11.472 utc msp getpemmaterialfromdir - debu 022 inspecting file etc hyperledger peer msp admincerts admin org1.example.com-cert.pem 2019-07-10 10 56 11.481 utc msp getpemmaterialfromdir - debu 023 reading directory etc hyperledger peer msp intermediatecerts 2019-07-10 10 56 11.482 utc msp getmspconfig - debu 024 intermediate certs folder not found at etc hyperledger peer msp intermediatecerts,unfortunately the directory also is a git repository and has a .git folder in it which is much bigger than the directory itself,here is a simple program to list all files and directory in a folder it seems less complicated than your code. it should be easy to find out by looking at the dirent structure for each entry in the folder whether or not it is a directory and if so abort,i need help in a vba macro that ll delete files in a directory that contains more than 2 _ and is older than 3 months old however there are some folder sub folder in the directory that must not be touched or modified, better solution instead of copying the linked files to the current project directory alongside the associated links copy the linked files to a hiddendebug folder within that directory or any directory you wish;keeping it in the current project directory makes it easier to manage and then to manipulate the paths during debug as i ll explain below,"
"directory","folder"," not there in project drawable clean, doesn overall,no more in files file faster, is not in file root files, do not overall, does not in files file faster, which contains zip_this_ overall, is shallower overall,nothing more then overall, will not in file root files,further deeper overall,","55662565,26124108,16864778,31904213,54933843,46530578,52197196,12645320,45985134,42704881,34350031,","go to root project cd android . gradlew clean in root folder run following command if there was index.js file react-native bundle --platform android --dev false --entry-file index.js --bundle-output android app src main assets index.android.bundle --assets-dest android app src main res if there are two files index.android.js and index.ios.js then run react-native bundle --platform android --dev false --entry-file index.android.js --bundle-output android app src main assets index.android.bundle --assets-dest android app src main res above two are single line commands;go to your project folder android app src main assets delete index.android.bundle and index.android.bundle.meta if assets directory not there create new one,following code allows to check if certain folder exists in apk bundle;aasetmanager_opendir will always return a pointer to initialized object even if the specified directory doesn t exist,i m starting in the base directory and i know that the the directory goes no more than 1 folder deep past the base directory,website root folder is a subfolder in your project directory named wwwroot by default but can be changed in project.json;this means that everything outside a website root folder is not accessible to the outside world,while amazon s3 does support the concept of a common prefix which can make things appear as though they are in folder directory folder do not actually exist;for example you could run a command like this aws s3 cp foo.txt s3 my-bucket folder1 folder2 foo.txt this would work even if the folder do not exist,put your software in the batch file folder and try again;your batch file may be working based on a working directory and because you run the batch file with software that is in a different folder the batch file s working directory does not match,change working directory to folder which contains zip_this_folder first cd desktop zip -r zip_this_folder zip_this_folder,the new directory is shallower than the previous one,actually adding a new resource folder does nothing more then adding it as a resource directory in build.gradle,this is because a submodules git directory is located at folder not inside the submodule itself unless you created your submodule on top of an exsiting repository in the first place in which case you can run git submodule absorbgitdirs to fix this;note that from a fresh clone the folder will not exist until you run git submodule update for the first time,if you really need to enter the sub-directory however you can create recursive calls and in the call you can check if the directory has further deeper folder by using directoryinfo.getdirectories,"
"directory","folder"," aren in file root files, history is easier in file root files, is not invariably in file root files,also faster in files file faster, is not in file root files, not in file root files, and execute the command in file root files, can not overall, not directly overall, resulting in output in path specified current, which has jasper jrxml in files file faster,","11653341,34236506,37959011,23086696,13053274,33422591,57300748,26054790,52266773,52451714,24183805,","assuming the folder aren t visible to your users give each user gallery a guid;then create a directory path using the first 3 characters of the guid like so,as described in moving files from one git repository to another preserving history this is easier if the file s you want to import are in a folder exporting importing a folder with a folder history is easier than just a file;goal move directory 1 from git repository a to git repository b,instead the files in these directory must be individually symlinked to the target folder which is either a preexisting folder or one that must be created as a regular folder on demand;that is the only way to guarantee that the existing content of the target folder is not invariably lost,rm -rf directory also works faster for billion of files in one folder,10k files inside a single folder is not a problem on ext4;it should have the dir_index option enabled by default which indexes directory content using a btree-like structure to prevent performance issues,if your logo is not in images folder then delete the reference for images folder;if your logo is in images folder in root directory not in your wp theme,android app src main assets if the folder assets doesn t exist then create the assets directory there;if it exists then delete two files viz index.android.bundle and index.android.bundle.meta then return to the root project directory and execute the command,if the command was successful the is an equivalent to if there are no errors the folder is now the current active directory so remove anything inside it rmdir;s q as it is the current directory it can not be deleted the hides the error output when the current folder can not be removed,your hadoop_conf_dir need to be spark s or hadoop s conf folder not the base folder from the spark extraction;this directory must contain core-site.xml yarn-site.xml hdfs-site.xml and hive-site.xml if using hivecontext then if yarn-site.xml is in the above location you don t need yarn_conf_dir but if you do set it it needs to be an actual directory not directly to the file,the subroutine renamefile is never executed if the specified folder does not exist or is currently not accessible;fifth solution the folder path is used on command dir which is executed with option s to search also in subdirectories of specified directory resulting in output of name of files with full path,you need to copy folder which has jasper jrxml files and put jasper jrxml files same directory of your jar file,"
"directory","folder"," is not overall, is not overall, which is way better overall,more than 1  in repo local git, to .tar file i in array files program, is no longer in file root files,more than one  in path beginning to, should not overall, that ll store overall,lower than  in files file faster, not overall,","52206580,24855611,13569169,57228073,15532326,37211666,50395713,2781823,11700961,54714657,27405441,","in intellij right-click your test resources or test java resources directory select the mark directory as... sub-menu select test resources root this directory will appear with a decorator graphic symbolizing that it is in the classpath in eclipse go to run- run configurations... in case of debug run- debug configurations... open run debug configuration which you use open classpath tab select user entries and click advanced... on the right in the opened window select add folder point to your src test resources this folder will appear under user entries then you should move it up to make it the first on your classpath;depending on your ide it is possible that your resource directory is not in the classpath,this is a snippet of code from an application i developed a little time as i had very few pictures put the assets folder and used the universalimagerloager lib to manage the loading of images to views;the internal directory is not accessible by the wizard when you create the first instance of your class database one that extends sqliteopenhelper your nome_banco.db is created in that folder data data package_name databases nome_banco.db if you install on avd you can navigate to that directory by ddms,basically you can set group owner and acl for the whole folder which is way better than setting everything to world writable on the host;i have not found any method to change the ownership of a nested directory,check it out no pun intended on windows run in cmd.exe otherwise usage # makes a directory forstackexchange with plug checked out git sparse-checkout plug # to do more than 1 directory you have to specify the local directory git sparse-checkout forstackexchange plug folder the git config commands are minified for convenience and storage but here is the alias expanded,concerning the basic fs node.js module i don t know if it is possible to read a directory i m not talking about getting all the files in an array using fs.readdir but handling all the files and their organization in folder;then when i convert the folder to .tar file i can compress it using gzip of zlib,when you move into the target directory the src main webapp folder is no longer available so the jsps stop working,so the common idiom for tasks like this is which would work even if folder were a complete path and more than one directory along the path needed to be added,you should never ever add the obj folder to version control as it is a folder completely managed by msbuild which will add and remove files and subdirectories whenever it likes to;the bin directory should not note the difference be versioned as it contains output instead of source files,since copying full folder isn t build in - you should write your own function to do so;make new directory that ll store copies of the files function mkdir,get-childitem server share folder1 succeeds set-location server share folder1 fails edit evidently it is possible to use set-location on directory lower than folder 1,use layout and layout-land folder not values and values-land;in those directory put separate versions of your my_layout.xml,"
"directory","folder"," structure is per repository in files file faster, structure then in project drawable clean, not in file root files, not in file root files, not only in permissions information hidden, is not in file root files, not in msp debu reading,better overall, were not set correctly in file root files, for running command process in path specified current, atob has more in files file faster,","2335441,48893308,28731696,1298254,10265852,53688155,56971061,46428335,30041566,50656521,41774940,","you have to look at your repository s folder not the program folder;the directory structure is per repository,as far as i know this problem occur when we paste image from explorer to drawable folder then it ask weather to copy drawable or drawable-v24 and if you have pasted images in both directory structures or have one more images with different directory structure then it will show this error make sure to copy image only in drawable directory structure,you should create a templatetags package under your app folder and put the tags.py in it;do not forget to include an empty __init__.py file in your templatetags directory otherwise python interpreter will think that it is an ordinary directory not a python package,to check if the folder exist use directory.exists method;look like the bleh in the path is a directory not a file,please note that older version of svn use to create a hidden .svn folder in all folder not only in the root folder or direct subfolders;you could also do an export command to create a full non-versioned copy in a target directory to avoid the manual delete of .svn folder,this is because the .htaccess file is disabling directory listing withing public folder for your second problem which is you are unable to visit laravel index page links this is because laravel rewrite url rules are not working and to enable them you sould enable apache2 rewrite module as follow sudo a2enmod rewrite then restart your webserver sudo systemctl restart apache2 now your problem should be fixed;for your first problem the public folder is not showing but you can visit it,stat etc hyperledger peer msp crls no such file or directory 2019-07-10 10 56 11.529 utc msp newbccspmsp - debu 02b creating bccsp-based msp instance 2019-07-10 10 56 11.529 utc msp new - debu 02c creating cache-msp instance 2019-07-10 10 56 11.530 utc msp loadlocamsp - debu 02d created new local msp 2019-07-10 10 56 11.530 utc msp setup - debu 02e setting up msp instance org1msp 2019-07-10 10 56 11.530 utc msp identity newidentity - debu 02f creating identity instance for cert -----begin certificate----- -----end certificate----- 2019-07-10 10 56 11.531 utc msp identity newidentity - debu 030 creating identity instance for cert -----begin certificate----- -----end certificate----- 2019-07-10 10 56 11.578 utc msp identity newidentity - debu 031 creating identity instance for cert -----begin certificate----- -----end certificate----- 2019-07-10 10 56 11.584 utc bccsp_sw loadprivatekey - debu 032 loading private key dfb17cf51dc061d585b4850599be0e4b8b7cc8cc363a67c23bc03c6c5393b0e0 at etc hyperledger peer msp keystore dfb17cf51dc061d585b4850599be0e4b8b7cc8cc363a67c23bc03c6c5393b0e0_sk ... 2019-07-10 10 56 11.590 utc msp identity newidentity - debu 033 creating identity instance for cert -----begin certificate----- -----end certificate----- 2019-07-10 10 56 11.591 utc msp setupsigningidentity - debu 034 signing identity expires at 2027-06-24 12 49 26 +0000 utc 2019-07-10 10 56 11.591 utc msp validate - debu 035 msp org1msp validating identity 2019-07-10 10 56 11.593 utc msp getdefaultsigningidentity - debu 036 obtaining default signing identity 2019-07-10 10 56 11.594 utc grpc printf - debu 037 parsed scheme 2019-07-10 10 56 11.594 utc grpc printf - debu 038 scheme not registered fallback to default scheme 2019-07-10 10 56 11.594 utc grpc printf - debu 039 ccresolverwrapper sending new addresses to cc orderer.example.com 7050 0 2019-07-10 10 56 11.594 utc grpc printf - debu 03a clientconn switching balancer to pick_first 2019-07-10 10 56 11.594 utc grpc printf - debu 03b pickfirstbalancer handlesubconnstatechange 0xc4203a2490 connecting 2019-07-10 10 56 11.596 utc grpc printf - debu 03c pickfirstbalancer handlesubconnstatechange 0xc4203a2490 ready 2019-07-10 10 56 11.596 utc channelcmd initcmdfactory - info 03d endorser and orderer connections initialized 2019-07-10 10 56 11.616 utc msp getdefaultsigningidentity - debu 03e obtaining default signing identity 2019-07-10 10 56 11.616 utc msp getdefaultsigningidentity - debu 03f obtaining default signing identity 2019-07-10 10 56 11.616 utc msp identity sign - debu 040 sign plaintext 0aa2060a074f7267314d53501296062d...6d706f736572436f6e736f727469756d 2019-07-10 10 56 11.616 utc msp identity sign - debu 041 sign digest e89b62d070a85640910f9f5f645001c6073874d4a31bebb32d07d3819b80816e 2019-07-10 10 56 11.616 utc msp getdefaultsigningidentity - debu 042 obtaining default signing identity 2019-07-10 10 56 11.616 utc msp getdefaultsigningidentity - debu 043 obtaining default signing identity 2019-07-10 10 56 11.616 utc msp identity sign - debu 044 sign plaintext 0adf060a1b08021a0608cb8897e90522...a4a5f110bbf8b5eeb10b57a114b4d087 2019-07-10 10 56 11.617 utc msp identity sign - debu 045 sign digest a49a923a48ea3b06d18d9eec625b752cefc3b76140e5bc6e6c6f4a1dc4bd503d 2019-07-10 10 56 11.617 utc grpc printf - debu 046 parsed scheme 2019-07-10 10 56 11.617 utc grpc printf - debu 047 scheme not registered fallback to default scheme 2019-07-10 10 56 11.617 utc grpc printf - debu 048 ccresolverwrapper sending new addresses to cc orderer.example.com 7050 0 2019-07-10 10 56 11.617 utc grpc printf - debu 049 clientconn switching balancer to pick_first 2019-07-10 10 56 11.617 utc grpc printf - debu 04a pickfirstbalancer handlesubconnstatechange 0xc4203a2dd0 connecting 2019-07-10 10 56 11.620 utc grpc printf - debu 04b pickfirstbalancer handlesubconnstatechange 0xc4203a2dd0 ready 2019-07-10 10 56 11.739 utc msp getdefaultsigningidentity - debu 04c obtaining default signing identity 2019-07-10 10 56 11.744 utc msp getdefaultsigningidentity - debu 04d obtaining default signing identity 2019-07-10 10 56 11.744 utc msp identity sign - debu 04e sign plaintext 0adf060a1b08051a0608cb8897e90522...f049159f678012080a021a0012021a00 2019-07-10 10 56 11.744 utc msp identity sign - debu 04f sign digest cb16bbfb813ac43bf639c51335f77e717e86385435709bc9dbb8929a0a45b539 2019-07-10 10 56 11.746 utc cli common readblock - info 050 got status not_found 2019-07-10 10 56 11.746 utc msp getdefaultsigningidentity - debu 051 obtaining default signing identity 2019-07-10 10 56 11.746 utc grpc printf - debu 052 parsed scheme 2019-07-10 10 56 11.746 utc grpc printf - debu 053 scheme not registered fallback to default scheme 2019-07-10 10 56 11.746 utc grpc printf - debu 054 ccresolverwrapper sending new addresses to cc orderer.example.com 7050 0 2019-07-10 10 56 11.746 utc grpc printf - debu 055 clientconn switching balancer to pick_first 2019-07-10 10 56 11.748 utc grpc printf - debu 056 pickfirstbalancer handlesubconnstatechange 0xc4203a3340 connecting 2019-07-10 10 56 11.757 utc grpc printf - debu 057 pickfirstbalancer handlesubconnstatechange 0xc4203a3340 ready 2019-07-10 10 56 11.757 utc channelcmd initcmdfactory - info 058 endorser and orderer connections initialized 2019-07-10 10 56 11.958 utc msp getdefaultsigningidentity - debu 059 obtaining default signing identity 2019-07-10 10 56 11.960 utc msp getdefaultsigningidentity - debu 05a obtaining default signing identity 2019-07-10 10 56 11.960 utc msp identity sign - debu 05b sign plaintext 0adf060a1b08051a0608cb8897e90522...0aee0140b36a12080a021a0012021a00 2019-07-10 10 56 11.963 utc msp identity sign - debu 05c sign digest 1ba003a36e3cc432819aaf51655c765cd96203cddf9ae451639c338f71077457 2019-07-10 10 56 12.001 utc cli common readblock - info 05d received block 0 2019-07-10 10 56 12.454 utc viperutil getkeysrecursively - debu 001 found map string interface value for peer.bccsp2019-07-10 10 56 12.455 utc viperutil unmarshaljson - debu 002 unmarshal json value cannot be unmarshalled invalid chars s looking for beginning of value 2019-07-10 10 56 12.455 utc viperutil getkeysrecursively - debu 003 found real value for peer.bccsp.default setting to string sw 2019-07-10 10 56 12.455 utc viperutil getkeysrecursively - debu 004 found map string interface value for peer.bccsp.sw 2019-07-10 10 56 12.456 utc viperutil unmarshaljson - debu 005 unmarshal json value is not a string 256 2019-07-10 10 56 12.456 utc viperutil getkeysrecursively - debu 006 found real value for peer.bccsp.sw.security setting to int 256 2019-07-10 10 56 12.456 utc viperutil getkeysrecursively - debu 007 found map string interface value for peer.bccsp.sw.filekeystore 2019-07-10 10 56 12.456 utc viperutil unmarshaljson - debu 008 unmarshal json value cannot be unmarshalled unexpected end of json input 2019-07-10 10 56 12.456 utc viperutil getkeysrecursively - debu 009 found real value for peer.bccsp.sw.filekeystore.keystore setting to string 2019-07-10 10 56 12.457 utc viperutil unmarshaljson - debu 00a unmarshal json value cannot be unmarshalled invalid chars s looking for beginning of value 2019-07-10 10 56 12.457 utc viperutil getkeysrecursively - debu 00b found real value for peer.bccsp.sw.hash setting to string sha2 2019-07-10 10 56 12.457 utc viperutil getkeysrecursively - debu 00c found map string interface value for peer.bccsp.pkcs11 2019-07-10 10 56 12.457 utc viperutil unmarshaljson - debu 00d unmarshal json value is not a string 2019-07-10 10 56 12.463 utc viperutil getkeysrecursively - debu 00e found real value for peer.bccsp.pkcs11.library setting to 2019-07-10 10 56 12.464 utc viperutil unmarshaljson - debu 00f unmarshal json value is not a string 2019-07-10 10 56 12.465 utc viperutil getkeysrecursively - debu 010 found real value for peer.bccsp.pkcs11.label setting to 2019-07-10 10 56 12.466 utc viperutil unmarshaljson - debu 011 unmarshal json value is not a string 2019-07-10 10 56 12.469 utc viperutil getkeysrecursively - debu 012 found real value for peer.bccsp.pkcs11.pin setting to 2019-07-10 10 56 12.470 utc viperutil unmarshaljson - debu 013 unmarshal json value is not a string 2019-07-10 10 56 12.471 utc viperutil getkeysrecursively - debu 014 found real value for peer.bccsp.pkcs11.hash setting to 2019-07-10 10 56 12.472 utc viperutil unmarshaljson - debu 015 unmarshal json value is not a string 2019-07-10 10 56 12.472 utc viperutil getkeysrecursively - debu 016 found real value for peer.bccsp.pkcs11.security setting to 2019-07-10 10 56 12.472 utc viperutil getkeysrecursively - debu 017 found map string interface value for peer.bccsp.pkcs11.filekeystore 2019-07-10 10 56 12.473 utc viperutil unmarshaljson - debu 018 unmarshal json value is not a string 2019-07-10 10 56 12.473 utc viperutil getkeysrecursively - debu 019 found real value for peer.bccsp.pkcs11.filekeystore.keystore setting to 2019-07-10 10 56 12.473 utc viperutil enhancedexactunmarshalkey - debu 01a map peer.bccsp map default sw sw map security 256 filekeystore map keystore hash sha2 pkcs11 map label pin hash security filekeystore map keystore library 2019-07-10 10 56 12.484 utc bccsp_sw openkeystore - debu 01b keystore opened at etc hyperledger msp users admin org1.example.com msp keystore ...done 2019-07-10 10 56 12.486 utc bccsp initbccsp - debu 01c initialize bccsp sw 2019-07-10 10 56 12.486 utc msp getpemmaterialfromdir - debu 01d reading directory etc hyperledger msp users admin org1.example.com msp signcerts 2019-07-10 10 56 12.498 utc msp getpemmaterialfromdir - debu 01e inspecting file etc hyperledger msp users admin org1.example.com msp signcerts admin org1.example.com-cert.pem 2019-07-10 10 56 12.530 utc msp getpemmaterialfromdir - debu 01f reading directory etc hyperledger msp users admin org1.example.com msp cacerts 2019-07-10 10 56 12.538 utc msp getpemmaterialfromdir - debu 020 inspecting file etc hyperledger msp users admin org1.example.com msp cacerts ca.org1.example.com-cert.pem 2019-07-10 10 56 12.548 utc msp getpemmaterialfromdir - debu 021 reading directory etc hyperledger msp users admin org1.example.com msp admincerts 2019-07-10 10 56 12.556 utc msp getpemmaterialfromdir - debu 022 inspecting file etc hyperledger msp users admin org1.example.com msp admincerts admin org1.example.com-cert.pem 2019-07-10 10 56 12.566 utc msp getpemmaterialfromdir - debu 023 reading directory etc hyperledger msp users admin org1.example.com msp intermediatecerts 2019-07-10 10 56 12.567 utc msp getmspconfig - debu 024 intermediate certs folder not found at etc hyperledger msp users admin org1.example.com msp intermediatecerts . skipping,and before copying the directory it is better to zip the entire stata8 folder - that will significantly reduce copying time since there are plenty of small-sized files,this will chmod all files in your html folder which is not recommended for production for security reasons but should let you see the files in that folder to be sure that isn t the issue while troubleshooting;in my case it was because the permissions on the root web directory were not set correctly,the first reason for not deleting a folder is used by this command line to delete all files and subfolders of the specified folder but not the folder itself;the folder is made temporarily the current directory for running command process which prevents the deletion of the folder itself,if directory folder atob has more than 1 item aaa.xyz let prune other folder file in same way,"
"directory","folder"," doesn in current success container,way more in files file faster, which is more in users likely home, that contains settings.py overall, structure is more in files file faster, which is harder in file root files, not in file root files,longer overall,more in system important systems, was not setup correctly overall, not in file root files,","36876497,26279419,21651351,51935176,36868698,46459455,43836305,44498194,31065144,27214727,45444727,","if you are trying to check for a certain filename you should check it in filenames and if it is a success the current folder is root;directory doesn t enter the picture,this folder has way more than 1 directory and has a bunch of files so apparently it snot reading something correctly,event if you decided to use c temp the folder may not exist and you may not have the required privileges to either create it or write to it;you could use system.getproperties user.home which will return the current users home directory which is more likely to allow you to write to it,a new .py file called with apps.py in the main folder same folder that contains settings.py you are put the apps.py module in the wrong location apps.py should be inside your app s directory,in order to only copy directory beginning with test you could do the following;if your folder structure is more than one level deept you could use the -recurse switch on get-childitem,running a directory from an ide a directory s the root project folder which is harder to deal with,copying an empty text file to every single folder did the trick - i was able to delete the complete node_modules folder;i m on windows 10 and i could nt delete folders with message directory not emtpy,the problem is that the dropbox folder is no longer in my user directory so r cannot find the directory.,i find the best way is to create a cache session folder in your systems directory is more safer i put important things like logs and cache in system rather than application folder,a the svn folder was not readable by apache user;b possibly the directory was not setup correctly,determine the full path with pwd var folder b4 qw_sstpn1jqd8ypb1zgc7vww0000gn 0 com.apple.notificationcenter db;this references a directory not a file yet,"
"directory","folder"," are usually in files file faster, which is no longer in public docroot meteor, called inbox in documents term appropriate, will not in context dockerignore aware, creates master in master branch ctrl+z,more overall, not overall,more than one  overall, is not overall,more overall, or not overall,","49639764,11937959,47415226,52066134,56548390,33242205,20269167,57178709,36202432,12469246,17021594,","directory that contains more than one module and sometimes other folders directory are usually kept in one of two places c program files python lib site-packages and,if this is the case move everything in the public directory one folder up so the case s in the docroot again and remove the public folder which is no longer needed,these days the folder is not called inbox but the name of the folder seems to be the bundleid of your app;the files that are being imported by the application are actually copied into a directory called inbox within the apps documents directory,so with buildkit if you do not have a copy command that includes back in your front image then that folder will not be included in the context without you needing to specify it in your .dockerignore file;there currently isn t a way to have two different .dockerignore files in the same directory that i m aware of,2- ctrl+z to stop after your project folder creates master directory folder you can see your project folder in your cocoa pods folder location .cocoapods repos download .zip from master branch your project folder 301 mb extract your project folder,i have makefile .mke on windows and i need to count directory in a folder which start with install and then make if clause to throw an exception if there are more than one directory like this,in this scenario the resulting location of your file is the iis process startup directory not a folder inside you web site;if you want to reach a file inside your web site folder you use server.mappath,but i am finding problem to decide the directory because classes are present in more than one folder,this function expects the folder to be empty;since your directory is not empty a fallback comes into play which relies on php s filesystemiterator,i have a directory containing more than 1000 folder,check if a folder exists already using this;dbmetadata has a property bool isdirectory which helps you to check if it is a directory or not,"
"directory","folder"," not overall, and not overall, structure s better in file root files,more in final projéčť characters, to base your resource in file root files, and not overall, is users user in user home private, not working;use in public docroot meteor,more than two  overall, not found for option overall, which isn overall,","57515886,35155294,14947156,17591179,9370706,57606421,13139636,23523608,47661761,18240726,31924937,","this class has properties that represent a directory including parent which returns a new directoryinfo for the parent folder;it also has a name property which is the name of the folder not to be confused with fullname which is the full path to the folder which we don t need,a temporary file that you can completely ignore named dependency-reduced-pom.xml will have been generated inside the root folder this is considered an open issue with this plugin;the shaded artifact will contain your initial pom unchanged inside the meta-inf directory and not the reduced pom this is not really important but better mention it - there was an issue about this that was closed automatically mshade-36,if a temporary folder is a symbolic link into another sdk folder inside the xcode directory structure the folder is a symbolic link into another sdk folder inside the xcode directory structure s better to copy the actual content from there into the new destination,i have a directory that contains more than 5000 subfolders....i need to get the name of the folder in the final level..,create at least one package to wrap your source because when jars are exported the src folder isn t included;so if you create your own default package then you ll have some defined root directory to base your resource paths on,for that reason a command line with dir and findstr is executed by for in a separate command process executed in background started with comspec c and the specified command line appended to get all file names to copy with filtering out all file names in source folder tree starting with folder path being equal target folder path;so executed by for is for example dir searches in specified directory c old and all its subdirectories because of option s for non-hidden files because of option a-d-h attribute not directory and not hidden matching the wildcard pattern any file name and outputs found in bare format just the file names because of option b with full path because of option s,found a dirty solution the preferences list .globalpreferences.plist located in library preferences folder of the user home directory contains preferred languages of the user default home directory is users user and if users user is root the path is var root but this path can be configured here method form nsstring can be used to get home directory,meteor public folder not working;use . public directory for serving static assets,i think there are more than two directory in your train and test folder,i am not an osx user so this platform might work differently than i think but it nowhere looks good when the linker tries to access folder which don t exist;ld warning directory not found for option,the command is trying to run the batch file from the current working directory which isn t necessarily the same folder in which the scripts reside;if you need to run another script from the same folder use the automatic variable to determine the parent folder,"
"directory","folder"," is not always in normal google application.startuppath, and clean the project in project drawable clean, is not in file root files, not overall, contains newer overall, does not in file root files, that shouldn in file root files, is newer in common bin past, is easier in permissions information hidden, is disabled click overall, exists it never in files file faster,","2049167,56436766,43762715,38114603,28189916,19571007,28899709,5957862,9245736,51665626,53577460,","if you want the application folder use the application.startuppath property;yes it s normal and no the current directory is not always the application folder,this problem occur when we paste image from explorer to drawable folder then it ask whether to copy drawable or drawable-v24 and if you have pasted images in both directory structures or have one more images with different directory structure then it will show this error make sure to delete the icon from drawable-v24 after moving it to drawable folder and clean the project and then run it will work,i just realized that in fact this directory is a symbolic link to a folder somewhere else;on the line lib ext git actually looks for a folder but a symbolic link is a file so my lib folder is not ignored,datasets in the work folder are stored in a proprietary sas format sas7bdat;the work library is essentially a folder in a directory not in a database,try to delete the bin folder on the target server before publishing and check.;the only problem i could see is if the local bin directory contains newer versions of existing components,if sd card is formatted as fat fat16 you can not put more than 65534 files in a folder but root folder have a limit of 512 files;if it s formatted as fat32 directory does not have file limit,then in the assembly descriptor i provide the overrides for individual folder that shouldn t have the default permissions;here the files in the bin directory will be given executable state for all users,put the file in some other common location such as in a folder on the c drive;just have the application work with the copy of the file in the output bin debug directory the directory that the application will be installed to - you can change the properties of the item in your solution to have the item copied to the output directory either all the time or only when the item in the solution directory is newer,asically i had copied the folder to a new location and modified the folder forgetting that the folder would bring along all the hidden .svn directory;once you realize how the folder happens the folder is easier to avoid in the future,but if you want to use your existing folder as a local repo of an existing remote repo follow desktop desktop issue 2883 drag an ordinary folder into the app or use add local repository and choose the folder you ll get a message saying this directory does not appear to be a git repository;and the add repository folder is disabled click create a repository here select the repository - repository settings.,as written your code says if this folder does not exist make the directory and copy the file;if the directory exists it never gets to the copy the file part,"
"directory","folder"," when doing docker overall, and creates more overall, named libs in files file faster, is for more overall,target in files file faster, it includes them as in files file faster, and not in file root files, and check i in project drawable clean, is part in path beginning to, but not overall, should be under version overall,","33687360,50919059,11571647,53469086,34044187,49148314,51459054,57140596,39694448,30195,2626911,","docker add will add when it is a folder not an url relative to the source directory that is being built the context of the build;so you need to be sure your current directory when doing docker build,each time you create a new xml folder in a directory from directory os.walk later enters your new directory and creates more xml folder since the ones you created are empty,now the final apk size is 486kb and files from libs folder are not included;this happens because you keep your modules in the directory named libs which is by default designed for the native libraries usually .so files,he resources directory is for the resources directory uncompiled counterparts so your less sass files and javascript should go there before being processed by mix;the storage folder is for more general file storage use but i would have a look at the possibility of storing these files with a third party service,file must be in the same or higher directory as target folder,however if the folder has more folder it includes them as well but i want to only include the files in the directory that was entered by the user,both dp0 can be replaced also by any other folder path ending with a backlash;the command for with the used options runs in a separate command process started with cmd c in background the command line the command dir searches for just non hidden files because of a-d-h attribute not directory and not hidden in directory of the batch file for files matching the wildcard pattern and outputs just file name + file extension to handle stdout standard output because of b bare format order alphabetically by name because of on,got solution for me go to your project directory and check if this folder exists android app src main assets i if it exists then delete two files viz index.android.bundle and index.android.bundle.meta ii if the folder assets doesn t exist then create the assets directory there;from your root project directory do cd android . gradlew clean finally navigate back to the root directory and check i if there is only one file index.js then run following command react-native bundle --platform android --dev false --entry-file index.js --bundle-output android app src main assets index.android.bundle --assets-dest android app src main res note that is one single command,second having at the beginning of your path makes it absolute so it s trying to serve from a folder that doesn t exist at your system s root;to make it serve such that your app directory is part of the path change your root to,mine does include hidden folder while zipping though i do have show hidden files enabled;it looks like the compressed folder shell extension ignores directory but not files whose names begin with a dot unless explicitly given as a parameter selected for the send to command,visual studio directory aren t just virtual folder though they can contain filter rules so when you add a file to your project it will automatically get added to the correct filtered folder;they can also specify whether the folder should be under version control or not,"
"directory","folder","sa44-3 not larger overall, is greater in files file faster, gives you less overall, doesn in file root files, not in path specified current, not in project drawable clean,more in files file faster, which is web.browser app overall, this makes go-rwx more in array files program, and creates a comma overall, and doesn in file root files,","27842314,50501293,47087058,29310302,50337439,7948964,11553430,35865110,50736089,288860,16713756,","i need to make sure that each of these top mail directory sa44-3 is not larger then 8gb so the script should calculate the size of each top level folder and if its over 8gb create a new one called eg,this error would occur when the number of folder in the target directory is greater than the number of items in your list,but the directory gives you less control of the folder s security,place the meta-inf folder containing persistence.xml in src main resources;just because the file is placed in a directory doesn t automatically mean it s in the classpath,it will be read-only and you have to put it into resources folder not src;if you want to store some session data you may want to use temp folder system.getproperty java.io.tmpdir or user home system.getproperty user.home or current working directory don t add any path then,your command will have created the directory structure in the current folder not the root directory of your computer which is what the missing is;the first command was right but because you are trying to create a folder in which is a protected directory you need to prefix it with sudo which is short for superuser do,i am using the bash ftp command to ftp files however i have a problem where i try to create a directory that is more than 2 folder deep,you would need a mechanism to get the list of images in the public folder using the fs package will do this for you;consider the following example which uses the fs package to read the public directory which is web.browser app from the server total unix path home user your_app_name .meteor local build programs web.browser app,notes especially when using -r to change entire directory this makes go-rwx more useful as the e x ecutable flag is usually only set on folder so they can be entered and program files. using 700 would add the e x ecutable flag to all files that don t have it yet which is usually not what you d want to do,i have a folder not the default sql backup location where transaction logs are deposited from a remote server;i am writing a c# app that loops through that directory and creates a comma delimited string of file names with paths fooa.txn foob.txn fooc.txn,if so you may want to loop through all files and folder in the root directory using os.listdir;the root path is not a file or a folder and doesn t have attributes strictly speaking,"
"directory","folder"," and not in permission error care,choose mark as  in file root files,more in files file faster,more overall, did not in files file faster, is no longer in common bin past, that has size greater in file root files, doesn in users likely home, whichever is more in file root files, are not in system important systems, are not in file root files,","26312769,52244943,6048084,27402039,54702514,15606529,56614812,33960777,24446302,48380186,18133034,","please take care you give permission to only images folder and not the other folder;as per the error i think that the images directory must have the read-write permission so that images can be stored into it,go to src folder and choose mark as directory then choose the option called source root that would fix your problem,i would recommend placing the common files in the same directory as the solution file or no more than 1 folder deep,to navigate to f you must go to c users folder a b c d e f within each directory there are a number of .html files and potentially more than one folder also,regardless it seems that i would need to be more cognizant of how rstudio establishes a folder which retains its initial settings;an adaptation to the email i sent my colleagues that were assisting me with a similar issue to review i was unable to open any files through rstudio because rstudio returned error messages indicating either that the file itself or the work directory did not exist,in the past a bin directory in the home folder was common to put the subl symlink in usr local bin but a bin directory in the home folder no longer exists in osx as of version 10.2 and the usr local directory is no longer in the default path,i am trying to list down all the files and its parameters within folder recursively to list all the files in the directory sub directory that has size greater than 10000 bytes and not modified more than 30 days,in your server s home directory which is the lowest directory for shared hosting users make a new file called phprc inside of the .php folder;if the folder doesn t exist yet create it,you can either prepend a dot to your image address which means go to parent directory meaning get out of css and then look for images or you can move your images folder to your css folder whichever is more comfortable for you,the simple answer you should not use project directory starting with a dot because for unix-based systems any files and folder starting with a dot are hidden by default;since gradle aims to be platform-independent it may happen that your project is used on such a system and some project directory are not shown to the user,if you want a true relative link that uses the current folder as beginning point;in case the files in your root directory are not used you can use a htaccess file to forward everything to mysitefolder,"
"directory","folder"," is much faster and easier in files file faster, not in file root files, should not in files file faster, but was usr share in file root files, hierarchy makes maintenance easier overall, but not in project drawable clean, is not necessarily overall, will not overall,c youruser in files file faster, are not overall,general in files file faster,","23371398,54150671,11120348,25487577,44868493,54968982,5612650,57343034,6643790,56828612,54085049,","if you ve a lot of files directory specify each one would take a lot of time;so removing that folder is much faster and easier,if you want to copy the folder specify the target as the folder you want to create copy .git . .git;you copied the contents of the .git folder into the dist directory not the .git folder itself,afaik cgi-bin folder is mostly configured to hold executable scripts;this directory should not contains any directly accessed non-executable files like pdf in your example,and the vim folder was also not in my root directory but was usr share vim vim73;i discovered by running scriptnames that the vimrc that it was loading was not from my root directory but was usr share vim vimrc,i like having source .cpp and headers .h in the same folder;this avoids a duplicate directory hierarchy makes maintenance easier,fastest way make a folder directory named raw under the res folder directory and put files of your desired like mp3 images etc it will be included to the final apk file as raw resource;you can also put some png images to drawable folder but not sure it would work in release mode latest version of android studio may give you error while generating signed apk if the size of files is too large not recommended increase your code base like increasing size of java files adding more number of lines to your xml files etc,a directory is a kind of folder;a virtual folder is not necessarily backed by a directory,for maximum flexibility use the layouts folder as mentioned by dan;if you need to provide custom routes to your users like domain.com then putting a file with a name _.vue in your pages directory will not work because you ll need it for your custom user routes,go to your user directory c profiles youruser there should be a folder called .m2 may be hidden,one of the problems is that you are hard-coding paths for just image 1.jpeg and therefore any other images within the folder are not moved;take a look at this code snippet this code snippet will scan the source directory containing several images and will move these images to the destination directory,you can find most of default email templates in edx app edxapp edx-platform lms templates emails if you can not see the changes immediately you may need to clear compiled folder directory,"
"directory","folder"," has already in repo local git, that contain more in files file faster,lower in files file faster, which is more in users likely home, contains we composer in files file faster, isn overall,greater in file root files, to save compile time in war dates tomcat, which aren in files file faster, it is safer in files file faster, is more in file root files,","51280396,57642034,47611724,21651351,54042498,55389998,29498098,12416954,6469588,11583834,25529560,","prerequisitions install git in your local machine check if the local folder already in a git repo in the directory of the folder you want to import to bitbucket repo execute git rev-parse --show-toplevel if it shows not a git reposiroty that means the local folder has not been managed in git repo;else if it shows a parent directory that means the local folder has been managed in git repo steps to import create a local git repo if the local folder not in a local git repo as the prerequisition 2 shows if the local folder has not been managed in a local git repo then create by below commands assume the folder myfolder is what you want to import and it s in c test example myfolder # in the directory c test example git init git add . git commit -am initial commit else if the folder has already managed in a git repo skip this step,power shell error picture of code and directory i would like to create a batch file that moves all folder that contain more than one file to another directory,i created a php file which deals with file deleting and uploading but i this file is one directory lower than the folder for files,you could use system.getproperties user.home which will return the current users home directory which is more likely to allow you to write to the folder,we got an error because we have missing vendor folder in we project the vendor directory contains we composer dependencies,i am assuming this is because your node_modules folder is getting uploaded as well;there is a skip_files option for the app.yaml which you might need to use to ensure that the node_modules directory isn t getting pushed,the genisoimage man page suggests using -r or -r which has better defaults for ownership in addition since the rock ridge standard supports 255 byte file names and folder depths greater than 8 directory and is a real standard unlike joliet,he key is to check the file dates in the tomcat directory of where you deployed the war;i noticed that even though i was deploying an entirely new cleaned war and deleting all folder there were still older cached files in there probably because eclipse keeps even though i was deploying an entirely new cleaned war and deleting all folder to save compile time thinking even though i was deploying an entirely new cleaned war and deleting all folder don t have any changes,maven needs the .svn folder when doing a release;also you need to have committed all your changes and not have files directory which aren t added yet,f a file or directory matches that string then it will not be copied;if you want to exclude folder it is safer to use the above string instead of simply folderc which might skip a file called folderc_listing.txt,i just explained the significance of build folder from my experience with java projects;here in php if you go through the ant file you will realize that build directory is more like the directory where all build-related scripts such as phpmd.xml phpmd.xml etc,"
"directory","folder"," doesn in files file faster, you have access in user home private, is better overall, listing not in files file faster, containing the class overall, and write a file in array files program, not in file root files, there is an even in files file faster, and try again in files file faster, named jdk1.8.0.jdk overall, they wont by chance in system important systems,","42027752,3514705,31729963,29838752,661143,15532326,7924724,55955003,25542334,40284513,48342681,","this is because moving projects between solution folder doesn t break any links between projects as they are stored as relative path hints in the project files;directory structure means nothing to vs,folder in private locations on the drive like in a user s home directory aren t necessarily accessible by the user that sql server runs as;it matters less what folder you have access to than what folder sql server has or should have access to,if i m not mistaken when capistrano makes the current folder the current folder is actually a shortcut for lack of a better term to the most recent release perhaps symlinked directory is better,the reason i think the problem has to do with the css folder is that only this folder seems to be unavailable;if you try visiting a 403 forbidden error is shown directory listing not allowed,therefore if you wish to have the compiler access com.stackoverflow.example.class present in the classes directory under project a but not in the build path of project a then you should add classes as a class folder and not classes com stackoverflow as a class folder;you can add a directory containing the class files to the eclipse project only if it is inside one of your eclipse projects either in a generated directory or in one you have created,the second node module will help us to read a folder and write a file;concerning the basic fs node.js module i don t know if it is possible to read a directory i m not talking about getting all the files in an array using fs.readdir but handling all the files and their organization in folders,this web.xml should be in a folder named web-inf inside your war and the compiled java class file should be in web-inf classes;the war file should be dropped in the webapps directory not the root directory,when you don t need empty directory there is an even easier way to get folders,normally we used to put this type of extra functions files under lib directory so you should create one folder and named one folder lib under app directory and put logger.js file under that folder and try again,however after installing this the old folder doesn t get updated and requires a manual update;to do this navigate to library java javavirtualmachines where you will see 2 different directory containing jdk 1.8.0 a directory named jdk1.8.0.jdk and one named where version is the release number for example 111,what you need to do is just store your credentials out side your main application folder i am not sure what system you are running on but appdata works well for windows;this way if the user does copy the directory they wont by chance copy your credeintals files,"
"directory","folder"," will be automatically in file root files, not overall, and mounts the smb in files file faster,more overall, which is more overall, and not overall, is not in master branch ctrl+z, but i no longer overall, which is never in file root files, is determined by mamp in file root files, not in file root files,","46272284,29475970,43401822,32585533,24883195,40374220,45002947,52996052,45931471,25224188,50934141,","the jar should be located in lib folder and not in bin folder see getting started;any jar file in such a directory will be automatically included in,starting with ios 11 you should only store user visible documents in the documents folder see wwdc 2017 fall video ios storage best practices;even if you had internally used files that were not easily reconstructed unless the intent was to eventually expose the user to them you d use the application support directory not the documents folder,if the folder doesn t exist then it creates it;then the command changes into that directory and mounts the smb,so now i just want to forget this virtual directory system and make the solution explorer to be nothing more than a folder tree as it is in real just like a windows explorer,also please note that folder is a gui term;what you are referring to is files and directory which is more universal,check for strictly folder that don t exist by adding nul to the end of your address;the addition of the nul allows you to find only directory and not the these scary extensionless files that look like folder,when other developers clone pull your personal repo package folder and foo folder will show in master branch;even through there has other ways to show only foo folder on master branch for other developers git fetch and git checkout origin master -- foo but the working directory is not the version as origin master,i continue to get adddir events when i create new folder in the volumed directory but i no longer receive unlinkdir delete events,just because a file exists in your project folder doesn t mean the webserver makes all of it available;looks like you re trying to access a file in the src directory which is never made statically available,the key was realizing that everything had to be done from the application folder itself not from within the mamp folder;as it turns out the issue was pathing and how the home root of the directory is determined by mamp,if you wanted to something more like require_relative you have to use some sort of workaround to turn it into an absolute path such as file.dirname __file__ which gives the absolute path of the folder containing the current file;give the path relative to the working directory not the file that you call file.write from,"
"directory","folder"," will be the bin in path specified current, contains higher level overall, doesn in file root files, not overall,structure more in files file faster, has no more in file root files, is not overall, and not in files file faster,more than 800  overall, did not in permission error care, did not in file root files,","51148417,33865125,50318615,19773536,7868608,52101504,42059742,24759868,56935901,57675329,15147366,","in your case the destination is a folder not file;in this case you may get some exception like could not find a part of the path f new folder while executing the application the current directory will be the bin folder,here is an example directory structure in which source code src is separated from temporary precompiled assets .tmp which are separated from the final distribution folder dist;the src folder contains higher level languages such as jade typescript and scss,so when you import the test_class package python will start looking for a folder called . test_class test_class to find the sub-modules;this directory doesn t exist and so the import fails,assuming that your images folder is the same depth on the tree as your css folder this should work for you;your reference needs to be relative to your css directory not your page location,but my directory structure is more organized its more like app lib mypackage folder,i would like to recursively search through project directory for feedback report folder and if that folder has no more sub directory i would like to process the files in a particular manner,this clearly indicates that there is actually no folder hierarchy for azure storage blob and the folder are just virtual directory for creating virtual hierarchy;furthermore if you are trying to use the microsoft storage explorer to create a virtual directory it also highlights to you that the virtual directory is not exist until you put a blob into it which makes prefect sense since the virtual directory in fact is just part of the name of the blob,this will however prevent browsing directly to a folder and getting a directory listing which should not be a problem;the next line in the htaccess will allow you to link to asset files contained in your admin directory and not have them get processed by laravel so that should not be removed,my folder structure looks like this d | - adc | - lh | - lh_a-dxb | - lh_a-dxb_mki-i | - lh_a-dxb_mki-i_ae | - pointofsalemanagesvrq_lh_a-dxb_mki-i_ae.xml .... | - pointofsalemanagesvrq_lh_a-dxb_mki-i ... | - pointofsalemanagesvrq_lh_a-dxb.xml | - lh_a-fra ... | - pointofsalemanagesvrq_lh.xml | - lx ... | - os ... | - sn | - pointofsalemanagesvrq_adc.xml there are more than 800 folder and 800 files. i need to rename the names pointofsale by adding adc_ to everyone except the folder adc all children example each directory each file example output d | - adc | - lh | - adc_lh_a-dxb | - adc_lh_a-dxb_mki-i | - adc_lh_a-dxb_mki-i_ae | - pointofsalemanagesvrq_adc_lh_a-dxb_mki-i_ae.xml .... | - pointofsalemanagesvrq_adc_lh_a-dxb_mki-i ... | - pointofsalemanagesvrq_adc_lh_a-dxb.xml | - adc_lh_a-fra ... | - pointofsalemanagesvrq_adc_lh.xml | - lx ... | - os ... | - sn | - pointofsalemanagesvrq_adc.xml my code everything works but the folder that are in lh lx os sn are renamed,i ve seen the security error when an asp.net application failed to generate temporary files xml serialization assembly in the following circumstances running under an account without a profile so no account-specific temp directory did not have permission to access the windows temp folder;it s a subjective question but i would treat its unavailability as a fatal error the same as i would do with a temp folder that is inaccessible due to security permissions,with a normal installation of python and pywin32 using the executables from the linked sites c python27 lib site-packages contained a folder named pywin32_system32 which contained the executables that needed to be copied to c python27 to solve the problem;with the active python installation this directory did not exist,"
"directory","folder"," structure is very in system important systems, not overall, structure not in project drawable clean, is not overall, name located in master in repo local git, not in project drawable clean,same as  overall, cannot overall,more in files file faster,such thing as  overall, names a better overall,","4896539,41977396,20796573,29454428,49572403,17175687,56440584,125045,11369555,56837425,57684911,","if the directory does not exist or the specified language is not located there ci will instead look in your global system language folder;file folder structure is very important,where blog is the name of the folder containing your new laravel instance;to install laravel directly within your chosen directory not in a folder within it as demonstrated above simply run the same command but this time without a folder name as in,you write two folder named driverlib and inc on the main folder;a folder name that begins with a is by definition at the root of the directory structure not within any other folder,android studio newly created directory not appearing in folder view;new created values folder is not visible in android studio,to get a folder from vsts git repo to your local directory you can follow below commands assume the folder name is myfolder and the folder name located in master branch,are below the top-level project folder not collateral to it;that s mainly because the current working directory is the first entry in sys.path by default which makes it very convenient to import modules and packages below that directory,is it like a directory is a folder that has one or more folder in it or its just same as folder,step 2 create trunk branches and tags folder - commit step 3 copy hook scripts to new repository;one of my hook scripts is to make sure that the items in the tags directory cannot be modified,on windows xp however it works fine on local file systems but when watching folder on a mapped network drive it throws an ioexception when trying to watch more than 50 directory,you can also use glue client to do the same thing with batch_create_partition or create_partition methods but this would require different inputs then athena client update 2019-07-03 if your data has structure like s3 my-bucket data 2019 06 27 00 00001.json s3 my-bucket data 2019 06 27 00 00002.json s3 my-bucket data 2019 06 27 01 00001.json s3 my-bucket data 2019 06 27 01 00002.json ... s3 my-bucket data 2019 06 28 00 00001.json s3 my-bucket data 2019 06 28 00 00002.json s3 my-bucket data 2019 06 28 01 00001.json s3 my-bucket data 2019 06 28 01 00002.json but you only want to have only 3 partitions year month day then definition of your table should take that into account then ddl statements for adding partitions should be remember that in s3 there is not such thing as folder or directory,because file1.txt as well as file2.docx are valid folder names a better approach would be just to do a simple regex check if input string matches this regex we consider that input string is a file system path and then you can try catch system.io.directory.exist to actually check if provided path is an existing directory,"
"dfa","nfa","expressive than  overall,far easier in easier representation equivalent,better choice overall,expressive than either  overall,larger overall,slower yet in slower powerful engine,faster overall, translation is the epsilon overall,smaller in simple transitions smaller,slower in slower powerful engine, does not overall,","36561846,198624,37891579,4809586,1408183,36630254,37892171,35856651,1217263,9192904,31037337,","the net effect is that the backtracking implementations i like that name better than traditional nfa are slightly more expressive than dfa implementations because traditional nfa can match regexes like which matches three or more word characters repeated twice something that can t be matched by a dfa,because representation of a given problem with a nfa is far easier than the equivalent dfa,dfa is a better choice over nfa because it has only one transition for an input while nfa can have many,as a result a posix nfa engine is slower than a traditional nfa engine and when using a posix nfa you cannot favor a shorter match over a longer one by changing the order of the backtracking search;traditional nfa engines are favored by programmers because programmers are more expressive than either dfa or posix nfa engines,if it can be practically determinized this give you a dfa that can be exponentially larger than the nfa then by all means do that,perl uses nfa which is slower yet more powerful than the dfa engine sed has,generally speaking dfa is faster but nfa is more compact,if you have a regular expression without any optional such as ab the nfa graph doesn t have to contain any epsilon edges;an important concept in nfa simulation and nfa to dfa translation is the epsilon closure a set of states reachable from a state by following epsilon transitions,in fact because you are eliminating the redundancy of epsilon transitions many simple dfa s actually get smaller than the nfa they represent but there is simply no way to guarantee that,regex is a nfa and is as such in most cases slower than a dfa or hand-written parser,if it finds a match then it goes back and matches the full regex using an nfa engine i think traditional non-posix but starting at the location where the simplified match occurred;this is much faster for both non-matches and matches than a straight nfa engine but still lets you use all the things in an nfa that a dfa does not support,"
"dfa","nfa"," which is easier in easier representation equivalent, is much more in simple transitions smaller,more states overall,typically more overall,","14827923,4451725,3979654,30114902,","stephen string search or a compiled dfa you can construct stephen string search or a compiled dfa from an nfa which is easier to make,the dfa is much more complex in terms of the nodes the nfa contains but has really simple rules since there is exactly one output state for any given input,if we express combinations of states in an nfa as states themselves we ve got a dfa with a lot more states than the nfa but which does the same thing,at the same time the compilation phase for a dfa is typically more complex than for an nfa and dfas don t have all the capabilities of nfas,"
"awt","swing","library easier in better newer old, gives you a much broader overall,more capabilities overall,buggier than  overall, has newer overall,much greater overall,more useful in useful, has no longer overall,newer in better newer old, has newer overall, occupies less overall,","30424479,542262,20927462,2994324,43474480,25959726,1090158,37291590,21279086,30941825,14627362,","i find that the swing library is easier to use than awt although you do still have to use awt for listeners,if you are planning to move your gui onto multiple platforms then go with awt;otherwise swing gives you a much broader set of components to work with,swing is relatively similar to awt apart from the fact that swing has more capabilities awt is probably the simplest form to start out with and has most if not all of the tools to complete your project hope this helps,performance swing components are generally slower and buggier than awt due to both the fact that cons are pure java and to video issues on various platforms,that is old code when using awt;swing has newer and better api s,in general avoid awt for swing which has much greater power and flexibility although it too is showing its age just less so than awt,as far as when awt may be more useful than swing -,applet awt is a relatively discarded technology;the newer japplet swing has no longer that large a browser support too,you can read all over the web that awt is old and deprecated and swing is old but newer than awt and should be preferred over awt whenever possible,keyevents are generally used in awt;swing has newer and better api s to use in most cases,awt componentsare heavy weight;swing occupies less memory space,"
"awt","swing"," jframe instead in jframe frame swing, will not in event thread frameworks,library much more overall,more in useful,better overall, is generally more in flexible richer out-of-the-box,newer in better newer old, is not overall,native than   overall,more features in flexible richer out-of-the-box, uses the platform overall,","47277584,3154872,30424479,408820,7069984,16781119,36696741,14627362,8642394,2377467,12145525,","consider using swing instead of awt jframe instead of frame;swing is richer,keep in mind that awt swing have a dedicated thread that does gui work handling event handlers repainting the gui etc;if you do long-running things on that thread swing will not repaint,the swing library is much more portable than the awt library,are there any cases where awt is more useful advised to use than swing or vice-versa,note first there is no combobox item in awt there is jcombobox in swing which is better to use than choice of awt,swing is generally more configurable and flexible to use;you should avoid mixing awt and swing components within the same ui ui tend not to play well together,yes swing has newer and better api s than awt,awt is not mvc based;swing works faster,due to swt looks more native than awt swing,in swing has more features than the awt components,swing was said that awt is faster than swing as swing uses the platform component but due the arrival of faster processor etc ..,"
"awt","swing","much more in flexible richer out-of-the-box,more time in flexible richer out-of-the-box, applications; has newer in better newer old,better features than  overall,more experience overall, but is more overall, solution; has newer in better newer old,much better in better newer old,far better in better newer old,far better in better newer old,worse technology overall,","19736083,13889116,43683431,43499648,42168264,10036091,43813030,16289744,21215282,9996090,39676981,","use a swing gui not an awt gui since swing is much more powerful and flexible than awt,swing will save your more time than awt as all the code of components of swing are purely written in java whereas of awt code of components are written in native language that is other than java thus compilation time of awt is more than swing,that is used in old awt applications;swing has newer and better apis,a panel has functionality the same as a canvas but swing is more advanced and has better features than awt,awt is really out of date people have more experience with swing no days and even javafx,the keylistener is an older interface from the awt days the keylistener still ok to use the keylistener with swing but is more of a general listener,that is an old awt solution;swing has newer and better apis,consider using swing which has much better performance over the old heavyweight awt,swing is far better then awt and should be educated to people who is writing gui-based java for the first time,swing is far better than the obsolete awt library,alternate way is to use awt - which is an older and worse technology than swing and in that case you can t use flowlayout - it puts all the components into one row,"
"awt","swing"," occupies less overall,more in jframe frame swing,other than the  in event thread frameworks,better in better newer old,richer in flexible richer out-of-the-box,approach closer overall,such as   in event thread frameworks,more overall,more onerous overall,better in better newer old,less memory overall,","14627362,27302462,13407535,9009156,2044311,3906154,52685420,20846514,12426434,29239458,416947,","swing occupies less memory space;awt occupies more memory space,swing has more or less deprecated awt so you should extend jframe instead of frame,if you need to modify swing components from threads other than the awt event dispatch thread use;swing is not thread-safe,swing is better and few people remember how to use awt components,well swing is richer in terms of out-of-the-box components than awt,the awt approach seems closer to bare metal but since jdk 6 brought a lot of improvements in swing rendering pipeline i would go the swing java2d approach,this applies to many other ui frameworks as well such as swing awt and the event dispatch thread,generally swing is more efficient and advanced than awt,use swing follow the threading rules which are hardly likely to be more onerous than awt and enjoy,swing or javafx would be better than awt,i mean by light weight i thought maybe the swing components occupy less memory than the awt components,"
"awt","swing"," has newer overall,general overall, has newer overall, does not overall,newer in better newer old,more comfortable overall, is much larger; also overall, has better api overall,","27640139,53309692,34051697,14627362,33978735,16509409,16810355,19336152,","don t use a keylistener that is old code when using awt;swing has newer and better api s,apple recommends using awt s filedialog instead of the swing file chooser because filedialog acts more like the native macos file dialog,that is probably the oldest solution and was used in awt;swing has newer and better api s which you should be using,awt does not work faster;swing componets are light weight,swing has newer and better api s than awt,i know you re thinking why did i use awt instead of swing but at the moment i am more comfortable with awt,awt is a thin layer of code on top of the os whereas swing is much larger;swing also has very much richer functionality,that is an old approach when using awt;swing has better api s,"
"mp4","webm","implementation more overall,more robust overall,faster standard overall, is higher quality overall, video doesn overall, file is bigger in bigger 720p 33mb,faster overall,better overall,smaller in bigger 720p 33mb,","25829657,24958247,42486772,6326524,23828748,52994952,30801590,32770855,23775693,","the mp4 implementation is more widely used and wouldn t require a webm fallback in most browsers,webm is usually more robust than mp4 which can be all over the place,probably webm will load faster than standard mp4 but i am not so familiar with webm format,be sure mp4 comes first;webm - webm is higher quality than ogg so browsers that support both will choose webm first if listed in this order,the mp4 video doesn t work under firefox 29.;but the webm version is ok,converted webm file is bigger in size than original mp4 original mp4 of 720p was 33mb but 640p webm is 76mb,also it looks like when i transcode webm to mp4 2.7 is 30 faster than webm,the ogv version of the video seems to run a little better than the webm which itself seems to run better than the mp4,i tried mp4 and webm and sometimes the file size after i convert to mp4 is smaller than webm but it is also sometimes larger than the original file size,"
"geocode","geocoding"," results more overall,more reliable overall, which was more overall,more overall,api better in api usage limits,more enough overall, api is getting much in api usage limits, is not overall, often returns more overall, which is less overall,better than  in google set apis,","26029945,26094097,12760132,7987892,42536489,47124371,2846508,34712450,47303391,13975365,54666724,","optional parameters in a geocoding request;bounds the bounding box of the viewport within which to bias geocode results more prominently,2 location + reverse geocoding - seems more reliable but requests location services on the device does it work on wifi only tablets and for it to be enabled and requires time to a fetch location b server call to reverse geocode, had originally used yahoo maps api to geocode a postcode and then use google maps for the map as google map s geocoding was woefully inaccurate;yahoo turned yahoo service off so i changed yahoo to the google local search version of geocoding which was more accurate at the time than google maps this was last year 2011,can the geocode gem support more than one geocoding step,the geocoding api works better than geocode normally but has usage limits and the implementation is bigger,i had a similar problem using ggmap geocode for a batch of locations where roughly 20 of locations gave the over query limit error even though geocodequerycheck would show more than enough geocoding queries remaining and the errors were sporadically spread throughout the locations not just the last 20,you need a service that does geocoding;yahoo s geocode api is getting much better results globally,the reverse geocoding requires an internet connection;but if you only need latitude and longitude using reverse geocode is not necessary,there s a note in reverse geocoding documentation;the reverse geocode often returns more than one result,geocoding api via rest is usually rate limited also due to legal reasons;you can use openstreetmap geocode which is less restricted and has a maintained cpan module geo coder many osm,google provides a set of geocoding apis that can be used to reverse geocoding but it s slightly more complicated to use but it s much better than geocode,"
"geocode","geocoding"," has greater coverage and better overall, is not sophisticated enough in google set apis, is not overall,","41829712,10853162,3491048,","the latency difference between place autocomplete and the new forward geocode increases further since the new geocode has greater coverage and better result quality but at the cost of somewhat higher latency,geocoding is notoriously littered with icky humanity;the consensus is that google s reverse geocode is not sophisticated enough to report intersections in one query,and things improved a lot but geocoding is not an exact science;in any case you might be using an old version of the geocode because i get a postal code with this request,"
"qplaintextedit","qtextedit","smaller overall, but is a lot overall,better then in text widget features, fixed the issue; overall, is better in text widget features,","11788422,33506433,32826981,21560813,35763363,","i have tried connecting the qscrollbars but the maximum of the qplaintextedit is smaller than the qtextedit s,you should give qplaintextedit a try;it uses the same technology as qtextedit but is a lot faster,if you want to display your file as plain text the widget qplaintextedit is better then qtextedit,as implied in the comments modifying the qtextedit to a qplaintextedit fixed the issue;qtextedit is not designed for handling very large paragraphs which is effectively what i was creating,it uses most of the features of qtextedit but with a vastly better performance;using qplaintextedit is better than using qlabel for displaying large text documents as the former has many more capabilities and features for customization and formatting,"
"double","int","larger values in range larger smaller,better in range larger smaller,bigger numbers in range larger smaller, arithmetic is far more in precision precise problems,slower code in faster slower machine, as not overall, not overall,more precise overall,precision less in part literal checking, is more in exact wall accurate,less in closest next rounds,","8424026,35717845,25060840,41996901,2242618,36407197,51347776,3878773,38489471,2044282,36745546,","a double variable can hold larger values than an int and is able to store an approximation of the value 4000000000,it would really depend on the processor and the range of the int which is better and using double would resolve most of the range issues,clearly calling int x on a double is asking for trouble since double can store bigger numbers than int,int arithmetic is far more precise and predictable;lastly you can always use a high precision library though i think double or long long will be good enough for you,using and at the same time to check equality on a int results in slower code because it takes two comparisons instead of one taking the double amount of time,int is an integral type while double is not;from the comments it seems like op is asking for standard quote which defines const int as a compile-time constant but const double as not,for any application-side date-time-values use an int unsigned a bigint signed a double signed or a decimal n p to simply store unix-epoch-deltas. make sure to show the resolution in a column name suffix if it is not seconds;examples mysql_row_created_at timestamp 3 default current_timestamp mysql_row_updated_at timestamp 3 default current_timestamp on update current_timestamp epoch int unsigned not null epoch_ms bigint not null epoch_us bigint not null epoch double not null epoch6 decimal 16 6 treat your database as much as dumb storage as possible,a double is a more precise type than an int in a general manner of speaking,however if you have large enough number the limit checking does not work because 64bit double precision has less bits usually 54 but this is not defined in standard for the fractions part than a 64bit int,the int value is less likely to with double since double is more precise and can represent many more exact numbers,the problem is that a positive double very slightly less than an int casts to the next int down but rounds to the closest int,"
"double","int","smaller in range larger smaller, is slower overall,smaller than your  in range larger smaller, so overflows 2.147483648e9 math.pow overall, you cannot overall,much bigger overall, cannot in f can not,smaller in number advance non-decimal, is not overall,better performance in performance version better,more precsion in precision precise problems,","28683183,7037750,13032415,50776212,42559548,39108379,27330169,31627536,12284419,3676567,38865387,","extra arguments to variatic functions with types smaller than int are promoted to int and passed as such and floating point types smaller than double are promoted to double and passed as such,the assembly s that or generated code be it the fact that you can fit less 64-bit int in a sse register or round a double to a 64-bit int is slower,and since float is typically promoted to double with varargs calls if your int is smaller than your double this will break,this is why the short overflew but the int did not;the first three outputs from both int and short are easy to explain -2147483648 your method returns an int so overflows 2.147483648e9 math.pow returns a double so formatted like this 2147483648 double casted to a long 2147483648 inside the possible range for long 32768 your method returns an int 32768 is inside the possible range for int 32768.0 math.pow returns a double so formatted like this 32768 double casted to an int 32768 is inside the possible range for int the hard to explain bit is the fourth result,byte is an object but his behaviour is different form int long and double you cannot use it for basic arithmetic operations;and even if you can assign an int number to a byte object like this,double is much bigger than int,since your to is 1 of type int you get overload version 4 with a return type of double;now it s entirely possible that given a float f float nextafter double f 1 is exactly equal to the original f it s rather likely that the next representable number in type double cannot be represented in float and that the conversion back to float rounds down,since an int is representing by a non-decimal number 1 and a double is represented by a decimal-number with precision 32 bytes more than that of a float 1.0 we can say that int s are less than or smaller than double s and by that logic int s can be promoted to double s and double s can be demoted to int s,the reason you re seeing similar initial output from nextdouble given similar seeds is that because the computation of the next int only involves a multiplication and addition the magnitude of the next int is not much affected by differences in the lower bits;calculation of the next double involves computing a large int based on the seed and dividing it by another constant large int and the magnitude of the result is mostly affected by the magnitude of the int,i m writing a c# class to perform 2d separable convolution using int to obtain better performance than double counterpart,this is important because if the int needs more precsion than 52-bits the precision is too low for a double,"
"double","int"," not in range larger smaller,better match than  in better function bullet,narrower than an  overall, which is usually in range larger smaller,representable as  overall, is faster overall,more overall,more in specific void argument,more overall,higher maximum overall,less in memory largest storage,","20645259,48855205,47705275,49140493,31507028,5199938,24857068,42046449,44725359,29551662,3027395,","read your bytes into an int which is always 32 bits in java then use this;you need double not float because single precision floating point won t necessarily be long enough to hold your 32 bit fixed point number,short int is an int type so bullet #2 kicks in and causes generation of a wrapper which calls double abs double and this wrapper is a better match than int abs int,roughly speaking the default argument promotions are the default declaration narrower than an int are converted to int and float values are converted to double,so use a float constant 1000000000000000000000f or 1e+21f instead of an int constant 1000000000000000000000;or if you re really using double which is usually a better idea than float use 1000000000000000000000.0 or 1e+21,suppose that int is 64 bits and double is an ieee double;then there are int not representable as double such as 2 53 +1 making the proposition false,the f is because there s no need for the operation to be done in double -- i expect there s more chance of an implementation where 1.0 i would be significantly slower for no benefit software float emulation indifferent optimization than one where 1.0f is significantly slower for no benefit if double is faster than float that s because you have fp hardware so conversion between the two will be very fast so not introduce significant slowdown,for floats and double it s more difficult as with plain int as these may or not may be in the host machines byte-order,we could check that void mymethod int i is more specific than void mymethod double a if any invocation handled by the first method can be passed on to the other one without a compile-time type error,and it shouldnt be too surprising that copying a double 3 strings and an int takes more time than copying a single double,a double has a higher maximum and presumably lower minimum than an int so there s no conversion necessary so you shouldn t experience any side effects of assigning a double an int so there s no need for an error,it implicitly converts an int to a double gets the double form of largest int that is less or equal to that double and converts that back to an int,"
"double","int","smaller overall,more in range larger smaller,representable as  in range larger smaller,slower in faster slower machine,larger overall,value bigger in range larger smaller, is always bigger overall,less memory than  in memory largest storage, is much greater in range larger smaller,more in closest next rounds,better in better match second,","30474373,30079823,53084231,16741312,33913731,37956224,14817393,48230236,3481703,7472356,30503277,","this is the reason for which if you use printf to print floats you don t need to say lf int the format strings and f is enough lf is needed for scanf however because that function stores a result and a float can be smaller than a double,operations on float and especially double cost more than on int,fails as expected since 9223372036854775807.0 is 2 63 these numbers are way out of the range where all int are exactly representable as double,incrementing the value might be a special case but it s possible that on your machine incrementing double is slower than incrementing int,bear in mind that if the double returned is larger or smaller than that which can be held in an int then the program behaviour is undefined,i wanted to check if a double value is bigger than maximum int value but because converting function does not return an optional value i am not be able to check by using optional binding,so on an 8 bit chars system if the sizeof int is 4 the maximum int is not necessarily 2 31-1 because the specification allows some of those bits to be used as padding. thus the only way to know for sure is to use limits.h;float double are also implementation defined although double is always bigger than a float,int storage may require less memory than double on some systems here only half the memory is required,the result might not fit into an int or a long;the range of a double is much greater,or math.ceil double which returns the closest int that is more than the double,why does f double is a better match than f long int 0 for f long,"
"double","int"," that supports more in possible size value, before computing the division overall,slower manipulations than on  in faster slower machine,more precise in precision precise problems, to do division in range larger smaller, arithmetic is generally faster in faster slower machine, is higher overall,slower in faster slower machine,larger numbers in range larger smaller, is smaller in range larger smaller,general in precision type integer.parseint,","48965200,48000861,41633258,25924215,13982430,23558616,2625948,10107000,5693480,51145298,31121609,","if the number is revised 1519490351963 this exceeds the greater value of an int 2147483647 so there is no conversion to whole a possible solution is to convert it to double another possible solution is to convert it to qvariant and then use tolonglong to use qlonglong which is a type of int that supports more bits,its also worth noting that because num3 is an int when you divide it by 45 you will get an int not a percentage;to fix this issue make num3 a double or cast either num3 or 45 to a double before computing the division,also after the first coercion from a side effect of a benchmark as noted above r will operate on double s and that contains slower manipulations than on int s,you used the double to calculate an int result the double is more precise than the int so you lost precision,a long or any int value cannot handle decimal points so if you have a number 0 x 1 when the decimal is chopped off you re left with 0;you re using unsigned long when you should be using float or double to do division,in addition to having different semantics from double int arithmetic is generally faster and the smaller size 32 bits vs,because a double is higher a double is higher will merge the int into the 0.0,even dividing int is slower than multiplying floats or double in some cases,because double can contain larger numbers than int or long,if this operation overflows this operation means the range of double is smaller than the range of int and we can use dbl_max -dbl_max as max_double_to_int and min_double_to_int respectively,i would recommend using int type keys long long for the map in first place and trim int type keys for double representation using a fixed precision for division,"
"double","int","types more instructions overall, w is a better in better function bullet,better in better match second, not in specific void argument,better in better match second,lower ranges overall, value doesn in 64-bit ieee-754 52-bit,wider range in range larger smaller, not in range larger smaller,less memory in memory largest storage,precise than long  overall,","6385312,53470524,22307506,55100761,28466553,25831721,28325866,5694078,46373477,21528753,45266203,","on a 32-bit machine doing addition and comparison on 64-bit int types takes more instructions than doing the equivalent on double s,because according to the rules of overload resolution void function int y int w is a better match than void function float y float w and therefore there is no ambiguity;calls function 1.2 2 and function 1 2.2 both have one argument that is an int and is the exact match with one of the arguments of void function int y int w and so only one type conversion is required double to int,imho int looks better than double in your task,with argument int it works perfect;sometimes reading the exception helps a lot - my rowindex was double not int,with gcc 4.9 it prints error because the second overload isn t discarded and int is a better match than double,if we use float data type in java we have to add f at the end of floating point literal as java assumes it to be double data type and gives an error why not do the same for short and byte as both have lower ranges than int,there are 2 32 int values therefore at least one int value doesn t have an exact float representation;i m assuming that by float you mean a 32-bit ieee-754 binary floating point value by double you mean a 64-bit ieee-754 binary floating point value and by int you mean a 32-bit integer,also given that double has a wider range what would one return for those out of range int values,xsd int not a derived type of xsd double;what is important in datatypes is the value 100 xsd int and 100 xsd double are the same value,int needs less memory comparing to double numbers,the prototype for main without arguments is int main void;the computation of pi 180 is performed as double which is less precise than long double,"
"double","int"," not in range larger smaller,little less overall,mantissa bits than  overall,smaller than  in range larger smaller,more bits overall, cannot overall, is bigger then overall,narrower in narrower 32-bit function,faster than  overall,usefull number-range than a  overall, or long in range larger smaller,","14714167,35802753,19689402,53247907,31756328,29592667,55255372,40269870,50677092,511948,57543236,","that being said if you work with double values instead of int you ll likely see the results you are expecting;the pow function works with float and double not integers,you will get 24 if pow 10 2 returns a double that is little less than 100 99.9999999998765 which when cast to int would be truncated resulting in 99 and thus you get,since double has more mantissa bits than int can have significant bits comparisions between double and int are precise in that regard,if you wrap a c type as an nsnumber and then unwrap it as above could the type change due to argument promotion which means that int types smaller than int get passed as int values float values as double in c,there is also the wrinkle that typically a double consists of more bits than an int does - so by printing in this way your code is interpreting random memory to the right of x as data,long long int is for large int cannot contain decimals;long double is for large double can contain decimals,3 one way is rounding your double value before you use this int initalizer from this double will be passed for int initializer to one decimal place and then round result by following rule for this purpose you can use rounded method with floatingpointroundingrule .tonearestorawayfromzero which is default value for this parameter also you can use failable initalizer since if double is bigger then int can be code crashes,but there was this legacy thing dating all the way back to k r c called default argument promotion that essentially says the compiler will implicitly convert all int types narrower than int to int if necessary to match a function signature and all floating point types narrower than double to double ditto,result summary numeric 5 million loops int 1.08 sec 20 faster than variant long 1.09 sec single 1.29 sec variant 1.34 sec double 1.37 sec numeric 25 million loops variant 6.16 sec variant is 9 faster than string string 6.76 sec numbers stored as text 5 million loops int 5.45 sec variant is 32 faster than int variant 7.99 sec therefore it it true arrays of non-variant data types are handled up to 20 faster than an array of variants,returning int as a double is the right thing to do here because int offers a much wider usefull number-range than a int could,as pow returns a double which has a much larger range than either int or long long int you could have,"
"double","int"," can handle - so in error hard smarter,range much smaller in range larger smaller,more specific in specific void argument, poer anyway overall, is a better in better match second, since long in better match second, is higher precision in precision precise problems,bigger than  in range larger smaller, is considered better overall,bigger in range larger smaller,more precision in precision precise problems,","47732661,27334337,2521374,18327151,17190680,47925622,49518458,25234889,14777718,20354046,29687757,","edit as hans passant pointed out you can get an overflow error in case both int add up to more than a int can handle - so casting at least one of them to double is the smarter move return double a + b 2,1 int range is much smaller than double and for pow 2 100 that fails,by contrast void doitagain int is more specific than void doitagain double because int is a subtype of double according the the jls,a double will move farther then an int so you will get more interations with an int pointer anyway,on glibc 2.21 it is an int;the manual says that using double is a better idea,note that long would be better than double since long can represent all int up to 2 63 - 1 precisely,so if you have a int and a double the double is higher precision so the result will have that form,types bigger than double and int are 64-bits,there is an implicit conversion from int to double but not the other way around;hence the implicit conversion from b to int is considered better and wins,you can convert your object into double and then into int but you should take care as double value is much bigger than int,double doesn t have infinite precision and you can t gain more precision than a double has by converting a double to a bigdecimal like you can t gain more precision with an int when you do which is 0.0 because it widens an int to a double,"
"double","int","such as  in floating-point fractions argument,less overall, multiplication is shorter in faster slower machine,larger in range larger smaller,much more overall,less specific in specific void argument,more space than an  in space members outofmemory,indeed more in specific void argument,value also larger in range larger smaller,much better in better match second, parentheses string comparison overall,","49814963,35931486,1965541,20181170,23555019,25924211,54374160,42046449,511928,17306689,47809177,","int is not fractions there are int division functions div and quot and their remainder counterparts mod and rem but that s not what you want here because you want a fractions result;the solution is to convert the int argument to a fractions type such as double using fromintegral,or a value for int a that is 6 less than double c,using double multiplication is shorter because the compiler uses the processor s floating point opcodes which probably run faster but actually i don t know than not using t for the same operation,many processors and or compilers implement long double types larger than any int type regardless of name and there are processors with 32 byte pointers,it seems to be that double is much more flexible than int,this is java detecting what you are doing and thinking it may be a mistake on your part - int s are less specific than double s,since the double takes more space than an int it occupies two of the members of the struct at the same time this may depend,an implicit conversion from byte to int is indeed more specific than an implicit conversion from byte to double according to widening primitive conversions rules,the largest double value is also larger than the largest int so it would have to be a long,except that double is much better than int for storing the join_angle in radians,int comparison -eq is equal to -ne is not equal to -gt is greater than -ge is greater than or equal to -lt is less than -le is less than or equal to is less than within double parentheses is less than or equal to within double parentheses is greater than within double parentheses is greater than or equal to within double parentheses string comparison is equal to caution note the whitespace framing the,"
"double","int","wider in range larger smaller,bigger overall,more in specific void argument,more in general most-specific type,slower than  in faster slower machine,greater value in range larger smaller,fancy as  overall,bigger maximum in range larger smaller,smaller overall,general overall, is greater overall,","41360457,11001264,19436196,15770825,51155879,5693478,49310455,37956224,23953851,52525223,3792915,","can be false when int range is wider than double uncommon and dx is a rounded value whose next higher representable value to 2 greater,with the cast that bad stuff is undefined behavior most likely double is bigger than int and it overruns the memory for i,i thought when i pass 1 as the argument java would have picked the first argument because int is more specific than double,but this isn t true because double is more general than int and therefore int is the most-specific option in this case,int represents a double performance hit listarray with iterator is 3x slower than int,double has a greater value range than int,this is exactly like the decision table above except the command is the one with an interface and subclasses - you don t need to do the switch because the command action subclass already has that logic for you for example if you need to make different decisions based on which command at multiple points int your code you might even want something as fancy as double dispatch which i won t go into here,i know that there s another way to check if a double number is bigger than maximum int value but i m curious as why it s happening this way,it doesn t matter if second member would be int short or whatever - as long as it is smaller than double sizeof struct will be 16,the i cannot be converted to double and the d cannot be converted to int,when you compare a double to a 64-bit int however there will be potential roundoff error if 32-bit int double is greater than 2 52,"
"double","int"," is more overall, quotes aren overall, is wider in possible size value, long float in range larger smaller,more than  overall,greater sizeof overall,shorter than  in range larger smaller, might not in large answer issue, is more in memory largest storage, is higher type in range larger smaller, and now overall,","30577548,43038793,56258932,56844327,57183426,11321381,43554894,31562686,52454347,48577225,44542353,","don t use int32 it s generally best-practice to use int in this case;though for your situation double is more appropriate,the value of an int chars constant containing more than one chars ab or containing a chars or escape sequence that does not map to a single-byte execution chars is implementation-defined;single and double quotes aren t interchangeable in c as they may be in some other languages,except it is theoretically possible that int is wider than uint64_t in which case the arithmetic would be done with int but that is still satisfactory performing a cast guarantees the arithmetic will be done with at least that width for real numbers the usual arithmetic conversions are largely if either operand is long double the other is converted to long double,the int type is smaller than the float type;in general byte short and chars int long float double,i can t work with int or double and use numberformatter because input could be more than int or double max values,alternatively if fieldsize is equal to sizeof double and sizeof double is greater than sizeof int then you are writing off the end of d_buffer and then something is corrupting your data,note that when float values are passed to printf float values are automatically converted to double just as numeric types shorter than int are promoted to int,using an int to represent the address of a double stored somewhere and attempting to cast an int to a is undefined behaviour in c++;an int might not even be large enough to hold a pointer address,that case is highly probable that representing small maps by an array of pairs of int double is more efficient both in terms of memory and in terms of cpu time,as here as i mention double is higher type it stores int float values as per you expected,the undefined behaviour as identified in other answers is explained on my intel platform with vc as that the f format specifier expects a double on the stack which is larger than an int so when the f format specifier retrieves the value the f format specifier retrieves more bytes than of an int and now assumes the next parameter at a diferent position on the stack causing z to be printed wrong it is not z that is printed,"
"double","int","faster in faster slower machine,less overall,better overall,lose identity as  in values strong idiomatic, and therefore overall, which cannot in range larger smaller, isn overall,much smaller in range larger smaller,wider overall, isn in range larger smaller, is not in object oggplayer iplay,","16298551,21470294,14285655,21592008,53695627,16268270,1218948,4181954,44424688,22772730,20372030,","the difference is that int are still faster than double because it takes very few clock cycles to do arithmetic operations on int,your nummines calculation will always return 0 because when you cast a double that is less than 1 to an int it will be set to 0 which means that the statement in your while loop will only be run a single time hence only a single mine being placed,note how i cast the result to int which is better suited for years than the double precision returned by extract,when you load the double and int values into the array the double and int values lose identity as double or ints--the double and int values are autoboxed and stored as instances of the abstract number class,the addition is performed using floating point arithmetic. with your particular values a s exact int value and the addition result s exact int value are not representable exactly with double and therefore the result is inexact,this will work as long as the double is not bigger than what can fit in an int;you are adding an int to a double so the result is a double which cannot be used with the operator,for example if coercing an int to a string box the int into an int and apply the rule for coercing an int to a string;or if coercing a string to a double apply the rule for coercing a string to a double then unbox the resulting double making sure the resulting double isn t actually null,don t forget that the range of int is much smaller than the range of double,so in your smartadder interface the method with signature add double a double b does not override the method add int a int b of your adder interface because double is wider than int,a float double isn t built like an int type;int s are straight-forward binary numbers maybe with a sign,well int is assignable from object so i can export as with no problem;however if i exported an and instead it will fail because double is not assignable to int,"
"double","int"," is less overall,wider overall,more overall,faster overall, doesn in range larger smaller,less in efficient modulus use, case is trickier in trickier easy infinities, vectors is faster overall, it is more in values strong idiomatic,more appropriate overall,greater in range larger smaller,","57062595,20821194,22203178,40405372,24334968,24945410,3997093,52166526,37412054,37951425,41806502,","here is a function which takes in an int tries to subtracts 21 from it and returns the difference if the int is less than or equal to 21 or double the difference if it is greater than 21,the most urgent one that comes to mind is that a double is wider than an int on your platform and garbage memory is read,double s length is way more than int that s why you should be checking int first,if such a hardware unit is present floats double can be faster than int unless there is also hardware int support,aside from the fact that you re comparing a double and an int and have no idea which one will be casted to the other s type you re comparing floating point numbers with the operator;comparing floats and double doesn t work like this because,because all int s can be upcast to a double without loss and the pow function on a double is no less efficient that that on an int,the int cases are easy;the double case is trickier until you remember about infinities,note that replacing elements of lists and double vectors is faster using rcpp while for int vectors base r solutions are preferred benchmark 1,2 unless you have a strong reason to use int it is more idiomatic to use double for spreadsheet values,also int are more appropriate than double numerics for this case of logical coercion,a double has a range that can be greater than any int type,"
"double","int","no better in range larger smaller,more troublesome overall, and long overall, is greater overall, type is wider in range larger smaller,bigger in range larger smaller, not overall, however is ieee standard overall,compare then as  overall, that is larger overall,greater in 64-bit ieee-754 52-bit,","30490916,44353937,12314984,31993049,18552018,32663774,26321638,53155981,51728329,1774432,22740584,","the conversion from float to long double is no better than the conversion from float to int since they are both not promotions,you are doing linear programming on int which is more troublesome than with reals double,since double is greater that int and long int and long will be turned into double in double long and int double,f the int is 0 then the two double are equal;if the int is less than 0 then val1 is less than val2 and if if the int is greater than 0 val1 is greater than val2,the range of exact representation of integers by the double type is wider since the value occupies 64 bits and there are 53 bits reserved for the mantissa,if you tell it the top of the stack has a double and double is bigger than int you ve now just told it to take extra bytes off of the stack which is terrible,the modulo operator is meant to work with int not double;remember double numbers are floating-point numbers so there s always room for numerical error if you use them,from significant digits and largest int by length for sas variables under windows | length | largest int | exponential | bytes | represented exactly | notation +---------+-----------------------+------------ | 3 | 8 192 | 2 13 | 4 | 2 097 152 | 2 21 | 5 | 536 870 912 | 2 29 | 6 | 137 438 953 472 | 2 37 | 7 | 35 184 372 088 832 | 2 45 | 8 | 9 007 199 254 740 992 | 2 53 so while all 16-digit hex values in c can fit distinctly in a 64-bit int not all such can distinctly fit in sas numeric those;the 64-bit double however is ieee standard and can deal with numbers up to 10 51 albeit with some loss of precision,your are comparing them as string so by alphabetical order 1xx is before 9xx you need to compare then as double or int so use comparator.comparing double valueof or int valueof,find the closest 32bit unsigned int that is larger than x;i use this to double the size of arrays the high-water mark,if a 64-bit long is written as an int and is greater than 2 53 ieee double has a 52-bit mantissa then reading it as a double will lose precision,"
"double","int","larger in range larger smaller,greater magnitude than  in range larger smaller,less space in space members outofmemory,more in specific void argument,more overall,higher magnitude in precision precise problems,more precision than an  in precision precise problems, would require indirect access in range larger smaller, is not in f can not,more complex overall,less in range larger smaller,","29017088,54472294,14638096,32293611,42209378,4130727,52909402,56397434,19659057,3632503,15949536,","when the result of math.pow i j a double is larger than integer.max_value and you cast to an int then the result is integer.max_value,keep in mind that double variables can hold numbers of much greater magnitude than int variables so the loss of data may be considerable,int takes less space than double on ram,this behaviour is due to the fact that int is more specific than double while there is no such comparison between int and boolean,you can t convert a double to int because the double has more information than the int,not really considering that a double can be a much higher magnitude than an int,you probably need arrays of int which makes some sense since it probably wants more precision than an int can give to be double or floats,indirect some processors may have a limited range for the immediate value for example 8-bits and anything larger such as int or double would require indirect access an addition word for the pointer address,list list int is cast to list seq int and then its map has the proper type for intseqtoint;your problem is that double is not a subclass of number,as for the last question floating point arithmetic particularly double precision is much more complex than int arithmetic so on a reasonably modern pipelined processor each instruction will take longer to execute,you really might need to up cast everything into a double and then check to see if the result is greater than or less than an int,"
"double","int","representable by  overall,array less in range larger smaller,larger in range larger smaller,higher in range larger smaller, arithmetic add in float significand types,bigger overall, quotes makes an identifier overall,less overall,smaller in range larger smaller,wider in range larger smaller,math faster in faster slower machine,","55447991,43258026,12015986,40734664,55578094,10951622,52582519,3481367,8103414,26209492,31346703,","double self.max int.min -9 223 372 036 854 775 808 float int.min -9 223 372 036 854 780 000 lower than int.min by 4096 thus not representable by int float int.min .nextup -9 223 371 487 098 960 000 greater than int.min by 549 755 820 032 thus representable by int int.max +9 223 372 036 854 775 807 float int.max +9 223 372 036 854 780 000 greater than int.max by 4096 thus not representable by int float int.max .nextdown +9 223 371 487 098 960 000 lower than int.max by 549 755 820 032 thus representable by int here s what that looks like in action which prints double nan as int nil double nan as int nil double nan as int nil double nan as int nil double + as int nil double +9 223 372 036 854 780 000 as int nil double +9 223 372 036 854 770 000 as int optional 9223372036854774784 double +1 as int optional 1 double +0.6 as int optional 0 double +0.5 as int optional 0 double +0.4 as int optional 0 double +0 as int optional 0 double +0 as int optional 0 double -0.4 as int optional 0 double -0.5 as int optional 0 double -0.6 as int optional 0 double -1 as int optional -1 double -1.5 as int optional -1 double -9 223 372 036 854 770 000 as int optional -9223372036854774784 double -9 223 372 036 854 780 000 as int nil double - as int nil,if a value in the double array is less than the int value at the same position of the two dimensional array then in that part of the boolean array the value would is true,double is larger than an int hence the overflow,and if the value of your double is higher than the range of an int then this can produce undefined behavior,this can also be applied to any implementations where long double has more than 63 bits of significand like powerpc or sparc there s a related question about calculating int arithmetic add 1 to uint_max and divide by n without overflow with also clever solutions,one of long long and double is bigger than 3 int and a multiple of 8,stick to consistent quoting and you should be ok create table trade prodid int references product productid transid int references transaction transactionid quantitypurchased int not null primary key transid prodid -- here ----- ------- -- ------;using double quotes makes an identifier in this case a column name case-sensitive,that wouldn t work on platforms where a double is less than 64 bits but it would work on many platforms without 64-bit int,in vararg functions such as printf int smaller than int are promoted to int and floats smaller than double are promoted to double,the range of double is wider than int,often using int math is faster than double,"
"double","int","less in faster slower machine, part is larger in hardware penalty for,larger in range larger smaller,longer than an  in long size_t zu, is a better overall,drastic than between short  overall,bigger overall, float unsigned long long overall,more than  overall,greater range than  in range larger smaller, string an so in specific void argument,","526070,47537306,34400483,53423672,51453223,56763785,26690571,49494540,55874696,50211053,57075289,","similarly when i cast a double that is less than the minimum possible int i also get -2147483648,but if the int part is larger than the maximum 64-bit int the int part s not that simple and you need to convert biginteger to float double which hardware doesn t support,because a double is larger than an int this accesses the 4 bytes of the original int and an extra 4 bytes on the stack,what is not stated is what happens to other larger types such as long long and long double and possibly long if it is longer than an int on the compiler,there is at least one argument for which the match is better than the match provided by any other viable function so when compiler checks first argument by first parameter of viable function it chooses that foo double double is the best match but when it checks the second argument it finds that foo int int is a better match,the clash between int and double is even more drastic than between short int and int,a double is bigger than an int,to ensure this the compiler uses the following rules if an operand is an int that is narrower than an int it undergoes integral promotion as described above to int or unsigned int. if the operands still do not match then the compiler finds the highest priority operand and implicitly converts the other operand to match. the priority of operands is as follows long double highest double float unsigned long long long long unsigned long long unsigned int int lowest in your case on of operands has type int and another one type size_t so max was promoted to type size_t and bit representation of -1 is the biggest possible size_t,the number of base 6 digits shouldn t be more than double the number of digits of a base 10 number and a 32-bit int has at most 10 decimal digits so just define your array to have 25 elements,so instead of using double and allowing for invalid inputs you should be using long instead which has a greater range than int,i m trying to parse a rdd seq string to dataframe. although it s a seq of strings they could have a more specific type as int boolean double string an so on. for example a line could be hello 1 bye 1.1 hello1 11 bye1 2.1 ... another execution could have a different number of columns,"
"double","int","widened type than  in general most-specific type,greater in range larger smaller,greater in range larger smaller, is a little more overall,greater precision than  in range larger smaller,stricter alignment overall,more overall, type not in range larger smaller,higher overall,larger data-type in range larger smaller, and float s overall,","54973870,6297636,526070,17438412,25636405,41379779,40663857,6986282,39709217,27064252,50599682,","double is more widened type than int,you should be using math.ceiling which will return the smallest int value that is greater than the double passed in,today i noticed that when i cast a double that is greater than the maximum possible int to an int i get -2147483648,if you were to use an int on an older 8086 or on a 16 bit processor microcontroller an int might be 16 bits;a double is a little more standard,and the second calculation 33 5.0 the result is actually promoted to double which has greater precision than int and resulted to decimal,on some real-world architectures double has stricter alignment requirements than int,the reason is simple double has different handling than new - mql4.56789 float the more than int et al,i guess that your input_arg should be defined as a matlab int type not as a double;it seems that you cannot use a double which is a floating point type as a representation of an address in memory which is inherently an integer value,i have a decent understanding on why there is an error because of how double has a higher precedence than int and how an int cant necessarily store a double value,you do explicit type casting to get your result in int because double is a larger data-type than int,the way functions without prototypes are called is the arguments are promoted lesser than int int types to int and float s to double s and passed to the function in an implementation-defined way,"
"double","int","to;use  quotes to create overall,faster in faster slower machine,larger than  in range larger smaller,more general type in general most-specific type,more than just  in float significand types,better in precision precise problems,less space in range larger smaller,larger overall,bigger maximum in range larger smaller, are not totally overall,slower in range larger smaller,","38953992,37742530,57138420,19823940,54574851,21886861,38246880,6765663,37956224,21807480,16741309,","x is of type chars chars to int is a better conversion than chars to;use double quotes to create a string literal,maybe division between double and int is faster than it between double,i am trying to round a large double larger than int allow so it will print without decimal places to the e13 power but i don t know how,double can in a way be seen as a more general type than int,x86 data types formats include much more than just int signed 2 s complement and unsigned binary int ieee float and double with sse and sse2 and x87 memory operands,double is better than int though as it will help you keep precision,1 if your data are int or some data form that takes less space than a double float,i understand that the compiler isn t happy because a double might be larger than an int can hold but this particular control is guaranteed to be a value from 1 to 10 so i know that it will be okay,so if i try to convert a double number bigger than maximum int it crashes instead of returning nil,as stated by ed heal in the comments you should however use int to store dollars amount in cents or according to the smallest of your currency but multiply all amount by 100 to do accurate calculations;float and double are not totally precise because of their binary representation,double is a different data type and generally much slower than int,"
"double","int","narrower fabs overall,less precise in precision precise problems, is not overall, required more overall, is larger in range larger smaller, and has only in precision type integer.parseint,less space in space members outofmemory, column named censorflag overall, is smaller size in range larger smaller,trickier in trickier easy infinities,more overall,","39100376,36801182,52690286,40185996,54537863,27747395,14674829,49184830,45198135,4194962,1400465,","code should use abs with int or narrower fabs with double fabsf with float labs x with long etc,since int is less precise than double i thought i needed to cast it when parsing it into a method,so to fix this either make min and max be double or leave them be int decreasing accuracy and print them as what there are namely int printf minimum d nmaximum d n min max;printing an int using a conversion specifier for double is not a good idea but invokes the infamous undefined behaviour,xamples with floating point include some cray supercomputers which natively support double precision floating point aka double in c++ and all operations on single precision aka float in c++ are emulated in software;some older 32-bit x86 cpus also worked internally with 32-bit int and operations on 16-bit int required more clock cycles due to translation to from 32-bit this is not true with more modern 32-bit or 64bit x86 processors as they allow copying 16-bit int to from 32-bit registers and operating on they with fewer such penalties,if a double is larger than the maximal int value converting it to int will give you the maximal int value,note that pow here takes an int not another bigdecimal;the double type in java is a ieee double and has only 53 bits of precision,int would take up less space than a double,in particular i assume i have a data frame set with a double column denoted as data and an int column named censorflag 0 value if censored 1 if not prefer this over boolean type,4 + 4 since int is smaller size compared to double padding of double size is done,note that the double is much trickier than the int s,i am guessing this is because modifying a double is a more complex operation than modifying an int,"
"double","int","more precision in precision precise problems, s not overall, not in object oggplayer iplay,lower in floating-point fractions argument,faster in faster slower machine,lower in range larger smaller,significantly faster in faster slower machine,slower than storing  in faster slower machine, not in long size_t zu,better in performance version better,general overall,","35525303,10306650,960895,13589067,24159693,18709820,25354205,10737929,52543712,36965719,54711020,","double can hold more precision than int,and you ll be fine as long as int do not overflow;you are unnecessarily using double s not to mention math.pow isn t the most effective and straightforward way to compute the power of 2,your oggplayer is an iplay double not an iplay int;you can t cast an iplay int to an iplay object or an iplay double to an iplay int like that,so no arguments after the 1st one get automatically converted except as defined by default argument conversions basically any int type with a rank lower than int to int and any floating-point type with a rank lower than double to double thank you pascal cuoq,int multiplications however are only 1.5 times faster than double multiplications,num is a of type const int which is of lower rank when compared to the type of z which is double,this can t help performance either since pow double int is significantly faster than pow double double in general,converting it in php between int and double will be slower than storing double in the database,d takes an int not a size_t should be zu;g takes a double not a long double should be lg,well in multithread the performance of the int version is 2.6 times better than the double precision,according to official flutter website only primitive types can be used int double bool string and stringlist so if you want to store binary data you should consider using sqlflite which is a self-contained implementation of the famous sqllite database for flutter,"
"double","int","better in error hard smarter, ...or even overall, which is a float in range larger smaller,greater in greater parameter area,compare against  overall,larger type in range larger smaller, is not in possible size value,less in part literal checking, that is greater overall, and promoting float s in range larger smaller,more efficient in efficient modulus use,","27318100,54579732,4391609,38741563,52260248,12931816,55635440,23227239,54557453,49213916,7998350,","is simply a hard error because f int is a better match than f double even though the f int overload causes a hard error further on,there is never never any reason whatsoever to square-bracket an int literal it s nothing more than a rather roundabout way to turn an int literal into a double since worksheet numeric values are variant double which is much better off done with an explicit conversion debug.print typename cdbl 42 double ...or even with a gasp type hint debug.print typename 42# double,your upccode int is larger than the long data type so i tried your upccode int with double which is a float but works,this causes a referencing to a memory area that was not completely initialized with value passed as parameter to the printf because double size memory buffer area is greater than int size,in my code i can work around the problem by using double or explicitely casting int to the template type t but when i try to work with i get into trouble for the methods which compare against int literals like the _div method above,the math.round double a method returns a long which is a larger type than int,since the value of the string is not an int but a floating point number we have to parse it as a double first;but to convert a string to the primitive double is not possible so we have to use the wrapper class double for this,the double literal 99.9999999999999 can be represented as a double which is less than 100 so the cast to int truncates the decimal part and 99 is the result,after further testing i realized any double that i cast to an int that is greater than integer.max_value just makes the int equal to integer.max_value. for example after further testing i realized if you try to cast a long to an int it seems to assign the int to a seemingly random number,these consist only of promoting int types smaller than int to int or unsigned int and promoting float s to double,if you want the modulus use as it gives an int and would be more efficient than double arithmetic,"
"double","int"," which is more overall, is a subtype in float significand types,smaller range in range larger smaller,better results overall, that takes less size in range larger smaller,less in space members outofmemory,better performance overall, doesn in precision precise problems,greater overall,bigger mantissa in range larger smaller,faster in faster slower machine,","24659070,56494904,1696724,5787578,50316247,17383158,29631941,51723101,32134782,2386794,12443760,","when you use dim dynarray this creates an array of type variant which can hold anything string double object etc.;if you explicitly declare the type dim dynarray as int then it ll only allocate enough memory to hold int which is more efficient,subtyping section where the the supertype relation is specified as the reflexive and transitive closure of the direct supertype relation 1 and the latter includes for primitive types double 1 float long 1 int so the method test int float is more specific than test long double because int is a subtype of long and float is a subtype of double,int can hold a smaller range than double,if you use int between -128 and 127 you will get much better results than double from -128 to 127 because the former uses caches values the later does not,in other words instead of throwing a runtimeexception your compiler converts the int that takes less size to the type that takes more size which is in your case a double,i don t understand how an int 63823 takes up less space than a double 1.0,byte takes less memory than double and int and provides better performance than bigger datatypes especially better than double,there are few problems with your code is not applicable to as double doesn t extend int,what i need to do it iterate over a list find the first int which is greater than 3 and is even then just double it and return it,the double has a bigger mantissa the int bits of the real number,on my machine the double swap loop completes 11 times faster than the int swap loop,"
"double","int"," limited only in exact wall accurate, may not overall, does not in range larger smaller,wider in range larger smaller,smaller in range larger smaller,smaller in range larger smaller,smaller values in range larger smaller,smaller in better function bullet,narrower than  in range larger smaller,less memory in memory largest storage, value floating po overall,","20847933,28010641,41810481,41360457,32021034,24119742,32946829,7089680,49473656,13566079,34396831,","instead use biginteger which uses an arbitrary number of bits to represent an exact int limited only by memory;double doesn t have enough bits 52 to exactly hold 65 17,there s also no guarantee how floating point types will be represented so it would be a blind shot to assume that converting int to double or even long double for comparison will solve anything;even double may not be able to represent all int values depending on how much bits does int contain,a big reason double ceil double does not return an int type is because that limited functionality is rarely needed;int types have or almost always have a more limited range that double,it is not exact on rare platforms where the range of int is wider than the range of a double s exactly representable whole numbers,in the third case double precision loss is much smaller than the int and you get something like,this imply that result of pow x y converted to int will be truncated because of double arithmetic and exponentiation that will return double value slightly smaller than int,another option would be to add a cast not recommended due to the same loss of information the error warns you about - the fractional part will simply be removed from the value and double can hold way larger and smaller values than int,the problem is that the precision of the data type int is smaller as the precision of double the function pow returns double therefore the value of binarychar i will be implizit converted to double and so on.,int types narrower than int are converted to int and float values to double,typically an int will use less memory than a double that is why one doesn t just use the largest possible datatype,when you convert 4.8657 double to int.the int value will be 4.primitive int does not store decimal numbers.so you will lose 0.8657;in your case 0.7 is a double value floating point treated as double by default unless mentioned as float-0.7f,"
"double","int","less in precision precise problems,more specific overall,range more overall, main doesn overall,greater than max  in range larger smaller, so there overall,type larger in range larger smaller,better fit in better match second, is a better in better match second, is not bigger in possible size value,type substantially narrower in narrower 32-bit function,","36164348,31627536,2582032,18423280,48395285,49274947,12323870,38239419,5014331,39285668,19591563,","further the precision of double may be less than int introducing other problems,for example an int is more specific than a double because 1 can be promoted to 1.0,double has range more than a 64-bit int but its precision is less dues to its representation since double is 64-bit as well it can t fit more actual values,console.read return an int that represent a chars so using if the user inputs 1 you may have a different value in your length variable;your static double main doesn t return a double,note it s not a good idea to cast it into int value because max double value is greater than max int value,because on most machines sizeof double is bigger than sizeof int so there s not enough room for the array elements after a while it should be,you need to use 2 even if the number of bits in your int type is larger than the precision of a double since the number of used bits in the most significant value might only be 1,so your compiler picks the double constructor as a better fit than the int one,f double;although f int is a better,the conversion from double to an int type changes the value by design think to 3.141592654 converted to an int;the conversion from long int to int instead may or work or may be undefined behavior depending on the platform and on the value the only guarantee is that an int is not bigger than a long int but they may be the same size,the int portions of the logarithms of 31 and 310 use different numbers of bits so there are different numbers of bits left for the significand so they end up being rounded to slightly different values. as long as the int type is substantially narrower than the double type the calculated limit will be much larger than the error in log10,"
"double","int","probably larger overall, takes longer in hardware penalty for,much wider in range larger smaller,much less in range larger smaller, is greater overall,typically smaller in possible size value,higher precision in precision type integer.parseint,simpler in long size_t zu,greater precision than  in range larger smaller,more bits in number advance non-decimal,much slower in faster slower machine,","30810025,31195152,1348958,45266203,56689895,20732505,19565419,32944022,51310956,14313668,15298409,","furthermore the representation of type double is probably larger than that of type int in your system 8-byte double s and 4-byte int s are common,for most modern since the end of the last century linux-compatible architecture there is mostly no penalty for smaller int types;for float types there might be architectures only supporting float not double by hardware or double takes longer to process,the range of double is much wider than the range of int or long,indeed fact is limited by the range of type unsigned int which is much less than the precision of type long double,a proposal can be compilation and execution as you can see i name the variable from their behavior making the code more clear contrarily to a b and c the right type for an index is size_t i do not use a literal int but i use sizeof in the for to follow the size of the array even if it is changed i use infinity to initialize min because any valid double is lower that it and -infinity to initialize max because any valid double is greater that it in case of an invalid input on scanf i flush all the line before to redo you can also just read a word if you prefer in the printf you do not need the l because printf cannot receive a float whose are transformed to double but the l is necessary in scanf to read a double,in contrast accessing the bits of a double as an int is usually in-practice safe even though it s formally ub because 1 an int is typically smaller or equal in size to double and 2 an int usually does not have any invalid bit patterns,you can assign the int s returned from integer.parseint string s to your double variables because double s are higher precision than int s and so type conversion happens automatically without the need for an explicit cast,in general casting from long to int is simpler than from double to int,recall that the int type may have greater precision than double,you can work around this issue if you ll advance to big int floating point numbers that store more bits than a double precision number,double is much slower than int,"
"double","int"," does not overall, if not overall,less overall, cannot overall,more space ints-an in space members outofmemory,faster in faster slower machine, variable cannot overall,less specific in specific void argument,smaller than  in range larger smaller,bigger than  in range larger smaller,easier overall,","57441498,43808329,14552079,33948620,1935145,24159693,34396685,42050694,52956861,47983443,37548426,","also from the same javadoc regarding the conversion d integral the result is formatted as a decimal int so the format specifier you need for int is d which you are correctly using however an int does not have precision;a float or a double may have precision but int does not,my opinion would be that these two should be of the same type and i really don t think it is realistic that these values would ever exceed the capacity of a 32 bit int if not 16 bit;not too mention that float and double are only approximations,i should point out previously in the code the double is less than 1 i.e 0.987 and is then multiplied by the number of decimal places to make it a real positive int though it s stored as a double still at that point,this is because we are outside the range where adjacent int are exactly representible in double;of course a 64-bit double cannot represent all 64-bit int,you re getting the outofmemory because if you declare int 1000 the memory is allocated immediately additionally double take up more space than ints-an int representation will also save you space,int subtractions are 2.5 times faster than double subtractions on my machine,you are trying to assign which is a floating point value a double to an int variable;a double is not an exact int so in general an int variable cannot hold a double value,so overloadedsingleparam double a is less specific than void overloadedsingleparam int a when an int value is passed as argument,in the variable-length portion of the argument list all integral arguments smaller than int are promoted to int and float is promoted to double,the second issue that you should take into account is your ans array you try to initialize account with values that are much more bigger than int or double capable to hold values that are bigger than 2 64,here for what it s worth is a pipes-csv variant which just compresses each parsed row into an unboxed vector of int s by hand this easier than finding double which is what this csv is really storing using readint from the bytestring package.,"
"double","int","larger in range larger smaller, is less overall, cannot in f can not,bigger in range larger smaller,smaller range in range larger smaller, does not overall,part less overall,compare eger1 with  overall, which cannot in number hashes expensive, and not overall,precise than  overall,","20732505,41379779,30514521,18580095,42537385,6917173,9081126,57323642,17779356,54912660,57007209,","a double is 1 typically larger than an int and 2 has some internal structure,if you set u.y and read u.x on a target where int is less than 64 bits wide the remaining bits of the object representation of u.x might be anything including a trap representation;or if you set u.x and read u.y the value will depend on the details of how int and double are represented,f int is visible within the definition context but not f double so overload resolution resolves to f int for both calls;f double cannot be found by adl because built-in types have no associated classes or namespaces,you could try using double which is bigger than a long and use only the int part,int or int32 has a much smaller range than double,since int does not have nan and both +0 and -0 will be regarded as just 0 such a method is not really needed for functionality;the compare in double has the same effect as,the int part is less than 2 and the double part is less than 3,spel evaluation error could not compare integer1 with double 2 and there is 1 more error in the log then there was an error where i was told that adding null and int type is not possible,in other words int is a number double is a number but int is not a double;you are trying to cast an instance of int to a reference double which cannot happen,define a type for the precision parameter int define a return type double;it was nearly correct use double and not float because this is the default floating point number in kotlin,why doesn t double print the entire number in second code- why is int more precise than double in that case,"
"double","int","more inexact overall,smaller in range larger smaller, s cannot in range larger smaller,more expensive in number hashes expensive,specific than  in specific void argument, quotes is there overall, and is commonly narrower in range larger smaller, is greater in range larger smaller, not overall, not in narrower 32-bit function, are not in range larger smaller,","14701685,10789396,51314811,30953112,53618332,49656418,41360457,51145298,47606479,53581817,4253527,","about the inexactness problem you should be aware that double can be more inexact than int,you could try to add some value to the double to make sure it s bigger or smaller than an int can be,double s being floating point values and having a range larger than int s cannot guarantee no loss in data so c# requires an explicit conversion usually termed explicit cast via the explicit operator to indicate to the compiler that we re okay with losing data,while both double comparisons and hashes are more expensive than int s the number of comparisons is theta n log n while the number of hashes is o n,int is more specific than double because there s an implicit conversion from int to double but no conversion from double to int,iam trying to retrieve the data from the table in json format with below query in sql server iam getting json response as where field id a id are declared as int in the database now i need those values should also be displayed within double quotes is there any way that we can handle in sql server 2016 itself to make clear all the field values irrespective of datatype should be enclosed in double quotes please do need full,with float x the conversion is commonly inexact on many platforms over the range of int as float has not more range than double and is commonly narrower,if those are smaller than dbl_max_10_exp the range of double is greater than the range of int and we can cast int_max and int_min to double,firstly need to call out in scala everything is an object there is no primitive type for your code it s int not like in java but scala need to compile to java bytecode to run in jvm since object consume more memory than primitive type so scala has specialized to solve this it means generate the primitive types parameter method when annotated with specialized with types;so for your code it s tuple2 and it s specialized for int long double chars boolean,0 is a 32-bit int you want 0d for 64-bit double s;you want to define transformtree for double not for an arbitrary type bound to type variable called double,the int and short are equal because upgrading a short to an int will not change its value;the reason the float and double are not equal is because their values differ slightly,"
"double","int","smaller sizeof in range larger smaller, only need 24x24-bit significand in float significand types,greater than eger.max_value  in greater parameter area,less overall,faster in performance version better,narrower in narrower 32-bit function,general in range larger smaller, technically isn overall, does not matter overall, is the way overall,slower in faster slower machine,","24146146,55623764,50786480,23666565,45876593,19591563,24194963,34094310,53758285,17600867,7896213,","since sizeof int is very probably smaller than sizeof double this is going to lead to horror,but general 32-bit int multiply has worse throughput than packed- float multiply;because the simd alus optimized for float double only need 24x24-bit significand multipliers per 32 bits of vector element,you re building up your total in a double and then doing this which is as radiodef said if temp s value is greater than integer.max_value int temp will be integer.max_value 2147483647 which when divided by 1000 with int division is .,as far as i m aware so long as the spacing between two double is less than 2 then int values stored as double should be exact - and though 10 14 is pushing it this should be an exact int since 10 14 2 46.507 2 53,the versions using diff are especially impacted ave_diff with int constants is about 2.5 times faster than the double contants version,i believe the following works as a sort comparison function for positive int provided the int type used is substantially narrower than the double type 32-bit int and 64-bit double and the log10 routine used returns exactly correct results for exact powers of 10 which a good implementation does,an int key is simpler to implement and easier to use and understand;an int key s also smaller 4 bytes vs 16 bytes so indexes will fit about double the number of entries per io page meaning better performance,there are two major differences i int require less storage space - this may be significant for larger vectors and ii when interacting with external code that has greater type discipline conversion costs may come into play;as is for coercing to a new class and double technically isn t a class but rather a storage.mode,i want to force users to enter a number and i m using that and it s working fine i want to force the user to make the input just two numbers int or double does not matter and those numbers must be between 0 and 20 or 0.0 and 20.0 e.g 0 or 0.0 1 or 1.5 1.0 etc . . . 20 or 20.0,adding to that int do not give you the decimal part of numbers so a number like 3.5 will get cut down to 3 using int;to fix this double is the way to go,with it is nub on and comparing big int is slower than comparing double,"
"double","int","choose between  overall,larger in range larger smaller,more accurate in exact wall accurate,larger in range larger smaller,much longer in longer w,higher in range larger smaller, is also better in better function bullet,specialised than  overall, or promoted float in float significand types, is less in range larger smaller, this is the default in range larger smaller,","54960060,43662319,37872559,35973092,30058172,43723712,14990231,51116357,12851435,13558575,30968855,","based on the result returned by eval i need to choose between int and long types - smallest possible that will fit the result converted to a round number as it s double,for platforms where int is larger than double it s obviously false,i figured out this was happening because i was storing the wall coordinates in double and although double are certainly more accurate than int they still aren t exact,if you are not storing infinity nowhere and use it just for comparison you can use double infinity which is larger than int and long max value,if they are double it takes much longer until w w 2 is 0 than with int,if your resulting double has higher int value then simple int math sometimes stops working,this example is exactly the situation described in the second bullet so the function returning double is clearly better than the other two;here s where it gets weird by a very literal reading operator int is also better than the template specialization because of the third bullet,this can be either a literal of type o or a binary expression containing expressions that represent more specialised number types than o int being more specialised than double,if instead of the double i pass an int not much changes but that significantly;i have looked at the generated code for many variations of types and count of arguments passed to printf and consistently the first double or promoted float arguments are passed in xmmn and the integer int chars long regardless of signedness are passed in esi edx ecx r8d r9d and then the stack,int values will be represented exactly as double numbers so long;as the absolute value of the int is less than 2 m the length of,the method should return a double not an int;when you do 1 a and you do not explicitly cast it to float double the default result will be an int this is the default type of a arithmetic operation in java between integral operands if not explicitly cast to something else,"
"double","int"," and check this condition;if overall, is less in precision precise problems, unsigned makes more sense here overall,wider overall,narrower than  in range larger smaller, doesn overall, pipe not overall, or not in range larger smaller, is able to represent less overall, is much larger so in range larger smaller,more than  overall,","17639702,24727996,42153082,29224191,28308615,46392697,45867552,32553395,52491373,3565246,48090955,","we could therefore iterate over every point if the coordinate system is based on int and not double and check this condition;if we are faced with double then a modified technique could be used to generate a contains method that will function correctly with high probability, and y are added as double and casting to int will saturate to the range;this is not as suitable for long s as precision of double is less than that of long but if the precision is not as important the precision will suffice,don t use m n on float or double;int unsigned makes more sense here,this conversion is legal because double are wider than int,int arguments narrower than int are promoted to int or to unsigned int if the type is unsigned and the type maximum value exceeds int_max and arguments of type float are promoted to double,this is a problem because int doesn t support division via but rather supports int division via div to avoid a certain class of bugs that crops up in languages that conflate the two;you have a two obvious choices namely convert your int s to something that does support division double or generate random double s in the first place,edit to clarify - bitwise operators will cast any value to a number value to 32-bit int to be specific;most likely you want to use a logical or operator which is || double pipe not | which as you ve pointed is a bitwise or operator,define a precision for which you decide whether a double value is an int or not;test whether the rounded value of the double you have is a correct result,as mantissa for the double in such a case has less bits than the int then double is able to represent less digits and a loss of precision happens,but a double is much larger so printf only gets a bunch of zeros,even with a tighter bound you still need to handle more than int and double,"
"double","int"," is a bit overall,smaller in range larger smaller, without giving nil overall, is longer in longer w, to float return overall,far greater in range larger smaller,faster in faster slower machine,more precision overall, is much more in range larger smaller,larger in range larger smaller, is not overall,","47977240,5772391,48097085,54034768,48576869,605547,594466,41420341,2398833,30706940,8554731,","double class a special class representing a float value this is especially useful when using capped collections where you need to ensure your values are always floats;int is a bit trickier due to the fact that javascript represents all numbers as 64 bit floats meaning that the maximum int value is at a 53 bit,these conversions promote float to double and anything smaller than int to int or unsigned int,answer int check string is int or not if yes then give you int and otherwise nil;but float or double can convert any number string to respective float or double without giving nil,0.5 is not int is longer int try to convert to double,i want to return different data types to abstract class bank but i can t do that .i did refer different return types of abstract method in java without casting this question because i have less experience in java therefore i did t understand that link s solution.please anyone help me to solve the issue code error test.java 18 error union is not abstract and does not override abstract method rateofinterest in bank class union extends bank test.java 19 error rateofinterest in union cannot override rateofinterest in bank float rateofinterest return type float is not compatible with int test.java 20 error incompatible types possible lossy conversion from double to float return 9.5,another reason is that the range of double is far greater than that of int,so i asked myself can a double be faster than a int,the problem is that i need to draw some simple objects points lines especially rectangles with double float decimal precision it doesn t matter with more precision than an int on a windows form i m using visual studio but i can t find anything,this doesn t change the binary value so the number displayed is the binary representation of a float or a double;the actual cast from float to int is much more complex in the generated assembly,because a double which f expects probably is larger than an int on your platform,the problem is not that you can t convert a double to an int round accomplishes that just fine it s that you re trying to do division on an int padlength - length string;the error message is just telling you that int is not an instance of fractional the typeclass for numbers that can be divided,"
"double","int","larger than an  in range larger smaller, can not in range larger smaller, is better in better function bullet, is larger in floating-point fractions argument, which is easier overall,smaller in range larger smaller,higher values overall,lossy conversion between  in specific void argument,more overall,smaller rank overall,way greater in range larger smaller,","51969789,14792021,57044680,31960150,16031002,38606192,17379031,55226963,9447869,25343540,605544,","because a double is typically larger than an int and because the struct is packed the field b is not properly aligned when the smaller field appears first,all larger int can not be represented exactly by a float therefore the calculation with numbers having 8 or more digits using float cannot be exact;double precision floating point numbers can represent numbers up to 2 53 9007199254740992 exactly,in java you can tell the compiler what kind of numbers do you have if you look at the signatures you have public static void foo int i long s public static void foo long i int d you have an overload of the method that s ok but now when you use the methods you have diferent types as arguments of foo you can do this in several ways or just foo 10 100l the l means your number is of type long the same can be done with 10.02f meaning float or 10.05d meaning double is better to use capitalized letter so the is no missunderstanding between 1l looks like eleven and 1l 1 long,therefore ieee 754 single precision floating-point format has 23-bits fractions and int type has 32 bits so if the int is larger than 2 24-1 the float type can t represent it exactly;but double floating-point format can since it has 52-bit fractions,if you want to map the range -128 to 127 to 0 to 255 i suggest you use an int which is easier to work with in java;as you have 32-bit or 64-bit registers using a short local variable doesn t help you as much as you might think unless you have a lot of t and i mean millions btw the byte code only allows for two sizes of local variables one slot boolean byte short float int reference and two slot long and double note reference uses one slot even on 64-bit jvms,that means a float will be a double and anything smaller than an int will be an int,to prevent the code from overflowing the int value space you should could change the type of res to double long anything that can hold higher values than int,and i get an error that says lossy conversion between double and int,as soon as the int needs more than 29 bits the atom is changed by the vm into a number type which is really represented as a 29 bit pointer to the actual 64 bit double precision float,for historical reasons you can not pass an int argument of smaller rank than int or a floating type of smaller rank than double to a variadic function,the range of double is way greater than the range of 32 or 64 bit int which is why std floor returns a double,"
"double","int"," cannot overall,finer resolution than  overall,implicit conversion between  overall,bigger overall,bigger overall, because now overall,larger 64-bit in range larger smaller, not overall, does not overall, which is much larger in range larger smaller,wider overall,","10680485,57732721,53263192,10556878,2551657,53186096,21071959,34853495,30584727,53488508,16964112,","method java.util.arraylist.add int java.lang.object is not applicable;actual argument double cannot be converted to int by method invocation conversion,to slow down actors in greenfoot you usually add a counter like this if you need a finer resolution than int multiples you can use double instead of int,in this case the conversion between int - double takes lower precedence because a user-defined implicit conversion between int - value exists,a double is also probably bigger than an int,edit actually my solution does not work because double can be very big much bigger than int and also very small like fractions,however this introduces manual memory management and this can become tricky in more complex code memory leaks double deallocations dangling pointers so a more c++ like solution would be assuming the pointers point to something more interesting than an int because now there seems to be no need for pointers,typically a double is 64-bit ieee floating point with roughly 52 bits precision and with range much larger than 64-bit int so magnitude is no problem,this is defined by the standard but it will in no way do what you are trying to convert an int to a string you need something like snprintf this is an example;also fabs returns double not int so you risk an overflow if you assign the value to an int,because the double is represented in a totally different format from the int even if you assume they have the same sizes;a double uses a floating point format whereas the int does not,int is a 16-bit signed int type that can easily fit into a double which is much larger than 16 bits,the widths are positive int not wider than the double significand 53 bits,"
"double","int","bigger overall,larger range in range larger smaller, by appending d in large answer issue, is usually in range larger smaller,smaller overall,quicker float in range larger smaller,","10531075,39937154,52641112,18066670,20963734,11286328,","a normal 3 will be treated as int but in the test function you are retrieving double which are bigger than an int on most platforms and you might end up reading wrong locations which inturn leads to exc_bad_access run time signal being generated,if one of the operands is a double the result of the multiplication would be a double which allows for a much larger range than an int,the answer should be because 10000000000 is int and 10000000000 is int can not be stored because 10000000000 is int is more than 2 147 483 647 that is why you are getting the error int number too large to fix the issue make the issue double by appending d,if you must use binary floating point double is usually a better choice than float unless you are dealing with a lot of numbers and know float has enough precision,on many systems int is smaller than double so if that s the case on yours this is very likely to be the cause of your crash,a solution using int only calculations should be quicker than float double calculations is,"
"uicollectionview","uitableview"," shows more overall, has a more overall,more visual flexibility overall, supports more overall, but not overall,more overall,more generic overall, has a layout overall, is way more overall, not overall, that doesn overall,","23079066,56965387,22590525,30758392,47921548,35645665,28599198,20980841,48112711,22428290,57126726,","you get the selected row instead of the item so for creating grid or modified items using uicollectionview is best;for the listing details of each item people use uitableview because uitableview shows more info on each item,uicollectionview has a more advanced caching scheme than uitableview or at least as it used to have which is the reason you see what you do,finally you might want to look into uicollectionview which provides a lot more visual flexibility than uitableview and can be seen as a sort of generalization of the latter,uicollectionview is like uitableview but uicollectionview supports more than single-column layouts,uicollectionview is a more recent addition 4-5 years old and provides a much more flexibility;yes uicollectionview duplicates much of the functionality of uitableview but not all,instead of using a uitableview in this instance perhaps you might consider using a uicollectionview - they are built for something like you describe more than a uitableview,uicollectionview is more generic than uitableview and you can custom almost thing on it,every uicollectionview has a layout object a uicollectionviewlayout subclass that describes where each cell should go in the collection view s scroll view and which cells are visible in a given cgrect;uitableview doesn t have this so it would be much harder to do anything that manipulates the position of its cells,i made the mistake at first to go with uitableview only to realize halfway that uicollectionview is way more powerful,if you want a 2-dimensional table then you want a uicollectionview not a table view;uitableview are strictly one-dimensional with optional grouping into sections,in practice though there are strange things in the uitableview implementation that makes similar things hard to do and would be much easier with uicollectionview that doesn t have those problems,"
"uicollectionview","uitableview"," but gives you more in similar control appropriate, is better overall, is more in similar control appropriate, is not overall,better candidate overall,","12565598,56481589,21313627,24967702,22944159,","a uicollectionview works very similar to a uitableview but gives you more control over how the cells should be displayed and aligned,this can be very helpful and logical with uitableview but layout calculation in case of uicollectionview is better to be determined by simpler calculative approach,uicollectionview is more appropriate for your problem;uicollectionview is like uitableview only loads cells displayed on screen,however in terms of intuitiveness and simplicity the uicollectionview is the better of the two options;if you feel comfortable with a uitableview then picking up a uicollectionview is not at all difficult,on second thoughts in your case uicollectionview is surely a better candidate than uitableview,"
"pagespeed","yslow"," addon gives much more in better detailed small,more reliable overall, requires a value overall,better in better detailed small,better overall,","584890,19182412,24483447,584890,27689780,","my testing so far google s pagespeed addon gives much more detailed analysis and smarter advice rather than recommending a cdn for small websites like yslow,i use firefox with firebug and yslow and think it s more reliable than pagespeed,this configuration satisfies both pagespeed leverage browser caching and yslow add expires headers;yslow requires a value greater than 7 days,in my testing so far google s pagespeed addon is far and above much better than yslow,imho pagespeed is better than yslow,"
"artifactory","nexus","better overall, and cannot overall, does since v.4.5 overall, may not overall, we have a repo overall,better overall,better overall, gives a much overall,","8953059,1039669,37615934,18843676,33084402,15488729,15226169,4092349,","nexus looks better choice than artifactory because of using filesystem instead of database to keep repository,that said i have never used nexus and cannot really help you with a proper feature comparison;here are some things off the top of my head that i really like about artifactory keep in mind nexus may have these features too,as of now nexus doesn t support cocoapods;and artifactory does since v.4.5,if you don t like big databases underneath artifactory may not be for you since it stores everything in jcr database although the authors stand behind their design choice;nexus is more file-system friendly and we ended up using it,this is solved by configuring your maven repository nexus artifactory from not allowing overwrite of the release repos;in nexus we have a repo for snapshot and one for releases,i would recommend artifactory much better than the nexus,i will not argue why and if nexus is better than artifactory as it would not be fair and would only cause flame wars,edit this is not true anymore as of 2017 nexus gives a much larger support for other build tools end of edit;artifactory provides an awesome hudson teamcity and bamboo integration and gradle ivy support,"
"cin","cout"," does more overall,faster in preferable situations practical, is faster in preferable situations practical,more practical in preferable situations practical, could be because syncronization overall,printf faster in preferable situations practical, is not part overall,","1736308,22112628,26448916,14757282,56210331,780111,21135541,"," remember many times when my solution didn t make it before the time limit just because of cin cout while printf scanf did work;besides that it seems normal at least for me that cout is slower than printf because cout does more operations,using printf isn t faster than cout but scanf is a little faster than cin 0.04s + - 0.05,hence cin and cout appear to be slower;however if the synchronization process is set to not occur cin is faster than scanf,there are obviously situations where in c++ scanf is preferable to cin i was wondering if there are any situations where printf is more practical than cout,to have the function more equivalent to the java version change your c++ main to note that by default c++ s console i o iostream cin cout is even slower than o iostream cin cout could be because syncronization with c s console i o stdio scanf printf is enabled to let a program not do weird things if both cout and printf are used here you can read about cout s performance when synchronization is turned off,scanf is faster than cin printf is faster than cout etc,you ll also get std cout for putting information back out when you choose to do so;std cin is not part of the language but is actually provided by libraries,"
"fink","macports","more packages in top access stable,more overall,reputedly more stable in top access stable,more in top access stable,less reliable overall,","4396834,388422,4527378,3822232,4396834,","on top of that fink gives you access to more packages than macports,i second macports it is more up to date than fink and closer to the freebsd ports system where i come from,because fink is reputedly more stable than macports and has many more packages,fink has more packages than macports,since i have seen many reports of the macports package manager being less reliable than the fink package manager i would suggest installing fink and then simply doing,"
"cudnn","theano","version more recent overall,version more overall,","42335562,46502997,","userwarning your cudnn version is more recent than the one theano,usr local lib python2.7 dist-packages theano sandbox cuda init .py 600 userwarning your cudnn version is more recent than the one theano officially supports,"
"landscape","portrait","wider in wider taller cell, mode is much more overall,lower in devices better phones,view less in mode bigger smaller,bigger in mode bigger smaller,smaller in mode bigger smaller, s width height overall,bigger than in  in mode bigger smaller,smaller in mode bigger smaller,more than 650  in mode bigger smaller,lesser space in wider taller cell,","19877662,649402,3222095,2952593,35218575,7752076,54909764,48783217,13646497,29184666,34962474,","as long as landscape means wider than taller and portrait viceversa it means to read the image into memory and check width and height,if you re working mostly with text as most programmers or other technical folks do or even documents then portrait mode is much more valuable;in fact the general trend in displays is all the wrong direction aspect ratios are squishing landscape displays to a mail slot to better fit the format of movies,without more detail it s hard to say why your approach doesn t work but my guess is that you are seeing the sprite positioning you describe as a result of the fact that if you don t change orientation the lower left in portrait is the lower right in landscape when rotated left it s the same point in gl space 0 0,or the tableview is covering it up because the landscape view has less vertical room than the portrait view,you are using the screen size to generate your placeholder image and the width of the screen in landscape is bigger than portrait,because in landscape your height is smaller than in portrait you need to have a value smaller than 1.0,this happens because portrait image doesn t have an info about its orientation at conversion stage and thus an image in portrait remains in portrait mode even though it s converted to ciimage or cgimage;to fix this you should compare standard landscape s width height with a non-standard portrait s width height and if these values are different rotate an image to 180 degrees cw or apply orientation case .portraitupsidedown,the values really seem to be correct in the landscape the r value is bigger than in portrait,obviously i want in portrait view the content area of this iframe to be smaller than in landscape,then you can write separate css for width less than 650px portrait more than 650 landscape,as landscape has lesser space than portrait you are able to view only top scrollview so only that portion would be scrollable,"
"landscape","portrait","wider than  in wider taller cell,mode lower then in mode bigger smaller,taller in wider taller cell,mode choppier overall,layout more space in wider taller cell, is more in mode bigger smaller, to fully overall, doesn overall,larger than  in mode bigger smaller,smaller in mode bigger smaller,larger in mode bigger smaller,","57190354,45437891,34425898,11651304,34962474,19742481,54376869,13689722,53103444,39829397,38283302,","but if they need to be different landscape orientation is wider than portrait for example so positioning can change then headers and footers need to be unlinked and each of them searched separately,you have this issue because height in landscape mode is lower then in portrait,i am trying to make all a row of 4 images all have the same height size i already tried to play around with the width and height using css but nothing seems to work the portrait images always end up taller than the landscape ones here s the code,if you switch between portrait and landscape when viewing this on an ipad3 you ll need to refresh the browser window between switches you can see that portrait mode is choppier than landscape,i guess you are able to see the complete layout in portrait just because portrait layout is having more space and your complete layout is fitting well but landscape is having lesser space,so my proposed solution that to run all the views in landscape and this one in portrait;having this view in portrait is more design logical to have the same orientation of the picker,i recommend using portrait or landscape to fully prevent your app from auto-rotation,just because you perceive it as landscape doesn t mean it s actually not rotated portrait;in short draw your splash screens as if they are portrait but draw them sideways,i don t understand why the font-size property on iphone landscape seems larger than portrait or desktop browser.,your imageview in landscape is smaller than portrait,i had an outlet of a viewgraph which was a subclass of uiview in which i drew some graphics.in landscape mode the size of the viewgraph is larger than in portrait mode,"
"landscape","portrait","mode greater in mode bigger smaller,higher in wider taller cell,more rows in mode bigger smaller,more in mode bigger smaller,orientation better in devices better phones,higher than wider  in wider taller cell,less in mode bigger smaller, pictures are unfortunately in mode bigger smaller,more space in mode bigger smaller,shorter in wider taller cell,longer than wide  in wider taller cell,","19364430,18352012,15130794,40165509,22694615,53452572,43578371,53029907,32267160,28429494,50933632,","the amount of spacing to the right of the uiview when in portrait mode is greater than i want for landscape mode and the distance from the bottom is less than i want for landscape mode,you may try to listen to uideviceorientationdidchangenotification within your subclass cell to rotate the webview and within your view controller with the table view and run a -reloaddata on your table view in order to properly resize the cells assuming the landscape webview is way higher than the portrait webview,portrait mode can display more rows than in landscape mode,in landscape it has more buttons in addition to what is there in portrait mode,currently i m using only the portrait orientation for all devices but the portrait orientation looks better on phones meanwhile landscape orientation looks better on tablets,the frame of one cell is wider than higher kind of landscape and the other cell is higher than wider portrait,on an iphone compact in landscape mode the status bar is hidden and the navigation bar height is less than in portrait,it works just fine with landscape on the screen however portrait pictures are unfortunately cropped by the display software,in portrait the spacing is decent but in landscape mode i feel like the graphs should be bigger and take up more space than in portrait mode which is simply not possible with the tab bar and navigation bar,the difference between a min and max dimension can be very large as the given area covered by a landscape cell is much wider and shorter than a portrait cell,we can think about images that are wider than they are long landscape or longer than wide portrait,"
"landscape","portrait","smaller overall,bigger in mode bigger smaller, there is more in mode bigger smaller,mode more in mode bigger smaller, is less in mode bigger smaller,more horizontal space in wider taller cell,bigger topbar in mode bigger smaller,simpler than  in devices better phones,smaller than the  in mode bigger smaller,narrower in wider taller cell, mode not in mode bigger smaller,","33138747,25076470,47614397,32246796,48435517,45057795,29092300,57199134,48293288,25353972,11350136,","but the only thing i get is that the activity is still in landscape but the rectangle now is smaller with the portrait dimensions and the camera becomes weird because the image is rotated 90 degrees and moves down when i move the phone left and up when moving the phone right,my problem with this is that the calculation works fine in the portrait mode but as soon as i switch to landscape mode the distance between bottom and button is bigger than in portrait mode,on the iphone 7 plus in portrait readable content guides are the same;as the view s margin guides but in landscape there is more white,its because the height of the screen in portrait mode is more when compared to height of the screen in landscape mode,since the screen size for portrait is less than 480px but if i rotate its more than 480px in landscape mode which makes it medium size as per tachyons,i want all of the images to fit inside the width of the text area which will change dynamically with the window size and i want them all to be the same height meaning of course that landscape images take up more horizontal space than portrait ones etc,topbar portrait height is bigger than topbar landscape height everywhere except iphone 6 plus,using landscape is generally simpler than portrait because the camera s native orientation is already landscape right,this makes the landscape photos look much smaller than the portrait ones,as you can see the bottom two photos landscape so have been rotated are narrower than the portrait ones at the top,first i think there s a typo in your question -- on a 640x960 display 640 wide is portrait mode not landscape;portrait is vertical landscape is horizontal,"
"landscape","portrait","general overall,taller in wider taller cell,mode less in mode bigger smaller, mode is more in mode bigger smaller,","48839964,29357275,15405098,1337085,","example i have one web page which displays contents which will be break into several pages for printing the first 2 part of contents will be printed in portrait and the others will be printed in landscape,portrait is taller landscape is wider,now my doubt is when i rotate it to portrait mode from landscape mode since the width of the textblocks present inside the itemtemplate of listbox is already defined when i rotate the it to portrait i am not able to see all the data present it cuts off since the width of the portrait mode is less when compared to landscape mode,it will adapt the video frame height to landscape mode and it will play the video in the landscape mode whether or not a portrait mode is more suitable,"
"compareto","equals","consistent with  in note consistent issues,less than  in objects object value,less in objects object value,less in objects object value,less in objects object value,less in objects object value,less in objects object value, but i surely overall,less than zero  in objects object value,less grater overall,greater less in objects object value,","50170257,47655599,35256125,8528899,27367410,13620180,2045608,55826886,48138459,15893212,27008416,","note that the fact that your compareto is not consistent with equals may create other issues so only do this if you understand what you are doing,see the definition of compareto t o returns a negative integer zero or a positive integer as this object is less than equals to or greater than the specified object,the compareto method returns one of -1 0 1 depending on whether or not the argument is less than equals to or greater than respectively,or is it compareto returns negative numbers 0 and positive numbers respectively indicating whether the calling object is less than equals to or greater than the specified object,the only significance of a compareto return value is if it is greater less than or equals to zero,this interface defines the method compareto t which will return a negative number zero or a positive number if the first object is less than equals to or greater than the other one respectively,in the compareto method you can then write the code which will call the getters for the year month and so on until it figures out whether the current object is less than equals to or greater than the object o being passed into the compareto method,technically the treemap uses the compareto method so you might get away with a compareto that s ok in itself only incompatible with equals but i surely won t recommend that,based on these objects the compareto object must return a value either less than zero equals zero or greater than zero,if this works it seems that u are still comparing the strings in your ordered list and u should now check if the return value of compareto is less grater equals to 0,in the compareto method you can decide which fields are used for the comparison greater less than or equals,"
"compareto","equals","less in objects object value,less overall,less in objects object value,less overall,greater than  overall,less in objects object value,less in objects object value,less than  in objects object value,greater in objects object value,less in objects object value,less in objects object value,","1168889,26910623,8694958,42863933,22202376,43948265,22743324,53894413,22186019,20005392,17790456,","compareto compares values and returns an int which tells if the values compare less than equals or greater than,you should be using compareto method for less than or equals or greater than,icomparable declares the compareto method which returns an integer that specifies a less than equals to or greater than relationship between two objects of the same type,and my guess as to why an element is being dropped is your compareto method never returns a 1 in any case so elements are always considered to be less than or equals to other elements which is probably screwing with the treeset,in general the javadoc for compareto contract must return a positive zero or negative number if this.property is greater than equals to or less than c.property respectively,note that in both cases you can take advantage of the fact that int also implements so you can use its compareto method to determine whether corresponding values in each instance of your class are less than equals to or greater than each other,implement compareto in temporary temporary.compareto left such that it returns a negative integer zero or a positive integer as temporary is less than equals to or greater than the left,returns a negative integer zero or a positive integer as this object is less than equals to or greater than the specified object. and hence that is considered to be the natural ordering of the obj when you override the compareto extending to a,the compareto method returns 1 0 or -1 depending on whether value a is respectively greater than equals to or lesser than value b,i know that in java there is a compareto method that you can write in a class that will compare two variables and return a value -1 1 or 0 signifing greater than less than and equals to operations,compareto returns a negative integer zero or a positive integer as this object is less than equals to or greater than the specified object,"
"compareto","equals","less in objects object value,less in objects object value,less than  in objects object value,less in objects object value,bigger overall, or greater in objects object value,worse overall,greater in objects object value,less in objects object value, parameter is older overall,less in objects object value,","13719521,30146917,51774071,1814118,40578389,47707287,16559648,21615168,2504816,55252858,22751028,","compareto method returns a negative integer zero or a positive integer as this object is less than equals to or greater than the specified object,is a generic type at compile time java will require that an instance provide an implementation of int compareto t o which from the javadoc returns a negative integer zero or a positive integer as this object is less than equals to or greater than the specified object,the contract of compareto only states a negative integer zero or a positive integer as this object is less than equals to or greater than for less than it doesn t matter whether it returns -1 -3 -5 or -1000000,per the page i linked to compareto returns a negative integer zero or a positive integer as this object is less than equals to or greater than the specified object.,we can call its compareto method to decide whether it is bigger than equals to or less than any int,unless you ve implemented your own compareto it s only ever going to return -1 0 or 1 based on whether the object is less than equals or greater than the other,when one of the strings is a prefix of another the performance of compareto is worse as it still needs to determine the lexicographical ordering while equals won t worry any more and return false immediately,you need to call the compareto method which will indicate the result of the comparison by returning an integer less than greater than or equals to 0,the interface defines one method compareto that must return a negative integer zero or a positive integer if this object is less than equals to or greater than the other object respectively,i know that when you compare two dates the result will always be 0 if equals 1 if the date being compared inside the compareto parameter is older and -1 if the date inside the parameter is more recent,with this rather than needing to implement a comparator all you need to implement is the int compareto class o method in the class which returns a negative integer zero or a positive integer as this object is less than equals to or greater than the specified object.,"
"compareto","equals","less in objects object value,less overall,slower overall,less in objects object value,greater less than or  in objects object value, is better overall,greater overall,consistent with  in note consistent issues,less overall,less overall,less overall,","43117226,17221209,29003698,14918703,16649591,16559648,11146653,54418168,22085195,29596189,22130261,","according to javadoc compareto needs to return a negative integer zero or a positive integer as this object is less than equals to or greater than the specified object,compareto defines whether your base student is greater than less than or equals to your studenttocompare and you can define these with any criteria you please,most likely hashcode will be faster unless for whatever reason calling hashcode + equals once is much slower than calling compareto log n times,compareto return a negative integer zero or a positive integer as this object is less than equals to or greater than the specified object,you would just create a method called compareto which you will have to write in your code and put some logic in there that says if this object is greater less than or equals to the object passed in,looking at the implementation of equals and compareto in java.lang.string on grepcode we can easily see that equals is better if we are just concerned with the equality of two strings,i am using the compareto method in java to try and check if a certain date is greater than or equals than 24 hours after another date,treemap uses compareto and the documentation warns you of problems if compareto is not consistent with equals that should be true,all i changed was the first compareto comparison to be less than instead of less than or equals to,compareto returns a number that is less than equals to or greater than zero corresponding to the first string being before the same as or after the second string respectively,i am using the min value because i read that the compareto method returns negative 0 or positive whether the first string is less than equals to or greater than the second,"
"localization","translation","more overall, but not overall,more just overall, is easier than ever in text images translation,easier in text images translation,better way overall,","4362280,48868778,8732514,29969570,539857,9722966,","this is where i learned that localization is a lot more than translation,see comparing translation plugins features regarding woocommerce;you can t translate content of posts or pages with something based on .po .mo files because it is only for theme or plugin localization but not for multi language content,i d have one project team responsible for localization which may well involve more than just translation - different countries have different legal requirements currencies payment providers etc. and one team for improving the code base,now the text is translation;and indeed in xcode 6 localization is easier than ever because all your localized material can be exported imported as a unified .xliff file.,localization it s easier to translation text than images,is the best solution to add a new column called engmaterial and add the english translation there or is it possible to solve this in a better way with localization in .net,"
"base","short","less than five  overall,worse design overall,bigger than the  overall, answer not overall,","10307766,9740151,51239291,26684032,","as the condition reads a loop can not be zipped tighter if the condition reads s either too short less than five base or the condition reads ends do not match,in short it s worse design to move things to the base class than it is to downcast to the specific class,my tabbar width is fixed in here so when i have long text in my tabbar it will not shown completetly i want to make my tabbar width is flexible base on the content so when my text is just short text the tabbar width will be small and when the text is long text the tabbar width be bigger than the short text tab,short answer not possible;deprecated tags are used by the android source inside comments as it can be seen in fw base core res values attrs.xml see line 3427 for autotext as referred to in the original post in fw base core res values public.xml and in other places throughout the project but it appears that the build tools used for building applications simply skip all comments meaning the annotiation tags get skipped as well in the process making this method fail,"
"dictionary","namedtuple","less overhead in data better lower, which is much more overall,lower overhead in data better lower,less memory than a  in data better lower,faster than  in faster efficient access, because as in faster efficient access, is much more overall,faster in faster efficient access, for renaming columns overall,slower than a  in faster efficient access,lighter overall,","36613126,24252016,9072260,52630768,46266028,56193886,51656917,40184533,54300159,47999145,38679970,","namedtuple should perform better less overhead than dictionary if the lists are long,a namedtuple is a tuple with a namespace like an object but not as heavy on your python processes memory;a full object stores attributes in a dictionary which is much more memory heavy,namedtuple have a lower overhead than dictionary since the duplicate keys don t have to be stored per item but have the convenience of named access,also namedtuple use less memory than a dictionary since it implements __slots__,you can try namedtuple namedtuple should be faster than dictionary e.g,a namedtuple gets to be way more memory-efficient than a dictionary because as far as memory layout goes it s just a tuple,however in some cases where i m writing tests i do keyword packing unpacking and in these cases dictionary is much more maintainable as i don t need to change to a 1 b 2 it also helps in some circumstances where i think i might want to turn it into a namedtuple or class instance at a later time,namedtuple is faster and significantly more memory efficient than a dictionary,alternatively you can use pd.namedagg essentially a namedtuple which makes things more explicit;it is even simpler for series just pass the aggfunc to a keyword argument.t lastly if your column names aren t valid python identifiers use a dictionary with unpacking pandas 0.25 in more recent versions of pandas leading upto 0.24 if using a dictionary for specifying column names for the aggregation output you will get a futurewarning using a dictionary for renaming columns is deprecated in v0.20,it comes in at 10x slower than a namedtuple fastest behind pure dictionary and 7x slower than standard intenum slowest behind numpy record,as namedtuple lighter than dictionary,"
"dictionary","namedtuple","better choice than  in data better lower,easier overall,better in data better lower,lighter overall,slower than  in faster efficient access,","55754244,40066792,30406083,18384001,49420287,","for example results in this list depending on how you are working with this data a namedtuple might be a better choice than dictionary,creating an instance of the namedtuple is easier than creating a dictionary,depending on your use case a dictionary might fit better than a namedtuple,2 you probably want a namedtuple - i m pretty sure they re lighter than dictionary and you can access properties using dot notation for which i have an aesthetic preference anyway,i think i know why namedtuple s access is slower than dictionary access,"
"distance","polygons"," or concave hull overall,greater than  overall,more x overall, offset however overall, is a little more in gjk minimal if, is less in points p1 p2,greater overall,less intuitive overall, is not in gjk minimal if,less in magnitude intersects radius, s a collision in gjk minimal if,","56303590,51935796,25954497,19611406,36024287,51973023,1120009,21541246,25714385,18668955,33377069,","use intersection area minus non-common area as a metric of similarity shapely can be used for that non-common area is union - intersection or simply symmetric difference final metric intersection.area - symmetric_difference.area intersection area this approach might be better than processing distance in some situations for example you want to prefer fewer points covering whole area over huge amount of very close points that cover only half of that non-common area only half of the area s more obvious way to compare candidates with different number of points but only half of the area has only half of the area s disadvantages too just draw some examples on paper and experiment to find them other ideas instead of using polygons or concave hull you can build a linear ring from your points and then use contour.buffer some_distance,i save the distance and if it s greater than distance i saved before i overwrite it and remember polygons,plenty of others don t use a delimiter at all and require you to start a new segment polygons if you re more than x distance away from the last point. furthermore these things often wind up being multi-gb ascii files so reading the entire thing into memory can be impractical,usually if you want to bias the depth against angle think of it like anisotropic bias for shadow maps you would use glpolygonoffset ... when you create the shadow map and in your shader you would use a constant bias or a bias that varies slightly with distance but not angle;the problem with polygons offset however is that it will disable hierarchical z-buffering when enabled so shadow map construction fill-rate will theoretically dip,the circle is trivial just check if the distance from your point to the center is smaller than the radius;the polygons is a little more complicated point in polygons,for example in your polygons there are 3 points p1 p2 p3 we calculate the distance between point p2 and the line |p1p3| if this distance is less than the given epsilon value p2 point is removed from polygons,calculate the actual minimum distance and continue through the sorted list until the maximum distance between the polygons is greater than the minimum distance found so far,the one drawback is that distance between vertices might be slightly less intuitive than polygons area but the two are proportional,offset the polygons inwards by distance circle s radius;if the resulting polygons is still a valid polygons is not self-intersecting and maintain the same vertices traversing orientation check if the circle s center is inside the offset polygons,if that distance is less than the radius of the circle then you know that the polygons intersects the circle,o for a polygons and a circle you can use gjk to find the minimal distance between that polygons and the center point of the circle then compare that distance with the radius of the circle;if the distance is less than or equal to the radius then the distance s a collision,"
"distance","polygons"," is smaller in points p1 p2, is less in magnitude intersects radius,","55357403,12513799,","if you want to do the same for lines and polygons and use the nearest distance shapely is your friend;now calculate the euclidean distance as the square sum of the differences over axis 1 the coordinates and retrieve the points where the distance is smaller than max_distance to compare the numpy solutions to the other answers in terms of speed i timed the answers for the same set of 1e6 random points the code above takes 49 ms the optimized solution by peter collingridge 44ms list solution by vurmax using list comprehension see below 2.88s 60x slower the list solution with peter collingridge s optimization 2.48s toy shapely solution by christian sloper 15.2s 300x slower,if the magnitude of the distance between the center of the circle and the center of the polygons is less than the radius of the circle the polygons intersects,"
"httphandler","httpmodule","still better then overall,more overall,","10348554,4628975,","i m not sure httpmodule is still better then httphandler,you can have only one httphandler but can plug in more than one httpmodule to examine and handle the requests,"
"objective-c","swift","more in familiar class easier, cannot in dictionary nsdictionary objective-c, it gets better in possible application better, provides alloc overall, and just overall, but have no counterpart overall,same than  in faster slower things, not overall,significantly more overall, when declaring the protocol overall,example more overall,","38606180,43648099,38347170,32705026,57389385,10379026,29445974,32540697,34358631,38274873,31149697,","im new to swift but i like it more than objective-c as it looks a bit like java does to me from syntax wise compared to objective-c,objective-c cannot see a swift enum at all;in swift an enum is an object type,in swift it gets better with nicer block syntax better type inference and auto-closures;objective-c is not known for it s pretty or terse syntax,as mentioned by others the whole point is that objective-c provides base class for all classes while swift does not;nsobject in objective-c provides alloc and init methods hence one can expect that a descendant from nsobject will either implement or inherit those methods hence you can have the shortcut-category you mentioned in your question,you can use only enums witch can be represented in objective-c enums without nested enums enums without parameters enums raw types should be integer therefore you can only create another enum witch can be represented in objective-c and just add method to convert to it and create somewhere swift method to another side,swift is not much friendly with macros;complex macros are used in c and objective-c but have no counterpart,still technically yes the library we re appears that an all swift solution is roughly 2x slower than a swift + cocoa solution which should be roughly the same than objective-c + cocoa when compiled with equivalent options iirc,create a swift file in your objective-c based project;no need to create bridging headers since they re used to see objective-c code from swift not swift code from objective-c,while i think the move to swift is prudent but i think it overstates the case to argue that swift is significantly more secure that objective-c,swift doesn t allow protocols to have optional requirements-- if the protocol declares something it s required;objective-c has long had the idea of optional requirements and swift recognizes this when you use objective-c when declaring the protocol,as you can see the swift example is more complex and error prone than your objective-c code,"
"objective-c","swift","interchangeable with  in objects existence several,more overall,faster than  in faster slower things, is a little more in dictionary nsdictionary objective-c, to give an answer in familiar class easier,older in applications professional work,long-winded than they  in familiar class easier, not overall,faster pure in faster slower things, which is better in better hint tool,easier in familiar class easier,","49021106,27213575,48051060,38399780,54414332,39811417,26002033,31102214,28699167,26591442,38170278,","so while cf types are not classes objects they act like objects and several are actually interchangeable with objective-c and swift objects,personally i prefer objective-c because you can use c very easily as anything that is legal in c is also legal in objective-c added to which swift is a more procedural in style where objective-c is quite clearly object orientated,i m confused because many people says that swift is faster than objective-c in general,in swift it will crash when you try to use an object before it is initialized;objective-c is a little more forgiving and will just do nothing if you tell it to use a nil object at least in the instances you are using,if you re like me and needed the searchcontroller to remain active while presenting modally another controller then do the following to get the same effect of presenting modally without doing so directly quick note not familiar enough with objective-c to give an answer in that but here s an answer in swift 4,as objective-c is the older programming language for ios or macos applications swift allows you to use those classes structs in your code,in objective-c something similar can be done with nsarray s various predicate-based and enumeration methods but they ll be a little more long-winded than they swift counterpart,the specific swift file showing the code required to interact with an nscollectionview is imagelistcontroller.swift;if you want to learn more about the new nscollectionview there s a whole wwdc session and the video is available from apple but the presenter uses objective-c not swift,it is possible for swift to be faster than pure objective-c in things that you would traditionally use c or c++ for anyway,if you want to do your app in swift which is better to understand if you have already programmed in java in my opinion use swift;if you want to do your app in objective-c well.,as you can see defining a singleton class in swift is much easier than in objective-c,"
"objective-c","swift"," is more in possible application better,more in faster slower things,less safe in possible application better,slower in faster slower things, and wrap it again overall,more in faster slower things,larger than  overall,more understandable overall,available with  in version available selection,more than  in applications professional work,safer overall,","34151048,24078665,41691607,24765759,26878163,27570223,57259652,29532756,49856370,32190188,35546224,","2 swift size 17 6mb;there is still a big difference if you ask me so i think for small projects objective-c is more suitable,and also apple does might not confidently announce that swift is more faster than objective-c in all the cases,swift in that case would not be safer but less safe than objective-c,i don t think that as of today you can run these tests and determine with any certainty whether swift 1.0 is faster or slower than objective-c,because it requires passing of c callback function and swift does not support it currently xcode 6.1.1;then i had to fallback to objective-c and wrap it again,swift s compiler is also doing a lot more than objective-c s compiler considering swift is more strongly typed and does not required specifying imports among other things,swift does not make swift code accessible to objective-c by default to avoid making its code larger than swift needs to be,those attributes let you create objective-c code which is more understandable by swift and complier warn you when you break the rule for example,what i found is this library have swift version however range selection version is only available with objective-c,writing swift 2.0 in my professional work my personal opinion is that beta s indeed ready for enterprise applications -- maybe even more than objective-c ever were,apple seems to claim that the optional type in swift is safer than nil in objective-c but i don t understand why this is so,"
"objective-c","swift","print more in readable print, classes provide information in type-constrained extensions objecttype,faster in faster slower things,application much bigger in possible application better,smaller overall, is generally overall, dictionary is more strongly in dictionary nsdictionary objective-c,much more in familiar class easier,more sensible in familiar class easier,better than  in possible application better,familiar with  in familiar class easier,","42978589,49523171,24078665,28741895,24150239,25827994,28191176,37682771,45358385,55584316,57683984,","objective-c print is more readable compared to swift,you need the latter for tasks like defining type-constrained extensions but you can t because swift doesn t know anything about the objecttype of any particular phfetchresult instance;the swift-evolution proposal that brought widespread objective-c generics import to swift 3 included a provision for letting objective-c classes provide information about their generic type parameters at runtime.,my view is that if in some cases objective-c is faster than swift it doesn t mean that all over performance of swift is slower,executable size of swift application is much bigger than size of objective-c application,the swift runtime is smaller than the objective-c runtime,no a class must be defined in swift or in objective-c not mixed language inside single class file sorry;as you may have already noticed a class in swift is generally made by a single file with .swift extension while objective-c class is defined using two files .h and .m,this is because the swift dictionary is more strongly typed than the objective-c nsdictionary,objective-c is much more free and easy with types and compilation checks where swift isn t,note that swift s arrays are much more sensible than objective-c s,if it is possible swift would be better than objective-c,i am an ios developer and am therefore familiar with swift and objective-c so i was wondering if there was a way to do the same thing using them,"
"objective-c","swift"," by making use overall,more familiar in familiar class easier,less familiar in familiar class easier, doesn overall, is a better overall,much nicer in possible application better, cannot in familiar class easier,faster in faster slower things,more comfortable in familiar class easier, but not overall,better in familiar class easier,","26486329,30496890,32918935,50364753,35735869,34829520,37763767,38899257,38241771,38147440,29049334,","swift does not use a preprocessor so pragma flags and macros are not an option;it is still possible though to provide similar functionality to the nlog and fixme macros used with objective-c by making use of the benign warning treating a forced downcast as optional will never produce nil,this article seems to suggest that this is possible by swizzling a fake location into apple s cllocationmanager class to be used by other apps but i am unfamiliar with the objective-c code more familiar with swift,i m learning swift and much less familiar with objective-c but for the life of me i can t figure out how they are trying to achieve the goal stated as build list of encodings sorted and including only those with human readable names.,first swift does not use the platform s calling convention;on macos c c++ and objective-c all use the x86_64 system v abi but swift doesn t,i suggest you learn a little bit of the objective-c messaging syntax since a lot of ios code is still in objective-c but don t worry a huge amount about it;swift is a better language,i find that objective-c is much nicer for working with core audio than swift,unlike objective-c which is a proper superset of c swift has been built as an entirely new language;swift cannot compile c code because the syntax is not compatible,the perf hit will be ridiculous i know apple say swift execute faster than objective-c but this is low level so it will be harmless,however i read that swift was the successor of objective-c and i would feel more comfortable with swift syntax,you can bridge variadic functions in c to swift but not the other direction;see using swift with cocoa and objective-c interacting with c apis variadic functions for more on that,swift also prevents us i believe from overriding a parent class s property but also still lets us observe changes to that property--this is much better than objective-c s approach,"
"objective-c","swift"," is faster in faster slower things,more overall, which is much more in dynamic better ways,-documentary more overall,more strongly in possible application better,less type-safe overall, is easier in familiar class easier, designers hadn t overall,easier in possible application better, has a handful overall,less dynamic overall,","28162623,30762258,51753377,45442390,33798907,30885637,47478289,27835223,28699167,30425603,24174697,","bottom line i would personally hesitate to draw any simple conclusions of swift is faster than objective-c or vice versa;i suspect that there are some algorithms applications where swift is faster and others where it is not,swift won t allow you to do absolutely everything that objective-c does but it will allow you to do almost everything and the code will be probably more robust considering that swift is more modern language with stronger typing than objective-c,also i m looking forward if there any better ways to do orm in swift currently the code does working with dynamic object creation but when comes with orm relation it really drove me nuts here just looking for ways to manage it in a easier and more manageable ways where currently it looks like there are not much functionalities to do meta programming a lot of other orm libs still using objective-c which is much more easier but boilerplate on dynamic instance creation or class inspection,objective-c -documentary provides more content than the documentary for swift,specifically it makes it possible to write objective-c code that plays nicely with swift which is more strongly typed than objective-c,so it should have always been gkagent under objective-c in order to be strictly correct however objective-c is less type-safe than swift so you could get away with it with the possibility that you d get an unrecognized selector exception if a gkagent object was ever passed to the delegate method and you assumed it was a gkagent2d,then the buffers won t need to be treated as arrays of pointers and passing arrays of say floats between objective-c ++ and swift is easier,the swift designers went through a lot of trouble to make sure that it s more than just objective-c without the constraints of c in fact i almost wish the swift designers hadn t said that since it s so often misquoted,swift is neither easier to read nor understand than objective-c,that s because objective-c is far less strict and is willing to do some implicit casting on primitives;with that said swift has a handful of types that are more equivalent to c c++ and objective-c s int and the other sizes of integers,swift seems to be less dynamic than objective-c in these stuffs,"
"objective-c","swift"," is much safer in possible application better, there is no operator overall,good with  overall,more effective overall,easier in familiar class easier,no longer visible overall, does not in familiar class easier,same as  overall,more in use,using  to do in reflection direction metaprogramming, it is more overall,","49117871,31039303,57437188,28556371,4850967,46861113,24457397,24925505,33544254,26256643,39085516,","however swift is much safer in that regard;note that in objective-c still accesses self,equatable is a swift protocol not available in objective-c which requires that there is a operator defined for the type;in objective-c there is no operator overloading,i am using swift 5 and not so good with objective-c so swift code suggestions would be fantastic,after reading some related articles i don t think swift is more effective than objective-c,download xcode free and learn objective-c or swift swift is easier to learn than objective-c,however when i switch to swift 4.0 the methods declared in swift is no longer visible in objective-c,note that swift does have a full implementation of generics;objective-c does not have generics so nsarray and nsmutablearray are not strongly typed in the way you desire,as to where you do this it s the same as objective-c;swift makes it easier as it does not require you to import header files so you can either put it in the file of the client the class that uses it or in it s own file,i use objective-c more than swift,swift currently doesn t do much in the direction of reflection and metaprogramming;you ll have to resolve to using objective-c to do this,one of the main differences between swift and objective-c is the type system and behaviour with nil;in objective-c it is more or less safe to use nil it may lead to unexpected behaviour but does not crash,"
"objective-c","swift"," bridging header overall, is not directly overall,application harder in possible application better,efficient as the  in version available selection,less in better hint tool,available with  overall,clearer in versions ports considerable,better in possible application better, api makes it easier overall,more readable in readable print,stricter in stricter type compile-time,","53393549,24042898,28549095,2917997,46952620,52482691,41127787,24049884,38056234,36220153,34910248,","on a personal note i have to say calling core audio api from swift is oftentimes more pain than gain;because of that it might be faster although a bit unsafer wrapping those portions of code into objective-c or plain c and exposing them via the swift bridging header if your project allows the swift bridging header,you can work around the issue by wrapping your c++ code with objective-c and using the objective c wrapper in swift;swift is not directly compatible with c++,injecting code in a swift application is harder than it was for an objective-c application but it s still possible, swift 2.0 version;the objective-c version is a little more efficient as the objective-c version does mutable operations on the string,it seems to me that swift would be way simpler to learn then objective-c for beginners i know because i learned it in like 2 months and swift is less complicated and better laid out then objective-c,you can check it in this answer in my scenario i am checking the error code if it is -999 then i just return from a function like following note unlike nserror of objective-c code property is not available with swift error,swift is clearer on versions releases and i keep asking myself if i am missing something related to objective-c since i can t find this information,so i ve started learning swift because it looks way better than objective-c,adding lightweight generics to your objective-c api makes it easier to interface with swift because your api gets translated more precisely,with swift the code is much more readable than with objective-c,swift is stricter about runtime type correctness than objective-c so duck typing alone is not enough,"
"objective-c","swift"," is not in dictionary nsdictionary objective-c,more secure overall,better in dynamic better ways,more in familiar class easier,cleaner in familiar class easier, doesn in type-constrained extensions objecttype, block calling convention in familiar class easier,slower in faster slower things,savvy with  overall,faster than  overall,better in possible application better,","48042552,34342193,43249602,24050500,33330810,49523171,46226995,37891632,54983281,28162623,27468779,","first of all a dictionary in swift is struct an nsdictionary is class;objective-c is not type-safe so it doesn t show an error,one of the reasons we want to move is because swift is more secure than objective-c,i feel swift is better than objective-c in many aspects but my major question is - will swift dynamic libraries support older version of swift,we ve seen that swift uses a more static method dispatch than objective-c which unless a class dervices from foundation nsobject prevents the style of swizzling based on remapping method implementations at runtime,usually i find swift s method naming to be cleaner than objective-c s but init methods can be an exception,in short objective-c generics are type-erasing swift generics are type-preserving;you need the latter for tasks like defining type-constrained extensions but you can t because swift doesn t know anything about the objecttype of any particular phfetchresult instance,you can use a similar approach as in swift blocks not working annotate the block with convention block;to use the objective-c block calling convention and explicitly cast,wouldn t swift be slower than objective-c in this case since it is layered on top of it,i m not too savvy with objective-c but would like to use it for this cordova plugin instead of requiring another plugin to bridge swift files,there are plenty of patterns where a naive translation of objective-c code will result in slower swift implementations;in particular far more dramatic than your example where the swift code was a little slower i ve been startled by simple situations in which it s quite easy to write a routine in swift that looks logically very similar or even more elegant but is really much much slower or at least prior to the app developer refactoring the swift implementation a bit;bottom line i would personally hesitate to draw any simple conclusions of swift is faster than objective-c or vice versa,better if you have a code swift is better but is not problem objective-c,"
"objective-c","swift"," as well overall,dramatically slower then in faster slower things, nsstringfromclass was fine; in familiar class easier, is much more in class equivalent cellclass,lot smarter in familiar class easier, doesn overall, becomes more in methods big casts,newer in faster slower things,easier in familiar class easier,more in possible application better, syntax yet in familiar class easier,","55938163,26990394,24537049,45875067,33758594,33979203,29063549,42119927,30210640,32043789,50125387,","i m not too experienced in swift i am objective-c but from time to time i need too mess with swift as well,the swift one is dramatically slower then objective-c implementation,see also get a user-readable version of the class name in swift in objective-c nsstringfromclass was fine;swift does not have the introspection capabilities yet as objective-c does,his works great in objective-c because the catchall type id has very lax type checking;but the equivalent any class in swift is much more stringent and does not lend the equivalent any class in swift to the same technique if at all,swift is lot smarter than objective-c about singleton class,that s because swift knows what a protocol extension is;but objective-c doesn t,a big reason you need casts right now in swift is because some objective-c methods return id which was no problem in objective-c but causes trouble in swift;i expect this to diminish as swift becomes more popular and frameworks are adapted,the swift compiler is doing a lot more and is quite a bit newer than the objective-c compiler so it unlikely it will be as stable fast for quite some time,on the other hand swift is easier to learn especially if you have objective-c background so it can be adopted during the time slowly and paralelly with using objective-c,if you get a difference between swift and objective-c it s more because swift is probably better on optimisation,rather than start from scratch with a whole new implementation it seems there s a way to extend kinwebbrowser but i m not totally familiar with swift syntax yet or objective-c and it seems that kinwebbrowser is written in objective-c,"
"objective-c","swift"," developers feel better;if i overall,use  constructs to do in better hint tool,more overall,available with  in version available selection,stricter overall,more in use, doesn in class equivalent cellclass,less in reflection direction metaprogramming,interoperable with  overall, is better in better hint tool, objects as overall,","38194589,37903333,29691943,47621197,27680835,33706305,46353161,31885528,48008009,25751738,24021418,","therefore the usage of nstimer is not more swifty but the need for late binding is more obfuscated to make swift developers feel better;if i wanted to use objective-c i d not be writing swift.,swift doesn t support objective-c s dynamicism;for better or for worse this means that in order to swap out the usage of this selector with a block you d probably need to perform some black magic to make it work and you d have to use objective-c constructs to do that,i also use objective-c more than swift but here s what my guess is as to what the objective-c code would come to,while direct equivalent is not available with swift this can be achieved relatively elegantly with objective-c and categories,furthermore swift is stricter than objective-c when it comes to initialisers,also note that i use objective-c more than swift,the cellclass parameter in objective-c is simply class with the swift equivalent being anyclass;unlike in swift objective-c doesn t support declaring that the value of class anyclass be any specific type,but swift is less dynamically typed than objective-c and has less support for reflection,swift on linux is not interoperable with objective-c it is interoperable with c,stick with objective-c for now;when you re comfortable with the ios sdk and swift is better documented you can consider learning swift,swift will incur this penalty in fewer situations than objective-c will for instance method calls to swift-only protocol methods do not hit objc_msgsend but if the protocol is declared in objective-c or if the swift protocol is decorated with objective-c such that the swift protocol can be adopted by objective-c objects as well then method calls to methods in that protocol adopted by swift objects appear to be dispatched via objc_msgsend,"
"objective-c","swift","familiar with  overall,more strictly overall, rendition isn overall,better deal in better hint tool,faster in faster slower things,compatible with existing  in stricter type compile-time, is dramatically faster in faster slower things,slower than the  in versions ports considerable, not in familiar class easier,familiar with  in familiar class easier, is more overall,","51564654,34158886,28783150,37177621,34341377,51233033,35641763,49742562,25999518,55467987,30388289,","i m not familiar with objective-c following is my swift definition http status enum in swift 4 based on httpstatusenum.strings,swift is more strictly typed than objective-c,perhaps you could consider a more literal swift conversion using private stored property which is handled by a computed property;note this is not thread safe but your objective-c rendition isn t either but it accomplishes what you asked for,or is it merely a hint to whatever tool converts between swift and objective-c to better deal with swift optionals,most importantly for your image processing app the compiler will optimize swift code to run faster than objective-c,you may want to look into swift which is much stricter about compile-time type safety than objective-c and is largely compatible with existing objective-c code on apple platforms,btw objective-c nsarray performance stinks;if you re going to use the native container objects in both languages swift is dramatically faster,there are many versions out there also swift ports but those are considerable slower than the objective-c versions,the idiom is for objective-c not swift;in swift init blocks aren t normal functions and don t return anything,since i m not very familiar with swift here s some objective-c sample code load html string get uiviewprintformatter and print,actually there is no difference at runtime between these two languages at delegate but swift is more easy to learn,"
"objective-c","swift","straightforward than  overall, project cannot overall,easier than in  overall, and not overall, is much more in faster slower things,larger overall, is just in familiar class easier, is much easier overall,faster in faster slower things,easier in familiar class easier, there isn overall,","53361976,50734962,53639728,55433401,52532028,34080597,36697425,48401319,24019229,45696488,41579960,","working with raw bytes in objective-c is generally a little more straightforward than swift,create swift framework geolens.swift when i am trying to access in objective-c project cannot call 2 methods createuser startsessionforuser getting error like this method cannot be marked objective-c because the type of the parameter cannot be represented in objective-c,in objective-c looking for class and #import file.h might have been easier than in swift where the imports are implicit,if i m correct the next lines read like put the outcome of in a new array named nameofarray if you know swift and not objective-c you may be thinking that arrays are value types so an assignment from one array into a var creates a new array;however in objective-c arrays are reference types and assignment of an value doesn t create a new array it just copies a reference to an existing array,i ve come to swift from objective-c and there s a lot of things that objective-c can do but swift is much more complicated,you will notice that swift bundles are always about 4-5 mb larger than their objective-c counterparts and this is precisely why,first note when you mentioned -ofast that s for objective-c not swift;the flag for swift is just -o,minimum deployment target for xcode 9 is 8.0 not 7.0 with coding there is no problem but xcode 9 storyboard add safearea for running layout properly in iphone x also i don t think there is users now running ios 7 regarding swift or objective-c you can accomplish anything with any of users now running ios 7 regarding swift or objective-c but swift is much easier all tutorial now written in swift for new features every wwdc,as craig revealed within the announcement of swift it is said to be faster than objective-c by far,early this month i started learning swift as i found it fun and easier than objective-c,#available and friends in swift are a simplification of and improvement on the more complex ways of dealing with new features and backward compatibility in objective-c;if you re using objective-c there isn t a direct equivalent to #available so you have to fall back to those old ways,"
"objective-c","swift","compatible with  overall,code slower in faster slower things,familiar with  in familiar class easier,less versión than  in familiar class easier, doesn in methods big casts, this is no longer in possible application better,greater overall, was not so overall,more comfortable in possible application better, as well overall,worse with  in faster slower things,","54893490,26991969,35700939,52326949,29699341,30348580,46952620,38108115,27609828,24579873,54996714,","swift generics do not get included with this as they are not compatible with objective-c,there are multiple reasons why the swift code is slower than the objective-c code,in case the method s helpful since you seem perhaps more familiar with objective-c than swift here s what your swift method would look like if we translated the method back into objective-c,as ekscrypto in swift 4 or later all functions need objective-c if you use less versión than swift 4 is enoguh objective-c in class,however note that optional methods do not exist in pure swift they are there only because objective-c have them so there is no big reason for swift to have special grammar for optional methods;also note that even objective-c doesn t have a special grammar for checking of existence of optional methods,note that with swift this is no longer possible and will result in a compiler error instead;objective-c is not as type-safe as other languages and does not care whether the delegate explicitly declares conformance using,i know that swift is buggy as heck i run into bugs everyday but ill bet objective-c was even worse when it started out benefits of hindsight so in a couple of years swift will be equal or greater than objective-c keeping its basic simplicity as well,can someone explain what someone explain what objective-c can support that swift fundamentally cannot when developing apps;what i used to take advantage of in objective-c was not so much dynamic typing as dynamic messaging,i have a objective-c application working fine and smooth to be more comfortable with swift i decided to write the unit tests for this app in swift,regarding your reply remember swift isn t fully baked yet and there may be many more apis available to us in future releases such as a unique set api reflection etc;this should not deter you from learning swift as well as objective-c so don t give up,i m not all that great with objective-c and worse with swift but i ve been using an updated equivalent for a while in one of my rubymotion projects,"
"objective-c","swift","more in objects existence several,","30021879,","no objects come magically into existence in swift any more than in objective-c,"
"piston","tastypie","better luck in similar luck query,more tightly overall,better in similar luck query,nicer overall,","2397232,10464883,15662815,12758060,","- similar to piston but i ve had better luck with tastypie,tastypie is more tightly coupled to the orm than piston but there are methods that you can define in a tastypie resource to specify how to handle create read update delete,edit i found out that piston was proposing query throttling but everywhere people say that tastypie is better than piston,i recommend to take a look on tastypie which turned out to work even nicer than piston it helps you also with things like versioning your api and mamange multiple endpoints nicly,"
"config","ini"," file not overall,less overall, upload_max_filesize is lower overall, it s probably better overall,simpler overall,slightly more overall,more overall,faster overall, store is more overall,","43680797,677459,48206983,21449065,17799411,8377835,31487131,2015715,4006152,","the way you are passing app.py tells uwsgi that it s a config file not a python callable;i prefer to use a ini file to configure uwsgi since it s cleaner than adding a bunch of command line options to the dockerfile,ini is really a much simpler format than xml if you have less than fifty config options and they are not nested,because your php ini config upload_max_filesize is lower than size of uploading image,if you intend manual config it s probably better to choose something else for example ini,it is similar to incron however config uses a simpler to read ini file instead of a plain text file,because spring s xml config is slightly more powerful than shiro s ini spring users are encouraged to use full spring config instead of shiro ini,i like that ini is more for config purposes but with todays interest in json it seems more logical for the config be designed with json in mind,since json can store arrays i can create quite complex config with it and it parses faster than ini files,the disparity between the a database config file yet having the remainder of the runtime options in the database seems silly;imo it s often done due to lazyness because setting up securely and modifying a file ini store is more involving than just another sql table,"
"passwd","sudo"," -- not overall, coming from standard input instead overall, there is better solution overall, to read the target overall, is not overall, question not overall,user postgres with  in following commands flask_admin,slighly more overall, i really overall, -u postgres createuser viter overall,usable with no  overall,","31889871,9600058,45072668,18148277,8368998,14264803,50196167,15868282,53520228,54894046,8398478,","some commands require sudo and so we set that up so that it does not require a passwd to be entered;alternatively you could run the jenkins slave jar after already having sudo -- not recommended,the best and much more secure way is to sudo visudo and attach nopasswd to get sudo not require a passwd;the way in the middle is using the sudo -s parameter that expect the passwd coming from standard input instead that from the terminal device,second from maintenance view once you change your passwd scripts suddenly stop working and you have to update them all;fortunately as you are already using sudo there is better solution,first most systems have configured sudo in a way to take the current user s passwd not the target user s one;to configure sudo to read the target user s passwd instead you must set,therefore multiple sudo calls are also not a problem;the passwd is not read from the standard input but from the current terminal,or you can check the man page for sudo and sudoers -- it s possible to configure accounts not to require a passwd and to limit them to particular commands when they do;this is a sudo question not a java one,nano etc postgresql 9.x main pg_hba.conf change peer in this line local all postgres peer to local all postgres trust restart the server sudo service postgresql restart login into psql and set your passwd psql -u postgres alter user postgres with passwd your-pass,automating sudo is slighly more complex if you need to pass the passwd but still can be done,for me logging in was as simple as sudo mysql -u root don t forget the sudo i really hope there should have been a message stating that ubuntu no longer used passwd when attempting to run mysql,sudo -u postgres createuser viter -s this also makes viter a superuser;then you need to set the passwd for viter to match what is used in database.yml by using this command while in psql as the postgres user yourpassword viter next create each db with viter as the owner while still in the psql terminal,be sure to let sudo only run specific commands without passwd not all;you can configure sudo to be usable with no passwd as described here,"
"passwd","sudo"," -u postgres createuser in following commands flask_admin,better overall, when using no so overall,user flask_admin with  in following commands flask_admin, is not in following commands flask_admin, su postgres createuser overall,simple as  overall, then digit systemctl overall, -u postgres createuser john overall, passing completely overall, -u postgres createuser in following commands flask_admin,","9723024,982185,44496443,53342169,57481041,57401159,48583923,50211566,31040218,53367465,53621261,","sudo -u postgres createuser pgs_root;set user passwd for the postgresql user,sudo is better in most respects but still requires a passwd which asroot does not,by the way note that sudo supports many authentication methods passwd is only one of them;in particular sudo may not ask for a passwd when using nopasswd so be sure as a minimum to check for the passwd prompt to be present before writing a passwd to a process,thanks to rfkortekaas help after changing the content in my etc postgresql 9.x main pg_hba.conf file on this line # local is for unix domain socket connections only local all all peer to local all all md5 after which i ran the following commands sudo service postgresql restart sudo -u postgres psql alter user flask_admin with passwd example dev 18,get into mysql instance sudo mysql -u root note mysql -u root will throw error error 1698 28000 access denied for user root localhost so use sudo to run this command;now to change the passwd of the root user i tried the following commands this has thrown an error error 1348 hy000 column passwd is not updatable this has thrown an error error 1348 hy000 column authentication_string is not updatable but the following command worked alter user root localhost identified by mypassword,setup postgresql phppgadmin first login as the postgres user and lets create a new postgres role user sudo su postgres createuser -p --interactive --interactive - adds some initial permissions -p - means assign a passwd the default installation of phppgadmin will automatically connect to postgresql server,as of february 2018 installing brew on ubuntu mine is 17.10 machine is as simple as sudo apt install linuxbrew-wrapper then on first brew execution just type brew --help you will be asked for two installation options for recommended option type your passwd if your current user is in sudo group or if you prefer installing all the dependencies in your own home folder hit ctrl + d,it was simply just gain super user privilege and then enable and start rsync process sudo su enter your passwd then digit systemctl enable rsync systemctl start rsync if you don t have a systemctl based terminal just use service instead,if you need to create an new user and passwd;sudo -u postgres createuser john -s,example 1 - simple remote command in this example we send a simple whoami command we re telling sudo not to issue a prompt and to take its input from stdin;this makes the sudo passwd passing completely silent so the only response you get back is the output from whoami,and of course add the username and passwd that you already created on postgres if you don t have an user you can create one like this sudo -u postgres createuser -s dev sudo -u postgres psql passwd dev #here you can type the passwd you want for this user q in this example we are creating a user named dev so you add dev to the username part and change the passwd for the one you typed,"
"passwd","sudo"," find -name vlc.png overall, slightly more overall, gives you more overall, or not overall,","27875674,47952616,17531361,39216914,","it may be simpler just to configure sudo such that a passwd is not needed;i assume your username is user1 and you wish that user1 may run sudo find -name vlc.png without a passwd,removing passwd control is even worse than using a standard passwd slightly more secure you can also make it so that only certain commands can use sudo,i would give sudo another look since sudo gives you more control;also the -s flag on sudo tells sudo to read a passwd from stdin rather than from a terminal so sudo would work in the context you give above,when you invoke a command with sudo you re asking sudo to elevate your privileges beyond what the account normally gets;this is an entirely different system with rules defined in etc sudoers which you should edit using sudo visudo that control which users are allowed to use sudo what commands they should be able to run whether they need to re-enter their passwd or not when using the command and a variety of other configuration options,"
"activerecord","datamapper","way higher overall,somewhat more work overall,idea definitely better in better newer smarter,much smarter in better newer smarter,reimplemented with  overall,faster overall,much simpler then overall, has a better overall,much more overall,more overall,model much easier in complex easier domain,","8169572,38681724,5351338,5351338,54269456,818424,4034591,36909196,2681920,18681836,4338143,","the level of abstraction of datamapper is way higher than the activerecord s,with datamapper this is somewhat more work than with activerecord which provides it s own db tasks,datamapper idea is definitely better than activerecord,datamapper is much smarter then activerecord,i ran into a jruby-specific problem with datamapper and so reimplemented with activerecord,i d recommend datamapper for orm not only it s way faster than activerecord but it s also very modularly built and plugins are actual gems that you can easily install,activerecord is much simpler then a datamapper but also much more limited,in my experience datamapper is nice because you don t need to write migrations;but activerecord has a better query interface,also datamapper is much more ruby syntax friendy and features like lazy loading when doing chainable conditions like activerecord in rails 3 are there from the beginning,i don t know whether this port of cql3 to ruby support activerecord it acts more like a simple datamapper but it worths having a look,from my point of view the datamapper model is much easier to grasp but since activerecord is the defacto standard it feels weird to change the orm just for this little problem,"
"activerecord","datamapper","potentially more in complex easier domain, is the better overall, does not overall, currently doesn overall,newer overall,newer in better newer smarter,general overall,","94070,8852991,3828932,4886354,3828562,3828265,5060861,","datamapper is potentially more complex then activerecord but it s a lot easier to develop your domain model and database asynchronous then with activerecord,if you are a beginner in the orm world i would suggest going with activerecord because it is simple and usually requires less code;on the other hand if you are building a large complex model i think datamapper is the better option,the datamapper is not more modern or newer but just more suited for an orm;the main reason people change is because activerecord does not make for a good orm,datamapper currently doesn t support this feature but it s on the roadmap and is referred to as embedded value;activerecord supports defining embedded values already,datamapper is newer than activerecord,so a question for those who know better is datamapper newer than activerecord,if you mean a data access object dao which is a class or classes to function as a bridge between the database records and your application objects try the datamapper or activerecord patterns;note that the activerecord pattern is a little older and falling out of favor with some so the articles about activerecord in php tend to be older and are a bit out of date,"
"ienumerable","iqueryable","better in better query linq,more useful overall,better in better query linq, has not replaced  everywhere in faster able specific,much more preferable overall, as much in larger filter possible, ours was on domain overall,more than just  overall,more than  overall,implementation more overall,more in larger filter possible,","7791066,33005821,36964658,43419334,9246022,50398305,23335882,3039295,52532812,9773250,13622131,","i have read about ienumerable and iqueryable i have read that iqueryable is better with linq because when you use where clauses it creates the exact query it needs but with ienumerable it retrieves all rows from database and then filters on memory,can you provide any useful example that shows ienumerable could be more useful than iqueryable,what i know is using iqueryable is better than ienumerable if another query is going to be performed,that is why iqueryable is considered faster because there is a lot less data that needs to be processed because you where able to ask a more specific question to the server;the reason iqueryable has not replaced ienumerable everywhere is because the thing you are asking a question has to be able to understand the question you are asking a question,i m not familiar with the infeasability of iqueryable but this blog post seems to indicate that iqueryable is much more preferable to ienumerable because iqueryable allows access to the underlying expression,but if the ienumerable is much larger than the iqueryable just filter the iqueryable as much as possible on the database side and then use asenumerable to pull it into memory and join there,the warning exists because an ienumerable can disguise something expensive such as a database call most likely an iqueryable and as ienumerable doesn t have caching as part of it s contract it will re-enumerate the source fresh;this can lead to performance issues later on we have been stung by this a surprising amount and we don t even use iqueryable ours was on domain model traversal,there is a very good linq video that i enjoy a lot- it hits more than just ienumerable v iqueryable but it really has some fantastic insight,putting the advices together no include but select to fetch only the used data no tolist but as enumerable to fetch data per page instead of all data return ienumerable after all the data is in local memory ienumerable can do more than iqueryable,entity framework s iqueryable implementation is more picky about creating new objects in objects than regular linq to objects ienumerable,i d say iqueryable is more suitable for interaction with a database than ienumerable is,"
"ienumerable","iqueryable","better in better query linq,smaller than  overall,way quicker in larger filter possible,more generic in generic instances, will do as result overall, is better choice overall,more in generic instances, gets only in faster able specific,faster in faster able specific, it gets harder overall, does not overall,","5182499,41962207,17920394,20529312,34637950,32652053,23475916,49018633,26992283,55017376,55679355,","the ienumerable side of linq which works on in-memory objects that are already in the heap will almost certainly perform better than the iqueryable side which exists to be translated into a native query language like sql,for your view to expect an ienumerable you need to change your view declaration to be also if you really need to perform a find use iqueryable first you really need to perform a find use iqueryable return the result as ienumerable since the payload on ienumerable is smaller than iqueryable,with really basic calls to the database iqueryable is way quicker but when do i need to think about using an ienumerable in its place,ienumerable is more generic than iqueryable though all instances of iqueryable implement ienumerable and only defines,5 the retrieved detached objects should be added to a returned list or you could yield return the results one by one with ienumerable is return type;returning an iqueryable isn t possible in this case but an ienumerable will do as result,n your case if you need only data on the client side then return list or ienumerable;iqueryable is better choice for data-tier server side because data-tier server side defer execution,ienumerable is more generic than iqueryable though all,iqueryable is faster than ienumerable if we are dealing with huge amounts of data from database because iqueryable gets only required data from database where as ienumerable gets all the data regardless of the necessity from the database,i assume applying iqueryable will be much faster than the ienumerable not to forget i was able to use iqueryable on the other entities without any issues,however when there s a few layers of indirection and a function starts returning an iqueryable instead of ienumerable it gets harder,an ienumerable object has everything in it to enumerate over all the elements you can ask for the first element of the sequence and once you ve got an element you can ask for the next element until there are no more elements;an iqueryable seems similar however the iqueryable does not hold everything to enumerate the sequence,"
"ienumerable","iqueryable","slower overall,better overall, not overall, is a better in better query linq,","17604913,30685017,1711204,23359554,","in simple words all operations on ienumerable causes simple iteratation over all elements well it s lazy ienumerable really slower than iqueryable,final result will be of type ienumerable and in terms of performance this would be better than iqueryable since here join is taking place between in memory collections,the where extension methods for ienumerable indeed take a system.func which is how you are trying to pass the predicate here;but you re working with iqueryable not ienumerable,so if you working with only in-memory data collection ienumerable is a good choice but if you want to query data collection which is connected with database iqueryable is a better choice as you want to query data collection which is connected with database iqueryable reduces network traffic and uses the power of sql language,"
"elasticsearch","kibana","new with  overall,higher than   in upgrade process versioning, that is newer in upgrade process versioning,more tolerant overall,less functionality than  overall, is much more overall,general overall, run curl overall, cluster including disk space; overall,","48927586,50323550,50323550,30607004,39650868,43265929,53202549,30283080,27852394,","i m very new with elasticsearch and kibana,however in order to facilitate an upgrade process where elasticsearch is upgrade first you can run a minor versioning of elasticsearch that is higher than kibana kibana 5.0 and elasticsearch 5.1,running different major version releases of kibana and elasticsearch kibana 5.x and elasticsearch 2.x is not supported nor is running a minor version of kibana that is newer than the version of elasticsearch kibana 5.1 and elasticsearch 5.0,odd that elasticsearch is more tolerant than kibana,if you would like a unified front-end for elasticsearch and opentsdb you could consider grafana which has support for both elasticsearch and opentsdb but less functionality than kibana in regard to elasticsearch,it s technically viable particularly if you re emitting json-formatted log messages but elasticsearch is much more tailored to storing and searching log data;there is an ecosystem of tools for storing and searching logs in elastic including kibana and graylog that make this even easier,push everything to elasticsearch i can push everything to elasticsearch and create graphs on kibana or grafana grafana seems more fit for the job,what you are seeing is not a parsing error shardfailures just means that the underlying elasticsearch is not ready yet;when starting kibana elasticsearch make sure your es cluster is ready before diving into kibana run curl -xget localhost 9200 _cluster health and in the response you should see something similar to this,you can certainly query elasticsearch manually if you want but you can get a whole lot more by using marvel to check the overall health and availability of your elasticsearch cluster including disk space;kibana doesn t really use hard disk space itself that s reported by elasticsearch,"
"google-chrome","opera"," produces smoother playback overall, has proper standalone overall,lighter than  overall,higher overall,more powerful in helpful mode powerful,longer in image longer performance,much more green overall,probably more complete overall, doesn overall,faster overall,release more popular overall,","55795461,53756837,56062501,14887100,7449937,44177682,3888711,17245353,40043690,46726727,37058103,","google-chrome opera produces smoother playback than firefox,and while pwa s work on android through firefox and opera only google-chrome has proper standalone support,i have used i m now using opera since it s more lighter than google-chrome,now this works fine in google-chrome and safari and i have had to use assitional css settings for firefox but ie and opera both display my ribbon div about 25px higher than in google-chrome or safari,since you noted thoughts on other browsers would be helpful opera s kiosk mode seems more powerful than google-chrome s,as you can see in the image google-chrome is running a lot longer than opera,especially in opera there is much more green in the painting than in google-chrome,opera probably is more complete to manage canvas than google-chrome but the problem is always to exclude the finger touch if i m using the pen,i d suggest supporting them both because opera doesn t support vapid but works with gcm;for google-chrome you ll need to either set a gcm api key or use vapid,note i recommend using opera - it is much faster than google-chrome as it doesn t have memory leaks and comes with a built-in ad blocker,for comparison 0.68 usage means this outdated google-chrome release is more popular than the current and previous versions of opera the penultimate version of safari on ios and ie 9 or 10,"
"google-chrome","opera"," is even worse overall, not only overall,darker than  overall, shows enter image description here overall,general in support moz o,more than only  overall,longer overall, uses fast history overall, works smoother in image longer performance, it s looks better in helpful mode powerful,compatible with   overall,","1947902,49846327,56484505,49719216,52445821,49397439,2294832,1195934,10871583,57095808,56787900,","i found that on google-chrome safari and opera many special keys don t get passed through to ajaxterm including backspace the arrow keys ctrl+c ctrl+h home end etc;opera is even worse when you press shift the keypress gets translated into p so you can t type capitals,btw in opera it is even worse - opera not only duplicates request but sends it many more times google-chrome is nice with sending it only once,in google-chrome the native checkbox is darker than opera s and edges one,this is my project url hosted this is what google-chrome shows me enter image description here and this is what opera shows enter image description here thanks,to clarify all browsers under ios will be reported as safari webkit all browsers under android but firefox will be reported as google-chrome blink google-chrome opera blisk vivaldi etc,for other browser that i ve tested opera dophin firefox i just stares at my redirect_uri and refused to go back to the apps note i already prompted user to click link for the redirection i would like it to work more than only google-chrome and samsung browser if it can works with maybe firefox custom tab and more it will be great,also nice to know is the fact that the latest version of google-chrome and also opera don t know since when but longer than google-chrome supports userscripts out of the box no need to install any add-ons,opera uses fast history navigation;google-chrome doesn t have a page cache 1 2,i think google-chrome has a performance problem with image processing;mozila firefox and opera works smoother,i ve got problem with my fullcalendar but only on mozilla firefox browser.i want to add 2 events first on date 17.07 to 20.07 another on 18.07 to 22.07. my browser in result show me first event on date 17.07 without end date and second correct result .i don t know why in another browsers google-chrome opera it s looks better,this is also compatible with google-chrome opera edge firefox and safari,"
"google-chrome","opera","general in support moz o,","52475927,","-webkit- - would be support browsers such as safari or google-chrome -moz- - would be supported by firefox -o- - would be supported by opera -ms- - would be supported by internet explorer browsers,"
"addition","multiplying"," which is somewhat more overall,slower simple overall,more in boilerplate operation complex,optimal than an  overall,deeper overall,slower overall,better overall,much faster overall, produces the less overall,more complex in boilerplate operation complex,","5381995,7464279,28311068,49691432,23555455,20209624,33582244,6831617,37674323,23188645,","multiplying a vertex with a matrix takes the equivalent of 4 dot products therefore you need the equivalent of 32 dot products to check all 8 corners;calculating the distance of a point to a plane takes a dot product and an addition which is somewhat more efficient in the worst case and much more efficient in the average case since you can often discard an object after clipping after one or two planes and never more than three,we don t actually multiplying it s slower than simple addition and as you can see we destroy temporary register t0 but don t touch s0 s1,however multiplying is a more complex operation than addition or shifting,i am writing some code right now that requires a lot of doubling halving operations because it is working on a dense binary tree and there is one more operation that i suspect might be more optimal than an addition - a left power of two multiplying shift with an addition,so we must show that a gradeschool multiplying circuit is o log n times deeper than an addition circuit,if multiplying is slower than addition then case 2 is slightly slower than case 1,so the compiler can t make the optimization because it can t tell if you wanted the exact behavior where multiplying is better or the implemented behavior where the scale of sum affects the result of the addition,you can try x1 c1 and then x1 + c1 but i don t think the addition is much faster than multiplying on todays cpus,multiplying .3 by 10 and adding .3 to .3 nine times you will see that;addition produces the less precise result because it involves eight,the boilerplate code would multiplying rapidly when the express get more complex than addition of two terms,"
"stateful","stateless","faster in knowledge session engine, works better overall,lighter in knowledge session engine,cluster often simpler then overall, service is way easier in service harder scale, service is typically harder in service harder scale,better than  overall,better overall,better in knowledge session engine,more appropriate overall,choose between  overall,","5497077,54548439,40551515,10669327,50655948,25236081,31567108,19457354,17227882,1177422,52122606,","for certain kinds of transactions a stateless session may perform slightly faster than a stateful session,an architecture that makes sense in flutter is as follows a top-level stateful widget stateful works better for hot reload a stateless one can cause issues something like myapp but change my to the name of the actual app,service to build drools knowledge and get session i prepared a stateless engine lighter than the stateful one,dealing with a stateless cluster is often simpler then dealing with a stateful cluster,a stateful service can not be scaled as easily;if you want to deploy more units on peek times a stateless service is way easier to handle,a stateful service is typically harder to develop and scale than stateless services,or in short stateless is better than stateful,but keep in mind that in many cases being stateful or not stateless is no problem and not all stateless applications are by definition better than stateful ones,in ejb3 there is no such thing as stateless is better than stateful session beans,do you have any situation where stateful is more appropriate than stateless,this post will also guide which one to choose between stateful component and stateless component,"
"stateful","stateless","more than  overall,simpler overall, deleter cannot overall, approach is worse overall,","55054661,29933428,34668970,32418078,","and that i should use stateless in simple things because props are imutable and also use more than stateful,while a lot of development has been done with stateless connections to solve most problems sometimes it s just simpler with stateful connections,a stateless deleter cannot remember to delete something else;you can add a stateful deleter to unique_ptr that supports aliasing but then you ll have to alias manually,also soap is stateful and http is stateless;an stateful approach is worse for scalability,"
"extends","implements","higher priority in better runnable thread,larger overall,easier in easier value listview,more preferrable in better runnable thread, a is a tad overall,less work than  overall,more sense overall, loadermanager.loadercallbacks choreographer in fragment1 fragment loadermanager.loadercallbacks,better option in better runnable thread,runable better in better runnable thread,more than  overall,","32471220,25098811,18354161,22595284,42084440,54156388,8032509,48117206,2782744,17311842,54136820,","which i think makes sense because extends from a class should take higher priority than implements from an interface,implements gives larger errors because i tried with extends,second in order to add or update the value of listview in general extends arrayadapter is much easier than implements baseadapter because of arrayadapter support add remove insert method by itself,they say that implements runnable is more preferrable than extends thread,if foobar implements a is s not obvious from the names that b extends a declaring foobar in such a way may make foobar implements a is a tad easier to understand but more often than not that just means you need to rethink the relationship between b and a,probably some debate can be had about that decision but i felt like that was a lot less work than extends each abstract enum class with a custom one of my own and implements the abstract functions,extends makes more sense to me here than implements maybe it s a typo in the book,public class fragment1 extends fragment implements loadermanager.loadercallbacks choreographer skipped 31 frames,why is implements runnable a better option than extends from thread class,i also know implements runable is better than extends thread,you may only rarely need to do anything more than extends the abstract classes with nothing more to implements than the common action,"
"extends","implements","mouseadapter easier in easier value listview, pdostatement has more overall,better in better runnable thread,better position overall, thread is the better in better runnable thread,faster overall,more succinctly overall,stronger relationship overall,better overall,runnable better in better runnable thread,better system in better runnable thread,","42584044,25340797,28491250,43306204,20982731,7194541,25498711,22268825,38187566,21962174,3311788,","note extends mouseadapter is easier since you only need to implements the methods you want to handle,this is kind-of the functionality you for instance see but you instantly already see that there it s called at an instance and the fact it requires to extends rather then implements pdostatement has more to do with the c-level implementation functionality of that class,i have read that implements runnable is better than extends thread but i have no idea of how,but if you d like to extends this api to be used by the external services then you ll be in a much better position with implements oauth 2.0 using doorkeeper because you can easily configure for example a authorization code grant for them,you could also so an anonymous class which implements runnable;but whether or not your use an anonymous class is orthogonal to the question of whether implementing runnable or extends thread is the better mechanism to define your thread task,is implements the rawcomparator that much faster than extends writablecomparator,more or less the same thing as the accepted answer can be implements more succinctly by extends the gridviewpager like this,a possible reason is that extends represents a a stronger relationship than implements although both represent is a or is a type of relationships,are there some solutions available to extends asp.net identity better than implements a custom solution as suggested in the links,implements runnable is better because you can extends other classes,i have not found a lot of times when implements an interface would be a better system than extends a superclass,"
"extends","implements"," abstractcollection is much more in fragment1 fragment loadermanager.loadercallbacks,","36830863,","that way you have to implements every method in the collection interface bt;making your class extends abstractcollection extends abstractcollection is much more easy since it does,"
"centos","ubuntu","less desirable overall,better than  overall,works with  overall,general overall,more consistent overall,worse than using  overall,system newer overall,slower overall, 6 using jeroen s overall, based but still overall,","263966,8709321,56768173,3842231,17406652,48180140,21462120,10574176,26697992,50282635,","in this respect ubuntu is less desirable on a server than rhel or centos which stick to more mature versions,i believe that ubuntu is better than centos when it comes to mono as it is much easier to setup and seems to be better supported,it works well with ubuntu 18.04 but it is not works with centos 7,if you don t have a favorite distro i would recommend trying ubuntu centos as you don t tend to be quite friendly to the beginner and have extremely robust community support,centos is more consistent and reliable because it is enterprise while ubuntu is geared towards desktops and personal computers,i m new in ubuntu previously i tried to set this up on centos because i m more familiar with centos but it was worse than using ubuntu because many dependencies are missing out of date,this is because your ubuntu system has newer system libraries than your centos one,is there anything obvious in ubuntu that would otherwise be making sfphpview- render run slower than centos,the init.d script provided for ubuntu does not help me much;yes i have successfully installed opencpu server on a centos 6 using jeroen s instructions and have the same question,the centos one was added as the binaries generated by the ubuntu image cannot run on quite a few linux distros easily due to high glibc version used by ubuntu;we have a pr which rewrites all our dockers as centos based but still need to test it more before merging,"
"whoosh","xapian","slightly more difficult overall, it certainly overall, is maybe not as overall,","10939202,48568914,30452913,","xapian is slightly more difficult to setup but is much faster than whoosh,some good ones i ve used are solr and xapian though if you re already familiar with whoosh it certainly sounds like the best option you proposed,for searching xappy xapian is faster there was no parallel processing used;but you see that the speed difference between xappy and whoosh is maybe not as big as you expected,"
"inputstream","outputstream","much less overall,select it as  overall,","17035499,7335227,","what i did not expect is that outputstream is much less bytes than inputstream although i have yet to make any modification on doc,map your image servlet on an url pattern of and get the image id or filename by request.getpathinfo and finally select it as inputstream from the db and write it to outputstream of the response,"
"virtualbox","vmware","faster in better slower server, does even overall, is easier in linux ubuntu easier,much more stable overall, seems much happier in linux ubuntu easier,slower in better slower server,better in better slower server,more features overall,worse overall,better in better slower server,less powerfull in free powerfull good,","630233,26948540,901826,43954151,11592396,46375001,3791405,55578,45324556,5763931,8107257,","in my experience i ve found that vmware seems to be faster than virtualbox although i don t have any hard data to back it up,i really miss this feature on virtualbox because i find vmware to be bloated and it slows the host system down much more than virtualbox does;as far as i know virtualbox does not support it but vmware does even in their free vmware,at home w ubuntu linux virtualbox is easier to maintain performs as well as i require and is free;vmware player is too feature limited,however i found that vmware is much more stable full screen resolution much much better to handle the iphone connection via usb and i didn t have yet any crash when on virtualbox it s quite often,vmware doesn t seem to have the same issue;running on linux virtualbox seems much happier and linux can be very light weight,virtualbox is slower than vmware,fwiw i have never gotten freebsd to work properly under virtualbox perhaps if you need that you would be better off with vmware which does,vmware has more features but costs 80 virtualbox on the other hand is more basic but is free for most users see virtualbox licensing faq for details,im trying very hard to like virtualbox but so far i find it so much worse than vmware in so many ways,i am not sure if vmware server will be much better than virtualbox,virtualbox free but less powerfull than vmware,"
"virtualbox","vmware","faster in free powerfull good, is way overall,more luck with  overall,faster in better slower server,use  player to resize overall,","630233,3690009,48583148,12349097,20993362,","even though vmware has been faster for me i still use virtualbox because it s good enough and is free and im cheap,and frankly if you re doing that then yes it s a real pita and vmware is way easier;my biggest beef with virtualbox is that you can t take a vm and move a vm plus a vm snapshots and branches to another pc,see this virtualbox ticket you may have more luck with vmware as indicated in the comments,from my experience vmware 5 is faster than virtualbox 4.2 rc3 and has better smp performance,just make sure that when you add the vmdk into vmware don t click upgrade format when prompted keep the current one in case virtualbox doesn t accept it;you can use vmware player to resize a vmdk,"
"gridview","listview","more overall, is better choice;as far in choice as repeater,simple  with  overall, height is greater overall,much finer in control finer display,better fit in control finer display, datagrid looks more overall,more overall,more overall, is not overall,better in choice as repeater,","41508471,8428492,48037510,49290425,31765196,13209588,49380974,1879340,2419913,19066399,16225337,","note that i do not want to bind a gridview directly to the view property from a resourcedictionary where the datatemplates x shared attribute is set to false as this leads to problems in the xaml designer view can t be shared by more than one listview,if paging sorting editing etc is needed then listview is better choice;as far as showing multiple tables goes you can use nested controls - for example repeater list-view nesting a gridview,i have a rather simple listview with gridview style,he reason listview and gridview can scroll in the first place is because they have a scrollviewer built in;when you place a listview inside a parent layout panel a stackpanel or a grid row if the listview height is greater than the viewport a parent layout panel becomes the full height of the listview,i believe gridview allows much finer grained control of display than does listview,here s another article that describes the listview control and why it s probably a better fit than the gridview,what looks more appropriate in you case is to use rowdetailstemplate to define a child datagrid gridview to display the strings collection displaying the strings collection at the same level as the other properties could be a hard task to do and doesn t make much sense . here a sample of how to define a datagrid within another one same thing using gridview listview datagrid looks more appropriate,regarding comparison between gridview and listview my experience is that listview is more lightweight as compared to gridview,the error is because the gridview is being applied to more than one listview,the gridview is visible by default and the listview is not;when we navigate to the page the visualstatemanager is called after the page is rendered and therefore when navigated to the gridview is visible for a short time and calls the loadmoreitemsasync method before it gets hidden,i think the listview is better than gridview in this situation,"
"gridview","listview","better in choice as repeater,better in easier better,easier in easier better, which is more overall, does not overall,","10809,37560070,36362028,50760810,32773730,","listview much better than the gridview dataview in that they let you write out clean html,i think using a gridview is better for your case than a listview,you may also consider using a listview which is easier to customize than the gridview,see the detailed tutorial about how to use one here you can use listview or gridview which is more suitable for you,a gridview is used as a supplement control to a listview to provide style and layout;the gridview does not have its own control related properties such as background and foreground colors font properties size and location,"
"matplotlib","mayavi","easier overall, is not yet overall,way better in visualization easy job,better job than  in visualization easy job,more options overall,better in visualization easy job, which is faster overall,","4739360,31682476,20386398,50337382,38790724,46457858,53565564,","mayavi makes it easier to visualize the data but matplotlib looks more professional,note that unlike matplotlib mayavi is not yet compatible with python3 and might not be in the foreseeable future so you ll need a python2 installation;matplotlib is not quite mature for 3d graphics,anyway if you re willing to do advanced 3d stuff mayavi is way better than matplotlib,for the visualization i am using mayavi since it was easy to set up and does a better job than matplotlib in 3d,mayavi has more options for moving the camera than matplotlib but it doesn t seem to have a way to rotate around the y axis so i m guessing i ll need to perform some complex calculations on the azimuth and elevation to achieve the same result - but i m at a loss as to where to start i m new to working in 3d space and my brain doesn t think that way yet,personally i ve used mayavi in the past and found the performance much better than matplotlib for 3d scatter plotting though the python bindings are somewhat in question moving forward i think,if you want to stick to python then you can use mayavi which is faster compared to matplotlib,"
"firefox","google-chrome"," does a better in better time tools, doesn in better time tools,faster in faster slower browser,fine with  in fine access mozilla,quicker overall,sometimes more lenient in forgiving error lenient,more handy tool in easier tool extensions, not overall, which is worse in faster slower browser, is simply more in forgiving error lenient, conversion is working better overall,","54466352,51849818,35114347,53226092,34985268,12488412,16682516,36543341,23770379,55955979,7432356,","seems like the google-chrome does a better job on my code while the firefox grimaces,it looks like it just works in google-chrome because google-chrome doesn t support allow from;firefox does the right thing here but you can intercept this header like any other with the webrequest api specifically webrequest.onheadersreceived,as you said google-chrome is faster than firefox so the webdriver is trying to interact with the dom before when elements are not yet visible exist,i had the same problem and could trace it to this javascript source line that worked just fine with firefox and google-chrome according to the mozilla docs this call has an optional 3rd argument,firefox seems to animate the element quicker than google-chrome does and so while a duration of 1s is enough for the animation in google-chrome firefox needs it to be 2s to be slower and show the effect,in my experience firefox is sometimes more lenient than google-chrome but i would definitely test in both,first just a heads up you may want to try firebug+firepath on firefox which is a more handy tool than google-chrome s developer tool,in firefox you get good output code from above but in google-chrome you got just next code;it is a bug for sure and it is a module related because for some reason google-chrome not read values which are set in module,though from a design perspective it may be easier to simply add the extra 5px to the width designing for the worst case browser to prevent wrapping in google-chrome which is worse than extra space in firefox ie,firefox seems google-chrome is simply more lenient,the service is still in beta trial and firefox conversion is working better than safari one;i can the service fully disclose what s the service technical solution but this being an so answer i can add a few details we have re-developed a full javascript api stack that mimics most of google-chrome extension apis for safari and firefox,"
"firefox","google-chrome","faster in faster slower browser, doesn overall,higher however in higher smaller bigger,faster in faster slower browser,situation better in better time tools,version better then in better time tools, but seems slower in faster slower browser, not invalving flash javascript in faster slower browser,not slower in faster slower browser,far more subtle in behavior edge visible,strict than  in faster slower browser,","44011008,1667192,16534059,6774018,43033977,15244665,8382400,23860302,2284148,45818759,50885045,","maybe google-chrome is working faster than firefox try adding a wait with expected conditions,for example google-chrome doesn t highlight the options in the drop down;firefox does but then it doesn t change them back if you move the mouse away and they are still pulled down,i just found out that if you re using google-chrome you trigger antialiasing at 49px or higher however with firefox it s set at 257px or higher,it s just a matter of preferences and browser implementation eg firefox works faster with brackets while google-chrome works faster with the dots,ps i ve noticed that in google-chrome situation is better than in firefox but new line still starts beneath tag,install firebug firefox version is better then a google-chrome one,it depends on the browser it definitely true for ie and firefox but seems slower on google-chrome,i just found a partial solution to google-chrome not invalving flash javascript extra dom manipulation with overflow hidden;firefox has fixed this bug,better use google-chrome even thought firefox is not slower than google-chrome for once probably the tracing for the image comparison pays off yay,it s clearly visible in firefox and is far more subtle yet also distinguishable in google-chrome,you may try the firefox browser which is less strict than google-chrome in this regard,"
"firefox","google-chrome","more in forgiving error lenient, requires --allow-file-access-from-files flag in fine access mozilla,higher in internet explorer global, logical is slower in faster slower browser, are rendering the stripe slightly in faster slower browser,better javascript in better time tools,more restrictive in better time tools, is understanding more thing then in thing reality face,less noticeable overall,stricter than  in seconds data stricter,more lax in strict ie configuration,","26973643,24550907,11488114,14871174,54793739,5204325,5020686,30537568,10511066,44613029,18223158,","google-chrome is more forgiving or you could call it less compliant firefox less so,ie doesn t allow file access when running locally and google-chrome requires --allow-file-access-from-files flag to be set when you launch the browser;firefox doesn t have these restrictions so your project would work as you expect there,most noticeably if you look or google-chrome firefox and then internet explorer 9 you ll notice that the terms and conditions are slightly higher than in google-chrome or firefox and thus slightly touching the main content area,on google-chrome ie bitwise is slower;but on firefox logical is slower,the results are clearly visible here you can see that although google-chrome and firefox are rendering the stripe slightly differently google-chrome is antialiasing google-chrome and firefox are rendering the stripe slightly differently google-chrome correctly so google-chrome and firefox are rendering the stripe slightly differently google-chrome looks much smoother than google-chrome and firefox are rendering the stripe slightly differently google-chrome did before,in my experience google-chrome has better javascript performance than firefox,no and there won t be any time soon because google-chrome s add-on api is more restrictive than firefox,often google-chrome is understanding more thing then it should and firefox is bringing you the reality back in your face eheh,support exists in firefox and my test just confirmed thisâ but the handle is a little less noticeable than in google-chrome,google-chrome is stricter than firefox when google-chrome comes to loading data from your local filesystem,it works in firefox because firefox is more lax about this security restriction and google-chrome happens to be more strict on mixed-content errors,"
"firefox","google-chrome"," is more overall,familiar with  overall, .sort is actually very overall,better in better time tools,slower in faster slower browser,better in better time tools,faster in faster slower browser, is no longer overall, has even fewer as far overall, doesn in implement parts implements,greatly faster in faster slower browser,","26248296,48026877,41656733,25067464,18308946,7876912,7057368,37037914,42550103,34712804,19769556,","so for example a hover rect doesn t work in firefox;google-chrome is more forgiving though so it will work there,so far i have the query to aws working and ajax request returning the audiostream resource to the browser but firefox is complaining about the format media resource could not be decoded error error code ns_error_dom_media_metadata_err 0x806e0006 i m not very familiar with google-chrome debugging but it does not work there either,ok as per mirko vukušić s comparison of this algorithm with .concat and .sort this algorithm is still the fastest solution with firefox but not with google-chrome;the google-chrome .sort is actually very fast and i can not make sure about it s time complexity,google-chrome shows it better than firefox firebug,firefox - good a bit slower than google-chrome,the google-chrome apis are currently experimental hence these tools are likely to be better developed under firefox,as google-chrome s v8 is dramatically faster than firefox s spidermonkey at the moment these things are constantly in flux pick the forward loop as it s faster on the slower engine,selenium can t launch google-chrome without google-chrome driver as google-chrome is no longer part of webkit and selenium can only launch webkit browsers by default;you should be able to launch firefox if firefox s installed without needing any additional binaries,google-chrome has a few proprietary selectors to change some shadowdom elements but not all;firefox has even fewer as far as i know and opera has none,google-chrome implements an offsetwidth in the svgtextelement prototype which allows jquery to return a width;but it s not a standard property and firefox doesn t implement it,i ve found that firefox is greatly faster with imacros than google-chrome,"
"firefox","google-chrome"," is actually in thing reality face,more flexible overall, doesn in forgiving error lenient, will ignore your code in higher smaller bigger,suitable as   in behavior edge visible,more overall, not overall,higher in higher smaller bigger,better in better time tools,reproducible more often in forgiving error lenient,better in faster slower browser,","5832294,22364875,45081016,46117499,52871796,7616566,19515590,14882752,37640574,25065194,19165746,","google-chrome doesn t agree about this for reasons i don t understand;i think firefox is actually doing the correct thing here,google-chrome is more flexible in this sense than firefox which is why it works there,google-chrome is somewhat at fault here as it should use the credentials mode of the request as part of the caching key so a non-credentialed request such as those sent by fetch shouldn t match items in the cache that were requested with credentials;i think there are other browsers that behave like google-chrome here but firefox doesn t,so if your timeout is set higher than 995ms google-chrome will ignore your code and puke on your nice clean empty console that you worked hard to keep clean;firefox is not much better and there are unreliable requests that just timeout for well beyond any patience i have and in doing so ignore the ontimeout handler,window.open is not suitable as google-chrome firefox and edge all block the popup which cannot subsequently be opened because of the navigation away from the original page,firefox is more technically correct in this case as it outputs the state of the object at each point in the loop whereas google-chrome is apparently waiting until the end of the loop to output each console.log but i m not aware of a standards specification that covers the console host object,firefox handles this syntax different from google-chrome as can be seen if you type the following in both console windows;this works in google-chrome not in firefox,for my website i noticed that tooltip delay in firefox was much higher than in google-chrome,evidently firefox debugger firebug is better than google-chrome debugger,this does not seem to occur in safari or ie and a similar permission error occurs sometimes in firefox but is reproducible more often than not in google-chrome,if you are able to enforce browser usage i find that google-chrome on android preserves line breaks when copying and pasting much better than firefox,"
"firefox","google-chrome"," doesn in better time tools,better overall,less in faster slower browser, does not overall, does not in faster slower browser, has a more overall,faster in faster slower browser,lower in older newer version,slower in faster slower browser,longer in older newer version,more in memory responsive high,","8812119,40426348,36332132,34676483,38884993,22143279,42117742,18068582,1233693,36987871,7159127,","try google-chrome to see the effect;nothing as far as i know firefox doesn t support those yet,i m sticking with firefox debugging of my angular 2 app outside of vs code as its user experience is better than google-chrome especially when working with typescript files so i ll stick with that for now unless i can figure this out,i need to do the same condition but for less than google-chrome version 46 and for less than firefox version 44,google-chrome instead emulates also user agent for example to trigger specific js ua checks that enable specific device detection features created by web application authors;currently firefox does not have this kind of feature natively but it was requested several times in official community and it seems in development,also tried the same extension with google-chrome because i read here that firefox doesn t support background yet the problem of connection breaking persists;the documentation you linked to is about a permission called background that google-chrome supports but firefox does not,google-chrome seems to have a maximum number of concurrent connections to one server of 6;firefox has a more granular configuration and distinguishs persistent maxium of 6 seen in about config and non-persistent the maximum numbers differed a lot in different sources,what we can see is that regardless of raw hardware power google-chrome seems to run up to three times faster than edge and also significantly faster than firefox all updated to the latest verion,i m seeing a strange issue on windows in google-chrome the framerate is almost 2 times lower than in firefox on mac google-chrome and firefox have similar framerates,firefox is slower than google-chrome which boats one of the highest javascript engines a modified version of webkit,firefox takes almost 3 times longer than google-chrome,firefox allocates lots of memory during that preloading up to 20-30 times more than google-chrome or safari do and even twice as much as ie,"
"firefox","google-chrome"," is still better in better time tools, is more in forgiving error lenient,slower overall, doesn overall,larger heights in higher smaller bigger,more overall,less overall,s much longer in faster slower browser,less in seconds data stricter, this is more overall,better than  overall,","55126010,56397117,18452965,13409241,34256894,10527980,42775173,17238511,640303,57295161,50291450,","considering that firefox is still better than chrom at debugging webfonts and that only today i have found add device pixel ratio in google-chrome s dev tools i wonder if i m overlooking something,my guess is either your post data or your response object is not considered valid json and that is why firefox throws an error maybe google-chrome is more forgiving,this is trickier to work around and you should file a google-chrome bug describing the situation and where it s slower than firefox but you could potentially reduce the amount of buffer uploads by looking into instancing or using uniform arrays instead of updating vertexes for positions textures,on my machine with no tel handler google-chrome simply does nothing ie9 says some content or files on this webpage require a program that you don t have installed;reasonable and firefox says firefox doesn t know how to open this address because the protocol tel isn t associated with any program,suffice to say ie and google-chrome start having problems at larger heights than does firefox,obviously google-chrome is more persnickety whereas firefox defaults to moving and sizing the printout as necessary,also about firefox i know still less than google-chrome and i m not sure about how it innerly manage resources among threads and if it is even possible to achieve what i want but if possible i d need to do the same work with firefox threads,i want to set my own time limit rather than use the browser s default i believe google-chrome s is much longer than firefox s,using the same data google-chrome does in less than 5 seconds what took ie and firefox 10 to 15 seconds to accomplish,follow these instructions google-chrome firefox this is more of a last resort,the easiest way i found to import url links list from excel or txt text document to google-chrome is open firefox no this is not a joke highlight all the cells with url inside in excel click hold on the selected cells and move to bookmark zone from firefox and release the click firefox understand what he needs to do and makes bookmarks open google-chrome three dots on top left bookmarks import bookmarks import from firefox firefox devs made this better than google-chrome clap clap,"
"firefox","google-chrome","increment with  overall,smoother than my  in faster slower browser, is better overall,bigger in faster slower browser, is more in better time tools, do not which is more in faster slower browser, treats have a higher overall, have slightly in faster slower browser,less forgiving in forgiving error lenient, dose not supports e.path so overall, has more overall,","53965580,15058125,53011919,21867035,51637626,11157350,25343946,33553280,27507680,57269585,18951075,","when i navigate to a real page not using figwheel with firefox the counter loads but does not increment timeout is printed once with google-chrome the counter loads and increments timeout is printed every second expected behavior i would expect the counter to get visibly increment with firefox and it does not,i m on a mac os x 10.8.2 running firefox 18.0.2 - 18.0.2 actually runs the game faster and smoother than my google-chrome 25.0.1364.99,threads -- threads shares a memory space it is an abstraction of the cpu it is lightweight processes -- processes have processes own memory space it is an abstraction of a computer to parallelise task you need to abstract a cpu however the advantages of using a process over a thread is security stability while a thread uses lesser memory than process and offers lesser latency an example in terms of web would be google-chrome and firefox in case of google-chrome each tab is a new process hence memory usage of google-chrome is higher than firefox while the security and stability provided is better than firefox the security here provided by google-chrome is better since each tab is a new process different tab cannot snoop into the memory space of a given process,google-chrome has a bigger time difference yet firefox is the one with the gap in the animation,we have observed metrics time taken for render grid in firefox is more compare to google-chrome,it would appear that google-chrome will fire the click and context menu events as the op is expecting.;but ie and firefox do not which is more the behaviour i would expect,for elements that are in the same node google-chrome treats later ones as if google-chrome treats have a higher z-index than the previous element even if it is not so;i has something to do with how google-chrome establishes a stacking context and also explains why preserve-3d is not required in some instances where firefox would require preserve-3d to display correctly,ie and google-chrome have slightly different implementations because of that;firefox does not have an implementation of innertext yet,actually your code doesn t work on firefox because he is less forgiving than google-chrome about errors,google-chrome supports both path it was google s original idea and composedpath;firefox dose not supports e.path so change e.path to firefox supports composedpath,alas there is still no really neat way to do this in google-chrome;firefox has more options.,"
"firefox","google-chrome"," headless sometimes in faster slower browser, is a bit more overall,filter slightly faster in faster slower browser, does not in security fussier resources,more in forgiving error lenient,less overall,easier in easier tool extensions,higher in faster slower browser, gives faster in faster slower browser,fine with  overall, combination is faster then overall,","53295012,42357097,42412501,54319614,43524743,16169171,7366195,2512461,48396410,51726656,53606554,","i had some benchmarks for google-chrome headless being faster than firefox headless sometimes back,firefox is a bit more complex,tldr in firefox filter is slightly faster in google-chrome that s the opposite,google-chrome does not allow loading of assets in your hard-drive due to same-origin security concerns;however firefox does allow it you can see that firefox has a few console.log s that get executed on completion,notably this doesn t happen in all browsers google-chrome renders more smoothly than ie and firefox for instance but is there any way i can make those white lines stop appearing when the user zooms in,update3 now it works here which means that i couldn t fully reproduce my bug to show it to you but in general the problem is that the height of 5x5 table in google-chrome is less from firefox on 22px which is 22px gap between table container and a table itself,i found programming google-chrome extensions easier than firefox but i couldn t come across something similar to xpcom in google-chrome,so i wonder the benchmark is also influenced by the installed extensions or google-chrome is really so much higher than firefox in performance,see the different results in firefox and google-chrome;i am confused why google-chrome gives faster results for non-recompiled regex though,when i use this on my style tag the font works just fine with firefox and google-chrome and edge but it s not working on ie so i seareched about it and it s looks like i have to use woff and eot in order to make it work with ie so i used this code but when i change my style to it s not working even on firefox,as you mentioned in your question geckodriver firefox combination is faster then chromedriver google-chrome at this point it is worth to mention that diferent browsers render the html dom in a different way,"
"firefox","google-chrome"," initially focused on peerconnection overall,slower in faster slower browser,less overall, not overall,more flexibility in fine document jquery-ajax, seems less in faster slower browser,chromium much better in better time tools,taller element in higher smaller bigger,less conveniently in better time tools,slower in faster slower browser, 14; has a problem in forgiving error lenient,","17659407,24051882,40264341,44521906,14040074,38626906,15934117,18185150,28388530,11577677,12626496,","when testing the data channels i would recommend using firefox since the webrtc work was split such that google-chrome initially focused on peerconnection and firefox on datachannel so firefox is still ahead in such that google-chrome initially focused on peerconnection and firefox on datachannel implementation of the datachannel,i have been working with linear parsers lately and noticed the performance in google-chrome v37 was much slower than in firefox v30,or does this mean firefox is less secured on that point than google-chrome,so this scenario may be left open for interpretation and google-chrome may not be violating any guidelines;however in another scenario where the height property was a factor firefox stuck with flex while google-chrome went with height why is firefox not honoring flexed div #39,firefox may be sniffing the document s encoding with more flexibility than google-chrome is,note the above returns reliable results in google-chrome and safari;firefox seems less reliable out of the box,also for animations google-chrome chromium behaves much better that is why i use it for development but do my general browsing in firefox,the reason this is happening is because firefox renders the select box as a taller element than google-chrome and the container of the blue text is getting caught on the select s container element because it s now sticking out lower than the others,what s up with google-chrome that the debugger behaves less conveniently than firefox,in google-chrome it seems .prototype is faster firefox shows no difference between the two although generally performs slower than google-chrome,does work in ie8 opera google-chrome but not in firefox 14;firefox has a problem with,"
"firefox","google-chrome"," it doesn in faster slower browser,richer in object binary range,less in faster slower browser,wider in higher smaller bigger, does not yet in object binary range,faster in faster slower browser,probably easier in easier tool extensions,just more in forgiving error lenient, it s more overall,bolder in higher smaller bigger, takes less overall,","45260467,2213961,8120047,35241820,8449950,37896235,17298652,20167596,53694606,16221732,4841196,","for some reason google-chrome reloads the tab when you click on it resulting in the default page being opened;when you use ie or firefox it doesn t reload the tab on click,i inspected the range object in both google-chrome and firefox and then noticed that firefox s range object is far richer than google-chrome s,the idea is that if they are using ie then they must install google-chrome frame and if they are using less than firefox 4 or opera 11.5 then they must upgrade their browser not not sure if the way i am doing that is correct,the same word in firefox look wider than in google-chrome,only google-chrome currently supports sending binary object types;firefox does not yet support sending binary types,however if thats the case does this mean firefox is faster in execution than google-chrome,firefox is probably easier than google-chrome these days,google-chrome is just more forgiving for some mistakes then firefox,on firefox a homescreen icon with a + appears in the browser for the user to install to homescreen - but it installs less like an app than google-chrome it s more like just a shortcut whereas google-chrome goes through a whole process,font-face bold in firefox is bolder than in google-chrome,for example try hiding a dom of of 200+ children in google-chrome vs safari vs firefox you will notice google-chrome takes 20+ sec safari takes 5+ seconds firefox takes less than 1 second,"
"firefox","google-chrome","larger than under  in larger text snippet, doesn in higher smaller bigger,faster in faster slower browser,bigger in higher smaller bigger,longer input in faster slower browser,slower in faster slower browser,even faster in faster slower browser, this is no longer in older newer version, image is displayed correctly overall, has a slightly more in fine access mozilla,everything more in different looks browsers,","57279199,53788026,12829930,30802594,30248543,16188505,4952787,16410555,29323536,11067557,3237600,","try to run this html snippet in google-chrome and then in firefox with an mobile view like iphone7 you will see that in the firefox mobile view the font size is much larger than under google-chrome,google-chrome ignores the writing mode of the cells firefox doesn t give the cells enough height and edge makes the rotated table super tall;css allows this but none of edge firefox or google-chrome does it right,google-chrome is just faster than firefox which just faster than ie at bit-wise operations,i have too problem with text in the firefox is bigger maybe bolder as in google-chrome .,google-chrome renders longer input fields than firefox and ie,svg performance in firefox is slower than in webkit google-chrome and ie10,in ie8 and google-chrome it runs even faster than firefox in general and this slow down never happens,it appears in newer versions of google-chrome this is no longer an issue;i have not tested firefox recently,however if your png has some other color profile such as adobergb then perhaps firefox is not compensating whereas google-chrome is;in this case the firefox image is technically displayed incorrectly although it will match rgb font colors in your html whereas the google-chrome image is displayed correctly to the creator s original intent for the colors but will not match the rgb font colors in your html which are specified in srgb,which seems to work just fine in google-chrome and firefox what i have access to;i am guessing that google-chrome has a slightly more strict base64 implementation and requires the padding,i have just noticed a site i am working on looks different in google-chrome than other browsers saf firefox everything is more saturated in google-chrome even a background-color rgb,"
"firefox","google-chrome","higher in higher smaller bigger,much better javascript in better time tools,more in forgiving error lenient,headless with  overall, doesn in internet explorer global,slower in faster slower browser, is not working well overall, is not in behavior edge visible,longer than ie7+  in older newer version,better in better time tools,more in easier tool extensions,","35735824,3136004,33759931,55536160,33374245,3156866,51672846,36739592,2248145,34924977,23064942,","what firefox displays is about 5 inches higher than what google-chrome displays,is it because google-chrome has much better javascript support than firefox,firefox which is more standards-compliant than google-chrome doesn t allow this,the tests that failed when ran by the automation server pass when ran manually - using test.only tried mostly headless with firefox and google-chrome but the same randomness happens with full ui as well,it s failing in firefox because you ve tried to reference the ie-specific global event variable which google-chrome also provides as a bone thrown to ie-specific code as sdgluck pointed out;firefox doesn t have that and so the code throws an error,firefox is slower than google-chrome in javascript,error on sigma not being defined firefox was having no trouble displaying the graph -- google-chrome was;i ll need to look into why google-chrome is not working well with sigmajs,google-chrome seems to allow this i do not know why but firefox is somewhat stricter;basically firefox is following the intended behaviour google-chrome is not,this alleviated some of our performance issues but even on subsequent refreshes our noticed ie6 taking substantially longer than ie7+ firefox google-chrome etc,also why does firefox perform way better than google-chrome on this benchmark,in google-chrome it seems more or less ok but in firefox when i first tried it it should give me not logged in my console,"
"firefox","google-chrome"," behaviour is more overall,wider in higher smaller bigger, maybe not in higher smaller bigger,more in faster slower browser,larger 15px in higher smaller bigger,older in older newer version,higher in higher smaller bigger, works far better overall,much ram with  overall,more forgiving in forgiving error lenient,more in faster slower browser,","29860103,46178781,42755741,17532421,19429156,44939382,7012987,25245512,55139250,20262398,35612869,","the html5 spec does not recognise the form controls as replaced elements and suggests that they be rendered as inline-block which would make google-chrome s behaviour correct but there are many ways in which all the browsers including google-chrome simply don t behave that way;from a backward compatibility perspective with old web content the firefox behaviour is more reliable,even if you give the inputs a attribute firefox renders them wider than google-chrome,there are some problems in firefox not edge google-chrome when expanding select fields - solution css select;some borders can can be visible some dissapear on firefox maybe not edge google-chrome,in google-chrome there is more files download 47 than in firefox ie 42,for example if you have a text size of 15px well firefox makes a larger 15px than google-chrome does,on stable versions of google-chrome the permissions show allow and it still won t connect even with firefox works older versions of google-chrome work,in google-chrome it is appearing higher than the original in firefox,in a nutshell google-chrome s printing capabilities are shocking;firefox works far better for printing but runs much slower,albeit it would induce some overhead to spawn the new webdriver browser client combination but that may provide the much needed cushion from cpu and memory usage as discussed in limit google-chrome headless cpu and memory usage selenium using too much ram with firefox incase the tests are not independent and the tests are based on the same session cookies etc parameters reusing the same webdriver browser client makes sense,i m taking a quite educated guess here that firefox is more forgiving with the irregular syntax than google-chrome,that means firefox is more than 7x faster than google-chrome here,"
"firefox","google-chrome","fine with  in fine document jquery-ajax,just stricter in faster slower browser,even slower in faster slower browser, not overall,higher severity in higher smaller bigger,slower in faster slower browser,smaller in higher smaller bigger,better in better time tools,perhaps more right in higher smaller bigger,more than  overall, does not in faster slower browser,","48420394,19391406,11057475,17430426,41705094,9050531,5699024,42863498,20625114,3081636,14858110,","i have a simple class and a mvc controller method i am calling the controller method via jquery-ajax this works perfectly fine with google-chrome and firefox,it is also proper browser behavior firefox is just stricter about it than google-chrome is,firefox is even slower that google-chrome,that s why most browsers can t animate cross-fade them via css only google-chrome has this non-standard feature;some more info on the topic css3 transition of background-image for firefox not working,google-chrome is reporting the missing key with a higher severity than firefox is so i want to eliminate that as a possible cause,use firefox rather than google-chrome - google-chrome with dev mode is much slower than firefox,if we remove the font-size it appears properly in google-chrome but shows up smaller in firefox,so firefox is better than google-chrome - but both suck in that regard,thus i think firefox is perhaps more right but other developers at google-chrome think otherwise,you can view a quick example live on cssdesk this method works in a lot more than google-chrome but breaks in firefox 3.0 and just doesn t work in a number of ie versions,google-chrome not supported google-chrome does not give access to clipboard;firefox there are steps here to enable clipboard access,"
"firefox","google-chrome","more limited in easier tool extensions,much more strict in faster slower browser,slower than the  in faster slower browser,nicer overall,helpful than  in forgiving error lenient,much slower in easier tool extensions, you see nothing overall,bigger in higher smaller bigger, from taking webaudio-generated source overall, it works strange. in image strange slide, is not overall,","27465781,24072476,40715077,9634984,54363932,8838478,54681411,8817800,26479138,54243841,3417323,","google-chrome extensions are considerably more limited than firefox extensions â â they can only hook the behavior of the browser in certain predefined ways,apple s safari applies almost no cross domain restrictions to files opened locally but firefox is much more strict about what it permits with google-chrome somewhere in the middle,google-chrome is extremely slow for your code path but grep seems to be 50 faster than array.filter here making google-chrome 900 slower than the firefox run,in my testing google-chrome is nicer it pops-up the unresponsive alert whereas firefox apparently doesn t care,also of note is that in this case i ve found firefox s error messages much more helpful than google-chrome s,because of how google-chrome s plugin system works development mode in google-chrome is much slower than in firefox or safari see here for more details,for some reason i don t understand google-chrome doesn t propagate opacity to children of an element with display initial;on firefox you see nothing on google-chrome you see text inside paragraph and nothing else,that happens because pixels size in google-chrome is bigger than in firefox,last i checked in google-chrome it couldn t process the output of webrtc through webaudio while firefox can;however there are two bugs blocking firefox from taking webaudio-generated source streams in a peerconnection one of which has now been fixed in nightly aurora and the other should be shortly,like this i get what i what in google-chrome but in firefox it works strange. in firefox when slide is moving image is not fixed,firefox extensions have much more power than google-chrome and have wider access to browser s internal api;firebug lite for google-chrome is not able to track http requests yet,"
"firefox","google-chrome","faster than  in faster slower browser, has not in implement parts implements,better in better time tools,more in extension addict advanced,wider in better time tools, is better in better time tools, doesn in difference fires greater,fatter in faster slower browser, is better in faster slower browser, installed jsonview in extension addict advanced,smaller left in higher smaller bigger,","50424378,40141741,26699714,6916614,12569898,8430879,49902742,6103317,25342494,44352103,25310350,","in most cases firefox runs blazor apps much faster than google-chrome or edge which implies that browser makers still need to improve even firefox can improve,there are also some parts that firefox has implemented that google-chrome has not for instance;ie and firefox implement the svg 1.1 definition of rect elements where x y width and height are attributes,in firefox it s much better than google-chrome but still not as black as ie,firefox has a more advanced plugin than the google-chrome extension but both work,after looking into the issue with the inspection tools of both browsers it seems that firefox is making the #main_nav_bar ul 10px wider than google-chrome,whether you use a plugin or write your own code css only is a no go for google-chrome safari and as you said firefox is better at dealing with it,just a guess but possibly the reason for the difference is that google-chrome fires the event after it adds voices to your page;firefox doesn t so it doesn t,it works fine even for italics but i m aware that the user s browser is being asked to make a best-guess here and have noticed that google-chrome will make individual chars appear slightly fatter than firefox when i do things this way,i did a couple of screenshots and firefox canvas was faster than google-chrome capturevisibletab;actually firefox is better suited for as-fast-as-possible screenshots since its canvas expose to privileged code the mozfetchasstream method allowing to bypass the actual bottleneck which is the base64 encoding of the image data,i then tried google-chrome which doesn t come with a built in parser installed jsonview extension and everything there was fine;did the the same with firefox installed jsonview and,why in firefox text smaller left than in google-chrome and opera right,"
"firefox","google-chrome","more strict in forgiving error lenient,better than  in better time tools,best less in faster slower browser, does not overall,better in better time tools,actually load in faster slower browser,better in better time tools, has an intent overall,lower in higher smaller bigger,more in higher smaller bigger, is not in faster slower browser,","5697000,49679541,12215314,50771588,27272144,31841419,25605880,57275380,25118060,3702732,31852458,","google-chrome is a bit more strict than firefox here,edit amazingly firefox seems to handle this better than google-chrome,the performance in google-chrome is best less so in firefox and ie,explanation firefox has a play button center poster and google-chrome does not;this is an obvious indicator that firefox has controls attribute that covers the entire tag and google-chrome does not it s controls are accessible only at the bottom where the bar is,it s not the first time that the plugin for firefox works better for me than the one for google-chrome,based on what you have provided i cannot tell if google-chrome actually does load faster than firefox,i have tried firefox and it better than google-chrome crash issue is not always happen but not solved this issue,strict-transport-security in the deployment recommendations of hsts preload list it is stated add the strict-transport-security header to all https responses in apache this would look like note i did not include the preload directive developers should read the hsts preload list s deployment recommendations first before adding that x-xss-protection if you are using csp without allowing unsafe-inline then you probably don t need to worry about x-xss-protection anymore google-chrome has an intent to deprecate and remove the xss auditor;firefox have not and will not implement x-xss-protection,it seems that in firefox ie the line-through style is a bit lower than google-chrome i am not sure if this is because of the implementation of different browsers or something,any ideas why google-chrome is apply more spacing than firefox ie,for backward compatibility ie still supports this model and google-chrome also has added support for this feature;but firefox is not supporting it,"
"firefox","google-chrome"," which has built-in profile overall,more detailed in forgiving error lenient,higher in higher smaller bigger,smaller in higher smaller bigger, this becomes a bigger overall,less overall,better in better time tools, which also in user folder file,less efficient in faster slower browser,better in better time tools,smaller in higher smaller bigger,","47303184,22889989,11208545,17458265,55251393,18204595,14895327,20521669,8264054,19931969,13139755,","further google-chrome cannot even select which profile out of many is selected when google-chrome is started from the os call url to be opened function and not started by user-activated clicking on a google-chrome application shortcut with specific profile selection instructions;naturally the latter works 100 wonderfully on firefox which has built-in profile selector after the browser has been started and regardless of which method was used to start firefox user click on firefox icon or os pipe of open url to firefox,google-chrome provides more detailed information about the ajax errors even when firefox doesn t say anything,on linux google-chrome displays text about one pixel higher than firefox and opera,firefox thumbnail 4 times smaller than on google-chrome,the current date is 2019-03-19 and the minimum date is set to 2019-04-18 next month however in google-chrome when you click on the date it opens up the native datepicker and it opens up on april with the invalid dates grayed out. datepicker in google-chrome in firefox with versions that support native datepicker however when you first click on the date it opens up the native datepicker and it opens up on the current month march with all the dates grayed out and the user will have to click to the next month in order to see valid dates. datepicker in firefox this becomes a bigger issue if the min date is several months to years from the current date in which case the user must keep clicking to view the next month only to find that they are all unavailable,in firefox it s less noticeable but in google-chrome it s very noticeable,google-chrome handles these better than compared to firefox,turns out i was wrong - user downloaded the file via google-chrome which also sets this flag;firefox doesn t set it i checked on the latest versions to date,so why is google-chrome less efficient than firefox and ie,in firefox the render of new tiles and the panning is laggy this is not the case when using maps.google.com so it s not an issue of google-chrome being better than firefox,my menu links seem in google-chrome correctly but in firefox my menu links are 1 px smaller than google-chrome,"
"firefox","google-chrome","quicker than using  in easier tool extensions,smaller in higher smaller bigger,nicer in better time tools,faster in faster slower browser, had less in better time tools,better in better time tools,typically narrower overall, does not overall,more padding in higher smaller bigger,more ram in memory responsive high,better in better time tools,","19350398,30222783,21204517,11881265,7589633,11662302,8618000,21278911,38729442,36921492,32425205,","also google-chrome has a great tool that allows you to edit css on the fly which i find more useful and quicker than using firefox s extensions,it looks like google-chrome is smaller in your screenshot there so firefox could just not be at the defined break-point yet,in firefox this might be handled a little nicer than google-chrome but rather than completely redraw an element you should try to append only what you need,rendering without buffering on google-chrome is faster than firefox so i m actually not sure why google-chrome is having such a problem with drawimage,google-chrome had less of an issue most likely due to tabs being in isolated processes;firefox 7 seems to have fixed this problem for firefox,when i ran the test individually with a timer google-chrome actually performed better than firefox hope this helps,try your current setup in google-chrome google-chrome typically has a narrower minimum viewport width and try something like min-width 600px in firefox and your styles should be applied as expected layout.css will be applied when the viewport is at least 600px wide,it appears there s a difference in google-chrome s webkit engine vs firefox s gecko engine;firefox continually runs to check a hover status while google-chrome does not,firefox s default textbox includes more padding than ie google-chrome,we use google-chrome though which i believe uses more ram than firefox,it seems google-chrome applies some formatting to render better than firefox,"
"firefox","google-chrome","more complete in better time tools, and scroll down in better time tools, not overall,faster than  in faster slower browser, isn overall, that s it......what more in faster slower browser,better with  in better time tools,more in forgiving error lenient,easier in better time tools, doesn in higher smaller bigger, folder is a folder in user folder file,","32095422,21588513,44418729,35612869,57403372,16136954,38681676,16420086,4472205,47880284,51331033,","pd note that firefox s developer tools are better and more complete than google-chrome developer tools but in essence are for the same purpose,indeed google-chrome doesn t have it but firefox 27 has it;for those who want to see this issue just hit his url in firefox and scroll down and up,so it happens i am dumb and the charset has nothing to do to google-chrome not loading;the real problem was the is line had style in it so google-chrome gets crazy but firefox was okay with it,to be more specific firefox was about 4x faster than google-chrome for both scenarios takes google-chrome about 1 seconds to process each scenario whereas it takes fixefox only 0.25 seconds,and with all set up correctly you might get something like most likely related to google-chrome devtools listening on ws 127.0.0.1 57725 devtools browser 34a42518-c3d9-4e14-af8e-9a137b11625b 0808 012434.304 info console 0 the content-security-policy directive prefetch-src is implemented behind a flag which is currently disabled. source 0 0808 012437.286 info console 240 no signed in google accounts available - visit accounts.google.com to ensure that at least one account is signed in otherwise no data will be returned from this api. source mss boq-identity js k boq-identity.identityyolowebmoduleset.en_us.fufh6x86rzu.es5.o am aw d 1 rs aoaemlh5bdy58s_qoulxsyv6tympthlvyw m yolo_frame_library 240;you ll get that error if firefox isn t installed or isn t accessible on your path,it is supported on ios safari google-chrome and some latest version of opera..;it s not yet adopted by ie and firefox that s it......what more one can ask than local db on browser which has relational db system...so u can query it easily and handle complex data....which is very tougher in key vale based systems,if you want to take the performance benefit of storing data in the same place each time then pass the variable as argument to the function the performance loss of doing that seems to be within acceptable limits although firefox seems to deal better with firefox than google-chrome,when i run my http web server on google google-chrome it gives more 324 error than in firefox,another alternative for javascript ajax is writing a google-chrome add-on easier than firefox or embeding a web browser within your application,google-chrome seems to always use value while firefox uses textcontent;since google-chrome doesn t look at the textcontent property range#selectnodecontents will select nothing on this browser.,your google-chrome folder is a folder named google-chrome located under your user profile .mozilla firefox g7fa61h3.default google-chrome,"
"firefox","google-chrome"," is slower than  too in faster slower browser,more picky in forgiving error lenient,more reliable in extension addict advanced,faster in faster slower browser,causes  to visit overall,faster than  in faster slower browser,different results between  in faster slower browser,better in better time tools, which is older in older newer version, uses the older in older newer version,smaller in higher smaller bigger,","51707529,39847666,38256817,36034808,53522619,51125795,49570822,45031412,54460199,24397037,8850307,","but speed of google-chrome is slower than firefox too,firefox is more picky about characters that are set in uris than google-chrome,2 install the firefox extension there s also one for google-chrome but the firefox one is more reliable -- so if you re a google-chrome addict like i am then use both,and of course the results - if they are reproducible - might suggest that google-chrome is faster than firefox or that firefox just prioritises timeouts over dom events,firefox is more likely that there is a cached redirect in google-chrome which causes firefox to visit a former redirect target directly without visiting the original url,so the winners are pure js and firefox 3x faster than google-chrome,when creating a 3d effect on some svg text by stacking a bunch of text-shadows through css i get very different results between google-chrome and firefox,also ie and firefox seems to work better than google-chrome,if you are seeing similar error on google-chrome on ubuntu the reason is probably you have a pre-installed version of google-chrome and firefox which is older,google-chrome uses the older gdi;firefox and ie use directwrite,if i remove this code the width of green image in firefox becomes smaller but it s still bigger than necessary about 100px the google-chrome images keeps unchanged,"
"firefox","google-chrome"," has own certificate in faster slower browser, will print the table overall, to replicate this so in post cookie iframe, v8 is more in faster slower browser, does not implement this yet overall,more in forgiving error lenient,better in faster slower browser,bigger height in higher smaller bigger, is better in better time tools,lower in higher smaller bigger,less frequent overall,","26046388,41993559,6060260,56781298,30315914,5375848,36229848,23845903,50109918,15483083,15113342,","in case of ie google-chrome uses windows certificate storage the same as ie the intermediate certificate is already installed on your computer that s why ie and google-chrome don t complain;firefox doesn t have that certificate installed and firefox has own certificate storage,the mdn examples of collections of primitive types worked on firefox but not on google-chrome;to have an output on google-chrome for your example what you could do is put the transportation variable inside its own array so that google-chrome will print the table,google-chrome to not add iframe url changes in the back button history;sadly i ve found no way to force ie and firefox to replicate this so i used the ajax post approach suggested above by arun,google-chrome node.js firefox apparently firefox wants the year to have 4 digits exactly while google-chrome v8 is more flexible,google-chrome does not implement this yet but firefox does;the following is a complete webrtc call that works in firefox note uses arrow functions,maybe google-chrome is more lenient when it comes to illegal characters in the url than firefox or something like that,the logs only seem to work with a google-chrome browser which is a laugh as usually firefox is better at running google apps than google-chrome,but in firefox the left-collumn gets a much bigger height than in google-chrome,in google-chrome firefox is better,in firefox it seems that this is 1px lower than in google-chrome and safari,this doesn t happen in google-chrome and is less frequent in firefox than in ie,"
"firefox","google-chrome","wider in higher smaller bigger,;it appears as if  in forgiving error lenient, and won t overall, behaviour is simplier overall, did not in forgiving error lenient,clearly less overall, were not overall, download here overall, was 74x slower in faster slower browser, and not overall, entry is no longer in faster slower browser,","19297982,45115475,53198244,56340749,3605068,40264503,41208515,1872424,24250456,21917518,49411140,","in google-chrome the button is wider than in firefox,this works in google-chrome but doesn t work in firefox;it appears as if firefox is ignoring the initial transform on the object and transitioning from 0 0 instead,it seems that google-chrome is stricter than firefox and won t allow this fix to work over regular http,based on w3c s specification cache.addall method google-chrome behaviour should be the right one;but firefox behaviour is simplier,your html was invalid and google-chrome did not accept it;firefox was more lenient in what it allowed,firefox is clearly less secured it deliberately allows something that google-chrome locks down,after further research i got to know that google-chrome was throwing this error for a special character æ which was part of my json response body content;however ie and firefox were not having any complaint about this,we recommend using google-chrome download here or mozilla firefox download here,google-chrome was 74x slower rendering svg over canvas;firefox was 150x slowe rendering svg over canvas,and for me this trick only worked in firefox and not google-chrome;however if in google-chrome you do shift+ctrl+n new incogneto window then go to your editor you ll see that it is working,which outputs the following user admin time 1521639136 ip_addy 127.0.0.1 browser firefox browser_version 59.0 os windows notice that the previous google-chrome entry is no longer in the file as it removes the previous entry and writes a new one my intended output is to display it like the following it does not have to be formatted with space it can simply be in one line and break line for the next input current problem when data is save to the file and it needs to be updated the file then gets overwritten with the new data and old data is disgarded,"
"firefox","google-chrome","smaller in larger text snippet, did; also in forgiving error lenient,more details in easier tool extensions,slower in faster slower browser, doesn overall, has a design overall,faster than  in faster slower browser,more time in seconds data stricter, i believe is less in faster slower browser, has more in faster slower browser, i tested so far in message handler body,","10007704,51757311,17608767,24708649,38482617,4804813,57108107,30634669,11790179,54385583,10091806,","while the rest of the browsers report the correct width of the document firefox reports a smaller one example at screen resolution of 1920x1080 ie google-chrome and safari reports 1920 while firefox reports 1903,when i tried to load the json in firefox i did not get any fatal errors however opening it in google-chrome did;google-chrome also gave a reason and doing this from memory but it had to do with security issues my guess to help prevent potential xss,i usually use google-chrome s developer mode which imho already gives even more details than firefox,update just tried it on firefox 30 and it did not experience the same slowdown in a worker thread but it was slower than google-chrome when run in the main thread,google-chrome doesn t require such initialization;firefox requires something that we call init here to be set in dragstart event to initialize the rest of drag events to occur,google-chrome has a design policy that forbids you from using xmlhttprequest to load local files;firefox does not have this policy,this javascript code is only three to five times slower depending on cpu closer in performance on higher end cpu s when run on newer versions of node google-chrome browser google-chrome version 75 is still about 25 faster than firefox version 68 than kim walisch s primesieve written in c and compiled to x86_64 native code,in firefox it takes some more time compared to google-chrome safari,the fact that you are using a different protocol is making firefox believe you are making a cross-site request;google-chrome i believe is less strict with this restriction when on local,i assume firefox has more strict cors requirements when compared to google-chrome,as you will see the message handler is called in firefox but not google-chrome;this is not working on any google-chrome i tested so far,"
"firefox","google-chrome"," and not in forgiving error lenient, as well in fine access mozilla, is better in better time tools,larger in larger text snippet,general in strict ie configuration,more padding in higher smaller bigger, is less overall,bigger in higher smaller bigger,much better in better time tools, does not overall,shorter overall,","31233819,54683390,51261506,20738370,54677492,2954659,35687675,11782208,8519584,40571005,20622748,","my core-list did work in firefox and not in google-chrome;i noticed that google-chrome gave me an error that no height was set even though i set height to 100,i am using inappbrowser to open an external url in my app but in this.inappbrowser.create function while passing url it is opening into a new window as localhost 8100www.google.com instead of www.google.com.i am using google-chrome but in mozilla firefox it is working fine i enter code herehave tried putting before the link like this this.inappbrowser.create my link but this is not working in firefox expected is link should open in google-chrome as well without putting before it in this.inappbrowser.create,believe me i m a google-chrome fan-boy myself but firefox is better in this case. here are the reasons firefox uses its own ssl cache which is purged on shift-reload,safari and opera work with the google-chrome setting but firefox and ie display the iframed page larger than google-chrome causing part of the text to be cut off,changing web.config security policy to object self fixed the problem in our case our could open pdfs in firefox and ie but not in google-chrome so google-chrome has a stricter implementation of the security policies,in firefox the input button has more padding than in google-chrome,once they do this it will work just like google-chrome;firefox is less liberal with permissions by design and unfortunately there is no way for a web page to carry non-persistent permission from one page to another or even from one call to getusermedia to another on the same page without user-consent for each call,in google-chrome the green div is bigger than in firefox i really don t know what is the problem i think in firefox it displays as it should but in google-chrome and ie9 it displays different,ie8 is the same and firefox is much better but i ve migrated away to google-chrome now so would like to stick with the one browser if possible,like google-chrome firefox has a use hardware acceleration when available checkbox in preferences advanced general browsing;however unlike google-chrome firefox does not require this checkbox to be checked for webgl to work.,however a sdp object local description by firefox does not contain datachannel at all and moreover the sdp is much shorter than google-chrome and less information bundled,"
"firefox","google-chrome","older in older newer version,faster in faster slower browser,more in better time tools,easier in easier tool extensions,more in strict ie configuration,smaller in higher smaller bigger,bigger in higher smaller bigger,better in better time tools, does not in forgiving error lenient,consistently better in better time tools,slower than  in faster slower browser,","6070609,19044179,25982053,41742075,17973121,11605875,4899792,25699066,23532944,25606508,54082486,","i believe the error might be how firefox and google-chrome handles these multiple times included jquerys and it might be that google-chrome uses the older and firefox newer version,the funny thing is that for abc_def property google-chrome is actually much faster than firefox and ie as i expected,my guess is google-chrome is caching more agressively and thats why you are seeing worse load time on firefox,i have been trying to do this in firefox webextensions but if it s easier with google-chrome i will try it that way instead.,i think from my tests perhaps firefox has more strict standards than google-chrome about cookie manipulation,in google-chrome the button should be a tad smaller than in firefox,i built a site and the problem is google-chrome display font-size 1px bigger than firefox,google-chrome dev tools are by far better than firefox ie whatever imho,google-chrome has its own implementation but you won t get it working firefox;this is not working because firefox does not support animations on background images,also it does appear that firefox consistently handles this better than ie or google-chrome,some developers there say that firefox performs much slower than google-chrome or edge in these scenarios but i haven t found a way to validate a difference personally,"
"firefox","google-chrome","smarter overall,slower in faster slower browser,more time than with  overall,more user in user folder file,more in image strange slide,more over  in forgiving error lenient,slower in faster slower browser, could do nothing overall,lesser extent in better time tools, you cannot in faster slower browser,better overall,","17538178,12583588,53629542,19007974,5645603,56091712,12150212,23435637,6583582,36669583,6433693,","i guess google-chrome is smarter about me being careless than firefox is,the reason google-chrome is slower than firefox is that the devmode plugin runs in a sandbox so calls and data have to cross the sandbox boundaries,when we simply changed a test context to firefox it turned out that all calls of findelement took 10 times more time than with google-chrome,ellipsis still wraps in google-chrome but is more user friendly in firefox by adding the .,firefox is more tolerant when it comes to image headers then ie and google-chrome,i created a rest api project and i m tring to access the https endpoint using curl like this i googled around and these are the steps i took to extract the certificate from dotnet and add it to my repository of trusted ca certificates this is what i tried to do to add the certificate i also tried setting the curl_ca_bundle env var like this i keep getting the same curl error more over firefox and google-chrome tells me the connection is not secure additional info references,i ve tested the sample on firefox and it s a bit slower than google-chrome but usable,edit this is simply a limitation of stun not of sctp so google-chrome could do nothing about it if they wanted to;firefox does not support sctp over tcp anyways,this is sporadic and i see it happening most of the time with firefox 4.0.1 and 5.0 and to a lesser extent with google-chrome,google-chrome might be different;i believe in firefox you cannot access indexeddb if you open an html page from file,given these last two complications i am leaning towards a browser plugin as the solution probably firefox since low-level http stuff seems to be better supported than in google-chrome but i am wondering if anyone has an idea for a more cross-browser-y solution and or if there is something out there that might provide a good api for this kind of thing,"
"firefox","google-chrome","stricter in seconds data stricter,worse in faster slower browser,faster in faster slower browser, is tighter overall,faster overall, doesn in higher smaller bigger,faster in faster slower browser, is that browser in faster slower browser,lower in faster slower browser,more-so in more-so pickier, does not overall,","43785589,6817093,1238849,43578419,7332444,22410157,41419724,20861987,11021028,35630482,35041472,","i d say that firefox is correct in being stricter than google-chrome here even though i m not entirely sure the base uri has changed and this new uri should be used for the relative uri #symboltype1 too,google-chrome is lightning fast when you use the dom api but vastly slower using the .innerhtml operator by a magnitude 1000-fold slower however firefox is worse than google-chrome in some areas for instance the append test is much slower than google-chrome but the innerhtml test runs much faster than google-chrome,the trouble is choosing a good value for x since for this particular page google-chrome is so very much faster than firefox which is faster than ie,even with this outwardly tight regular expression google-chrome and firefox s implementations interestingly allow for e here presumably for scientific notation as well as - for negative numbers and google-chrome also allows for;whereas firefox is tighter in rejecting unless the,when i posted this on facebook someone said that since firefox is open source project developers optimized math.min but google-chrome didn t since google-chrome is just a modification of chromium but beside that above statement is not quite right that makes no sense because that doesn t explain the reason why google-chrome s and firefox s math.min a b performs in similar speed and google-chrome s math.min a b and firefox s performs in same speed because if firefox is faster than google-chrome then google-chrome s math.min a b should be much slower than firefox s,that blue border is google-chrome specific firefox doesn t show it;google-chrome adds the border to links,this makes firefox 50x faster than google-chrome and 70x faster than safari is there any known reason for that,the only problem with google-chrome firefox is that browser version compatibility is a pain in the tail,firefox text is 1px lower than in google-chrome and ie,it looks like it shows for firefox more-so than google-chrome but it might help,google-chrome adding origin header to same-origin request;testing on firefox confirms that firefox does not set origin on same-origin get or head requests but google-chrome does,"
"firefox","google-chrome","higher than  in higher smaller bigger, it does trigger overall,next issue with  overall,general in better time tools, doesn in older newer version,more in better time tools, but not overall, although not in behavior edge visible, is more used partly in easier tool extensions,better overall, and doesn in strict ie configuration,","54565425,56814946,19832393,6399867,25324323,18528300,32539274,42426495,11062366,15999099,45045428,","for some reason google-chrome shows this span unusually higher than firefox,when the user press at editicon the function onsubmit is triggered automatically and when i remove the type from iconbutton it doesn t do it it s only triggered once. but the bigger problem is in google-chrome it does trigger but the editwindow keeps open but on firefox it closes. i don t know where is occurring the error and don t know why this is happen,this is part of the problem as firefox requires it google-chrome does not - this explains why it works there;the next issue with firefox is that overflow hidden set on .home-auto-interactive causes all descendant elements to be flattened according to the spec w3c transform-style,i would also recommend using firefox with firebug or firefox google-chrome with the inspect element context menu so you can see what styles are being applied to the various elements on your page when what styles don t act like you expect,google-chrome which uses a newer version of v8 throws an error if you don t use new;i m not sure why firefox doesn t throw an error though,my firefox adds more chopping to video playing itself google-chrome plays the video much better,the log location is clickable in google-chrome but not in firefox;i have used floatinglomas solution with some tweaks as it does not quite work on firefox the stack is slightly different,this includes safari 10 google-chrome 55 including opera edge 13 on the desktop and ios 10 and google-chrome on mobile;this also includes firefox although not mentioned,a similar tool is now present in firefox you had to use firebug before but google-chrome is more used partly because of google-chrome more powerful js engine v8,graph beeing cut in google-chrome is a known issue google-chrome does not respect some print css and works much better with firefox not all versions still,google-chrome usually use configuration from ie;and ie may be more strict then firefox and doesn t allow authentication with bad ticket,"
"firefox","google-chrome","faster in faster slower browser,brighter in forgiving error lenient,slower in faster slower browser,forgiving than  in forgiving error lenient, this works much better in better time tools, never downloads more in faster slower browser,much better in better time tools, but not in internet explorer global, debugger was much clearer in better time tools,better browser in better time tools, is much worse in faster slower browser,","5746611,32877975,4398883,3008645,50523442,53560991,29925542,35118008,28505632,30843036,6465004,","i use google-chrome for development as i find it runs faster than firefox and ie so it is a bore to guess where the icons are,in firefox and other browsers the background is much whiter and brighter than in google-chrome,as you can see ie 8 is about 5 times slower than firefox 3.6 and almost 20 times slower than google-chrome 9 at least when using sunspider tests,most likely your post value has one or more characters in it that are breaking the html of the page and ie firefox are being a bit more forgiving than google-chrome is,actually the fix was simple by changing the event to mousedown like that at least for firefox and google-chrome this works much better,this works perfectly well in firefox but google-chrome never downloads more than 10 files,on my computer firefox performs much better than google-chrome on these tests using jsperf,it is working fine for me on internet explorer and google-chrome but not for mozilla firefox;in case of firefox asking for username and password i am providing it but it is not working,google-chrome debugger gave some pretty lame info;firefox debugger was much clearer,although firefox is a better browser for developers than google-chrome cause it gives you more tools to work with,at least this is how it works for me in firefox;surprisingly google-chrome is much worse in it from my experience,"
"firefox","google-chrome","taller in higher smaller bigger,more strict in strict ie configuration,pickier in more-so pickier,forgiving than  in forgiving error lenient, calculated not overall,significantly faster in faster slower browser, doesn in message handler body,more overall,faster in faster slower browser, did well in memory responsive high, it also overall,","4134107,7377302,28008564,55376251,17834786,7221963,19922990,3288456,30830253,27277070,12843558,","currently with the 10px top padding the cells in google-chrome look taller than in firefox,firefox and ie9 require a proper domain to run from presumably because they are more strict than google-chrome in the way they associate the localstorate object to a domain in google-chrome it doesn t need to be a domain as such,apparently google-chrome is pickier than firefox,google-chrome and other chromium based browsers are known to be more forgiving than firefox and others,in firefox i m using v20 the dropdown position its top left at the bottom left of your while google-chrome set its top right at the top right of the select positioning itself above the;plus firefox use the inherited width the one firefox calculated not the one you set of the to set the dropdown s width thus it s overflowing outside of the window,on google-chrome it s significantly faster to do this using 1 0 but on firefox it s slightly faster to do this using bool,google-chrome doesn t provide the response code or body if it s a 400 code error;firefox provides the response code but not body,firefox produces more space compared to google-chrome,here is a simple performance test showing that in google-chrome filling is faster than clearing i am not sure what goes on with google-chrome and canvas nowadays but in firefox clearing is many times faster than filling both significantly faster than google-chrome,google-chrome did well and was the most responsive;firefox did not as well as google-chrome high memory consumption but worked,i tried several tests of google-chrome v ie9 and google-chrome would not work under any configuration with the root password set;i also tested firefox it also worked fine the issue is only with google-chrome,"
"firefox","google-chrome","general overall,better in better time tools, is slower but still in seconds data stricter, does not overall,more strict in forgiving error lenient, is not in faster slower browser,slower than  in faster slower browser, is more in higher smaller bigger,more finnicky in forgiving error lenient,thinner in higher smaller bigger,bigger in higher smaller bigger,","43891268,30315914,43566160,47863428,35358677,29248433,47943274,51781208,43724970,12303054,11542460,","with my pc i also see difference between firefox and google-chrome;the result of google-chrome looks smoother,my knowledge of firefox is better than google-chrome so my apologies if i missed something,in google-chrome it takes about 3 seconds on my machine;firefox is slower but still executes one million if statements in about 0.2 seconds and ie one million if statements in 0.1 seconds,i will have stick with google-chrome for profiling our app;after further looking into this i came to the conclusion that firefox does not support runtime analysis for web workers at the moment,is this due to firefox being more strict than google-chrome,you can record your test in ie and play it across firefox google-chrome using selenium components for coded ui cross browser testing which can be found here;recording on firefox google-chrome is not supported by coded ui,why google-chrome on canvas operations is consistently slower than firefox,for some reasons firefox only accepts first request and the rest is denied due to cors problems meanwhile google-chrome is more to be 50 50 - some request go thru and return data from backend properly but some are blocked,in general i have found google-chrome to be more finnicky than firefox,even then google-chrome s fonts look thinner than in firefox or ie,here the working header request with firefox bigger than google-chrome request,"
"firefox","google-chrome"," is position unavailable meaning probably overall, which uses a lesser in better time tools, just launches more overall,easier in easier tool extensions,smaller in higher smaller bigger, indexeddb is slower in faster slower browser, is faster overall, uses a more in user folder file, using -webkit-background-clip text in higher smaller bigger, doesn overall, is more in different looks browsers,","9105666,7483347,6092869,14747312,12990034,49376927,22298391,40479283,3232713,46149428,52248534,","your script seems to be working in google-chrome but not in firefox;the error i am getting in firefox is position unavailable meaning probably that firefox s location service doesn t have access to my wifi network,the open source browsers are webkit which is used for safari and the mozilla flavoured firefox;there is also chromium which is the open source version of the google flavoured google-chrome which uses a lesser javascript engine squirrel as opposed to v8 and does not have the multi-separate-thread modularisation of google-chrome,firefox 4 has started doing the same thing in fact if you look in your task bar you ll see a plugin-container.exe process for firefox when flash is running;google-chrome just launches more chrome.exe processes,with firefox it was very easy i just grabbed the apk from their ftp server run the adb install command one would think it should be even easier with google-chrome but it s really not,google-chrome renders passwords smaller than firefox,in both firefox and google-chrome indexeddb is slower than localstorage for basic key-value insertions and it still blocks the dom,i think it mostly depends on the interprter engine in browser which responsible for interpreting th scripts which server sends to your browser and another thing about firefox as you have mentioned the extension in firefox is also responsible for the extension in firefox and another thing which i think matters a lot the developement of the browser as you can see google-chrome has same ui on all windows linux mac since the extension in firefox is mostly written in java so the extension in firefox requires jvm to run hence it is slow as compared to firefox which is written separately for each os using the respective oss apis hence it is fast since it directly interacts with os than interacting with jvm which in turn interacts with os like in case of chrome.still google-chrome is faster in incognito mode because of the extension in firefox intelligence to keep frequently data cached if you ll see the temporary files created by google-chrome you ll see the temporary files created by google-chrome are huge in size as compared to that of firefox,if using firefox you can delete or modify the default download behavior by file type;google-chrome uses a more simplistic approach it just downloads to the destination set as your download folder in your google-chrome settings,if you only need this for a small amount of text like a heading you should be able to achieve the effect in safari google-chrome using -webkit-background-clip text and -webkit-gradient;firefox doesn t support this yet.,here is a snippet which only works in google-chrome showing a few examples of weirdly accepted and refused mime types;firefox doesn t respect the mime type and doesn t triggers its magic number s checking on bloburis.,comparing the logged values on firefox and google-chrome console should demonstrate the different behaviors;i should also mention that according to my experience knowledge firefox is more standards-compliant than other browsers,"
"firefox","google-chrome","more picky overall,smaller in fine access mozilla,faster in faster slower browser,longer in extension addict advanced,less width in higher smaller bigger,testet with  in user folder file,successful with  overall, doesn overall,easier in better time tools, launcher doesn overall, is honoring the img overall,","35236659,23072956,23763890,3591151,19814480,49915381,49634173,15984559,41484342,19261409,33811151,","i had a hiccup in firefox but found a fix after figuring out firefox is a bit more picky than google-chrome when defining svg attributes,my question is similar the same page in my pc renders fine firefox google-chrome but the same one on a server rendered smaller by firefox google-chrome ok,i am not concerned about the fact that firefox runs faster as browser js implementations will vary so much as the wide spread of results encountered in google-chrome that makes getting an accurate result impossible,this is due to the simple fact that firefox has been in the extension business longer than google-chrome or ie,firefox has like 40px less width than google-chrome in columns which is causing problems,here is what i came up with this is how the downloaded foobar.xml file looks then testet with google-chrome version 65.0.3325.181 firefox 59.0.1 and microsoft edge 41.16299.371.0,i am using redhat 7.4 and i ve been through a lot to uninstall and install google-chrome and firefox got partially successful with google-chrome and this is what i tried,thus when you login with google-chrome wp sends a login cookie to google-chrome starting a new session;when you then access the site with firefox firefox doesn t have a login cookie so wp prompts you to login and then sends a login cookie to firefox also starting a new session,firefox is easier and better than google-chrome in finding the element from the webpage,the google-chrome launcher also has the same issue but you must have it installed in the appdata folder of your profile which is the first place the google-chrome launcher checks;the firefox launcher doesn t take into account 64 bit windows,google-chrome is honoring the img s percent max-width when computing the minimum intrinsic size of the flex item;firefox is not,"
"firefox","google-chrome","always higher in higher smaller bigger, doesn overall, doesn in internet explorer global,more forgiving in forgiving error lenient, 64+ not in better time tools, sometimes has better compression overall,older in older newer version,slower in faster slower browser, works more overall, is simply more overall,more characters overall,","7676281,11167187,12282599,12391385,54162786,34257361,40642495,16639931,54949272,11564150,19754809,","the position of the list in firefox is always higher than that of google-chrome no matter how i padding or margin the list,code that relies on the trick does not work in google-chrome because google-chrome does not support the inline processing of xml within javascript;this is a perk of firefox which supports e4x while google-chrome doesn t,internet explorer and google-chrome automatically make global variables for dom elements with id values;firefox doesn t because no standard says it should and it s a goofy idea anyway,i guess google-chrome is a bit more forgiving than firefox,your example would then remain the same only for the fieldset selector you have the following css properties edit flexbox on element is supported only on firefox 64+ not yet supported on google-chrome;i found this after posting and testing to see if it works in google-chrome as i tested it only on firefox before posting,note that it provides better results smaller size only in google-chrome;firefox sometimes has better compression than canvas-png-compression as for 0.0.3 version,i would like this function to work in firefox ie google-chrome etc but at the moment it only works in google-chrome and older than firefox version 49,why is firefox so much slower than google-chrome on my laptop,google-chrome prioritises sequentially for this reason while firefox works more in a parallel fashion,google-chrome is correct in rejecting it;firefox is simply more forgiving,firefox encodes more characters than google-chrome,"
"firefox","google-chrome"," is not in post cookie iframe,higher than  in higher smaller bigger,less close overall,less pleasant overall,better in better time tools,higher than in  in higher smaller bigger,higher baseline in higher smaller bigger,more strict in faster slower browser,older issue in older newer version,objects better in better time tools, issue not in post cookie iframe,","48659153,48523291,32568215,21798628,12899440,14882752,14870041,2821210,32256754,39372828,56787379,","google-chrome is not adding the cookie for post requests;in google-chrome firefox the was ltpa2 token cookie is not set for xmlhttprequest,the reason is that firefox find the textarea somewhat higher than google-chrome does and as such there is no space for the footer,btw google-chrome is less close to w3c specs than firefox de 42+ i d opt to start using firefox for your initial testing and then test it in ch to detect quirks,i m trying to add a ribbon to a page using css3 linear-gradients but the rendering in google-chrome looks a lot less pleasant than its firefox or ie alternative,transparency is also an issue however google-chrome seems to handle all three better than firefox,so i researched a bit and fortunately tooltip delay in firefox was much higher than in google-chrome s easy to fix,ie firefox higher than baseline google-chrome on baseline,google-chrome s parsing may be more strict than firefox is at having things like trailing commas as specified by the standard at as stated in danilo celic s answer,i have encounter a problem with google-chrome that is similar to an older issue with firefox at this url scrolling blocks javascript execution in firefox,performance is browser and device dependent for example firefox handles objects better than arrays while google-chrome prefers arrays,google-chrome displays the disappearance of the cookie immediatly;for me it seems this is an firefox issue not updating the developer information correctly,"
"firefox","google-chrome"," and doesn in forgiving error lenient,larger in larger text snippet,slower in faster slower browser,taller in higher smaller bigger,faster in faster slower browser,less overall,larger in higher smaller bigger,larger in higher smaller bigger,much as  in seconds data stricter,s example with  overall,more sluggish in better time tools,","44562130,5780924,16481858,6557784,4213247,46691875,15382071,8840940,49505333,54839707,14176911,","above example works in google-chrome and doesn t in firefox;firefox is throwing error as it doesn t find event parameter in svgelem.onmousedown,this is because in ie and firefox the footer link text is larger than in google-chrome and safari therefore the margins that i ve set up for the icons do not work,using firefox even it s slower than google-chrome it s still more respectful of privacy,by highlighting the shape of the two elements you can see that in firefox left the button is 2px taller than in google-chrome right,right now google-chrome is faster than firefox sunspider tests so your statement it loads faster in firefox is not really correct,but firefox keeps being less responsive than google-chrome or ie,arabic default font in google-chrome is larger than it is in firefox,i am checking it in firefox and google-chrome and see that in google-chrome spacing between letter is larger than in firefox,it seems like if sockets are disconnected by the server after one minute of inactivity that would apply to ie and edge just as much as google-chrome and firefox,as others have mentioned in the comments try selenium you ll need google-chrome or firefox driver installed here s example with google-chrome to get you started for more instructions see documentation,firefox on nexus 7 is a bit better at producing reasonable sample rate readings but the updating of the display is even more sluggish than google-chrome,"
"firefox","google-chrome"," version is much better in better time tools,general overall, solution seems easier in easier tool extensions, is more in internet explorer global, has similar behavior in behavior edge visible,slower in faster slower browser, does not overall,earlier overall,smaller in higher smaller bigger,fine with  in fine document jquery-ajax,bigger in higher smaller bigger,","21286020,19355167,54710896,10899741,24645106,23311171,4267506,1996484,11778005,57711081,7229568,","there is a firebug version available for google-chrome firebug lite but the firefox version is much better and i d highly recommend you at least try a firebug version available for google-chrome firebug lite out,basically safari google-chrome opera is used in mobile devices and device features works fine with these;firefox browser is not a better option for mobile devices,for firefox there is browser_console;from first look the google-chrome solution seems easier,when a cookie has a bad date from an de-synchronized host machine which happens frequently in my virtual machine environment google-chrome correctly rejects it which causes the php session variables to be lost;firefox is more lenient,as noted in that bug page google-chrome has similar behavior on other special elements like and though it does happen to accept custom display styling on for some reason;firefox does not in any case. to fix this you need to set display flex on a wrapper-div inside of the instead of on the itself,do you have any idea why google-chrome loads javascript function much slower than firefox,this would probably explain why google-chrome allows the xhr but not firefox;google-chrome considers the subdomain part of the same origin but firefox does not,i suspect that this may be the case because ie and google-chrome need to know about the content of the div earlier than firefox,in google-chrome the font appears to be 1px smaller than firefox and ie and i cannot see why,when we re trying to load our discovery document with ie11 we get the following error script 7002 xmlhttprequest network error 0x4c7 the operation was canceled by the user. we have an angular app where the user has to authenticate himself before he can use the website. in our angular app we use the angular-oauth2-oidc library where the error occurs when the loaddiscoverydocument method is called. the app as well as the loading of the discovery document works perfectly fine with google-chrome and firefox,why height in google-chrome is bigger than firefox of input,"
"firefox","google-chrome","console more information in easier tool extensions, did not in fine access mozilla, it doesn overall, is more in forgiving error lenient, is stricter in higher smaller bigger, have shipped html in forgiving error lenient, does well; is more overall,faster in faster slower browser,slower in faster slower browser,higher security in higher smaller bigger, does not in higher smaller bigger,","21147622,54189679,50562139,48877569,18856869,42202002,50692770,3164817,10706070,28196310,5629571,","the google-chrome console gives more information than the firefox console,it worked fine in google-chrome but had no response using firefox;turns out firefox did not trust the self-signed security certificate,in google-chrome it happens at zoom level 125 on a standard 1080 screen;in firefox it doesn t happen at all on any zoomlevels and edge has it at various zoom levels,another attempt of mine is to open it in a modal iframe with angular-material setting up the src with the xml data both tests opens the xml with the url but with this error error loading the stylesheet unknown error 805303f4 i have test it in firefox and google-chrome and google-chrome is more clear with the error details in the console unsafe attempt to load url route-to-xsl.xsl from frame with url data text xml .,in google-chrome you can supply none or only one parameter to addeventlistener and no errors will be reported;however firefox is stricter and will spit out the not enough arguments error,no browsers other than google-chrome have shipped html imports support;firefox doesn t support html imports unless you enable the dom.webcomponents.enabled flag,it sounds like work should be on google-chrome for you since you use firebase and other important things that google-chrome does well;firefox is more than capable for music and hangouts and non-dev stuff,also firefox 4 has hardware accelerated canvas that is marginally faster than ie9 and a lot faster than google-chrome,and also does anyone know why is firefox so much slower than google-chrome in 3d rendering,found that the issue stemmed from using the firefox webdriver which apparently has much higher security than its google-chrome counterpart,when a button element is left to the native style remove the border line-height is ignored by both browsers weirdly google-chrome also ignores the height but firefox does not;as soon as the button is custom-styled google-chrome picks up the line-height but firefox does not,"
"firefox","google-chrome","lower in better time tools,consistent with   in behavior edge visible, has tighter security overall,better performance in better time tools, element worked only overall, before finding a solution in faster slower browser, and not in behavior edge visible, is more in forgiving error lenient, is smaller in higher smaller bigger,more in higher smaller bigger,faster in faster slower browser,","18394282,49744903,17812053,39681968,57733609,23640336,32604219,48777275,48522568,14686908,27318095,","phantomjs achieves a better accuracy than firefox slightly lower than google-chrome but in around half the time of firefox,however the found that the behavior is not completely consistent with google-chrome firefox and edge either,if i run this in firefox it works;google-chrome has tighter security restrictions for loading scripts from local file systems,of late firefox has had much better performance than google-chrome,i had a similar issue but the headless worked in firefox and not google-chrome;the xpath for the google-chrome element worked only in headed mode,google-chrome would not display the files and ie compatibility mode would corrupt them;i did not test it in firefox before finding a solution,as mentioned in the question itself this behavior is visible only in google-chrome and not firefox for reasons unknown to me;firefox produces an output similar to the expected one even when a extra padding + margin is added to the body and b when the image itself is wrapped inside another container which also has padding + margin,google-chrome is more picky about js errors than firefox,the footer height is set to height 40px and in google-chrome is ok however in firefox is smaller around 35px,the issue that i encountered was that google-chrome was giving me height about 300px more than the firefox ie,for example using renatoargh s test google-chrome is faster with option 2 firefox with option 1,"
"firefox","google-chrome","uniform with  in behavior edge visible,more overall,shorter than in  in faster slower browser,slower in faster slower browser,greater than on  in difference fires greater, does not overall,worse in faster slower browser,faster in faster slower browser,general in forgiving error lenient, is much more restrictive however in faster slower browser,better imo in better time tools,","52489012,38297450,53803274,20740967,49976270,33965054,39862190,20655397,54049759,49822000,23245240,","this is a shorthand rule that breaks down to flex-grow 1 flex-shrink 1 flex-basis 0 source 2019 update since the posting of this answer in 2018 it appears that google-chrome s behavior has changed and is now uniform with firefox and edge,firefox can get away with more than google-chrome before aborting,in ie it is shorter than in google-chrome and firefox,spidermonkey firefox is slower 50 of google-chrome max speed but the speed is consistent,interestingly on google-chrome the difference is much greater than on firefox as i just tried out,google-chrome does not implement this yet;firefox has an implementation of the identity provider part of the spec which seems to be enabled by default,firefox gsap performance is still much worse than in google-chrome but google-chrome gsap does still lag every few repeats or so while in google-chrome the css animations do not,now when clicking a time less than 1 millisecond will be displayed however it obviously takes nearly a second on my computer until the changed color actually is displayed where btw. google-chrome seems to be faster than firefox,to find the unique css selector i would recommend using google-chrome or chromium i am yet to find how to get the css selector in firefox,still if you want do that you could build a browser extension - firefox extensions and ie extensions can access local resources;google-chrome is much more restrictive however,google-chrome sends back and html collection that is longer really better imo than firefox ie or chromium,"
"firefox","google-chrome","better in better time tools, does not do so in faster slower browser,larger in higher smaller bigger, does not overall, does however in higher smaller bigger,slower in faster slower browser,faster in faster slower browser, benefited a bit overall, does not in higher smaller bigger,smart than  overall, was even more in forgiving error lenient,","45403936,15385339,24077137,39299284,35546457,22182454,18351894,45274720,24945814,57136103,582486,","firefox works a bit better than google-chrome but the settimeout just seems to delay the final result being shown rather than delaying each step,i guess ie or google-chrome starts redirect processing right after;it knows it s a 307 redirect while firefox does not do so until it,i ve found that the font-size in firefox is a bit larger than in google-chrome,google-chrome considers the key to be the meta key while firefox considers it to be the os key;thus firefox does not set the metakey property to true because from firefox s point of view the meta key is not being pressed,when looking at the headers for each you can see that googleadservices doesn t send any cors information which should cause google-chrome to not load the resource;this is what firefox does however google-chrome seems to require the attribute to properly block,also google-chrome s map function is up to 2x slower than firefox on my machine,in my experience google-chrome will be a lot faster than firefox to debug huge js files,firefox benefited a bit less dropping from 5600 ms to 4800 ms;anyway the combination of those two tweaks gave me a 33 speed boost in google-chrome and a 14 boost in firefox,google-chrome adjusts the file size to 0 when you delete a file;sadly firefox does not do this,i m assuming google-chrome will be a little more smart than firefox and in the bookmarks change that link to,i have already run into problems with old javascript code which i thought was standards compliant by firefox or opera standards - which are pretty good but google-chrome was even more picky,"
"firefox","google-chrome","higher in higher smaller bigger, was faster still in faster slower browser, will show warnings long overall,typeof slower in faster slower browser,more detailed in easier tool extensions, but not overall, couldn overall, in almost in faster slower browser,choose between  overall, does not overall,info than  overall,","11648488,23417828,30911294,20680334,39137371,18596158,7015700,53084314,48262240,57676956,22248369,","i know different engines render fonts differently and but the issue i m having is that firefox renders the font higher than google-chrome - at the size i am displaying the font this is very noticeable 3 or 4 px - which on a button means they re totally misaligned,i don t actually know how long but the asm.js optimizations are new enough that when i last tried the asm.js optimizations in early april 2014 the asm.js optimizations was significantly slower on the command line with node.js than in the browser with google-chrome and firefox was faster still,google-chrome will show warnings long before that but still accept them;firefox will not accept sha-1 after january 2017,in google-chrome typeof is slower than the other two but in firefox it is the fastest,use firefox network in development tools and check your requests sometimes it just sends options request in cross origin i insist on firefox because it s network tool is much more detailed than google-chrome and safari,i tested the nverba fork of that code which updates the code to reflect the improvements in the visualization api and it functions in google-chrome but not firefox;since the code used to work in firefox i assume that either an update to firefox or the canvg library broke functionality in firefox there s nothing specific in the visualization api that would cause functionality in firefox to break,google-chrome is apparently reads my mind.;since my success callback was part of a json object firefox couldn t find whatever default it was looking for and therefore didn t fire anything,you re using firefox which is slower than google-chrome in almost all real-life applications,now i can choose between google-chrome and firefox but choosing google-chrome will bring up the maps app,according to the discussion about this google-chrome bug google-chrome has a hard limit of 500 peer connections and closed connections are also included until they are garbage collected by the browser;it seems like firefox does not have the same problem,debug with firefox using firebug debug will give you more info than google-chrome on this type of error,"
"firefox","google-chrome","security fussier in security fussier resources,earlier overall,general in easier tool extensions, but not overall,better in better time tools, not overall,greater in higher smaller bigger,further overall,faster in faster slower browser,more overall, shows better in faster slower browser,","20009049,4265710,54228975,37446769,17510122,52173134,13198764,9492029,5299127,6222562,27070735,","i ve learned partly by trawling this site that the google-chrome security is fussier and the app loads correctly without errors in firefox and ie but i can t find any resources that are loaded from a non https source,if google-chrome seems to finish earlier than firefox then that is the freezing glitch in action,for more information i d recommend googling google-chrome developer tools or firefox developer tools depending on your browser,after playing around with different sticky and fixed footer solutions i found that applying a flex of 0 0 auto to the footer to let google-chrome act the same as firefox;it seems that with a flex-shrink of 1 google-chrome shrinks the footer when in this position where firefox doesn t and as the default value for flex is 0 1 auto this caused the footer to be mishapen on google-chrome but not firefox when in this position,bad buffering can also be a result of a slow computer because statistics show that google-chrome is better for a normal-fast computer and firefox works best on slow computers,i have to access through excel vba to a web that is only compatible with google-chrome or firefox not ie,i ve ran into out of memory errors on firefox with canvas heights greater than 8000 google-chrome seems to handle much higher at least to 32000,you can see if you look at the bottom and right hand edges of both google-chrome lines up perfectly but firefox seems further in both directions and i cannot work out why either in firebug or google-chrome dev tools,but i guess google-chrome i have version 10 should be much faster than firefox 3.6,i ve been trying to sum up values over a swipe and see how different browsers report values and they vary a lot with safari reporting order of magnitude bigger numbers on almost all platforms google-chrome reporting quite more like 3 times more than firefox firefox being balanced on the long run but quite different among platforms on small movements on ubuntu gnome nearly only +3 or -3 seems like it sums up smaller events and then send a big +3,however if you try to drag the carousel in google-chrome browser while it s animating you will see same jump lag problem;again firefox shows better and smoother animation with this library,"
"firefox","google-chrome","quicker even in better time tools, is better in internet explorer global, but not in different looks browsers, but does work overall, does not in forgiving error lenient,better in better time tools,better in better time tools,better in better time tools,lower in forgiving error lenient,faster in faster slower browser, will change the behavior overall,","18477622,55119589,5070003,51009362,32919699,3341954,25318564,12542302,46124506,5150147,36823113,","the most interesting thing i noted is that certain test combos in firefox are much quicker than even google-chrome,nvda is used more frequently in combination with firefox than with google-chrome or internet explorer because its compatibility with firefox is better,the mathjax fonts don t include u+25fb and so google-chrome is showing the missing symbol;it looks like firefox is finding the character in a different font and using that or perhaps you have stix fonts on the machine running firefox but not the one running google-chrome and mathjax is using that,webm works in google-chrome firefox not safari;i converted to mp4 mp4 h.264 which does not work in google-chrome firefox but does work in safari,google-chrome chromium allow folder upload when webkitdirectory attribute is set;firefox does not allow folder upload currently though user can upload multiple files at single file dialog when multiple attribute set set allow_dirs attribute at firefox,they use iframes in firefox which seems better but in google-chrome they do this by putting manually defined span and font tags,and additionally the algorithm that google-chrome uses is better than the one in firefox at least for handling the resizing of text,firefox handles this better than google-chrome does but neither exactly the way i want,open up the fiddle in both firefox google-chrome to see what i m talking about - in firefox the positioning of the background image is much lower than in google-chrome safari,the results varied from a browser to another firefox 4.0b12 is faster using but google-chrome webkit and opera is faster when using,the above extract would imply that the behavior in firefox is correct whereas the one in google-chrome isn t but as boltclock points out in this answer the spec s editor s draft has been updated and the updated text seems to imply that google-chrome s behavior is the correct one;maybe firefox will change the behavior someday but since this spec has not reached maturity it may take time,"
"firefox","google-chrome"," that adds more information in easier tool extensions,","52476811,","i m making a webextension for google-chrome and firefox that adds more information to github,"
"parsley","robotlegs","more overall, has fewer overall,","6585643,3392475,","parsley is more robust and adds a lot more features than robotlegs which is why its been used in the world s largest flex applications,robotlegs has fewer features and a smaller learning curve;parsley has more features and a steeper learning curve,"
"gwt","requestfactory","more complicated systems overall,slower in better counterpart db4o,better in better counterpart db4o,","4926459,14364885,9420697,","with requestfactory you pay an up-front startup cost to accommodate more complicated systems than gwt rpc easily supports,gwt requestfactory by design is slower than gwt rpc and gwt json etc,from gwt 2.1 requestfactory is better counterpart than gwt rpc when you use db4o together,"
"gprof","profiler","way better results overall,","6664948,","for time measurements use google s cpu profiler it gives way better results than gprof,"
"many-to-one","one-to-many","side much more overall,general overall,much easier overall,","25251777,55395960,3114169,","inverse is for bidirectional associations and most often it s on the same side with cascade but that s because the many-to-one side is much more efficient to control the association than the one-to-many one,and which fields should be made one-to-many many-to-one,fifth many-to-one is much easier to use correctly in nhibernate and i assume hibernate than one-to-many collection mapping,"
"boolean","int","greater than 0  in sum string greater,many member with  in string accepts object,longer last case in indexing faster small,better overall,more in can not error, once is even less overall,more time in time logic objects, isn overall,less than any  overall, len more information overall,general in can not error,","52257323,48207058,46043964,4545595,2469679,12879437,35320435,54950845,55454395,50007251,48187580,","in python 2 -1 string compares greater than 0 int - it raises a typeerror in python 3.x - and in both cases a non-empty string evals to true in a boolean context,i have third party object which contain so many member with int string and boolean,but wait it s not really constant for boolean arrays and why does int array indexing take longer last case than boolean array indexing even if it has to process 5 times less elements,also your examples about why int would be better than boolean are kinda flawed,from the link which is mentioned in question my question is that microsoft says the c# type keywords and their aliases are interchangeable but why we need aliases from my point of view boolean is more meaningful then bool and int32 is more meaningful then int then why aliases,but take care you should not use this approach to functions that are not uniform in argument set say one function has one int argument and another zero arguments - that would throw an exception;but given your conditions i d go with boolean if statements because checking a boolean once is even less expensive than calling a function through a variable,since the logic is the same i thought evaluating boolean objects takes more time than int equivalence true 1 and false 0 therefore i came up with the following benchmark and it turns out that i was correct,the scala standard library s built-in to-binary-digits string formatters tobinarystring for the int types byte short chars int and long are very limited;and an implementation for boolean isn t provided,for example any boolean evaluates as less than any int,for ignoring case boolean regionmatches boolean ignorecase int toffset string other int ooffset int len more information or try to create a regex pattern dynamically from one string and compare with other ...though not an efficient approach,can anyone help me out posting source code below model db creation typer converter getting error as error 492 29 error incompatible types object cannot be converted to string error 492 29 error incompatible types object cannot be converted to boolean error 492 29 error incompatible types object cannot be converted to int,"
"boolean","int"," using another custom overall,less space overall, which is easier overall,public booradio as  overall,more time in time logic objects, because right in can not error,more memory overall, enums not overall,better performance overall, indexing making my method more in indexing faster small,true more meaningful overall,","16961860,4528318,16031002,50694928,21107085,43054708,18152578,430261,30976199,48435149,19790845,","since this is an int and not a boolean like the checkbox expects you may have to use the grid events such as cellformatting to read the value from the data source and set the checkbox accordingly but i m not sure;or if this is an object you could expose the int value as a boolean using another custom property,keep in mind that depending on the use and on the system using it while a boolean takes less space because it s just a single bit depending on the implementation an int is the native word size of the hardware,if you want to map the range -128 to 127 to 0 to 255 i suggest you use an int which is easier to work with in java;as you have 32-bit or 64-bit registers using a short local variable doesn t help you as much as you might think unless you have a lot of t and i mean millions btw the byte code only allows for two sizes of local variables one slot boolean byte short float int reference and two slot long and double note reference uses one slot even on 64-bit jvms,to have it available within the code module only private booradio as boolean to have it available within the whole vba project all modules user forms and for the workbook and worksheet events public booradio as boolean when vba starts all boolean variables get initialized to be false likewise all int or long get initialized to be 0 string to be etc.,however the second loop causes the additional task of casting each of the values between a.length and 0 to a boolean which is more time consuming than int comparison,to be able to express what you were trying to do another operator that takes two bool operands would have to be able to do more than just boolean because right now your has a bool and int around another operator that takes two bool operands,int occupy more memory than boolean so the heap got corrupted,basically c has two main simple data types int and floating point numbers though various precisions;everything else boolean enums not simple but it fits etc,a boolean would most likely not yield better performance than int since the excel formula engine is dynamically typed,for a small number of selected elements int indexing is faster than boolean indexing making my method more efficient than tai s method even after tai optimized his code,boolean true is more meaningful than int or chars 1,"
"boolean","int","greater in sum string greater, format as overall, not in string accepts object, isn in type standards truth, is greater in greater data entry,faster overall,use  to model overall,array more in type standards truth, which read less nicely in space bits value,longer overall,compare  with  overall,","23406215,48280229,45656959,2638231,15028456,41089110,56759546,42517950,27182943,42760245,56461145,","if the boolean value is true and the length of the string is greater than the int value print the sum of the float and the int,if your coordinates are each n-bit int this should be more space-efficient than boolean format as long as your sparsity is less than 1 n - 3 or so for 32-bit,in this context position should be an int not a string or a boolean;compoundbutton.ischecked returns a boolean not a string so you cannot write,int isn t a boolean and if you want for some reason to store boolean as int then you should hone your own standards of how the truth and not truth are represented;boolean true is not a number it is an instance of the boolean type,in the key flag you convert the value of the flag route data entry to an int and then to a boolean depending if an int is greater than zero,the int version seems to execute much faster than the boolean one,solving int constraints is significantly harder than solving pure boolean constraints and when you use int to model boolean like this the underlying solver simply explores the search space that s just unreachable,but counting the number of trues in a boolean array is more difficult since the accumulated type int differs from the source type bool,the thing is you re not using bits but int;so either you want to revert to using boolean which read less nicely or you want to use some expression like 0 if x else 1 to flip your elements,so boolean operation is quite longer than for int or float,looks like you re now trying to compare boolean with int,"
"boolean","int"," but not in type standards truth,more efficient in space bits value, doesn in type standards truth,better in type standards truth,more overall,less in can not error, is faster overall, indexing if efficiency overall,less in space bits value,less space in space bits value, indexing is faster in indexing faster small,","2215783,7221963,54600914,35052403,29899349,20412664,35732564,54356405,22715785,23069431,48435149,","so yes it seems that the ternary operator is causing the constants to fix their types as int and disable the implicit type conversion that you would otherwise get from constants that fit within the smaller type;i m using vs 2005 for and i can reproduce for bool boolean but not for true,i always assumed that boolean were more efficient than int at storing an on off value - considering that s their reason for existence,vartype function returns an int indicating the subtype of a variable;isnumeric returns a boolean doesn t tell you what the type of data user entered only if it passed or failed numeric check,perhaps using int type if values are all numeric would work better than boolean for a and b,since your patterns are just 0 and 1 values you can think of them as numeric values long is a 64bit int which is more than enough which can be converted later to a boolean matrix,which is invalid as a boolean cannot be less than an int,still int is faster;so what about creating the boolean mask,performance note for larger data sets you may find int indexing more efficient than boolean indexing if efficiency is a primary concern see efficiently return the index of the first value satisfying condition in array,a boolean takes less space than an int,it is saying this because using the int to store the value 32 bits is going to use less space than a boolean 256 as each allocated boolean will take up 8 bits 8 256 2048,then method 2 significant performance improvement this uses the same idea expressed in the last section of my answer comparison to tai s method that int indexing is faster than boolean indexing for small number of expected elements to be selected and avoids creating an initial index at all,"
"boolean","int"," bingos is greater in greater data entry, based solution overall,more in can not error, aren in string accepts object,more space in space bits value, is false so you even overall, is less in sum string greater, makes cleaner overall,better overall,","35220300,8866125,28614411,48032098,18429507,18183724,52783327,53681490,38837450,","this would require you to get rid of the boolean;you dont really need it either as you can always check if int bingos is greater than 0,the pure int solution is faster by a factor of two compared to the math.ceil solution;thomas proposed an int based solution that is identical to the one i have above except that the one i have uses a trick by multiplying boolean values,you can encode up to 32 boolean in an int as follows the method won t error on more than 32 boolean s but won t be able to unpack correctly,so int accepts a string or a number;boolean aren t strings but they are in fact numbers,would a boolean array of size 32 take more space than an int variable for example,update answer for your comment in case you have non-boolean value or default value zero for int do not fit your requirements with defaultifempty you can provide own default value if there is no items matching your condition;default value for boolean is false so you even don t need any conditions - just select first or default isdefaultlimit value,the entire program is meant to take any number of int and first find the sum and average then print the square root of each int then print a boolean true or false if any int is less than 20 and finally print a boolean true or false if any int is between 10 and 90,i made some minor changes -use enum instead of 4 boolean makes cleaner code -do not use me.refresh it s slow and does things you do not need here -it s better to use not two int lists but one point list hope it helps,there is one case when boolean or int works better than boolean and int,"
"rythm","velocity","faster in benchmark normal page,faster in benchmark normal page,faster in benchmark normal page,","9926106,13906205,9596890,","rythm is a high performance 2 to 3 times faster than velocity pure java template which use razor like syntax,rythm is a strong typed java template engine using razor like syntax with high performance 2 to 3 times faster than velocity and fm,the benchmark shows rythm is 2 to 3 times faster than velocity on a normal page,"
"filewriter","printwriter","more effective overall,better overall,","21198091,20182239,","printwriter is more effective than filewriter and filewriter is needed anyhow but this will work also if you want to do multiples people then just for-loop it and dont forget to close,i m not sure if printwriter or filewriter is better but printwriter worked for me,"
"cpython","pypy","worse than  overall, is slower because here overall,better overall,faster overall,more overall, does not yet fully overall, also becomes fairly in faster slower jit, 3.5 is slower in faster slower jit,faster in faster slower jit,higher recursion overall,faster in faster slower jit,","56637652,31826941,18958181,11125957,36091217,12867428,8797731,40708984,8452592,7541324,34837400,","interestingly pypy is worse than cpython with __slots__ and stays stable for extreme field counts,the answer is simple here - pickle on pypy is slower because here - pickle on pypy s implemeneted in pure python as opposed to c in cpython,if pypy succeeds to be better than cpython in general which is questionable the main weakness affecting its wider adoption will be its compatibility with cpython,i finally ran it through pypy and was delighted to discover that when the lists got really deep pypy was running significantly faster than cpython,further the relative speed of profiled code may well differ hugely between them - pypy code is low-level and so introducing profiling is likely to slow it down relatively speaking more than cpython,cpython s reference counting scheme for memory management arguably has more predictable performance impacts than pypy s various gc systems although this isn t necessarily true of all pure gc strategies;pypy does not yet fully support python 3.x although that is an active work item,so how is it possible for pypy to be faster than cpython also becomes fairly obvious,it also clearly demonstrates that cpython 3.5 is slower at this than 2.7 which is sad but expected;pypy is not only a solid 5x faster than either of them but all three algorithms perform equally well,pypy is now faster than cpython in most cases,pypy has a higher recursion limit than cpython normally,pypy which in general is much faster than cpython is considerably slower for this use case,"
"cpython","pypy"," has much better overall,faster in faster slower jit,faster in faster slower jit,slower than  overall, s object model in things pypy support,more architectures in better ways important,much more overall,slower in faster slower jit, 2.7.5 as in module slower database,better overall,more complex in faster slower jit,","10820455,5843950,7991345,47666565,6675428,12867428,18958181,26515134,21270502,12575001,43456211,","it s very hard to do correctly with cpython;pypy has much better sandboxing capabilities,cpython is faster than pypy on the two tests slowspitfire and waf,i presume it s why is pypy faster than cpython some of the time,increment -3 to -2. 2 5 0 1 -2 - jump 4 steps forward escaping the maze. the code edit i compiled and ran the code with cpython that dropped runtime to 2.53s but that s still almost an order of magnitude slower than pypy,running pypy on cpython wouldn t help besides slowing things down terribly because the api wouldn t interact with pypy s object model it would interact with that of the cpython host environment;pypy has alpha-level support for the cpython extension api via an emulation layer called cpyext,cpython runs on more architectures than pypy and has been successfully adapted to run in embedded architectures in ways that may be impractical for pypy,secondly the current version of pypy consumes much more memory than cpython in a rather large set of cases,i haven t tried comparing the two but this pypy bug seems to suggest that multiprocessing in pypy is slower than in cpython,but for a really useful explanation you d have to tell us exactly which database and library you re using and which python version cpython 3.3.2 s csv module seems to be a lot faster than cpython 2.7.5 s and pypy 2.1 2.7.2 seems to be faster than cpython 2.7.5 as well but then either one also might run your code faster too and so on,now there are some implementations like pypy rpython which run a static-typing phase and should favor much better than cpython here,the pypy jit for python is also much more complex than cpython but also typically much faster â increased complexity is a fairly typical cost for speed. the four levels of disassembly for julia code give you access to the representation of a julia method implementation for particular argument types at different stages of the transformation from source code to machine code,"
"cpython","pypy","slower than the  in faster slower jit,slower in faster slower jit,faster in faster slower jit,generally faster in faster slower jit,quicker than  overall,faster overall,faster in faster slower jit,currently more in faster slower jit,faster integer-arithmetic in faster slower jit,slower overall,faster in faster slower jit,","9408295,26457906,6636272,7529473,53484314,19998762,10727166,9934528,45490403,18958181,2703428,","but not the other way around then the pypy runs hugely slower than the cpython interpreter,as gnibbler pointed out cpython is slower in the simple implementation but pypy is jit compiled for much faster code when you need it,pypy compiled with jit is almost always faster than cpython frequently by a large margin,i saw that pypy is generally faster than cpython,use pypy as interpreter it is up 6 times quicker than cpython if it is not enough use a quicker language than python,so at this point in time pypy is just over 9 times faster than cpython in this micro benchmark,you might want to try running your trainer under pypy if you aren t already -- it s significantly faster than cpython for some workloads,pypy is currently more than 5x faster than cpython on average,in the competing programming a lot of problems are never meant to be solved with cpython but with pypy which has a faster integer-arithmetic and a git-compiler but otherwise a python interpreter just as cpython,long-term evidence is showing that pypy runs certain python codes slower than cpython and this drawback seems to be rooted very deeply in pypy,just keep in mind most of the time c is faster than python but then again most of the time pypy is faster than cpython,"
"cpython","pypy"," uses just-in-time jit overall,faster in faster slower jit,less memory in memory data structures,faster than  in faster slower jit, is only overall,faster than  in faster slower jit,more overall,faster in faster slower jit, sum is slower in module slower database,better in better ways important,faster in faster slower jit,","10175756,14024189,5495318,619544,31538804,50776649,7067001,30679809,24399639,5923232,25363585,","the real speed difference comes from the fact that unlike cpython which is interpreting whole program as bytecode pypy uses just-in-time jit compilation into machine code for rpython parts;pypy isn t python interpreter implemented in python it s python interpreter and compiler implemented in rpython which is a restricted statically typed subset of python,if that isn t sufficient a lot of standard python code can be run on the pypy implementation which generally faster than the cpython implementation,when people talk about pypy using less memory than cpython this is a major part of what they re talking about,part of the compiler toolchain includes an experimental jit generator now in the compiler toolchain fifth incarnation and starting to work really well - the goal is for a jited pypy to run much faster than cpython,the documentation at is misleading the beta versions of vmprof on os x or freebsd are only available for cpython not for pypy;vmprof on pypy is only enabled on 64-bit linux right now,also pypy works in the vast majority of cases and is usually much faster than cpython,profiling is known to slow pypy a lot more than cpython,under cpython tests run 4 times faster than under pypy,also in cpython using sum lets your code run in the c internals so it can be very fast;but in pypy sum is slower than just writing a straighforward loop that the jit can turn into a wicked fast native loop,the waf benchmark has less of a pronounced difference in performance and i d guess that the answer would be more complicated some factors pypy does better some factors cpython does better and overall cpython comes out slightly ahead,unfortunately as martijn pieters noted there are no accepted solution for python 3.x and only one for python 2.x and according to the amount of memory spent for solving it numerix could have used psyco the library on which pypy is based much faster than cpython,"
"cpython","pypy","faster overall,better than  in better ways important,smaller overall,faster in faster slower jit,slower in faster slower jit, does not overall,faster in faster slower jit,faster in faster slower jit,faster in faster slower jit,faster in faster slower jit,faster in faster slower jit,","5843950,16536668,8675877,18949049,32777213,53062101,5318157,37192125,17483632,28404279,25713282,","judging from the benchmarks posted on the pypy speed center it appears as if pypy is faster than cpython for all but two of the tests presented,but pypy s still far from being better than cpython in a lot of very important ways,i know many of pypy s data structures are actually smaller than cpython s but again that has nothing to do with the jit,that site does not claim pypy is 6.3 times faster than cpython,this is why pypy may be slower than cpython sometimes it needs a warm-up phase in which it can actually optimize repeated operations,for example pypy does jit compilations cpython does some low-level tricks;no python cpython does not do variable inlining reason for this is that you can run your script with python -i non-optimized.py and you should be able to reach all vars,pypy s jit can make python code execute much faster than cpython,some people may argue with me on this one but i find pypy to be faster than cpython,however consider that pypy might do the linear search 100 times faster than cpython then a few times might be dozens,can someone please help to how use that script using the pypy as i heard it is much faster than cpython,note that there are plenty of python implementations other than cpython out there - for loopy code pypy tends to be much faster than cpython,"
"cpython","pypy"," is faster than either especially overall, doesn t support in things pypy support,slower in faster slower jit,faster in faster slower jit,faster in faster slower jit,more secure overall,less memory in memory data structures,faster in faster slower jit,faster in faster slower jit,faster in faster slower jit,slower in module slower database,","8500158,52082427,29966745,14294643,6726347,10588174,8675877,3163538,40708984,18047406,25681097,","oh and it can use cpython for it innermost loop or psyco - but pypy is faster than either especially on 32 bit systems,many things that are builtins in cpython are plain old python code in pypy;1 of course that isn t always an option pypy doesn t support 3.7 features yet there are a few third-party extension modules that are still way too slow to use it s harder to build yourself if you re on some uncommon platform so let s go back to cpython,jython is more unpredictableâ sometimes almost as fast as pypy sometimes much slower than cpython,pypy is supposedly faster than cpython while gevent is based on co-routines and greenlets which supposedly makes for a faster web server,you could try running it in pypy - for some cases it can be significantly faster than cpython,edit again one completely different approach would be to use pypy s sandboxing mechanism which should be much more secure than cpython plus a sandboxing module,if the dominant memory usage is program data structures then i wouldn t be at all surprised to find pypy using significantly less memory than cpython whether or not the jit was enabled,for python there is a pypy project which it includes jit making possible the code to run faster than in cpython in many cases,pypy is faster than cpython s sum intrinsic because it can figure out that all the elements of the array are numeric and slice out a bunch of per-element overhead,i ve tried using pypy because i ve heard its faster than the cpython interpreter but still no good,as mentioned by ifloop this would be running a cpython c extension module on pypy which often works not always but is slower than on cpython,"
"cpython","pypy","more overall,","45117672,","i did a search on the web but i was unable to find any evidence that suggests that pypy s memory usage is much more than cpython,"
"decimal","ieee","less than 0  overall,more complex overall,slower in useful precision 64bit, numbers cannot in useful precision 64bit, floating point number in useful precision 64bit,less in useful precision 64bit, might not overall, floating point number overall,more than 15  overall,less in useful precision 64bit,","55193139,46150959,10562448,49225330,45255249,26032849,25164304,10702338,50333529,5729100,","above number.max_safe_integer for example the precision is less than 0 decimal places to convert a number to a string with a fixed number of decimal places use .tofixed as epascarello suggested as for doing financial calculations some say you should never use ieee 754 floats such as javascript s number s although many of the largest companies in finance do just that,double has its ieee floating point definition which is also much more complex than decimal,decimal types libraries are fantastic for financial applications because we re used to dealing with the style of rounding required in financial stuff but there is the cost that they tend to be slower than ieee floating point,most decimal numbers cannot be represented exactly in binary format so they have to be rounded;ieee 754 defines several possibilities for this procedure i think you see this one citation from wikipedia,many decimal numbers cannot be represented exactly in this representation;so the compiler uses the nearest available binary ieee floating point number that is available,the maximum representable value with ieee 754-2008 binary32 is so the base 2 log of a number stored in binary 32 is less than decimal 128,if an approximate result is good enough and there are lots of calculations where it is you can use double but be aware that it s a binary floating point number and accurate rounding to decimal might not always be possible;although in certain cases it might work in general there is no way to determine force the decimal precision of a double value or indeed any ieee floating point number,decimal cannot use the fpu;you get best performance with float or real which map to a standard ieee floating point number which is supported by the fpu,note using parsefloat ieee 754 double precision floating-point means you have a maximum of 52 bits of precision which is a bit more than 15 decimal places,the third line displays the data with the maximum useful precision - an ieee 754 64bit floating-point number has slightly less than 16 decimal digits of precision so all those digits of the literal in math.h are pointless perhaps they can be seen as future-proofing against a possible future redefinition in a format with more precision,"
"cakephp","codeigniter","slower overall, which is simpler overall, seemed much more overall,somewhat harder in better great thing,lighter overall,more complicated overall,lighter overall,lower learning overall,newer overall,much more lighter overall, gives you more overall,","7222395,8768026,2579251,6403035,139120,4095770,6021229,3794813,3015992,14226938,110605,","cakephp is considered slower than codeigniter but you can tweak it to enhance speed,for that you would use a php framework;i would suggest starting with codeigniter which is simpler than zend;i don t have experience with cakephp so i won t comment on that.,i don t know too much about codeigniter but i remember ruling it out when i was evaluating frameworks myself about a year ago;cakephp seemed much more developed at the time,why do people say cakephp is somewhat harder than codeigniter for people new to mvc,i suspect codeigniter doesn t have quite as flexible a structure it s smaller and lighter than cakephp but a quick look at the cakephp manual to see how behaviors components helpers and the vendors folder may be helpful,i have been reading about cakephp but it seems a bit more complicated than codeigniter,i personally use codeigniter which probably falls in to the heavy framework category but is at least much lighter than cakephp and lighter than zend too i think,codeigniter has the lower learning curve overall so i would recommend it over cakephp,cakephp is newer more feature rich and heavier than codeigniter codeigniter is designed to have a much smaller footprint so you will most likely find yourself creating functionality in codeigniter to match cake s,in my view cakephp seems much more lighter then codeigniter but i never used either one so i can t judge,i tested cakephp but it s too much a la rails style and i didn t like it;codeigniter gives you more freedom to do whatever you whant,"
"cakephp","codeigniter","better than  in better great thing, felt much more overall,more experienced overall,","52881,855268,8779290,","also i don t think it is so much that asp.net mvc is so much better than cakephp or codeigniter ruby on rails etc the great thing about asp.net mvc and other asp.net-based technologies such as monorail is that developers who are using asp.net now have the option of following the mvc pattern using tools and languages developers who are using asp.net are familiar with,cakephp is a good ruby on rails imitation;i found codeigniter felt much more like php in a good way it s documentation is also my favourite of any project i ve used,in my opinion it will probably be easier to integrate with codeigniter although someone more experienced with cakephp might prove me wrong,"
"concave","convex","difficult than a  overall,simpler overall,","6941020,14583110,","but determining a concave hull is far more difficult than a convex hull,the check for convex polygons your triangle is simpler than for concave ones see first linked article,"
"mergesort","quicksort","better in simpler in-place parallelizing,more space in worst comparisons complexity,better in worst comparisons complexity, poses much more in simpler in-place parallelizing,generally more overall,quicker in faster algorithm slower,better in simpler in-place parallelizing,better in worst comparisons complexity,more complicated overall, generally performs better overall,much faster in faster algorithm slower,","497819,5784610,9023456,50361950,24173092,42183380,19853740,35228495,680613,3685845,8535540,","previously discussed on so why is quicksort better than mergesort,mergesort may use more space than quicksort i m not entirely sure and merge may be better for linkedlists,for instance quicksort can outperform mergesort although mergesort is provably better than quicksort in the worst case,i generally recommend starting with bottom-up mergesort in haskell but heapsort isn t a bad choice either;quicksort poses much more serious difficulties,also note that quicksort is generally more optimal than mergesort see this as well which explains why it s taken advantage of when sorting primitives,that way quicksort can reach recursive base case more quicker than mergesort,is quicksort always better than mergesort,for example locality of references has influence on cache hits or misses which is the reason why quicksort performs better than mergesort,quicksort is also more complicated than mergesort especially if you want to write a really solid implementation and so if you re aiming for simplicity and maintainability merge sort becomes a promising alternative with very little performance loss,quicksort generally performs better and takes less memory but many examples have worst-case performance of o n 2 which almost never occurs in practice;mergesort doesn t perform quite as well but is usually implemented to be stable and mergesort s easy to do part in memory and part on disk that is use the built-in sort function,i have read that quicksort is much faster than mergesort in practise and the reason for this is the hidden constant,"
"mergesort","quicksort","more overall, is faster even in faster algorithm slower, which does not necessarily in faster algorithm slower,better overall,quicker overall,faster in faster algorithm slower, does but it does also overall,faster in faster algorithm slower, is faster in faster algorithm slower,algorithm faster in faster algorithm slower, is done in place in faster algorithm slower,","47536371,1560687,57014989,497967,497794,90477,47731412,29218730,19106889,20811630,35189561,","i do know though that quicksort has more compares but less swaps than mergesort which i learned from another stackoverflow discussion quicksort vs merge sort,quicksort is a partitioning sorting algorithm you might refer to mergesort which also is a partitioning sorting algorithm the biggest difference is probably the speed quicksort is faster even though both of the biggest difference are o n log n,quicksorta performs marginally better than mergesort for large datasets on my system but performance will depend on the actual implementation of quicksort which does not necessarily use a quicksort algorithm,the biggest difference that can be produced between the two of them will always be to quicksort s detriment and it involves lists that are already largely sorted or contain a large number of ties when quicksort does better than mergesort the difference will not be nearly so great,i had been taught that quicksort is almost always quicker than mergesort and i understand that there is some debate on this topic but i at least expected it to be closer than this,as many people have noted the average case performance for quicksort is faster than mergesort,quicksort is faster than mergesort because it has no loop in it s recursion where mergesort has to copy it s element in aux array and one more thing o nlogn is number of compares that mergesort does but it does also 6nlogn array acceses,it might be helpful to see why quicksort is usually faster than mergesort since if you understand the reasons you can pretty quickly find some cases where mergesort is a clear winner,quicksort this is a bit tricky to implement efficiently with lists but it is possible;i won t discuss it because it s not a good early programming project and mergesort is faster in many cases,the quicksort algorithm is faster than mergesort which is what sorted will get you when called on a sequence of objects via java.util.arrays.sort,mergesort - in general mergesort is consistently faster than quicksort however quicksort is done in place and doesn t require allocating memory unlike mergesort,"
"mergesort","quicksort","worse overall,faster than  in faster algorithm slower,better in faster algorithm slower,more sensitive overall,better in worst comparisons complexity, does not overall,not better in worst comparisons complexity,better in simpler in-place parallelizing,less comparisons in worst comparisons complexity,faseter than  overall,consistently less recursive in faster algorithm slower,","7770477,5070199,29218730,18427291,36054413,25656262,70627,13096603,29243439,7372734,42183380,","this means that while the two underlying sorts it uses mergesort and insertion sort are both worse than quicksort for many kinds of data timsort only uses them when it is advantageous to do so,quicksort generally runs faster than mergesort but under some circumstances mergesort can degrade to quadratic running time,quicksort has better locality of reference than mergesort which means that the accesses performed in quicksort are usually faster than the corresponding accesses in mergesort,quicksort is more sensitive to input sortedness in a positive way than mergesort,i know mergesort is better since it is stable and doesn t have n 2 as worst case but i required to implement quicksort,for instance quicksort does not have a tight bound unless we are talking specifically about either best average or worst case analysis as it s ω nlogn in the best and average cases but o n 2 in the worst case;on the other hand mergesort is both ω nlogn and o nlogn therefore it s also θ nlogn,quicksort is not better it is well suited for a different kind of application than mergesort,i ve looked at the question at why is quicksort better than mergesort,mergesort uses about 30 less comparisons than quicksort,but remember mergesort is not in place mergesort require 2n memeroy space.and mergesort also need to do many array copies which we don t include in the analysis of algorithm.in a word mergesort is really faseter than quicksort in theroy but in reality you need to consider memeory space the cost of array copy merger is slower than quick sort.i once made an experiment where i was given 1000000 digits in java by random class and it took 2610ms by mergesort 1370ms by quicksort,quicksort consistently has less recursive calls than mergesort,"
"mergesort","quicksort","usually better in simpler in-place parallelizing, is generally faster in faster algorithm slower,simpler in simpler in-place parallelizing,consistently faster in std compiler odd, has less overhead so overall,simpler in simpler in-place parallelizing,slightly slower in faster algorithm slower,faster in faster algorithm slower,slower in std compiler odd,faster in faster algorithm slower, is more overall,","29218730,70440,33319313,23501734,77945,10955685,5608850,30014574,39156435,4292700,497933,","quicksort usually is better than mergesort for two reasons,quicksort s because quicksort is generally faster that people use quicksort instead of mergesort,i would say that the quicksort is simpler for parallelizing than the mergesort,when comparing my quicksort implementation with std sort on my compiler and my implementation of mergesort i noticed an odd pattern on large data sets when operating on 64 bit integers quicksort is consistently faster than mergesort,quicksort has less overhead so with small n and slow computers it is better;but computers are so fast today that the additional overhead of a mergesort is negligible and the risk of a very slow quicksort far outweighs the insignificant overhead of a mergesort in most cases,parallelizing mergesort is simpler than quicksort in-place,mergesort is slightly slower than quicksort but it does not have quicksort s susceptibility to pathological cases,for the 10 tests on the same list the results should be quite the same at least all showing that quicksort is faster than mergesort or vice vesa,when comparison function is a callback function like in quicksort libc implementation quicksort is slower than mergesort by 15 on random input and 30 for already sorted array for 64 bit integers,quicksort is implemented well it is typically 2-3 times faster than mergesort or,as a result for relatively small datasets quicksort is more likely to get cache hits and therefore just tends to run faster on most hardware;mergesort is still a pretty good solution for large data sets or other data structures like linked lists as your experiments confirm,"
"mergesort","quicksort","faster in faster algorithm slower,worse than  in faster algorithm slower,worse in worst comparisons complexity,more comparisons in worst comparisons complexity, has a shorter overall,faster in faster algorithm slower,faster in faster algorithm slower, is not overall,more natural overall,worse in faster algorithm slower,better overall,","13096603,34237672,487454,14349085,37746747,70440,961662,52282899,14807074,29218500,7372734,","depending on where i look people say quicksort is faster than mergesort due to its locality of reference cache hits etc,insertion sort for example has an average time-complexity of o n 2 worse than quicksort or mergesort but as an online algorithm it can efficiently sort a list of values as it are received as user input where most other algorithms can only efficiently operate on a complete list of values,given that it is possible to vastly reduce the likelihood of the worst case of quicksort s time complexity via random selection of the pivot for example i think one could argue that mergesort is worse in all but the pathological case of quicksort,interestingly quicksort performs more comparisons on average than mergesort - 1.44 n lg n expected for quicksort versus n lg n for mergesort,hat you do not need a second array to store like mergesort you do not need a second array to store like mergesort is calculated inside the same memory;what does author mean by quicksort has a shorter inner loop than most,quicksort is usually faster than mergesort just because it s easier to code a tight implementation and the operations it does can go faster,in most cases quicksort will run faster than mergesort even though the worst-case execution time is longer,quicksort is slow for lists mergesort is not in-place for arrays;short answer quicksort is advantageous for arrays in-place fast but not worst-case optimal,mergesort is more natural to implement for linked lists but you can do quicksort very nicely,scenarios when quicksort is worse than mergesort,that s hard to say.the worst of mergesort is n log2n -n+1 which is accurate if n equals 2 k i have already proved this .and for any n it s between n lg n - n + 1 and n lg n + n + o lg n .but for quicksort its best is nlog2n also n equals 2 k .if you divide mergesort by quicksort it equals one when n is infinite.so it s as if the worst case of mergesort is better than the best case of quicksort why do we use quicksort,"
"mergesort","quicksort"," is more overall,more comparisons than  in worst comparisons complexity, but asked a question overall, it is easier in simpler in-place parallelizing,better in simpler in-place parallelizing, is worse overall,slower in faster algorithm slower,slower in faster algorithm slower, is usually faster overall, is more in simpler in-place parallelizing,better in worst comparisons complexity,","16398609,50745491,16720979,52771693,1527161,5715112,24290704,23501734,52712620,16594411,16943949,","note this may reduce memory usage if you have a really large dataset it is actually used to handle such situations but may perform worse than raw quicksort beacause of the split cost which becomes linear if you copy over the subarrays and the multithreading overhead;consider that inplace mergesort is more space-efficient when applied to large arrays,under one plausible definition of average case quicksort is expected to do about 40 more comparisons than mergesort,there are numerous problems with this question starting with the fact that you ve implemented a very slow version of quicksort but asked a question about mergesort;mergesort is not typically implemented as a tail recursive algorithm,see multithreaded quicksort or mergesort it is easier to pass several parameters to a thread using a control struct,why is quicksort better than mergesort,your implementation may of course have stack unwinding while in mergesort the sorting is done on the way up the splitting step does not move elements at all but on the way back up you need to merge two sorted lists;as for the performance comparisons it is certainly true that the worst-case behavior of quicksort is worse than that of mergsesort but the constant factor for the average case which is observed almost exclusively in practice is smaller which makes quicksort usually the winner for generic unsorted input,i have been testing the practicality of openmp gnu parallel sort algorithms in the c++ standard library and have found the parallel quicksort algorithm to be significantly slower than the mergesort algorithm,however on smaller int sizes quicksort gets slower and mergesort gets faster,a good example of this is quicksort versus mergesort;they both have an average runtime of o n log n but quicksort is usually faster,quicksort and heapsort are both normally considered in-place and heapsort can be implemented with o 1 extra space i was mistaken about this earlier;mergesort is more difficult to implement in-place but the out-of-place version is very cache-friendly - i suspect real-world implementations accept the o n space overhead - ram is cheap but memory bandwidth is a major bottleneck so trading memory for cache-efficiency and speed is often a good deal,an interesting answer about this can be found at why is quicksort better than mergesort,"
"mergesort","quicksort","faster in faster algorithm slower,better in worst comparisons complexity,slower than  in faster algorithm slower,better than  in faster algorithm slower,worse complexity in worst comparisons complexity, is fast in faster algorithm slower,faster in faster algorithm slower, is the way overall,faster in faster algorithm slower,faster in faster algorithm slower,not better in simpler in-place parallelizing,","30014574,4819756,54067493,7678974,201171,5223158,4819185,29606081,42767458,16308408,77945,","to be specific the quicksort runs faster than mergesort in the first test case and loses badly in the following 9 tests,technically mergesort has a better time-behavior î nlogn worst and average cases than quicksort î n 2 worst case î nlogn average case,however at least quicksort is not slower than mergesort for the random case,it s not a question of is map reduce better than mergesort or quicksort because map reduce is just a tool for implementing a sorting algorithm like mergesort or quicksort in a parallel way,quicksort is worse complexity than mergesort in the worst case.,quicksort is fast when the data fits into memory and can be addressed directly;mergesort is faster when data won t fit into memory or when it s expensive to get to an item,from what i ve read i was expecting quicksort to be faster than mergesort but on my code it is not so i assume there must be a problem with my quicksort algorithm,quicksort cannot be used on lists;for lists mergesort is the way to go as long as the list contains more than one element split it into two lists sort each of them then merge the two sorted lists into one since both lists are sorted only the heads have to be compared,quicksort is approximately 40 faster than mergesort on random data because of fewer data movements,normally quicksort is faster than mergesort which is faster than heapsort,quicksort is not better than mergesort,"
"mergesort","quicksort"," is not overall,faster in faster algorithm slower,more efficient in worst comparisons complexity,slower than data.list.sort  in faster algorithm slower,","32334651,7818843,13096603,52237695,","therefore quicksort is not an option;so a variant of mergesort is used the current java versions use timsort,you can also see that for smaller collections quicksort is faster but then mergesort takes the lead but all of this is case specific so take your time to study all 4 algorithms,purely in terms of the number of comparisons performed is mergesort always more efficient than quicksort,the results were as follows data.list.sort 0.171s mergesort 1.092s 6x slower than data.list.sort quicksort 1.152s 7x slower than data.list.sort,"
"altitude","distance","larger overall,important than  overall,more overall, always means more fuel overall,","27203554,54537656,7121938,54209853,","because most of the case zero-altitude distance is quite larger than altitude itself so normally you can ignore altitude,altitude gain is a rather important metric in the analysis of certain activities often more important than distance,a negative verticalaccuracy signifies that altitude is invalid whereas normally a smaller but positive value of verticalaccuracy actually means that altitude is more precise since it s the vertical distance that it may be off by - i ll leave the discussion as to why this measure is called verticalaccuracy and not verticalinaccuracy for some other time,however if i put all the flight records together the dataset contains many airlines so the flight distance vary from each other the fuel cost positively changed with the altitude because usually the longer the flight distance is the higher a plane would fly and a longer flight distance always means more fuel cost,"
"memcmp","strcmp","better overall,simpler in lengths faster simpler,faster in lengths faster simpler,","13095552,41420603,1128221,","rest assured though that strcmp is better equipped in the general case for string comparisons than memcmp is,memcmp is simpler than strcmp and can be implemented even more efficiently in places where the strings are known to be properly aligned,if you always keep track of the lengths of your strings you can compare lengths and use memcmp which is faster than strcmp,"
"aes","tripledes"," is not overall, is better then just overall,more secure overall,older overall,","45797696,4360130,3868099,15109802,","aes is not be supported on client side for gwt but you can use tripledes;tripledes is also very much secure implementation,des is a very weak algorithm you might want to think about aes rijndael;even tripledes is better then just plain des,i personally would use aes for my encryption as it is lighter and more secure than tripledes in fact i think it is the de facto algorithm at the moment,but tripledes is older and weaker than aes,"
"gridview","repeater","fewer overall, is more overall,faster in difference big understanding, is easier and is very overall,familiar with a  overall,faster in difference big understanding,far better overall, doesn overall,general overall, is a better overall,lower level overall,","7328194,4448792,38065946,29321755,721563,21944601,10578443,2770491,2867802,29301738,26385838,","repeater has fewer templates then gridview,for me datagrid or gridview can be used when you need to display data with able to edit each row sorting paging etc;if you want just to display one table repeater is more prefered,in my understanding repeater is most suitable since it faster than gridview,i would recommend you to use an asp repeater because finding controls in an asp repeater is easier and is very similar to the asp gridview control,maybe a repeater is better but you will be fine with a gridview if you are familiar with a gridview,the repeater is still faster than the gridview but the difference shouldn t be big if you code it right,in simple words we can say performance of repeater is far better than gridview,if you want more flexibility in the output than gridview provides take a look at repeater;since the repeater doesn t directly implement paging you ll have to supply your own next and previous buttons,also you can use templatecolumn with gridview and add your labels into this template column;but imo repeater is simpler to customize your view by templates,it is possible to create a table with repeater - how to create a three column table in asp.net repeater but it will require a bit different markup with properly set table tags;and perhaps gridview is a better alternative to manual table creation,each button raises the correct event but the events never reach the gridview because they are handled at a lower level by the repeater,"
"gridview","repeater"," gives you more overall, why not overall,better decision overall,","2171798,350167,1123182,","the gridview is for tabular data only and does a lot of the work for you like binding data automatically to columns;the repeater gives you more control over the result but you have to do more because nothing gets binded automatically,if you don t need any of the features of a gridview why not use a repeater;a repeater keeps it simple for implementation but also allows you to have full control over the generated source,a repeater might be a better decision than a gridview as they are friendlier for using custom layouts,"
"memcpy","strcpy","faster in faster blocks memory, is faster in faster blocks memory,more efficient overall,faster in faster blocks memory,slightly faster in faster blocks memory, is simply easier overall,faster overall,slower in faster blocks memory,usually more efficient in performance difference efficient,faster overall,prefer  over  overall,","39286524,10513207,35394906,24966865,23762261,7059973,33590309,23201978,41749210,24990268,57347984,","information - use memcpy as it s faster than strcpy and we know,strncpy - memcpy is faster but you need to know the size of the input string;strcpy may be the fastest if your string sizes vary much because it determines the required size while copying,memcpy can be more efficient than strcpy since rep movs is highly optimized on intel cpus esp,on almost any platform memcpy is going to be faster than strcpy when copying the same number of bytes,because of the above replace strdup with strlen malloc memcpy memcpy is slightly faster than strcpy,nstead of overwriting the first element with the last using memcpy instead of overwriting the first element with the last using memcpy can also be done with strcpy and assignment of the priority like in your remove;using memcpy is simply easier,notice that memcpy is faster than strcpy unless the source string is much smaller than the buffer s size which is rarely the case with ip addesses.,memcpy is rarely slower than strcpy or strncpy and often significantly faster,performance difference memcpy is usually more efficient than strcpy which must scan the data it copies,if size is known normally a non-naive implementation of memcpy is faster than strcpy since it takes profit of the cpu s data bus size,anyway as you already have to determine the length prefer memcpy over strcpy,"
"memcpy","strcpy","natural than  overall,faster in faster blocks memory, is better overall,slower overall, already knows the length overall,faster in faster blocks memory, is more overall,faster in faster blocks memory,slightly faster in faster blocks memory,safer overall,more time in performance difference efficient,","55838662,24966693,38255275,4361629,611542,24966693,53579067,9760567,15176895,27222337,30186543,","if source_str is certain to point to a properly-terminated c string of no more than length - 1 characters and if it is it string value that you want to copy then strcpy is more natural than memcpy,so i feel that on x86 memcpy is faster than strcpy,you take a small performance hit that you don t if you use strcpy;if you compute the length of the string for unrelated reasons or have the length of the string from other resources it s unclear to me whether memcpy is better or worse than strncpy,memcpy is not really any slower than strcpy,memcpy can be faster than strcpy and strncpy because memcpy does not have to compare each copied byte with 0 and because memcpy already knows the length of the copied object,is memcpy usually faster than strcpy on most real platforms,note that i deliberately did not use strcpy because if length is known memcpy is more efficient and if not known strcpy is unsafe risking undefined behaviour due to writing beyond target array bounds,but sometimes memcpy performs faster than strcpy because it moves blocks of memory at a time which allows it to perform some optimization i will not go into details here,we keep track of what cmd s length ought to be in a variable and copy the string with memcpy which is slightly faster than strcpy and does neither check string length nor copy the extra zero at end of string,your macro with memset and memcpy was not any safer than strcpy,memset behaves like strcpy but the difference is that memcpy copied the data as it is byte but strcpy copies the formatted string as well so takes more time than memcpy to execute,"
"memcpy","strcpy","more complicated overall,slower overall,faster in faster blocks memory,faster in faster blocks memory,","25303036,24967980,3942421,25242721,","the reason for not having strcpy i m guessing is that strcpy can be replaced more efficiently with memcpy for constant strings and if the string is not constant strcpy is a bit more complicated than memcpy anyway so not as beneficial to make inline optimisations for,for example for small amounts of data an memcpy optimised for large amounts of data may be significantly slower than a strcpy that wasn t optimised for large amounts of data,memcpy is usually faster than strcpy for longer strings,memcpy is faster than strcpy and also enforces you to specify a buffer size,"
"max","range"," which holds the greater in greater larger smaller, is greater in greater larger smaller,greater in greater larger smaller, which takes more overall,smaller than my  in greater larger smaller, is less overall, is more overall,higher overall, go to middle  greater in greater larger smaller,greater int in greater larger smaller,bigger overall,","52112785,18163668,43411471,55613098,48696242,54148360,49894557,8013367,51866431,41511932,24089593,","when a range of values is scanned to find the maximum we usually compare each element to a variable let s say max which holds the greater value found so far and which is updated when a bigger value is found,scanning backwards we can stop our scan when d is greater than the middle of the array s range;or when the current max is greater than the length of the possible sequence for d greater than the largest indexed difference,it works like range so i need to pass it a number one greater than the max number in the current index,so i have to write two views view 1 view 2 create or replace view comment_max_id_view as select max pkid from comments group by session_id here the problem is if i use this view to select for a particular period then the comment_max_id_view is doing a full table scan instead of just the selected date range which takes more than 5 minutes to get the result,and then i join this range with my max value on where the range values are smaller than my max value,say you now want 0..9 so you modulo by 10 and now your upper part has a range 0..4294967 42949672 100 as max is less than 16m we can now bring in the next byte,if the date range is more than 31 day s i need to restrict the enddate to max of 31 days,at first i tried a solution using a where i would select every coordinate in range check if they were valid if they were i would call a from them to the center position and count the number of steps if they were higher than my max range i would just remove the coordinate from my list,if value is greater than current max go to middle range greater than current and go back to step 2,the problem is it is overflowing therefore it works fine when the result returned is in int_max range 10 9 but negative value is show in case of value returned from function is greater than int max range,that occasional max threshold is actually bigger than the range of small instance,"
"max","range","larger than  in greater larger smaller, birth ends up greater in greater larger smaller,greater than   in greater larger smaller,more than the  overall, not overall,higher in greater larger smaller,less overall, then take cell value in formula possible cell,lower overall,greater in greater larger smaller,greater in greater larger smaller,","37806605,32895040,48195154,47622708,39988761,39502592,36562247,47823995,8013367,28488349,35075746,","and this number is larger than max so this number doesn t fit into the range,that will be your range;if max birth ends up greater than min death then there is no overlap,here is the code which i was trying it is complicated condition like greater than range max on the basis of condition number this function decides condition meet or not and return true or false,so if your new type is unsigned following rule #2 if we subtract one more than the max value of the new type 256 from 800 we eventually end up in the range of the new type with 32,max reflects the reality of the values actually used by ruby when iterating over the range or testing for inclusion in the range;in my opinion end should reflect the actual maximum value that will be considered inside the range not the value used at the end of the definition of the range but i doubt that ll change otherwise it d affect existing code,if you disable elastic axis and make sure you specify a range that is higher than the max value you can select the point,take your large function and compare it to the max from your source range if it is less than max display none if otherwise have it display the max,i need a formula to distribute values across a range while accounting for the max value possible conditions to check if the cell value is less than max then take cell value else subtract cell value from max and check conditions if there is a remainder in the subtraction and add to the next row,i have this grid each coordinate in the grid can be either closed or open i m tying to using an open coordinate find all the open coordinates around the first that are valid and the walk range between then is equal or lower than the max walk range,the question i have is i would like the calendar to only display the min and max date range for the from date and to date any thing greater than the max and anything less than the min should be greyed out,i ve got a constant defined data_out range and cnt is incremented on clock and never reaches value greater than max range of data_out,"
"max","range","greater in greater larger smaller,harder with  overall,less overall,smaller in greater larger smaller,less than  in formula possible cell, .boxed is still better overall,greater in greater larger smaller, is less in greater larger smaller, and then in first program numbers,greater in greater larger smaller,less overall,","20943998,14964979,19947660,21562035,50278224,27803385,43950275,19388029,40545362,38423673,1386828,","i want to validate a number as decimal up to two decimal and minimum range is greater than 0.00 and max range is up to 99.99,the benefit of backbone models is that those are are dynamic so you can create a parser to read the validations form the server and add those validations to the backbone model in the way your pluggin need it it kind of easy if you only use required minlength max length and a regex but gets harder with range or some other kind of validations,the python built-in range will make a series of values for you starting with adding k each time and stopping with the last multiple of k that is less than max x,to create equal bins you can simply first define a min and max value which is slightly smaller than both range,when a new task is added to a queue i want to run it if the number of ongoing tasks is less than max the reason i can t use atomicinteger or longaddr is that they only allow you to compare against a specific value instead of a range of values,a boxing conversion s not the most beautiful code but intstream.range 0 max .boxed is still better than,third you initialised end to len lis2 this will give indexerror list index out of range if you are trying to search for an element which is not present in the list and is greater than the max element in the list say 23,to start off get the range of the two date series for our purposes column 1 is revenue and column 2 is traffic;for each series get the max value of the series or 1 if the max is less than or equal to 0,there is also another option and that is convert the lines to coordinate system of the other bbox and then compute points for each line where one of the x y z min and max and then just test if other two coordinates are in range or not;the initial value of the min max should be the first vertex not hard-coded 0 1000,a larger range means a greater max value and a smaller min value than decimal,the range function produces a list of integers from 0 less than the max len s by step 3,"
"max","range","larger in greater larger smaller, is lower in greater larger smaller,larger in greater larger smaller, you made earlier in first program numbers,larger overall,shorter than   overall,much more straightforward overall, is not a scala.collection.immutable.list indeed overall, is one less so in greater larger smaller, is way too overall,better compression with  overall,","29827603,52769950,3567637,55187150,18854408,49095912,24873884,56634095,19910684,21032989,56758103,","in general you want to have a range slightly larger than x max - x min and then divide the range into the desired number of bins,what happens when the range has a negative number or the max is lower than the min,so if you re getting a step exceeds the specified range error i d guess that the default step value 1 is larger than the max of the range the result of .,your program is creating a range of numbers between 1 and 19 range go to one less than the max number you specify like your program is initializing the variable total to equal 0 for i in a you start looping through the range you made earlier the first iteration the next and so on until you are selecting only the data where the modulo of 5 or 3 is 0,i ll leave the rest up to you ov checking for error conditions such as more than 1 column passed in the range or range of unequal size or a max # of items returned being larger than the range size,hence if i set only a subset of values as shown below i get an exception as below org.springframework.batch.item.file.transform.incorrectlinelengthexception line is shorter than max range 581 org.springframework.batch.item.file.transform.fixedlengthtokenizer.dotokenize fixedlengthtokenizer.java 109 org.springframework.batch.item.file.transform.abstractlinetokenizer.tokenize abstractlinetokenizer.java 112 org.springframework.batch.item.file.mapping.defaultlinemapper.mapline defaultlinemapper.java 43 org.springframework.batch.item.file.flatfileitemreader.doread flatfileitemreader.java 180 do we have to do anything special to fetch only subset of values,figuring out the necessary exponent range is much more straightforward if you can describe the max and min possible absolute values of your input you can easily find suitable corresponding binary minimum and maximum exponent,luckily the range class offers the handy method tolist which you can leverage for converting the range into a list and resolve the problem like in the following code snippet;2 to max is not a scala.collection.immutable.list indeed but a scala.collection.immutable.range more precisely an instance of scala.collection.immutable.range.inclusive as mentioned in your error message,also note that i changed your random range to 1 - 7;the max is one less so in your code you were never getting a number 6 for an image,your formulas are evaluating 900 000 more cells than your range is way too large need to determine the max date,better compression with range constraints adding a self-imposed constraint partitions are sorted from largest to lowest part we can deduce for p in parts num size max constraints 2 3 can be applied recursively to all by noting that is in therefore we can compute better imin imax and inject better imin imax in the previous implementation live demo todo sanity checks the provided implementations can go into undefined behaviour if the partition is not sorted or the partition index is not valid,"
"max","range","greater in greater larger smaller,higher than the  in greater larger smaller,higher in greater larger smaller,smaller potential overall, is more then overall,greater overall,length more likely overall,more cells than the  overall,larger in greater larger smaller,smaller than  overall,less overall,","20007371,56917843,30607810,28844504,52557469,27926019,27711026,56050896,47389759,53501759,13264779,","keep in mind that the cast int d will not throw an exception if the value of d is outside the range of an int - if which is greater than the max value of an int the resultant cast will be -2147483648,but if you multiply the first with the second number the result is never higher than the max value of the range,i am trying to create a figure in which the colorbar will extend beyond the data range go higher than the max value of data,the recursive cte is troublesome because it is limited to a max size of 32 767 much smaller than potential range sizes and has the very real possibility of being very slow,according to this 5 2 is widened to 10.0 and which is equal to 10.0 if you are comparing the mixed data types then the result will be considered on the basics of data type which is having long range so in your case float range is more then int float max number can be -- 1.7976931348623157e+308 int max number can be -- 9223372036854775807 thanks,since your container is sorted you can use std max_element on a range ending with the first element greater than your max use std find_if with a lambda or std lower_bound to get this range,a sample of visual inspections suggests that the max length is more likely in the 200 to 300 character range,use intersect and do a binary search the above should work for any arbitrary range and it does not iterate through the entire range nor does it use cells.count which can raise an error if there are more cells than the max value for a long integer,but with this the last range is larger than the max value,the max values of the rgb channels are lower than that of the n channel but look at the mean values of the rgb channels the max values of the rgb channels are much smaller than max 2 you have a high dynamic range hdr image here and want to compress its high range to 8 bits for displaying,if max z1 z2 range is less than or equal to table2 max value2 where table1.type table2.type,"
"max","range"," have not already overall,greater in greater larger smaller,larger in greater larger smaller,greater overall, is greater overall,smaller in greater larger smaller, are not overall,not greater in greater larger smaller, not overall,greater in greater larger smaller,larger overall,","43850350,17885817,17068322,46003981,14353636,28482152,21185303,45418162,21021881,27101914,2022693,","the variables min and max are the highest and lowest port numbers that you want to use and a check also follows to ensure that all the ports in the chosen range have not already been taken;this is done by comparing the range that has been selected from variables min and max producing the variable range and the number of ports that have already been used variable used,an automatically adjusted range will always be the next power of 10 greater than max value,if you date range is larger than the two years displayed just drag the formulas down to expand the max range,issue here is i need to submit myform but because of min max attribute i get this tip value must be less than or equal to 20 or value must be greater than or equal to 10 for the inputs submitted lesser or greater than the range provided in min and max attributes respectively because of this my form does not get submitted,as commenters have said using modulo to reduce the range of rand will not give you a good random distribution;also be sure to check that max is greater than 0 otherwise you will get an error,if you want to exlude a number 4 that means the range is smaller by 1 so use r.nextint 5 and if the result is the excluded number then return the max allowed which is 5 because it will never be generated because you used max - 1,as has been pointed out the question is for range not for detecting single point inclusion;however it s simply an elaboration of that because range min max should return true if and only if both min and max are not included -- you simply have to test both points or alternately if one of the a range in the database endpoints is contained within range min.max,2 that the min range is not greater or equal to the max range.,and be carefull the range is min;max not min,avoiding variables and functions cross join the table against some other table s to generate a range of rows where the range is greater than the max value of quantity,basically if you know that your events are never larger than a given duration you can search for a bounded range that s larger than the max duration then add restrictions to get rid of the extra stuff that matched,"
"max","range","bigger in greater larger smaller,","17936725,","a signed 64-bit integer range from âˆ 2 63 to 2 63 âˆ 1 the absolute value of 0x8000000000000000 or âˆ 2 63 is 2 63 is bigger than the max 64-bit integer,"
"jlist","jtable","better jcomponents in rendering strategy listcellrenderer,better in rendering strategy listcellrenderer, is not overall, which is better in rendering strategy listcellrenderer, is better in rendering strategy listcellrenderer, is not yet overall,better overall, provides a slightly better in rendering strategy listcellrenderer,","6823252,18589264,48037999,27036926,21437005,2054971,8598358,28492260,","i think jtable would be better jcomponents as jlist because there you can implements tablecelleditor maybe example with similair output here,i don t really think that jtable will help you here anyhow better than jlist - it has the same rendering strategy,you could use the key bindings api to add support but it s not intuitive and now you implementing functionality you d get for free using a jtable based solution anyway generally speaking it s not intuitive as jlist is not meant to be editable so most users won t have that expectation mindset and it could cause frustration for the user as their expected results don t meet what your program is now doing,again use a listcellrenderer to customise the look of the jlist to your needs;you should also consider using a jtable which is better at displaying structured data see how to use tables for more details,don t use a jlist;a jtable is better for displaying data in columns,jlist is not yet editable and might never be;jtable would handle the layout problems for you and you can easily access the values via the table,a jlist would probably be better than a jtable,se a jtable with a jradiobutton in one column;not much better then the jlist but the jlist provides a slightly better visual cue,"
"drawimage","putimagedata","much slower in old browser earlier,faster in clipping version function,much faster in clipping version function,slower in clipping version function,slower in old browser earlier,faster in clipping version function, is slower overall, is much faster overall, is far faster in use getimagedata othercanvas, is easier in use getimagedata othercanvas,faster in clipping version function,","7074620,37245491,7722892,7380496,17406346,14511199,22136011,18303534,7639431,28769477,41723100,","that beeing said putimagedata is much slower than drawimage at least when i tested them earlier this year,using the clipping version of drawimage will be faster than putimagedata,as of right now drawimage is much faster than putimagedata,right now putimagedata is much slower than drawimage as you can see here,the only problem that as far as i know using drawimage is slower than using putimagedata and it was required in old browser versions like firefox 2 or such,i have seen that the drawimage function is really faster than the putimagedata,that way you can just drawimage the offscreen canvas to the onscreen canvas--efficient because the gpu can do the blitting instead of burdening the cpu;putimagedata is slower because it involves the cpu fetching data from the pixel array setting that data in your temp array and transferring the temp array to the onscreen imagedata,if you get curious and discover the putimagedata function in the canvas 2d api please note that drawimage is much faster which might be unintuitive,if this is the case definitely just use drawimage othercanvas x y instead of using getimagedata and putimagedata if possible;using drawimage is far faster,don t use getimagedata putimagedata for clipping since drawimage is easier and faster.,using drawimage canvas is much faster than using putimagedata,"
"drawimage","putimagedata","more performant overall,faster in clipping version function,","22793378,13668442,","is putimagedata ... more performant than drawimage ...,drawimage seems to be much faster than putimagedata,"
"bellman-ford","dijkstra","more similar overall,algorithm more efficient in negative edges googling, isn overall,approach better in negative edges googling,better in negative edges googling,faster overall,slower overall,better choice in negative edges googling,better than  in negative edges googling,slower overall,","30176723,19482317,27025683,19482317,19482317,36437805,20273939,40843559,27025683,12665448,","in fact i think it is fair to say that bellman-ford is more similar to dijkstra because of its use of iterative relaxation,after a lot of googling i ve found that most sources say that the dijkstra algorithm is more efficient than the bellman-ford algorithm,that makes your worst-case performance worse than bellman-ford which is therefore preferred for graphs with negative edges;this doesn t mean that your modified dijkstra isn t useful,surely there is some situation in which the bellman-ford approach is better than the dijkstra approach,but under what circumstances is the bellman-ford algorithm better than the dijkstra algorithm,since a proper implementation of dijkstra is faster than bellman-ford use dijkstra unless there are negative weight edges in the graph,the only issue with applying that technique for the single source shortest path problem is that reweighting with bellman-ford takes o mn time which is slower than dijkstra s o m log n,however if g is guaranteed to have only non-negative weights g is non-positive weights then dijkstra s algorithm could be better choice over bellman-ford,your modified dijkstra can perform much better than bellman-ford if you have very few negative edges and or if those negative edges are separated from the rest of the graph by expensive paths,bellman-ford as suggested in your question tends to be slower than either dijkstra s or a - it is primarily used when there are negative edge-weights which there are not here,"
"client-side","server-side"," is trickier overall, redirection is probably more overall,slow as  overall, and use the api overall, than simply overall,script way less secure in script browser ps, disable not in script browser ps,validation more secure in user responsive proper, code is cleaner overall,general in validation client-side security, you store an array overall,","4010171,55033022,52174683,30137992,54640291,16814826,2914583,24450617,4771738,56553305,55929508,","you can do this client-side with javascript;doing the same thing server-side is trickier,if you use cookies you may have to provide a disclaimer. if you want users to be redirected with client-side javascript such as by assigning to window.location.href when the visitor visits the landing page and the server has no need to see their locale information on every connection then use local storage;server-side redirection is probably more elegant but both methods are ok options,two of the sub-selects were too slow as client-side queries in access so i created them as server-side views which worked better but still had room for improvement,if your transcoding system has a restfull api like bitcodin which is used in this tutorial or any other service you can do your application also client-side and use the api calls to get the state of your transcodings etc;however using the api you can do the same also server-side whatever fits better for you,if as indicated in your comment to this answer this needs to be modified server-side input#destinations needs to be filled out server-side than simply include that input in the partial-view and read it from there and ignore the code that modifies it client-side,ps again i am aware that relying on client-side script is way less secure compared with handling all execution from within the server-side,if you want to perform a client-side disabling you ll need to use script for that or perhaps use a readonly attribute;specifying is like a server-side disable not a client-side disable,server-side validation is more secure than the client-side as the user cannot see the code even he does a view-source,to that extent i had to make a few changes to my server-side presentation layer but as a result i think my client-side code is cleaner and more focused on processing actual user events,any data validation must be implemented server-side and should be implemented client-side too,so the productviewmodel can stay the same but then we introduce a simple orderedproductviewmodel to represent the ordered products when calling order the concept of a cart is strictly client-side but if we do have a cart model that exists server-side just not an entity so the product list still returns a collection of productviewmodels to the view;when the user goes to add a product to the cart client-side you store an array of objects consisting of the product id and quantity,"
"client-side","server-side"," rendered code overall, rendering is a better overall, call isn overall, validation isn in validation client-side security, language not overall,smaller overall,validation more in user responsive proper,also faster overall,better overall,greater overall, code doesn overall,","48999745,38805354,1891994,13993216,2324542,42743255,12484431,5365998,9174795,39749944,24693609,","in asp.net web form server-side id and name won t be necessarily same as client-side rendered code,having that said there are situations when client-side rendering makes sense when some content is too long to load at once and it is better to show the page and a progress indicator we are loading your data...;nevertheless for most content-oriented websites and portals nearly complete server-side rendering is a better way to go,server-side it ll work - assuming that the server-side call isn t an ajax postback cause then in reality the actual dynamic insertion is still client-side;client-side it won t you can t simply inject java-script into a page after it s already loaded and expect it to execute,client-side validation isn t a security measure it s just a better user experience;server-side validation is the only real validation,you cannot affect client-side print settings with a server-side language not even vb.net;the web server and the browser run in different machines and neither html nor client-side javascript define such features,since servers omit the working copy you should generally expect server-side bare repositories to be smaller than client-side non-bare repositories,client-side validation is more responsive to the user but always back it up with proper server-side checks,as a general rule of thumb at least in security you should trust no user so i think it would be the wise choice to validate the data client-side it s also faster and then if the first validation passed validate it server-side to double-check or to have a safety net if the user has javascript turned off something you don t see that often,are client-side binding grids better than the server-side ones,is there a way to load it in node.js or is there another proxy that can be used to monitor changes in array for server-side code that will work far greater than client-side code as it will be receiving yuge requests,if the outcome of any client-side code is a boolean true value server-side code runs;otherwise server-side code doesn t run,"
"client-side","server-side","more valuable overall, does not overall,script more reliable in script browser ps, mail letting the user overall, not overall,javascript much more overall,way more overall,more critical in things critical malicious, it is more overall, and not in validation client-side security, but requires a bit overall,","1141264,2766529,9866616,31440012,32886678,1322011,33539901,47184617,42347995,55679542,29300711,","not all of the reasons given are necessarily valid but one important one is that unless you re google server-side cpu cycles are a lot more valuable than client-side cycles so it s easier to have the client compile optimize what is quite often dynamically generated html javascript rather than the server,another difference is that hiding content client-side does not remove it from the dom - the content is there just hidden;hiding controls server-side prevents their content from even being emitted to the html,i am curious to know if detecting the visitor browser with client-side script is more reliable than server-side script,from a web page you can only choose between client-side mail letting the user confirm what everyone s browsers re sending and losing some proportion of respondants who aren t in a position to send mail or a web-accessible server-side mail forwarder either a third-party service or your own server in which case you have to work out the security yourself,you just need an app token no user token to do this search but using this client-side is a bad idea since it would expose your app token publicly;simply do this server-side not client-side,that said client-side javascript is much more common and if you were using server-side javascript you would probably know the answer to your question,the server-side way is more reliable and browser-independent while the client-side approach will decrease the amount of incoming traffic to server,how you handle things server-side is far more critical than client-side validation which can and will be ignored by malicious users,i would advice to switch to server-side processing of table data instead of using client-side it is more preferable if your application works with a lot of rows more than several hundreds,as a side note if you are validating any kind of input you have to make sure that you also validate it on the server-side and not only client-side validation;client-side validation is just for ux and can be easily escaped,this will make it easy to group the radio buttons on the client-side but requires a bit more work to determine which button was selected on postback;when i needed this behavior i found this behavior easier to keep the radio buttons as server-side controls and just enforce the button group w jquery,"
"client-side","server-side","slower in things critical malicious,secure than  overall, validation is more overall, it doesn overall, code is a lot overall, doesn overall, validation is required simply in validation client-side security, doesn in access token endpoint, doesn overall,faster overall, is also harder overall,","9306245,54998951,4534715,22378439,11709903,8260741,778759,47502642,9400244,1598764,53808458,","since you don t want to repeat yourself it s very tempting to put all your validation on one side or the other but there are tradeoffs either way and it is true that server-side validation is going to be slower than client-side,server-side scripting is more secure than client-side scripting as the server side scripts are usually hidden from the client end while a client-side script is visible to the users,client-side validation is more of a convenience to the user than anything else;then the first answer would be are you checking on the server-side,this would assign a javascript click handler to the client-side rendered output of button1;of course button1 still causes a post-back but since it doesn t need to do anything server-side it doesn t even need to be a server-side control,the code you provided is html which is on the client-side meaning that you wouldn t really process the text user s details there;i m sure that if you tell us what the server-side code is a lot more people will provide input,the only thing i think of that is that you actually never start receiving the file because the server-side doesn t read the command get foobar.txt so the client-side freezes on sending the command;the existence of the file at the client-side might be from previous tests,server-side validation is required simply because client-side validation can be disabled;client-side validation isn t required but it makes your application more responsive during error handling as you no longer need to do a form submit to the server and wait for the resulting page to come back,javascript on the client-side doesn t have access to anything server-side unless you create an endpoint;client-side javascript is really not much more capable than a user with a mouse and keyboard,if you use a service reference to generate your client-side proxy be sure to check reuse types in referenced assemblies when generating the proxy;the class and object you created server-side doesn t exist client-side,while not really a console skulpt.org runs python code client-side with no plugins or anything which makes it a lot faster than a server-side prompt,client-side is also harder to scale however this may not be an issue if this is a small personal project;i personally am using server-side sessions in my current rest api because authentication was easier and i wanted the scalability,"
"client-side","server-side","easier overall,tabletools more overall,more powerful overall,more responsive overall,more overall,better overall, not overall, is probably easier in access token endpoint,shorter overall,probably more common overall, brings more overall,","33536598,18486205,1204968,14265942,26689582,12896810,14024272,52474427,10490295,12135445,54060048,","i think this method of manipulating the dom from the client-side is easier than using regex from a server-side language,since eventually i d have a button that runs some script on the smeow values and send it to a different request i had a look at the tabletools plugin but it doesn t really play well with server-side tabletools is more focused on the client-side source,my guess is that server-side implementation may turn out to be more flexible and more powerful than client-side as i can add functionality to the server easily as long as the client understands it,client-side will also be more responsive than server-side because there s no request-response but that s really only a perceived performance issue for high-latency connections,in theory encoding client-side is no more dangerous than encoding server-side,server-side checks are better than client-side,if you want to reconnect the web page in other words make your browser send another request to server with window.location.reload or some other method standard play.libs.comet.ondisconnected handler is of no use to you - its domain is a server-side not a client-side;to make your client-side manage possible blackouts by itself you may need to implement heartbeat scheme,you could do it client-side but you d need to acquire an access token with adal.js msal.js and then request for the file and set it as the content of the img element via javascript;server-side is probably easier to implement point the img to an http route in your back-end acquire an access token call the api and return the bytes of the image as a result from the route,if getting these in sync is an issue make the client-side timer shorter than the server-side one,storing that info on the server-side is probably more common with the client-side only given his the session cookie,in some cases i use same approach but i recommend to do same approach on client-side via javascript if same approach s possible and my next recommendation is to send all forms via ajax always;less code on server-side brings more benefits less cpu and memory consuption on server-side output of view can be cached better performance smaller transmission data here is a flash module with same functionality as in express.js but i don t recommend it for rendering some flash messages you can use controller.repository object or model or you can create some view engine helper,"
"client-side","server-side","earlier overall, is definitely more overall,general overall, fiasco makes more sense overall, is usually faster and therefore in script browser ps,validation faster in validation client-side security,easier overall, offers a better overall, technology including asp.net overall,better overall, flow is considered more overall,","18131172,5338890,57319763,7666354,7078933,24450617,24469630,33839606,14587506,29544898,42088650,","the problem is that the server-side selectedindexchanged event triggers earlier than the client-side change event so the data never reaches the server-side,or client-side you can include a javascript snippet which calls the js function you mentioned;for server-side server-side is definitely more complicated,cookies are primarily for server-side reading can also be read on client-side localstorage and sessionstorage can only be read on client-side,i will try to break this whole client-side server-side fiasco down now so this whole client-side server-side fiasco makes more sense,parsing xml at client-side in browser is quite slow and should be avoided;delegating this to server-side is usually faster and therefore more user friendly big files will cause your browser stop responding for a long time,client-side validation is faster than server-side because the validation takes place on client side on browser and the networking time from client to server is saved,now if you aren t generating your html server-side and are instead writing it by hand you can make your life just a dash easier with a client-side transformation like this,client-side offers a better user experience and slightly less server load but client-side s at the expense of all of the security concerns that server-side validation addresses,but the client-side code isn t the problem here anymore;one of the examples you had shown could work well with any server-side technology including asp.net,first vb.net can be used to perform this xslt transformation which is really simple as you said just applying the xslt stylesheet at server-side if it is considered better than at client-side or even just sending the processing instruction for that to the browser,the server-side flow is considered more secure,"
"add","subtraction","assignment lower procedure overall, 1 so overall,worse overall,","25073892,39589499,16841814,","addition subtraction assignment has lower procedure than simply add operation,this took 6 steps but leads to a number that could be arrived at in 5 steps subtract 1 divide 3 times add 1 so clearly we should not perform the addition;subtraction is always better,addition and subtraction is worse as these have to be done in sequence of two operations and the second operation requires the first to have completed - this is not the case if the compiler is just producing two add operations on independent data,"
"eventlet","gevent","way slower overall,choose between  overall,lower in use performant lower,lower than  in use performant lower,","12669971,52180348,45035647,49078655,","but this popular wsgi benchmark says eventlet is way slower than gevent,if you want to use threads as your execution pool you can choose between eventlet an gevent,the use of gevent is also a performant option but slightly lower than eventlet,the use of gevent is also a performant option but slightly lower than eventlet,"
"build.gradle","gradlew","general overall,better overall, 3.3.0-alpha10 is questionable. better overall,general in javacompile.setdependencycachedir method incremental, could not overall, while building your application overall, will not in level issue task, is not overall, plugin doesn in build application system, doesn overall, does not overall,","50759336,45317837,52412828,50533293,29777213,41408942,45587505,54744136,54663417,44258248,41411677,","it seems like you are using an ndk versioning that is no more supporting some abis armeabi in you error log so you have to upgrade build in build.gradle project .... to the latest one classpath com.android.tools.build gradlew 3.1.3 and in gradle-wraper.properties and finally you have to add abifilters in android block of your build.gradle module app depending of wich abis you need,i naively assumed that gradlew is better than ivy then when i created my build.gradle the dependency management is even greater mess than of ivy s for the first look,when reviewing there some to be several possible causes you are using the wrong buildtoolsversion which can even be omitted in the build.gradle - in order to use the latest version matching api level 27;using com.android.tools.build gradlew 3.3.0-alpha10 is questionable. better use stable version 3 1 4,please use task.dolast action instead. at build_3icce6xsgr7rsvstratajpe9b.run project build.gradle 138 the javacompile.setdependencycachedir method has been deprecated and is scheduled to be removed in gradlew 4.0. incremental java compilation is an incubating feature. the taskinputs.source object method has been deprecated and is scheduled to be removed in gradlew 4.0,in your root build.gradle make sure buildscript section contains jcenter repository and andcom.novoda bintray-release 0.2.7 classpath;it looks like gradlew could not find this plugin in all specified repositories,although build.gradle has a higher priority so the value in androidmanifest.xml will be ignored by gradlew while building your application,is going to extend you the script you have applied it to then if your test.gradle is in the root and you want to access the task defined in that script from your project or any subproject you will need to apply it in the build.gradle in this project because otherwise gradlew will not read it automatically and configure the task;build.gradle is read automatically therefore extending it with your script is going to configure the task you want to use and have it visible for gradlew,if the build.gradle is not to complicated i would just rewrite it as pom;if you do not want to do that one could have a closer look at the commands you mentioned afaik mvn clean and gradlew clean do the same so no need to worry,for me removing apply plugin io.fabric from the library s build.gradle file but leaving it in the application build.gradle solved this error;it seems the new gradlew plugin doesn t work if the setup is different from the one recommended here,i modified my build.gradle not app build.gradle to force all dependency to react-native to specific version;since gradlew doesn t support declaring repositories on a per-artifact basis yet,the order of declaration in a build.gradle does not matter;gradlew build lifecycle has two phases configuration and execution,"
"build.gradle","gradlew","more than 1  in level issue task, file is anywhere lower overall, not overall,general in javacompile.setdependencycachedir method incremental,general in javacompile.setdependencycachedir method incremental, plugin not in level issue task, version is lower overall, is higher overall,general in javacompile.setdependencycachedir method incremental, as follows for more in level issue task, plugin not in level issue task,","45854712,55324661,34782711,49551663,53386846,43776471,56272085,55431625,53064776,52697405,43776158,","please note if you have more than 1 build.gradle files module library and module yourapp you need to add the following code in all gradlew files to make the following code work,install the latest version and for android after manual linking installation if your minsdkversion in the top most build.gradle file is anywhere lower then 19 please turn your minsdkversion in the top most build.gradle file is anywhere lower then 19 to 19 because that s the lowest version for which react-native-razorpay supports;then clean the gradlew and build the gradlew again i.e go into android and gradlew clean and in the root project folder - react-native run android,in your build.gradle not the app gradlew it should look like this;make sure your gradlew module is up to date,please use task.dolast action instead. at build_2i8nzouqc2wrs6o6f4wz2xu2m.run project build.gradle 138 the javacompile.setdependencycachedir method has been deprecated and is scheduled to be removed in gradlew 4.0. incremental java compilation is an incubating feature. the taskinputs.source object method has been deprecated and is scheduled to be removed in gradlew 4.0,please use task.dolast action instead. at build_6jy1i81bonl94r9ffpxyja5iq.run e avtar all project latest code mindfulscholar platforms android build.gradle 141 the javacompile.setdependencycachedir method has been deprecated and is scheduled to be removed in gradlew 4.0. incremental java compilation is an incubating feature. the taskinputs.source object method has been deprecated and is scheduled to be removed in gradlew 4.0,here is the sample project level build.gradle that resolved my error-;issue error with gradlew plugin not getting updated resolved - if you are getting this error you should update the plugin version to 2.1.2 as henry has mentioned or 2.3.1,you may also have the following in your build.gradle where gradlew version is lower that 4.1,i understood from similar issues that the version of com.google.firebase firebase-auth should be set to at least 11.6.0 but the version in my app build.gradle is higher 16.0.3 as you can see my app build.gradle dependencies my project build.gradle dependencies classpath com.android.tools.build gradlew 3.3.1 classpath com.google.gms google-services 4.1.0 any help would be appreciated,please use task.dolast action instead. at build_dwomais0s9mkq91rc7xy5fcb5.run project build.gradle 137 the javacompile.setdependencycachedir method has been deprecated and is scheduled to be removed in gradlew 4.0. incremental java compilation is an incubating feature. the taskinputs.source object method has been deprecated and is scheduled to be removed in gradlew 4.0,in the case of android projects you can fix this error by changing the project module gradlew file build.gradle as follows for more informations please refer here,here is the sample project level build.gradle that resolved my error-;issue error with gradlew plugin not getting updated resolved - if you are getting this error you should update the plugin version to 2.1.2 or 2.3.1 be careful it is 2.1.2 and not not 2.12,"
"build.gradle","gradlew"," uses to actually in build application system, might not overall, error more in level issue task,simple as  in build application system, file not in build application system,","31792893,47583520,50166596,53463332,21360039,","error cannot find symbol class means your build.gradle file doesn t contain a reference to the classes that your source code refers to;adding a library to the project structure will only affect the ide you re using and not the actual build script gradlew uses to actually compile your work,depending on the environment in which you re working just adding a dependency to build.gradle might not be handled automatically by ide;please try to run gradlew build from commandline - it should work there assuming that you imports are fine in the source,once i did i get this gradlew error more than one variant of project mymodule matches the consumer attributes - configuration mymodule debugapielements variant android-aidl - found artifacttype android-aidl but wasn t required. - required com.android.build.api.attributes.buildtypeattr debug and found compatible value debug . - found com.android.build.api.attributes.variantattr debug but wasn t required. - required com.android.build.gradle.internal.dependency.androidtypeattr aar and found compatible value aar . - required org.gradle.usage java-api and found compatible value java-api . - configuration mymodule debugapielements variant android-classes - found artifacttype android-classes but wasn t required. - required com.android.build.api.attributes.buildtypeattr debug and found compatible value debug . - found com.android.build.api.attributes.variantattr debug but wasn t required. - required com.android.build.gradle.internal.dependency.androidtypeattr aar and found compatible value aar . - required org.gradle.usage java-api and found compatible value java-api . - configuration mymodule debugapielements variant android-manifest - found artifacttype android-manifest but wasn t required. - required com.android.build.api.attributes.buildtypeattr debug and found compatible value debug . - found com.android.build.api.attributes.variantattr debug but wasn t required. - required com.android.build.gradle.internal.dependency.androidtypeattr aar and found compatible value aar . - required org.gradle.usage java-api and found compatible value java-api . - configuration mymodule debugapielements variant android-renderscript - found artifacttype android-renderscript but wasn t required. - required com.android.build.api.attributes.buildtypeattr debug and found compatible value debug . - found com.android.build.api.attributes.variantattr debug but wasn t required. - required com.android.build.gradle.internal.dependency.androidtypeattr aar and found compatible value aar . - required org.gradle.usage java-api and found compatible value java-api . - configuration mymodule debugapielements variant jar - found artifacttype jar but wasn t required. - required com.android.build.api.attributes.buildtypeattr debug and found compatible value debug . - found com.android.build.api.attributes.variantattr debug but wasn t required. - required com.android.build.gradle.internal.dependency.androidtypeattr aar and found compatible value aar . - required org.gradle.usage java-api and found compatible value java-api . here is the project s build.gradle,as of gradlew 4.9 application plugin understands --args option so passing the arguments is as simple as build.gradle src main java my app.java bash,note in gradlew build system minsdkversion and targetsdkversion defined in androidmanifest will be overridden by what you define in your module s build.gradle file;so define them in build.gradle file not in manifest,"
"deserialization","serializable"," is the better overall,more in control process better, has a completely overall,longer equal overall, never used a package in constructor package default, does not handle problem properly in constructor package default, is even more overall, method is more overall, it gets default value in field bean value, not overall, not overall,","50047901,20996013,19204335,18810000,30205743,21373335,45020324,48366008,17360772,14569386,49393803,","maybe serializable deserialization is the better way than save java code at the file,side note check-out the json.net serializable which gives more options and better control over the deserialization process,the other is that while datacontractjsonserializer writes the local offset during serializable it doesn t use it properly during deserialization;it just assumes that if any offset is provided that the time should be local - even if the computer doing the deserialization has a completely different offset,i have an issue with serializable in c# .net where if i serializable in one stream multiple references to a same object these references are no longer equal after deserialization,i think the default constructor is only necessary for deserialization never used a package for deserialization;serializable shouldn t need it,java serializable was an old mechanism create decade before annotation;i guess that is why default serializable deserialization does not handle problem properly,using the same code for serializable and deserialization is even more of a pain i cannot off hand figure out how,also i do not know which serializable deserialization method is more suitable for such a use case,output is 10 because your object is in memory serializable doesn t erase transient fields it s just ignore them on serializable;output is 0 and 10 0 because y is transient field it s not saved on bean serializable so upon deserialization it gets default value witch is 0 for int,i have rolled back the project to the initial state without classes map or custom serializable and changed the struct type to class type and it worked;in resume this exception error is related to structs deserialization not with interfaces deserialization,see the serializable check where the check is called and where deserialization happens in a non-local cluster;i believe that local mode clusters only test serializable not deserialization,"
"deserialization","serializable"," package needs a default in constructor package default, is not overall, not overall, is not more in complex easy, is harder overall, code looks more overall,better control over the  in control process better, becomes a bigger overall, is much faster overall, is not overall,harder in harder self-made notice,","30205743,13659233,52234222,26236624,4103211,19277574,54538157,1687594,24075797,34027280,3019036,","serializable shouldn t need it;the default constructor is redundant because if the deserialization package needs a default constructor it obviously attempts to create an instance using the default constructor to afterwards set the field values which can t work with final fields,the key here is using the serializable to write an anonymous object that includes only the desired properties;note that readjson is left unimplemented - i am assuming that deserialization is not required at this stage,1 there is a special case though when you can register only an abstract class for the purposes of serializable deserialization not for kryo.copy though;this case is when your serializable is the same for all subclasses and during deserialization you can decide which subclass to return based on the data,and deserialization is not more complex,if you are worried about instantiating too many serializable you can always keep them in a dictionary keyed by type;deserialization is harder as you have to know the type of object first - if you can t get the caller to tell you what type to expect you will need to read the data before you try deserializing to determine the type,i ve reworked it slightly so that the deserialization code looks more like the deserialization code serializable counterpart,this gives you much better control over the serializable deserialization process and makes your class simpler with less boilerplate code,serializable deserialization becomes a bigger bottleneck when you scale out to a session server or use sql server to manage session state,have you considered creating an object structure for these files and serializing these files java object serializable and deserialization is much faster than parsing an xml this is again considering that these 500 or so xml files don t get modified between reads,of course since serializable is not the same as;deserialization i don t understand how this is an explanation for the,deserialization is harder than serializable,"
"deserialization","serializable","more memory than the  overall, is more in complex easy, using getter in property setter getter, is more overall, also doesn in field bean value, only is a feature overall, doesn overall, which is also less overall, side has less support overall, is slower then in slower next request,quicker in slower next request,","51106572,20598437,52322978,49312600,17360772,13038245,36061774,51939635,8189201,51025149,1092020,","also due to deserialization of the data it will take more memory than the serializable disk data,serializable is easy but we have to deal with complex data structure implementations std string std map for example that play with allocated buffers;deserialization is more complex and you need to rebuild your object with all its members plus references to vtables .,recently i had the same issue using jackson-annotation 2.9.0 and lombok 1.18.2 this is what worked for me so basically adding the annotation means that the property may only be written for deserialization using setter but will not be read on serializable using getter,i also tried but that gives the error impl doesn t use types inside crate i could probably live without deserialization since serializable is more important at this point,output is 0 and 10 0 because y is transient field it s not saved on bean serializable so upon deserialization it gets default value witch is 0 for int;10 is because z is static field thats mean it is not wired with instance serializable also doesn t erase it,you might need a different view on deserialization and not just on serializable and that would be a jackson 2.0 feature - supported by spring 3.2 and backported into spring 3.1;using a view on serializable only is a feature since jackson 1.4,looks like yaml serializable doesn t like it;i used deserialization with specified type as you mentioned and this is what i get,you could also go through serializable form deserialization which is also less risky and handles more situations the code there uses a slightly different format though so you ll probably have to adapt it,for serializable filtering properties blog entry should help;deserialization side has less support since it is more common to want to filter out stuff that is written,the problem occurs when a serializable is slower then the next request s deserialization,the serializable seems quicker but deserialization much slower and the app is doing more deserializing than serializing,"
"deserialization","serializable","more control in control process better,much faster in property setter getter,general overall, binary is normally faser overall, is harder in harder self-made notice,","3210800,12316966,49963452,6761005,25836693,","you ll need to implement serializable to have java handle the serializable or externalizable if you need more control over the deserialization process,if the taskproperty class is decorated with datacontract attribute and with datamember attribute for each property the deserialization is much faster than when it is decorated with serializable attribute,it comes with some serializable logic added via annotations to specify what should be ignored during serializable and what order should be applied during serializable and deserialization i want to change these annotations without having to creating my own class with all the other logic copied over,you need to find the balance in serializable and deserialization performance speed ease of working with the data size of files number of files and management of files and ability to use external libraries;when it comes to file foramts and serializable binary is normally faser than json which is normally faster than xml,i wold not bother with self-made serializable;notice that designing deserialization is harder than serialization...,"
"firefox","opera","just far more access overall,larger fonts overall, did get a much higher overall, is not overall,less such overall,general overall,lower overall, is usually less overall, mp4 is not usually overall,even worse in worse faster framerates,slower in worse faster framerates,","3323922,2292967,43112690,35473861,19552846,19355167,26089655,176144,20625815,1451062,1278273,","firefox just provides far more access to its own internals and system to the extensions so its technically possible to do more with them than with opera widgets or google-chrome extensions,update under linux gentoo amd64 it s the same - opera reneders slightly larger fonts than firefox but nothing that looks odd,while firefox showed no difference between the two;strangely though opera did get a much higher score across the board,you can add same for few more browsers ie opera safari... to be sure that script will run on users machine;script will not run if firefox is not preinstalled,for example in the classic browser share example if the firefox is at 45 i want to be rendered inside the pie distance -40 may be and where the slice is less such as opera at 6 - i want it to be shown outside with a connector,basically safari google-chrome opera is used in mobile devices and device features works fine with these;firefox browser is not a better option for mobile devices,in opera it goes little bit lower than in firefox,second safari has the greatest standards support combined with a marketshare which isn t negligible opera is just hard to justify for cost benefit reasons;third in my experience going from safari to firefox is usually less problematic than the other way around,opera also doesn t support mp4 and i can t find evidence that they will use the os s decoder if available;regarding firefox mp4 is not usually supported,the problem with this code is that it doesn t quite work on firefox and works even worse on opera,for example i ve read that opera and ie will fix framerates slower than firefox,"
"firefox","opera"," did not overall,faster overall,worse overall, before got bigger overall, also has a limit overall,significantly faster in worse faster framerates,bigger overall, can easily overall,even fewer overall, but not overall, messages wether i overall,","46535557,19554859,9978052,5819252,3563703,17023332,17721821,14139617,14056377,13054335,396899,","for some reason edge google-chrome and opera were able to ignore maybe that old version of jquery address while keeping jquery javascript to work normally;but mozilla firefox did not behave in the same way and jquery javascript was not working at all in mozilla firefox,but if firebug donâ t enabled after launch firefox â time equal 2ms that faster than opera and google-chrome and difference between empty filled array disappears,only firefox and opera seem to have this issue opera even worse than firefox but i don t care that much about opera,well if you take a look at this statistic it shows that opera is getting more known and popular since 2002 maby not that fast but it still growing;i m one of the people who used to use opera before and it has some realy nice features but since the people who used to use opera before got bigger it not as fast as before but it supports the maskerading to firefox and ie,firefox also has a limit of 50 cookies per domain;opera is lower at 30 cookies,it seems to be slower in google-chrome and firefox but is significantly faster in opera,on your screenshot div width in opera is bigger than in firefox and google-chrome,many modern browsers google-chrome firefox opera allows user to edit page on the fly so many modern browsers google-chrome firefox opera can easily remove the maxlength attribute,firefox has even fewer as far as i know and opera has none,i added a hidden text field which worked in firefox but not opera;opera forgets the hidden field values but not text field ones so i wrapped the text field in a display none div which remembers the value ok but javascript fires before the formfield has its value updated by the opera browser,download firefox messages wether i be using ie8 partner build firefox 3.1 beta 2 or another browser like opera safari,"
"firefox","opera"," has better browser overall,","17296826,","if the columns should be uniform using the multicolumn module might be a better way to go and this will only work in google-chrome opera and ie10 firefox has better browser support,"
"multiplying","subtract","faster overall, the now overall,more reliable overall, has significantly better overall, sequence was slower overall,greater than 9  in first field value,greater in first field value,weaker than  overall,faster overall,bigger overall, is a better overall,","12977648,53643341,417342,52005974,22076715,50384078,32910443,5269766,29260876,38324792,2665997,","p is sometimes chosen to be 31 because not only is it prime but a compiler resolves it to a bitshift and a subtract which is much faster than a multiplying,to start here is the first two arrays where array1 0 0 will be used to subtract the the input value from array3 0 0 and then array2 0 0 will be used multiplying the now subtract value from array3 0 0 to give the new output of array3 1 0 in other words these calculations will get array3,adding and subtract logarithms of factorials then taking the exponential at the end is more reliable than multiplying and dividing factorials directly,if range limits aren t a problem you could even fold the subtract in using an;simd fp multiplying has significantly better throughput than integer multiplying so it might be a win overall even with the extra cost of an fp round-to-nearest instruction,shift and add subtract sequence was slower than the lea method above,multiplying all odd positions indexes 1 3 5 etc. of input by 2. if any of these multiplied entries is greater than 9 subtract 9,subtract first field from the other and if the value is not greater than 0 multiplying by -1,similarly if you skip the five first elements your loop takes o n-5 time but that too is the same as o n because adding or subtract a constant is even weaker than multiplying by a constant,subtract is faster than multiplying,the logic is multiplying by 2 whole number 0.1 2 0.2 and if it s bigger than 1 subtract and continue,zero or multiplying by one;this also reveals that subtract is a better search term than minus,"
"multiplying","subtract","smaller than so  overall,","47817772,","you can also get rid of the expensive operation if you check and subtract 1000000007 because re is smaller than 1000000007 before multiplying it by 2 it is smaller than so subtract 1000000007 is enough complete program,"
"gridbaglayout","miglayout","probably easier overall,less difficult overall,much more overall, which is more overall,","12658326,7514203,16595066,4699921,","edit for clarification miglayout is probably easier to use than gridbaglayout but if you get familiar with gridbaglayout it s not that hard to use and it doesn t require a download since it s part of the core java library,look at the quick start guide on the miglayout site it s a lot less difficult than gridbaglayout and much more powerful,you can also check out the open-source miglayout which is much more convenient that gridbaglayout and also a bit more powerful,for more complex layouts i often used gridbaglayout which is more complex but that s the price;today i would probably check out miglayout,"
"feed","rss","more value overall, makes it more overall, module provides more flexibility overall, doesn overall,more overall, is more overall,more hotfixes than a  overall, string takes more overall,easier overall,faster in faster fwiw reader,prefer atom over  in faster fwiw reader,","4832591,48138913,4902145,5235559,43525024,3789816,2166220,26399169,5462449,37597683,242991,","the specification of atom feed offers more value than the rss one with internationalization content types and other things and every modern feed reader supports it,so if your rss feed is at the atom link element should be this including an atom link element in your rss feed makes it more portable and easier to cache,the views rss module provides more flexibility in terms of what you re able to include in the feed,you d gain compatibility and durability because rss is more defined things will change less but if the feed doesn t include what you need the feed doesn t won t help you,as you noticed yoou cannot create an rss campaign feeded by more than oone feed,rss is more simple that s where its strength is;atom is better defined yes but that s the problem they made it easy to write a very complex feed when at the end of the day you want a simplified summary,if microsoft provided a rss feed people would install way more hotfixes than a rss feed people should and we don t want people to unintentionally break people own apps and not be able to find out which of the 30 hotfixes people installed is to blame,for example if your screen is 600 pixels wide and if the rss feed string takes more than 600 pixels to display you will not be able to see the end of your rss feed,if your service already has an rss feed things are even easier with the rss reader integrated in the buzzbox api,the atom feed option seems to load faster than the rss one fwiw,i produce a feed reader and when scraping feed always prefer atom over rss,"
"feed","rss"," not overall,more overall, not overall, does not overall, isn overall, page doesn overall, isn overall, not outputting properly overall,","22476583,19577065,5982355,20713,939691,2446978,5833841,12008463,","now the rss feed button in your browser should ve become active because an rss feed is available for the page;it shows 10 latest news items in the feed not 5,google does this because their feed api can parse more than rss,the link to the rss feed is working properly and there is information at the address;so i started looking around and i found that the problem is with the rss not the code well the original problem,as there are quite a few deprecated versions of rss you might conceivably have to support rss feed that vary quite a bit in their formatting details;rss does not have a registered media content type,rss is there to keep you up to date;if a feed publishes 10 items an hour but only shows five you ll miss five of those items and the feed isn t serving its purpose,in the case of the rss feed to make sure that you are getting something back;it s possible the rss page doesn t have the same javascript components and that it returns an empty string,that is an xml feed rss is always xml;your issue is likely that feed isn t a real url even if safari handles it,rss feed not outputting properly causing your site not to show the right information improper tag use or unescaped characters;may not be updated right away i know a few feed that update on a weekly basis,"
"keyboard","mouse"," but still overall, interactions set up earlier overall,general overall,happier with the  overall, i ve found py overall,slower overall,non-obvious issues with  in non-obvious issues events, not overall, which gives you more overall,more accessible in dropdown menu accessible,faster overall,","34759200,15262701,51424640,3320135,1946361,6731523,51262647,40886823,36475044,35051381,1295425,","the default keyboard s a little bit easier than the mouse but still a manual task,then update this current object reference during appropriate mouse event methods;now you can use this current object in your keyboard interactions set up earlier,for neovim-qt gui client you can change the font by ctrl + mouse scroll if you put the following to ginit.vim for those who prefer using keyboard there is a nice way to use numpad s + kplus and - kminus obviously you can replace consolas with the font you prefer,if you re happier with the keyboard for me a mouse notepad++ has to be emacs,for the mouse i ve found pymouse which seems to work i haven t fully tried it a small hack needed for the click cf the issues;for the keyboard i m not sure xlib can do the job,this is a radical concept i know but the mouse is slower than keyboard shortcuts,i would suggest that you don t mix setlayout and setcentralwidget as this will lead to sometimes non-obvious issues with mouse and keyboard events,you may also poll the mouse polling is not always a good idea anyway with method pressedmousebuttons of nsevent;as you may notice keydown tracks keyboard not mouse so you need to track the mouse,and then all your users are relying on your left right controls no matter whether all your users re using a mouse or the keyboard which gives you more control over how it looks at each stage,i need to change my dropdown menu which works well with a mouse but i would like to make it more accessible with keyboard,i prefer the command line because i have a shell window open in the source anyway to run builds and tests and along with file name completion using the keyboard is faster than using the mouse for me,"
"keyboard","mouse","accessible by  in dropdown menu accessible,accessible by   overall,more important overall,prefer  over  overall, events is more in non-obvious issues events, driver is better;i overall,much more slower overall, is faster overall,much better overall,faster overall, a selected tag value overall,","48541026,51181013,923190,53717027,10697578,13327353,45399198,30738710,3982894,414289,56378101,","in this case maybe calling combobox hide will also work this assumption is correct as you can see if you change the selection by keyboard in which case the dropdown is not open the combo is still accessible by keyboard and mouse,its not accessible by keyboard mouse,also in this case keyboard input is essential and more important than mouse interaction,my mind simply refuses to accept the fact that people who wrote ipdb god bless them prefer mouse over keyboard or that i m the only one who pastes code into debugging sessions edit before posting the question tried all ctrl-,in this way you can send any keyboard and mouse events as well;ignoring mouse events is more difficult,so creating your own mouse driver is better;i needed a safe way so simulate mouse keyboard behavior for my bot and i wrote a detailled article on my blog,doing it with mouse is much more slower than it can be done with keyboard,the ctrl key is usually close by my keyboard hand and in combination with the mouse is faster than the page up page down keys,it recommends using ctr cmd + 1 to use the keyboard this is much better than having to use the mouse but i m still looking for a just do the right thing most of the time automatic solution,the keyboard is almost always faster than the mouse,i need the user to be able to select using mouse or keyboard a selected tag value and be able to copy the text somehow,"
"keyboard","mouse","events more overall,more overall,slower overall,","10389173,161193,31884800,","this approach using purely images screenshots and generating mouse and keyboard events is more similar to manual testing activities performed by real people which have just monitor mouse and keyboard,for example do you use keyboard more than mouse,the last fallback is to go the snail way - to mouse over to click on the red close window button but any mouse movements are slower than a keyboard shortcut,"
"division","modulo"," not in remainder property operator, will also in integer multiplier value,much faster in faster solution computer,more overall,much harder in number bits harder,higher priority overall, gives the game away overall, is greater in integer multiplier value,faster in faster solution computer,more overall,better straight in number bits harder,","19809895,43707441,20258656,31210691,12786755,13440423,13496434,27178198,27589182,16247449,29476678,","modulo returns the remainder of the division and the division operator returns the quotient of the division;you should use modulo not division operator,the symbol is for modulo not division;as your two variables a b are both integers the result you will get from a division will also be an integer which in the case of 300 400 is zero,since bit wise operations can be done very fast and division operations are relatively slow this type of modulo is much faster than doing a division,modulo is more mathematical if you like while the remainder in the c-family is consistent with the common integer division satisfying and this is adopted from old fortran,programmers like to use this property to speed up programs because it s easy to chop off some number of bits but performing a modulo is much harder it s about as hard as doing a division,is the same as because division has higher priority than modulo,second your algorithm is trial division not the sieve of eratosthenes and will have time complexity o n 2 instead of o n log log n;the modulo gives the game away,multiplier the integer division of the value by the divisor;if modulo is greater than zero the multiplier is incremented,if you compute modulo a power of two using bitwise and is simpler and generally faster than performing division,inverse modulo for 300 time take 1.422 seconde more than executing division sub and multiplication 10k time even the core of inverse modulo is build with same division and sub and multiplication functions and for this number it just do 150 time inside while help plz why,modulo gives you a remainder which is why it s better than straight division in situations where you re number of elements can change,"
"division","modulo","slower overall,remainder than  in remainder property operator, is faster in faster solution computer,better overall,faster in faster solution computer,really more overall, is a lot more overall, will not overall, solution is so much overall, is remainder in remainder property operator, is faster in faster solution computer,","27980441,50438042,14032673,8914667,48103,10063634,31929434,31985034,1892498,57169762,56457683,","so in simple terms this should give you a feel for why division and hence modulo is slower computers still have to do long division in the same stepwise fashion tha you did in grade school,the division property conceptually is more about remainder than modulo,all we have to do is unconditionally add n and do another modulo;that s unlikely to be the best solution unless you have a computer where division is faster than branching,using an extra variable to avoid the costly division and the resulting time was 18.9s so significantly better than the modulo with a statically known constant,a side effect of many division routines is the modulo - so in few cases should division actually be faster than modulo,python respects this definition whereas in most other programming language the modulo is really more like a reaminder after division operator,this is not an approach you can count on because division with floats is imprecise;using a modulo is a lot more reliable,so as long you have a perfectly representatble floating point number there is a good possibility that the division modulo will go through smoothly;however when you try to divide or take modulo by 0.1 or 0.2 there is always a roundoff error in its floating point representation which means the division will not be complete and hence the result will not be true,just use integers and division will just work for you;except of course the modulo solution is so much more elegant,the or modulo is remainder division,since std mt19937 is superior to rand the answer should be modulo is faster than division,"
"diagram","entity-relationship"," do not in parallel models data, modeling is more overall, or not overall,better than   overall, model supports many-to-many binary overall,more overall, modeling isn in parallel models data,","25642147,12446158,26491618,656377,47726681,23722201,25808990,","entity-relationship modeling is very parallel to object modeling except that object modeling models behavior as well as data and entity-relationship modeling only models data;the problem with learning entity-relationship modeling for this purpose is that in the present state of affairs most of the professionals who use entity-relationship diagram do not use them to depict a conceptual model,first entity-relationship modeling is more than just an erd;we tend to try to put the entire entity-relationship model on one diagram,an entity-relationship diagram is more abstract;sql server let you create a physical data model diagram that could come from a entity-relationship diagram or not,another article s the only conceptual modeling tool available for relational data the others - erwin and another article ilk - being at best logical modeling tools with nothing much better than entity-relationship diagram and ddl synchrnoization which are also provided by object role modeling tools,entity-relationship diagram aren t a good fit for document databases;the entity-relationship model supports many-to-many binary as well as ternary and higher relationships composite keys and attributes on relationships all of which aren t directly supported in hierarchical network model dbmss,oo class diagram is more abstract and has more features than entity-relationship diagram,unlike most people i prefer to make a sharp distinction between diagram that depict an entity-relationship model and ones that depict a relational model;contrary to prevailing opinion entity-relationship modeling isn t just relational lite,"
"maven","pom.xml"," has also overall,more overall, is not overall,more work in eclipse right click, to expect your source overall, file doesn overall, app is a dependency overall,up-to-date with  overall, it couldn overall, this is more ore overall, and has no value overall,","15988729,11371300,19163398,28884953,10906635,24161287,54199786,56654200,34682937,55190576,15680657,","provided-scope dependencies are indeed inherited from parent pom.xml but not from pom.xml defined as dependencies and i consider that a maven weakness;given that maven has also difficulties in adding modules as dependencies across module hierarchies i can t say maven is a sophisticated tool to manage multi-module projects,it has some downsides- namely working with both ant and maven so the actual pom.xml is more difficult to comprehend but it does allow for more flexibility than is otherwise possible with maven,maven has a concept of importing dependencies;the documentation says that this feature is for cases where extending a base pom.xml is not possible,converting to maven requires more work -- afaik there is no official tool or wizard that allows you to generate a full pom.xml configuration from an existing eclipse project,otherwise maven will not know where to find your source code;it is possible to configure a pom.xml to tell maven to expect your source code to be in different source folders but it is highly recommended that you follow the standard maven directory structure,when you run the goal archetype create-from-project maven generates a pom.xml file for building the archetype at target generated-sources archetype pom.xml and then runs the package goal by default on this pom.xml;the generated pom.xml file doesn t have project.build.sourceencoding or any other property defining encoding and that s why you get the warning,when i launched jacoco dump i successfully connect to my container and a jacoco.exec file is created locally not empty my app is like that docker container wildfly where i need code coverage local machine my maven application local machine my selenium maven project where i execute jacoco my plugin config pom.xml edit the jacococli generate a report so my exec file is correct edit ok so i think i find my issue the problem is my maven app is a dependency of my selenium project it seems that jacoco-maven-plugin is unable to use a dependency to get all the class files to create the report but with ant there is more options so here is my solution i finally have the report automatically generated by the pom.xml thanks,also i m displaying my main pom.xml file below and it has compilation errors on pom.xml and both ndash i ve tried the following solutions eclipse blue maven project configuration is not up-to-date with pom.xml failed to resolve version for org.apache.maven.archetypes repository element was not specified in the pom.xml inside distributionmanagement element or in -daltdep loymentrepository id layout url parameter error failed to execute goal org.apache.maven.plugins maven-deploy-plugin 2.7 deploy default-deploy on project dfc_app deployment failed repository element was not specified in the pom.xml inside distributionmanagement element or in -daltdeploymentrepository id layout url parameter,i was struggling with it trying to start it up using maven and i got the same result as yours unable to find a suitable main class;i spent several hours trying and changing stuff and after adding the property mainclass to the pom.xml it couldn t find it,however if you really need to use maven for that you might use the following pom.xml this is more ore less an equivalent of the build.gradle file,in another word unless the profile is activate by default maven doesn t know about it you might be tempted to activate everything by default in your case but bear in mind only one profile can be activate by default at the time;your problem is svnbranch from trunk is only present in your child pom.xml and has no value therefore maven only acts on the gav and not the classifier,"
"maven","pom.xml"," to build java in easier dependencies version, doesn overall, makes life easier in easier dependencies version, coordinates more information in deploy phase information,select run as  in eclipse right click, is not running so overall, is not in easier dependencies version,classifier construct overall, is easier better overall, and not overall, will not overall,","52381120,2371109,18420717,55350701,53335898,14885437,48715988,16450819,15402153,6815409,8353812,","you can find information about required libraries inside pom.xml it is much easier to use tools like maven to build java applications,but maven doesn t;to solve the problem either remove the jta exclusion in simple-persist pom.xml that s the quick and very dirty fix or fix the hibernate dependencies to make them converge that would be the right fix,using maven makes life easier in such cases;opening the pom.xml in ide like eclipse sts will give you a better picture like following -,you can configure the deploy plugin to provide the maven coordinates more information can be found here and the file name definition with in the pom.xml itself and invoke it to the install phase maven profile should be helpful in achieving this task,and setup project.follow this steps import existing maven project or create a maven project goto project explorer and right click on your pom.xml then select run as maven install,the pom.xml is in memory as a java object tree of the project setup at build time only when maven reads the pom.xml and creates the model;at runtime maven is not running so pom.version has no value,maven is not bash;pom.xml is an xml file it has tags some of the tags are filled with text,maven uses the classifier construct for artefacts build from the same pom.xml but differing in there content for example one for jdk1.6 or jdk1.7,the sensible thing is to use ant or maven maven is easier better in most cases;you write a build script build.xml or pom.xml respectively and run the builder ant or mvn,this is implied by romaintaz s answer to the same question maven - skip parent project build;but just to make this explicit in the parent pom.xml it s imperative that you specify the packaging element as pom.xml and not jar or war,pom.xml is a maven build configuration file;if you are not using maven adding a pom.xml will not help,"
"maven","pom.xml"," and not in eclipse right click, will not in available public irrelevant, it is just in easier dependencies version, file not in eclipse right click, not being in synch;i in eclipse right click, is a lot in easier dependencies version, make a section overall,familiair with  in easier dependencies version,deeper overall, or move the resource overall, is not good enough in easier dependencies version,","20547404,45552630,37102794,52674654,6496030,42786343,41118445,55504717,43611728,56215961,24107357,","that is an important thing to keep in mind the maven pom.xml is the lead in this kind of project setup;if you want settings to change try to do that through the pom.xml and not through eclipse project settings directly or doing a project update might revert what you have changed,you can refer to this answer for additional details on how to create such a repository maven add a dependency to a jar by relative path;the sql server drivers are not available in public maven repositories so unless you have a private maven repository to which you ve added that dependency including it in your pom.xml will not result in the jdbc driver being available at runtime,with maven it is just simpler to gather mongodb java driver dependency;you just need to declare mongodb java driver dependency in your maven pom.xml file,the reason you cannot build the plugin with mvn install is because you have configured maven to expect the plugin to already be built;this section belongs in a different pom.xml file not the plugin pom.xml file,i would guess that your issues arise from the eclipse project and maven pom.xml not being in synch;i would suggest that you use the m2e plugin to keep maven and eclipse in synch,using maven is a lot easier with managing dependencies like this;example pom.xml,maven isn t perfect but i highly recommend using either maven or gradle;to define multi-modules from the root pom.xml make a section like this,i m not so familiair with maven but added the following dependencies to my pom.xml i expected that the olwapi-contract dependency contained the test package,when maven is going deeper it falls back to normal pom.xml,by default maven will not process copy resource files from the directory with sources;you need to explicitly configure it in pom.xml or move the resource files into folder that will be of a resource type,maven uses this variable to build your project so java_home and the compiler plugin version must match;just changing the java version in your pom.xml is not good enough,"
"maven","pom.xml"," files quite in need new experience, but not in easier dependencies version, profile only in profiles profile profiles.active, plugin define them later overall,accessible by  overall, and is newer overall, central has many more overall, downloads the dependency now overall, extract is mostly overall, shows an older overall,click run as  in eclipse right click,","25010461,15988729,49733603,37507413,51150873,10691669,53525328,55973057,56867043,47064313,50447614,","if you re new to maven but have some experience with pde build then maven is harder;for maven pom-first there s a need to tweak pom.xml files quite often,maven expects a strict single-rooted hierarchy that is only suitable for the simplest projects;provided-scope dependencies are indeed inherited from parent pom.xml but not from pom.xml defined as dependencies and i consider that a maven weakness,i got many jobs and here are just two of them my application.yml config spring profiles.active activatedproperties and pom.xml some profiles from big list of them i want it to be like spring profiles.active eodt0 eodrepo instead of activatedproperties if i check more than one maven profile only one will work,maven will not do this however if there s no value available at this time;therefore you can remove these properties from pom.xml and let maven plugin define them later in build lifecycle,i have configured jar.version as dynamic input to pom.xml i tried to configure maven goal in eclipse s run configuration as i also tried to add jar.version 1.0.1 in parameter section of maven run configuration. but this parameter is not accessible by maven build process,i also answered this other question about .exe files but generating a console .exe without gui wrapping a java command line application with launch4j and maven;this one have a complete pom.xml and is newer,if you read the travis log you can see project name missing project description missing project url missing license information missing scm url missing developer information missing invalid pom.xml io reactivex rxjava2 rxjavafx 2.11.0-rc31 rxjavafx-2.11.0-rc31.pom project name missing project description missing project url missing license information missing scm url missing developer information missing missing no javadoc jar found in folder maven central has many more requirements than bintray i suggest you look through the minimum requirements here,first of all please make sure that the dependency that you ve added indeed accessible by maven even without ide just run something like mvn package to see that maven downloads the dependency now if the project was imported into idea by opening the pom.xml intellij is supposed to build its internal data based on the information that was found in pom.xml so the following might help maven - reimport all maven projects - this will trigger the synchronization process settings - build execution deployment - build tools - maven maven home directory - intellij might be configured with a different maven than you think you use has something bundled with ide,given that the pom.xml extract is mostly about a custom plugin and its configuration you will have to develop a matching gradle plugin to replicate the features of the maven plugin;maven plugins cannot be used by gradle,the first issue is that the maven command is not compatible with the version of cucumber;the argument cucumber.options was introduced in version 1.1.1 please see this thread but the pom.xml shows an older version,wrote the code responsible for sending a post message using the library i am not sure it works tbh since i could not run it built the maven project from within eclipse by doing pom.xml right click run as maven build .,"
"maven","pom.xml"," will not in eclipse right click, should be jar in easier dependencies version, do not overall,now more parsing overall, repo available server in available public irrelevant, is not overall, takes precedence; in deploy phase information,simple as  overall, is not yet overall, is not overall,more suited as the  overall,","14247594,4821539,42576636,14727072,48768469,45169443,55858093,56113068,20304476,25540809,6072962,","put the following plugin declaration in your maven pom.xml build file;running wsgen then wsimport then wsgen then wsimport will achieve what you need but you maven will not let you build a module twice,your maven project doesn t seem to be configured as a eclipse java project that is the java nature is missing the little j in the project icon;to enable this the element in your pom.xml should be jar or similar,i found a trick so that maven do not troncate the url used for pushing the tag;i precise both push and fetch scm url in the pom.xml using the same url,once the set of projects has been validated maven now does some more parsing of those pom.xml files to construct the list of build extensions if any and the list of plugins,since the classes may get recycled or the groovy class loader isolation magic loads it differently our environment eclipse neon 3 uses groovy 2.4.5 internally starts with a jdk 1.8.0_121 jaspersoft studio 6.3.2.final we have a 6.3.0 server but full neon support was added in 6.3.1 so we took the last patch versioning of 6.3.x we had issues before where newer studio versioning did not work with older servers so we are more careful with upgrade here may be irrelevant it was enough to just depend on the slf4j api 1.7.12 in our case in the pom.xml we have both provided jasper libraries added to the project jaspersoft server library jasperreports library dependencies we also depend on server functionality so we also have jasperreports-6.3.0.jar in the pom.xml and some not via some public maven repo available server jars as a user library to the project which we copied over from the server no groovy reference in the pom.xml,i like this because i don t have to install maven and i don t have to add the maven integration plugin to do this simple thing although the maven plugin does have other features that better or worse might be interesting to some folks;there is a 3rd option use the build step called invoke top-level maven targets in a free-style project where you select a maven installation see below then set its goals clean install or clean install -f subdir pom.xml if your pom.xml is not in the root,execution order in maven is based on two components first the lifecycle phase to which the execution is bound for executions bound to the same phase the ordering in the pom.xml takes precedence;maven doesn t allow the same plugin to be configured twice so it s not possible to move the docker stop execution below spring,it is quite enough to have only selenium-java it will resolve via maven transitive dependency mechanism given you mention that you have to manually download selenium standalone server you don t need this selenium-server dependency as well so it should be as simple as pom.xml test class demo more information remote webdriver selenium with java chromedriver - getting started,force import the maven pom.xml using the reimport all maven projects button in the maven tool window on the right side by default;if the pom.xml is not yet showing click the add button navigate to the pom.xml file and add it,maven ---- update project;if your pom.xml is not specific as to the version of the maven-resources-plugin that version will come from the superpom,its will depend on the complexity of your builds if you have basic java builds then maven might be more suited as the pom.xml files maven build filds are pretty simple but if you need to do a lot of custom specific stuff you may well end up having ant scripts to perform also if you already have one system in place there isnt necessarily a huge advantage of swapping,"
"maven","pom.xml"," file contains a lot in easier dependencies version, doesn overall,project more in easier dependencies version, or not in need new experience,get  to publish in easier dependencies version,easier in easier dependencies version,discoverable by  in easier dependencies version, this is logback.xml overall, file not in easier dependencies version, not yet in deploy phase information, that made the problem in easier dependencies version,","15279160,812117,26857936,43722797,47338098,33131999,49250729,55743594,11282002,1726531,25493378,","maven is the paramount of a method of defining your build using a formal grammar in a standard manner;your pom.xml file contains a lot more than just your build it is the identity of your artifacts the project metadata the modules and the plugins brought in,this will tell maven to use jfoo 1.0.0 up to jfoo 2.0.0 so when jfoo releases version 1.2.17 you ll be fetching that in the next build assuming your settings are set to check versions each time;this pom.xml doesn t have to actually build anything,keep in mind that a maven project is more than just the pom.xml file it also conforms to the maven standard directory layout and more but that should get you started,the maven-jar-plugin provides the capability to build jars files if you define that your project is packaged as a jar file maven will call implicitly to this plugin;we don t need to define it inside pom.xml it will be downloaded and executed when maven needs it.so does not matter whether you use this inside your pom.xml or not it will get automatically loaded by maven if needed,when publishing to a maven repository using the above method the pom.xml will not have the proper version;to get maven to publish the proper version use the flatten-maven-plugin,you would be able to circumvent this by first running mvn clean install on magnicompcommon s pom.xml then on model pom.xml but this is much easier done by invoking maven directly on the root pom.xml,package org.yaml.snakeyaml.external.biz.base64coder does not exist means that you do not have the dependency specified in your pom.xml file or is not discoverable by maven,i am using maven in spring boot project. this is pom.xml this is logback.xml this is my main class the problem is when i am running my project on my local machine i got logs like vahan logs are generating but i wonder when i deply my project on testing server i got blank log file and no logs are printing in log file. please help me,realize that maven is not magic;and that if you are going to use maven and eclipse everything has to be driven by the pom.xml file not the other way around,set up a public maven repository deploy the parent pom.xml to it and put the repository information in the child poms;you can tell maven to look for the parent pom.xml on a relative path before it checks repositories this is to help you when you have local changes to the parent pom.xml not yet deployed,yes i know that is the default scope of dependencies but for some reason maven was not capable of the job;see a piece of my pom.xml that made the problem disapear,"
"maven","pom.xml"," projects click next click overall, file; packaging lasts more overall, file is only then overall, prerequisites approach overall, to change the version overall, will not overall, but make an activebydefault in profiles profile profiles.active,familiar with  in eclipse right click,","54137404,8325803,26860045,18055010,23303384,15652914,10842047,53451676,","type maven in the search box select existing maven projects click next click browse and select the folder that contains the pom.xml file click finish if all of these steps worked you will see a screen similar as below,part of the configuration is dispersed along in the pom.xml file;maven packaging lasts more because the filtering,maven doesn t yet know where the pom.xml file of the parent is;it has to use the block to figure out where the pom.xml file is only then can it read the contents of the file,the maven pom.xml prerequisites approach will be lighter weight as the maven pom.xml prerequisites approach will not require the maven-enforcer-plugin to be downloaded,when a jar is in the release repo maven doesn t automatically download a newer revision if the user is using an older revision;maven will integrate with your revision control system and automatically update the pom.xml to change the version to a non-snapshot number,however you say that removing the deployment and then re-deploying actually works so i m not sure how the re-weaving gets done if you don t execute a maven build before redeployment;i m not familiar with aspectj but myeclipse does not execute maven goals to build and deploy projects though you can add builders which may invoke maven externally - note that the maven project builder doesn t execute maven goals so processing specified in your pom.xml will not be executed automatically though obviously you can run maven goals from the run as menu item,maven doesn t support any sort of list or way to store several properties tags inside one tag;however you don t need to duplicate the plugin configuration you can just move it entirely into the profiles and not have it defined in the main pom.xml at all alternate still remove it from the main pom.xml but make an activebydefault profile which has the default plugin configuration,there it says in pom.xml add the following xml stanza between i am not familiar with maven or gradle or how it works nor did the documentation of maven really help me with importing this library into my java project in eclipse,"
"sublimetext","vim","better overall,easier overall,more accessible overall,","13118840,39787498,32047973,","textmate isn t known to perform well on large files but sublimetext supposedly performs better than vim on large files and yet supports legacy textmate syntax parsers,personally i m using sublimetext right now which i find easier than vim for searching and navigating the code but everyone has their taste,sublimetext or webstorm are far more accessible than vim so it usually happens with their editor rather than mine,"
"neo4j","orientdb","result way longer then overall,faster in ti faster depth,better overall, is less overall,slower in ti faster depth,lower overall,more fancy overall,","26497068,45860902,39445325,11074230,14565718,34219834,23779937,","as you can see the neo4j result is way longer then the one from orientdb,using orientdb that is supposed ti be faster than neo4j for depth i m seeing a slow performance,i read in some blog that orientdb performs at least as good if not better than neo4j while also offering more complex types like mongodb would,orientdb or neo4j are graph databases that can be embedded;orientdb is less mature but has a better license,i found that orientdb is too slow at least much slower than neo4j even on relatively small 150k datasets when searching records by text pattern despite the presence of indices,the number of requests orientdb could serve is consistently 3 to 5 times lower than neo4j,orientdb looks more fancy and advanced but not as popular as neo4j and is not supported on heroku,"
"h.264","mp4"," encoded video overall,better support in avi format free, and even overall,more often overall,format better in avi format free, does not overall,","29948715,9734251,31648732,11314411,16599515,28076784,","mp4 is not really a format but a container that can hold video in different formats;firefox supports only h.264 encoded video,- you can also consider using webm which is a free alternative to h.264 and has better support on some platforms than fragmented mp4,mp4 is a container and the video and audio in it may be encoded with various codecs which you will need to check your browser supports most support h.264 and even the same codec using different settings baseline vs mainline profiles,h.264 is more often encapsulated in mp4 container and vp8 in mkv,it doesn t support avi at the moment but imho mp4 format is better suited for h.264 anyway,yes h.264 is not royalty free but wrapping it an an mp4 does not remove that restriction;.264 is just h.264 nauls saved in annexb format,"
"hstore","jsonb","much more expensive overall,more compact overall,","42787991,7934577,","i admit that gin index on hstore is much more expensive than its equivalent for jsonb but even then it is faster to just do seq scan on hstore than use index on jsonb,json jsonb fields support any json-encodable data type not just key value pairs but also tend to be faster and for jsonb more compact than hstore,"
"acid","consistency","principles much more easy overall,more overall,","17914452,45723184,","using a database system such as sqlite or mysql that follows the acid principles is much more easy as the database system guarantees consistency atomicity of the transactions isolation and durability,however the isolation part of acid sounds more like consistency model in particular the sequential consistency model,"
"lodash","underscore.js","more in latest easiest features, does not overall, causing a bunch overall, composability is better in accident composability backbone, has better performance in accident composability backbone,better choice over  in details blog comment,faster implementation overall,more modular overall, is faster overall,faster overall,even more features in latest easiest features,","13898916,33736552,50140592,27514972,17225303,45621592,17156415,33964991,32190714,21148491,28931507,","because lodash is updated more frequently than underscore.js a lodash underscore.js build is provided to ensure compatibility with the latest stable version of underscore.js,debounce.cancel is implemented only in lodash with this commit underscore.js does not implement it;if you re using underscore.js or lodash here s a quick and elegant way to solve this problem,if you re using lodash the function signatures are different-- underscore.js allows you to define a this context as the third parameter whereas lodash s doesn t;so in our case our lodash was overwriting their underscore.js causing a bunch of weird issues.,this is apparently no accident if these are left out of underscore.js lodash composability is better than features,therefore in my app backbone doesn t know if it is underscore.js or lodash as long as i am passing _ using amd;i started to use lodash just because i read lodash has better performance,i believe lodash is a better choice over underscore.js more details in this blog post,lodash is another faster implementation of underscore.js that will provide a lot of utility methods for working wit arrays objects functions etc,lodash is more modular than underscore.js underscore.js around 5kb lodash around 17kb but is generally lighter because you only include the specific modules you need,based on why lodash is faster than native foreach post maybe its s justifiable to use lodash in favour of both underscore.js and native foreach to loop,if i remember correctly lodash argued they were faster than underscore.js because the use non-native functions for key operations,the easiest solution is to just replace underscore.js with lodash which has even more features than the latest underscore.js,"
"lodash","underscore.js","better version overall,more convoluted overall,general overall,faster in faster stunned simple,more overall, _.isfinite doesn overall,better solution in details blog comment,slower overall,faster in faster stunned simple, but is more overall,","43641416,23922041,35574474,18300039,42076586,38500450,32372660,27342533,20321282,54071171,","if you re open to using lodash which is a better version of underscore.js imo this can be expressed fairly simply using flattendeep on each element of your multi-dimensional array,basically collections are things that implement some kind of iterable interface and they internally use the same iteration method though lodash source is a bit more convoluted than underscore.js,you can transform data using lodash or underscore.js;here is an example built using lodash btw i added one more array element to show how groupby works,i am stunned right now seeing a lodash performing 100-150 faster than underscore.js in even simple native functions such as array.every in chrome,given that this is 4 years old and lodash has more or less taken the place of underscore.js i thought i would share this solution using lodash,underscore.js _.flatten is deep by default while lodash is shallow;underscore.js _.isfinite doesn t align with number.isfinite _.isfinite 1 returns true in underscore.js but false in,edit as pointed out in the comment lodash is likely to be a better solution than underscore.js,lodash is definitely not slower than underscore.js,the solution from charliefl is approximately 100x faster than underscore.js depending on browser in this case and lodash being up to 2x faster than underscore.js,there is a fantastic library called ramda which is similar to underscore.js and lodash but is more powerful,"
"debian","linux","worse than on  overall,older overall,compatible with  overall, has got better overall,smaller base-image than the  overall, sid is more overall,","22438068,43331973,34383821,636468,46709154,53943481,","personally i develop on the windows but deploy on debian where couldn t find the scrapyd package at all at the time i was working on the scrapyd package and noticed that performance of scrapy is considerably worse than on linux box so you might reconsider your target,if your system linux is older like debian wheezy for example your glibc version will be older than the required one,i installed a new virtual machine openvz containter ct with debian 8 and had the same problem at start i thought that the problem was with the upgrade but having the same problem with a new fresh debian 8 install i have search for new solutions;the problem it s that i was running proxmox 3.2 which uses a linux kernel not compatible with debian 8,os upgrades always have the potential for subtle bugs whether it s windows linux or anything else;debian has got better than it used to be in this regard,the first line in the dockerfile is widely used to have a separated tag with the postfix -alpine in the first line in the dockerfile to indicate that alpine linux is used which is a much smaller base-image than the debian image,i recommend installing and using some linux distribution in particular because debian sid is more developer friendly and provides valgrind which is very useful to hunt bugs,"
"drag","move","smaller in press release order, was greater overall, is significantly less overall, lasts for more overall,bigger parent in press release order,general overall,more difficult overall,slower overall,","24920106,12221360,57627154,33061269,29649620,57708587,9984591,44699825,","it ends up in a state of confusion...you basically cannot release the drag of the table it keeps getting bigger and smaller as you move your mouse - but you can never let go and release it to get the size you want,in the mousereleaseevent i check to see if the overall move was greater than at least a tiny amount,if i drag a card i will move in the direction of the mouse however the distance move is significantly less than the distance the mouse move by the time the mouse has reached the edge of the screen the card has only move one cards width,otherwise you can update the velocity so the object move towards the touch position;finaly on mousebuttonup set the velocity to the average velocity of the last frames up to a max of 10 if the drag lasts for more than 10 frames,i reimplemented the mouse move press and release events for the inner widget in order to be able to move it inside its bigger parent with drag drop,read more here i would recommend using drag and drop feature to move files into xcode,this is easy to implement with buttons select items click up or down buttons to move - just loop through and apply the move to each selected item but much more difficult with drag reordering,i got a draggable object done with jquery ui and when i drag it it s way slower than i move my mouse,"
"kerberos","ldap"," not overall, which takes a lot overall, was always more overall,more than  overall,far more overall,more overall,","26296778,40782208,46188971,29680565,46188971,242546,","3269 is not kerberos this is ssl-backed global catalog;pure ldap not kerberos,if you are using kerberos ad ldap then create a user there setup kerberos which takes a lot more effort,kerberos is the inside-the-corporate-network industry standard single sign-on protocol;ldap was always more of a directory look-up protocol,active directory is far more than ldap - at the very least you d need kerberos but active directory also tightly integrates with dns both srv and a records which d require significant trickiness to fake over an ssl connection,in short as an authentication protocol kerberos is far more secure out of the box is de-centralized and will put less load on your directory authentication servers than ldap will,one might want to define kerberos access to more than one ldap server,"
"pandas","seaborn"," is a lot overall, is more overall,more flexible overall,much easier overall, which allows more customization overall,","50025985,51964508,24921847,38947307,56723020,","there are just so many features that you can control with the plotting capabilities of pandas which leverages matplotlib;i found that seaborn is a lot easier to produce pretty charts and you have a lot more control over the parameters of your plots,seaborn is more for making the plots more readable with less direct intervention in the script and generally gets the most mileage when dealing with pandas dataframes for example yields as to how to set the styles the way you want for the variables you re trying to show that i m not sure how to handle,for similar functionality that speaks pandas but has more flexible features you could use the facetgrid object from seaborn,i also think working with seaborn is much easier using pandas dataframes and not numpy arrays,so if you are using pandas for basic plot you can use matplotlib for plot customization;however i propose an alternative method here using seaborn which allows more customization of the plot while not going into the basic level of matplotlib,"
"apc","memcached","much more faster in slower single process, is faster in slower single process,generally less overall,faster in slower single process,slower in slower single process,faster overall, u cannot overall,faster overall,more overhead overall,faster overall,faster overall,","11584538,17118318,10858440,13751524,17295791,22253724,37253509,7465290,1935264,1825484,18910135,","as you know apc is much more faster than memcached if we re fetching keys from a single server,apc is faster on a single server - but memcached is distributed,the big caveat here is that apc generally has less memory available for storage though. default being 32mb memcached s big adavantage is that while still fast it s distributed so if you have multiple servers running they could share this data,it s slower than apc but it s faster than memcached redis etc,memcached is in-memory too and a bit slower than apc,better use apc on apache server it will be much faster than memcached because you don t need to make a tcp ip call to connect to memcached,i d suggest using memcached especially if you re concerned about performance;whilst you are correct that apc u is a lot faster than memcache you re not taking into the account that by the time you re worrying about these metrics you will be running across multiple servers and apc u cannot be shared across nodes,memcached is faster than apc as zend_cache backend but you still need apc extension installed even in development mode to get a great speedup of your code,memcached has more overhead since you have to use a tcp connection to access it versus just a function call for apc xcache shared objects,a quick googling says that apc is 5 times faster than memcached,please don t mention memcached i am already using it but it is not suitable as direct replacement of apc because i am doing many apc_fetch calls because apc is much faster than memcached for this purpose,"
"apc","memcached","faster in slower single process,more in opcode caching memory,more functionality in slower single process,more in opcode caching memory,more overall,slower in slower single process,faster in slower single process,more functionality overall,less feature overall,","9884780,15117742,1803139,4999069,1935264,1808814,2353190,10135428,1825484,","in my case apc is 59 times faster than memcached,memcached is more for caching database queries and improving performance in that regard while apc is more for improving performance of php code,memcached has more functionality but is intended for distributed environments while apc works on single servers only,apc is more an opcode caching system than a key value memory database like memcached altough it can be greatly used for both purposes,memcached is more along the lines of a distributed object cache vs something like apc or xcache which stores php bytecode in memory so you avoid having to parse it each time,memcached around 4-5 times slower than apc but run as a single process that can be touched everywhere in my environment,apc access is a bit faster something like 5 times faster than memcached if i remember correctly as it s only local -- no network involved,apc gives you some more functionality than memcached,apc have less feature than memcached and is easly to use and optimize but this depends on your needs,"
"dsa","rsa","lot slower in generation testing note,much slower overall,older overall,faster in generation testing note,smaller overall,shorter overall,more compact overall,slower than  overall,keys much riskier overall, is considered more overall,slower in generation testing note,","12594809,24689036,10115891,5868759,7911156,2917821,4599528,57153085,43577180,37479236,7911156,","i have made some testing and it came out that rsa is lot slower than dsa,this is why rsa is much slower than dsa,i guess rsa is older and dsa is newer,a dsa signature generation could be somewhat faster than a rsa signature generation maybe up to twice faster,dsa has signature that is independent of key strength and is much smaller than rsa signature for equivalent security rsa 1024 1568 vs dsa 192,dsa signatures are signficantly shorter than rsa ones,use dsa it tends to be more compact than rsa,dsa in general s slower than rsa and ecdsa dsa in general has more perfect or broken states than rsa and dsa in general key generation is several orders of magnitude slower than rsa and ecdsa,that continued usage of 1024-bit prime field elgamal or dsa keys is much riskier than it is for 1024-bit rsa all are still commonly used because once a successful attack has been conducted against a single well-chosen prime field all users of that prime field may be affected at little additional effort.,yes rsa is considered more secure;in october 2014 openssh 7 the default with ubuntu 16.04lts has disabled default support for dsa,however dsa verification expect verification calls to be 100x issue is about 10x slower than rsa verification,"
"dsa","rsa"," signature is faster in generation testing note,less overall, or better overall,better choice overall,","55579437,16525310,46125270,41289399,","note that checking an rsa signature is faster that checking a dsa or ecdsa one for respective keys length corresponding to the same security level,however in this case dsa key size is less than rsa key size,after this because there are more options than ssh-dss dsa the client openssh v7 should connect with rsa or better algorithm,rsa is also a better choice than dsa because it has much better breadth of support for signatures still considered secure by nist,"
"mouse","trackpad","#inner overall,much more overall,higher overall,general overall,","39257760,32828353,13442793,31543961,","let s boil your trouble down to this if the mouse is over #inner you can t use the usual methods spacebar arrow keys trackpad wheel to scroll #outer up and down,the trackpad is much more sensitive and can get the breakpoint increment correctly but the mouse wheel ends up scrolling through the section much quicker without correctly keeping up with the proper frame rate so i never end up reach the final frame by the end of the section,unfortunately the trackpad s scrolling deltas are orders of magnitude higher than a mouse s so the scroll speed is psychotically high,so for a mouse since it scrolls at an interval move 10 y every scroll position it isn t noticeable;but for a trackpad scrolling is at a much finer grain,"
"margin","width"," was greater in right div greater,less in padding css directives,greater than the  in right div greater,less in 16px 20px x,more in viewport body iphone, is no more in auto flex item,greater in right div greater,size greater content in content border padding,less than  in right div greater, is greater overall,s more related overall,","953927,38602970,56463628,22302383,13059986,49916613,1567157,7190768,19753594,22980115,45996349,","you said you specified the height to 800px and wanted the div not to stretch when the width was greater than that.;to center horizontally you can use the margin auto,i notice that when the arrow block is rendered when width is less than 296 px inside the carousal there is some space between the left border of the carousal and the arrow block even though i did not add padding or margin,give the heading a right margin equal to or greater than the width of the floated element,instead of the reaching 100 width the margin is causing less width for them due to the 20px x 12,you re negative margin is more than half the viewport width of an iphone therefore it is dragging it out of view,you container is set with display flex and .hero-image is a flex item and it s width is no more 100 like a default div but it s equal to size of its content and that s why margin auto is centering your element where you don t need to specify a fixed width,give the sample text div a right margin greater than the width of the right-floated element,the first-child column has 10 pixels or whatever the margin size is greater content area width than it s siblings,margin 0 auto brings any block element to center if margin 0 auto has some width applied which is less than margin 0 auto parent element s width,t also trims the width by a percentage referred to as the margin;the margin is only used if the margin is greater than zero and you would specify a number like 0.20 twenty percent,margin using percent doesn t work cross browsers on flex items so if your margin s is more related to the viewport use viewport units vh or vw else you can combine px with css calc width calc 33.333 - 20px to match an equal gutter between the items,"
"margin","width","less in content border padding, 16px is more in 16px 20px x,bigger as  in content border padding, might is bigger overall, is bigger;interestingly you can actually overall, adds more overall,higher in right div greater,more difficult overall, is less in screen margin size,animation longer overall,more div overall,","17646403,57380606,55876410,23069175,30645551,31236608,41787489,45232082,21763325,23095425,17188618,","this means they won t float next to one another unless you set the width of each so that the combined width + margin + border + padding of both blocks is equal to or less than the width of the container,because width 100 plus margin 16px is more than 100,this creates the box model i just described where the content width is equal to the width style and the element gets bigger as margin border padding is added on,i noticed your website width might is bigger than the browser;instead of using margin -8px on you .container and .footer just set,you must set the leading and trailing numbers to zero never negative even though the width is bigger;interestingly you can actually set the leading trailing numbers to any positive value try say 50 and it gives you kind of a margin of the bounce,adding margin adds more width making the total width of the two elements and their margin greater than 100 pushing the elements onto separate lines,now if the div with has margin 660px from the left it should be hidden because it s margin is higher than the maximum width of the main container,but the margin is more difficult to include in the width because it is on the outside of the box,if the width is less than the screen size;margin auto,my guess is that they are technically happening simultaneously but that the margin animation takes longer to complete than the width animation,how to place 2 div side by side one div is of 75 width and the other is 25 width.one more div with margin 0 auto,"
"margin","width"," which is more in auto flex item,larger than window  in right div greater, is wider in content border padding, is larger than div in right div greater,larger overall, is larger in right div greater,auto better overall,less in large h3 classes, is greater in right div greater,less than the  in value base widget,greater than the  in behavior negative example,","15604026,48178566,15262835,24136478,28288003,22419247,28907018,31763862,14349123,57024512,53185715,","use min-width and or max-width to give that element a defined width which is more flexible than a single width value and then give that element a margin 1em auto style to centre it horizontally within the unused width available in it parent element,what you are doing is saying the element should be as wide as the browser window 100 of the window width and then also saying that outside that width the element should have more margin in your case 15px on the right and 260px on the left and that is streching the page causing the total widht to be larger than window width,to make sure the margins shrink before the content use a fixed pixel width for the content and margin 0 auto;and when the pixel width is wider than the screen make that width smaller,the whole problem is because of the div 2 height and width is larger than div 1;as we know padding width and height is not added to the element width and height as it is done for margin,when the screen width becomes larger than 500px the contents of the media query are ignored and the margin becomes 20,tables are one such instance that don t seem to align without margin auto;also make sure your div width is larger than your button width,first one margin auto is better if you can know width of container list and you don t want centre content of this container,you have to take into consideration that borders take up width and that the margin of 1 of either side of the large container means that it can only be less than 100 width,the width of the span content plus the left margin is greater than 1000px and so 1000px wraps spans to the next line,so i called a mediaquery to get the screen size and base the widget off that margin just slightly less than the width of whichever screen it is on to make the value responsive,with both boxes floated right the second box is placed on the left of first box +------------------------------------------------+ | +-----++-------+| | | red || black || | +-----++-------+| | | with widths added on the boxes the second box is pushed on a new line +------------------------------------------------+ |+----------------------------------------------+| || black || |+----------------------------------------------+| | +----------------------+| | | red || | +----------------------+| | | with negative left margin width added on second box the second box is placed next to first box +------------------------------------------------+ +----------------------++----------------------------------------------+| | red || black || +----------------------++----------------------------------------------+| | | | | | | | | a rough explanation of this behavior is that when you add a negative margin equal to greater than the width of the element it apparent width becomes 0 and browser will fit it on the same line,"
"margin","width","smaller as  in content border padding,less than 30  overall, is 400px more or less overall, 10px is more then in screen margin size,bigger overall,larger overall,more in images space image, is greater in right div greater,longer than the  overall,nicer in padding css directives, that is less in right div greater,","55876410,52795696,44292992,49206210,33567403,10486251,15199420,26823180,55894855,604314,29478000,","with this style the whole element will only be the width of the width style and the content area gets smaller as margin border padding are applied,as you use the margin of 1.66 that s why you must have to use less than 30 width in div.main-column,on a 5 screen roughly the width is 400px more or less and the height is 600px or more;so a left margin of 909 will push it off the screen area for good let alone the top margin,for width 100 in some screen margin 10px is more then 1 so,what i do not understand is why the size of the point and the text is not the same and why the margin can be bigger than the width of the figure,by giving the middle indicators div a left and right margin slightly larger than the width of the left and right button divs you allow it to float up between the two and take up as much space as possible,50 width is more than enough for the images to fit even with the image margin,therefore assuming the container s width is greater than the image s height margin -50 0 should be enough,of course you need first to check there is enough place before to write a word supposing the separator are only spaces all the separators are produced even consecutively or at the beginning of a line in case a word is longer than the width a word is not cut case of azeryuiop in the execution later a proposal from your code where the program receives in argument the left margin the width and the string compilation and execution,edit someone else suggested css and i believe that using the height and width directives from css will give you the same result as using the same attributes of the tag but doesn t limit you strictly to images and a will play nicer with any margin or padding directives you might use,if the content leads to a width that is less than 100 of the table s containing block then setting the left right margin to auto will center the table within the containing block,"
"margin","width","lesser in right div greater, and not overall,smaller than the  overall,larger in right div greater,larger than the  in content border padding,slightly wider overall,more overall, and is floating left in right div greater,bigger overall, that is greater in right div greater,more div with 980px  in right div greater,","27755071,29956814,52863925,30945014,27317590,16430191,30378885,27351057,6746105,18518651,48091291,","one way of doing this is checking if left margin of the inner div is lesser than the width of sidebar and then only add class adjust,also note that when a margin is specified in terms of percentage the margin s value is computed as a percentage of the width of the container;even for the vertical margins the percentage is applied on the width and not the height of the container,using autoscale sales the pdf document smaller than the width of the screen it basically adds a margin around the pdf,the the main content div needs to have left margin larger than the width of your sidebar,the width padding and margin together make the box model so that the total width of the element is larger than the width of the screen,another option is not to float the main content column but instead assuming it s the right column to give it a large left margin that is slightly wider than the width of the left column,if browser width is more than 768px then set a 110px margin,the second div input-btn-col is covering the div with the select element select-col because it has 100 width and is floating left and it nested div has a margin on the left,the box is never too small infact the margin of the adjacent div is bigger than the width of ul div,you can try giving an id to each one of the divs or style each one of the divs for margin that is greater than or equal to the image width.assuming you image width is 50px this code shall work,you can do it by adding one more div with 980px width to wrap inner container so applied margin will work,"
"margin","width"," is less in large h3 classes, is smaller in behavior negative example,greater in right div greater, without having any scroll overall,greater in right div greater,bigger in right div greater, auto is actually overall, auto is less overall, set as auto in content border padding, has a lower overall,greater overall,","44478884,53794835,5494802,12265317,39685256,36665967,37772133,35180854,49351837,50917019,15830195,","while width is less than 780px h3 will have this classes;2 - you will have problems with the margin in the container i would add padding to the container instead of using margin in the boxes inside,here is a basic example as you can see the trick is to define the margin considering the width of the container 100 and we will have two cases when the width is bigger than xpx we have a positive margin and a normal behavior with spacing when the width is smaller than xpx we will have a negative margin and will have the overlap effect without wrapping,the title goes in a h1 for example and the text in a p or div so set these two elements to have a left margin greater than the width of image a,simply put when you want an image to go to 100 of the monitor then you need to put it into a container with width as 100 and margin auto not fixed width and no position at all;you will see if you increase the width of the fiddle box then the image automatically increases width without having any scroll bar,i just made the left margin of div2 greater than the width of div1 and it worked for me,the reason why your website is x-scrollable on all devices is that you forgot to add to and the row inside it has 100 +15px left margin +15px right margin which is bigger than window width,for a horizontally centered image - display flex and justify-content center on the parent element of the image that has a specified width;however for your example i used margin auto because margin auto is actually simpler,width + translate is maybe not the best way;block and margin auto is less tricky,you need a div as a wrapper for your webpage and set a fixed width or max-width if you want the content to be able to become smaller on smaller screens with margin set as auto for margin-left and margin-right to keep the page centered,also make sure to include jquery this is possible paste this beneath all other scripts to your .userhasscrolled class you can set something like this --first answer-- what you ask for is media screen and max-width 767px while the screen width has a lower size than 767px you can define an overwriting css style like this but this causes another problem while the navbar is closed the main class will still have a margin of -336px and that s not what you want i guess,i m using the function window .resize to detect if the total width of all open windows plus some margin is greater than the window s width then hide the first chat window or show the last one if there s space for more windows,"
"margin","width","greater in right div greater,less than the  in right div greater, you wouldn in value base widget,less in viewport body iphone,more than 100  overall, is higher in images space image, is greater in right div greater,less overall,smaller than thus  overall, cannot overall,greater in right div greater,","5555847,10692069,36258545,35299571,54776321,57242304,24136478,19968815,56535571,20738305,17308362,","a common solution is to float your fixed-width column left or right and give the other column a margin equal or greater than the width of the fixed-width column,to ensure that the element s maximum width is less than the width of the parent allowing some space between the content of the element and the borders of the parent;margin 0 auto,so to calculate the width including the outline you need to explicitly add it;note even if you decide to take the elements width including margin you wouldn t get the outline-width value by just using outerwidth true,the elements are sized relative to the entire viewport but the body width is less than this because of the default margin on body,it is this line the takeaway is that something with a width of 100 and a margin will end up being more than 100 width of the page,i tried calculating the images width and the margin space between them to set the limit value for the right button so slider doesn t slide past it but that doesn t work if you see it on another device because it s width is higher,as we know padding width and height is not added to the element width and height as it is done for margin;so the scroll get the scroll width and height of the div 2 and as the div 2 height and width is greater than div 1 padding to right and bottom is not visible,just so you know you want the total width object width + padding + margin + border of both columns to be less than the width of the container,when max-width is smaller than width of .divs is effectively smaller than thus margin 0 auto works,since the smaller child view is 50dp you have added a margin of 130dp the total width it needs is 180dp but the parent view itself is only 150dp wide;that is - the child plus the margin cannot fit inside the parent,once the negative margin is equal or greater than the width of that element further negative margin has no effect,"
"margin","width","more in padding next total,bigger overall,bigger overall,clipping earlier overall,less overall, you used earlier in right div greater, is more overall,greater overall,greater in right div greater,less in right div greater, is more than 700px then in images space image,","25428575,26513621,28122212,15982149,14087592,33119632,18757820,42291487,39193574,37293218,38682542,","and if they aren t next to eachother that means your padding and margin is more than the 5 width of total width so reduce them,when i launch it with firefox or internet explorer the box gets immediately to a huge distance from the margin much bigger than the width of the screen and now i noticed that if i run it with stackoverflow s snippet function the box does not move,but in general this wouldn t work because of the specified width is bigger than the real one and exceed it so there s no margin to auto it and center the content which found in div,so it would also be possible to have all three textblock columns starsized but that means width clipping does occur earlier and there is more margin,essentially my logic is when the window is re-sized and the width is less than 810px then remove all css styling and add margin top items 2 through infinity,ositioning the element 20px from left and right and setting width to auto will make it get the right width while respecting the gap you want it to have on either side;and margin auto 0 would work as well simply undoes the margin you used earlier to center the element,i believe it was only one single page whose width is more than the screen s width;and the default view is achieved by setting the margin or other property which is for you to findout using trial and error,on release the drawer should slide open if its margin is greater than half its width and should close if its less,you have given width of 50 with additional margin of 5 to each of the element which is greater than 100 width,but if the screen width is less than 200 then it will be from left view edge margin to right view edge margin.,for example let s say width of one image you want is 200px then also you d like to have some space as margin between images too so let s say if browser width is more than 700px then we ll have images on the same row else we ll have them one below the other,"
"margin","width","bigger overall, which is better overall,smaller than the  overall, is no longer overall,bigger overall, is greater in right div greater, which makes everything wider in padding next total, read more overall,","40405403,35524094,49909518,55663570,9126839,19985007,45347273,51700720,","aside of the left floated div i have another div article with a margin a little bigger than the width of the floating div just usual for a two column layout,using a left margin of auto and a right margin of 0 the div will be flushed to the right;note that i also used a padding of .5ch for one half of a letter width which is better than 3px hardcoded,t s rendered with a gap usually called margin or padding between the bullet and the content;so because the is inside the the width is already smaller than the width of,with the top chart labels set at a precision of 8 and the bottom chart having a max of 5 digits 10 000 as the max the width of the bottom chart can be made equal to the width of the top chart by including the following however if the number of digits in the bottom chart s valueaxis labels changes then the left margin is no longer correct,anyway i just set the margin to a value a little bigger than the width of the image and overlap is eliminated only in the very specific case we want,ox-sizing border-box sets the height width of an element and takes into consideration the padding as well as the border width;the scope of an element s margin is greater than the element itself meaning the element itself modifies the flow of the page and the element itself surrounding elements therefore directly altering the way the element fits within the element itself parent relative to the element itself sibling elements,the padding and margin are added to that width which makes everything wider than you would think,this creates columns of equal width and your buttons reside within each columns;you can add spacing between your buttons by providing a margin read more here,"
"dozer","orika","faster overall,better overall,faster in close manual slower,slower in close manual slower,","12161589,10046175,23957375,22939629,","wel i have used both orika and dozer i can say orika is at least 10 times faster than dozer in my project after the replacement,performance will depends on your application use cases basically orika perform better than dozer or other mapping reflection based frameworks because it use bytecode generation behind the scenes,orika is way faster than dozer and quite close to manual mapping,i tested orika and it was a lot faster but was slower than dozer with small collections,"
"hash","md5"," though not in sha1 better algorithm, is harder than just in sha1 better algorithm, is not overall, not in sha1 better algorithm,such as  in second time sink, is still overall, is not overall, is not in value original chance, is not in output functions cryptographic, method is slightly less in sha1 better algorithm,much more difficult in value original chance,","48803120,34448106,25237459,21598834,30219361,2517670,14936670,12465713,25859376,15072525,2718168,","yes on most cpus sha-256 is two to three times slower than md5 though not primarily because of its longer hash,it is not probable to make a change and have the same hash even with md5;note that finding a collision to a known hash is harder than just finding any collision,the md5 hash is not available for every object;while most objects have an md5 hash many objects do not,they should be hash not encrypted;md5 is a hashing algorithm but a very weak one that is entirely unsuitable for protecting passwords with today,so if it takes 1 second to generate each hash it becomes a bigger time sink than it would have been had you used a fast hash such as md5 of a version of sha,while md5 has been broken those breaks only allow you to create two different files images with the same hash but not with a predetermined hash;what you want is called a pre-image attack and since md5 is still unbroken concerning that attack you need to bruteforce,you can lookup in the database as the hash will be unique;md5 is not ok for hashing numbers anyone can reverse a numerical md5,finding the original value from a hash function is in general not possible;md5 is not an encryption,this is because cryptographic hash functions are designed with certain properties which also explains why md5 is not suitable in such a role;the ideal cryptographic hash function has four main properties,the security of the hash method is slightly less important for a key derivation function;it s probably best to choose one that matches the required key size although use of md5 in general should be discouraged,obviously md5 is much more difficult to reverse even in this anything that will hash to the right value sense then a simplistic algorithm like take the last two digits,"
"hash","md5","function more in sha1 better algorithm, which is faster in sha1 better algorithm,secure  than  in sha1 better algorithm, takes longer in output functions cryptographic, value is somewhat greater in value original chance, is likely slower overall, was broken but is still in sha1 better algorithm,cake more overall, is less overall,no longer overall,faster than other  in sha1 better algorithm,","3153718,5177403,51749626,38574726,15732823,10178330,2329665,9267163,13677411,8704228,4025516,","if the hash function is more complex and cryptographically strong md5 or sha1 then it is theoretically not possible,was getting weird cases where a different app engine instance was seeing different hash;using md5 which is faster than sha1 faster than sha256 for now,yes using a salt hash is hugely better than an unsalted one but sha is a much more secure hash than md5,a cryptographic hash takes longer to calculate but changing just one bit in the input will radically change the output and for a good hash sha-512 there s no known way of getting from output back to input;md5 is breakable you can fabricate an input to produce any given output if needed on a pc,an md5 hash is twice as fast;the chance of a collision two different text producing the same hash value is somewhat greater though,lso both the hash you cited have issues md5 more so and should not be used if there is any risk that someone could have any incentive to create documents having the same hash yet being different this is readily doable with md5 which is completely broken from a cryptographic point of view and possibly not far away with sha1;basically what you propose hashing then compare the hash is likely slower than plain compare unless you read from a really seek averse media and have issues of a really seek averse media own,storing hash password is better since it hides the password from prying eyes of dba s;also yes md5 was broken but is still used to this day,the hash cake generates are more complex than md5,7 of a single md5 hash is less than 1 4 kb,but if you still wish to figure out the phpbb password encryption hash it is no longer md5 in version 3.0 or higher and is a custom hash,in that case some hash functions are somewhat faster than other md5 being one of the fast functions but md4 is faster and md4 is simple enough that md4 code can be included in any application without much hassle,"
"hash","md5","actually less secure overall,using  to check in sha1 better algorithm, which really overall,faster in sha1 better algorithm, is faster operation overall, which is generally as in good hashing security,value longer in output functions cryptographic, algorithm is a lot in good hashing security,better than using  in input element safe,implementation significantly more secure in sha1 better algorithm, technique is usually more overall,","18852096,37462987,18658100,3737518,53184132,3126262,31445149,10438915,53837331,10934748,32065449,","double md5 hashing is actually less secure than a single hash with some attack vectors,using hash to check your files are downloading okay is probably fine depending on the hashing algorithm you use;while hash collisions are possible with for example md5 i do not think this is very likely,so with this knowledge of why hash are usefull you get to the next point;if you consider a 16 byte hash such as md5 which really shouldn t be used for cryptographic purposes but you already knew that that gives you 128 bits of entropy,this generally uses a hash algorithm that is much faster than md5,if your response is really big i would recommend switching to md5hex assertion tick save response as md5 hash box at the advanced tab of the http request sampler calculate md5 checksum of the file the approaches differ depending on the operating system and software you have as the majority of the world is on windows i d suggest winmd5free check out how to use jmeter assertions in three easy steps for other options and more detailed information replace response assertion with md5hex assertion and use the checksum from step 2 instead of full response the idea is that equal strings have equal hash and comparing 2 short hash is faster operation than comparing 2 big strings,you can t reverse a one-way hash theoretically you can find a plaintext that has an equivalent hash which is generally as good but you can t in any reasonable amount of time so you just need to set a new password and email it to them as a temporary and or just provide them a link to reset their password;md5 isn t encryption - it s a one-way hash,so i guess it s the md5 in the dump output cause the conflicts and the hash value is longer than md5 outputs,the class also commits a computer science no-no of using a cyptographic hash function where cryptograpic security is not required;the md5 algorithm is a lot more expensive to run than a regular non-secure relatively,personally your form error handling such as if it s empty in this case can be remedied by adding required at the end of each html form input element this is what i d do secondly md5 isn t safe for hashing passwords you re hashing a password not encrypting it thirdly here s a way to hash the password from the form using bcrypt which is much better than using md5 hashing,the wrapper uses these in such a way that even the md5 implementation is significantly more secure than a simple hash,the hash technique is usually more cpu-intensive because you will need to hash every file uploaded but you can then provide the hash to your clients to check the integrity of the file,"
"hash","md5","slower in sha1 better algorithm,smaller than   in sha1 better algorithm, is not in good hashing security, is not overall,general in good hashing security, algorithm not in sha1 better algorithm, is still more overall, is no longer overall,simply more overall, is better in sha1 better algorithm, has always in value original chance,","12183240,16418738,18043466,2920482,43479013,6046512,2717463,21832859,8646550,36731059,2672933,","the fact that bcrypt produces hash slower than md5 because of security reasons is also clear for me,linux command uses key as is if is length smaller than md5 hash block length 64 bytes otherwise is uses md5 key as key and not key derived using cryptderivekey rc4 md5 key like in your implementation,another thing to note md5 is not a good hash function for passwords because it is much too fast;a really fast hash function is a hash function where trying a large amount of passwords from a generator is a trivial task,computing an md5 hash of your text-including-the-to-be-computed md5 hash is not possible;well adding a md5 hash to a text changes the text and thus the md5 hash,this hash function can no longer be used for cryptography because the collisions are very easily found but if you need to use it just for your own checksum where collision attacks are not an issue i recommend using md5 even today in 2017 provided that you have found a fast implementation of this famous hash function,first of all md5 is a hash algorithm not an encryption algorithm.;if you really need to use the md5 algorithm i don t have any solution for you,note that while some people even trying to answer you re question think that md5 is insecure the reality is that md5 is still more than adequate for most purposes although i d recommend one of the more recent hash if you run paypal or control the launchpads for a fleet of nuclear weapons,the hash idea should work minor note that sha-256 wouldn t present collisions md5 is no longer secure but sha-256 wouldn t present collisions md5 is no longer secure depends on the trade-off if the service involves something really really critical a client-side ssl certificate would be the best way to proceed,crypt with hash is simply more expensive than md5,then when testing the password for correctness you hash the password for correctness the same way and then compare the results -- sha1 is a common hash for this md5 is better than nothing,also given a hash you cannot get the original value well you can always try cracking it that s the purpose of a hash;the result is always the same because an md5 hash has always the same length,"
"hash","md5","stronger in sha1 better algorithm,less cryptographically in good hashing security, outputs is much greater overall,such as  overall,no longer secure in sha1 better algorithm,better options overall,less likely in sha1 better algorithm,shorter  than  overall,larger in sha1 better algorithm,better  than  in sha1 better algorithm, was specifically overall,","6879159,3837891,1240875,50133362,13102496,11420347,8822558,54612789,43414199,52940857,16042217,","use hash and choose hashing algorithm that suits you well if possible something stronger than md5 but don t go all the way to sha512 either,if you don t need the cryptographic properties then a non-cryptographic hash or a hash that is less cryptographically secure md5 being broken doesn t prevent it being a good hash nor still strong enough for some uses is likely to be more performant,moreover md5 is made to make it difficult to find any such reversed hash however there have been attacks that produce collisions - that is produce two values that hash to the same result but you can t control what the resulting md5 value will be;however if you restrict the search space to for example common passwords with length under n you might no longer have the irreversibility property because the number of md5 outputs is much greater than the number of strings in the domain of interest,i explored hash based algorithms such as md5 and bcrypt,md5 is no longer secure and it can be broken with a good gpu no typo you can use your graphics card to break a hash,of course any hash algorithm is going to have some chance of collision but you have better options than md5 that would still satisfy the 1024-byte limit,other hash functions such as sha-1 also have hash collisions although it is much less likely than md5,this probably won t be of use to the op since they were looking for 2 way function but may help someone looking for a shorter hash than md5,any hash function like md5 sha1 sha256 etc obviously cannot be 100 unique - because they have a fixed length and due to the there must necessarily be non-unique results for input content that is larger than the hash,you should also strongly consider a better hash than md5 as it s considered weak these days,gumbo - no md5 is not broken but the main purpose of hash like md5 now and in the past was for file checking as you know to check if the contents of the file can be trusted not password storage as hash are very quick to decipher since you wouldn t use bcrypt to check the contents of file since you wait for 30 - 45 seconds.;so this means that a hash was specifically meant to be read quickly,"
"hash","md5"," has more overall, or not in value original chance, is effectively overall,less secure in sha1 better algorithm,faster than cryptographic  in sha1 better algorithm,bigger in sha1 better algorithm, sha1 is what i in sha1 better algorithm,longer in sha1 better algorithm, is not in sha1 better algorithm, is much better in sha1 better algorithm,slower sha1 in sha1 better algorithm,","34457553,1715461,28213014,44348142,51975751,12615409,1786423,9572031,20416824,33063682,25440325,","sha1 whole file hash;this came after the md5 because the md5 has more address space,a md5 hash has no internal structure or any kind of signature that allows you detect if a 128 bit value is a md5 hash or not;a md5 hash is a 128 bit value,as you can see the hash is effectively an md5 hash of the email address;md5 hashes cannot be decrypted,if you want to compare you re better off putting your result through getstring and compare the much less secure hash even less secure than md5 already is,it s just that no effort is made to make them resistant to cryptographic analysis that could lead to constructed collisions . take a look at these pages which have extensive info on recent developments in general these hash are going to be slower than an ad-hoc xor implementation but much faster than cryptographic hash like sha and md5,it s like complaining that a sha256 hash is bigger than an md5 hash - yep it is but only one of them might slow the attacker down for a while,use a content based hash sha1 is what i use;md5 is weaker and faster but on modern processors speed isn t a concern,because sha256 hash are much longer than md5 hash you can detect the hash algorithm by looking at the length of the hash,further no one-way hash should be a reversible function;md5 is not a reversible function,one hash is enough;for security md5 is not the best method hash is much better,also the hash algorithm itself is much slower than sha1 md5 sha1 md5 are made for speed - not a useful property when storing passwords,"
"hash","md5"," will not overall, is not more overall, considering it output in output functions cryptographic, password cannot overall, not in good hashing security, have not overall, will certainly overall, is much faster in sha1 better algorithm,much more in sha1 better algorithm, and then overall,more secure than  in input element safe,","2683666,13798043,47912639,4389022,20760206,22923388,8262666,8156982,2329664,15271067,52722701,","as a general rule a good hash will not hash two similar but unequal strings to similar hash;md5 is good enough that this isn t a problem,make sure adding an md5 hash is not more overhead than you save though,it can be quite much faster than md5 sha functions and it produces good hash considering it output but it produces a smaller range of hash 64-bit output and as such is more likely to cause more conflicts,and it makes sure that a hash password cannot be uniquely reverse engineered;md5 is a one way hashing function which will guard your original password more or less safely,if you pass a string you get a hash which cannot be reversed. it can be used for comparison purposes;some of the hashing algorithms are md5 not considered secure sha-1 sha-256,basic straight hash have not been considered safe for years;md5 has been outright broken for years,no in general the hash doesn t change by move_uploaded_file somehow magically;but if you compute the md5 including the file s path the hash will certainly change if the file is move to a new path folder,instead of storing a list of links as strings store a list of md5 or sha1 hash for those links;comparing a hash to a list of hash is much faster than comparing a string to a list of strings,with a salt md5 is much more secure than without but you re better off using one of the sha hash such as sha256managed,in short if md5 isn t good enough for collisions use a stronger hash if the stronger hash are too slow then use a fast hash with low chance of collisions such a as md5 and then use a slower hash such as sha-1 or sha256 to reduce the chance of a collision but if sha256 is fast enough and the doubled space isn t a problem then you probably should be using sha256;hash functions are generally written to evenly distribute the data across all result buckets,just using the basic example on that page would be more secure than md5 back to your question yes if the password is saved as an md5 hash into the database you have to convert the password in input with the md5 function and then check if your hash is valid,"
"hash","md5"," will not in sha1 better algorithm,shorter than   overall,stronger overall, ing still overall,safer in sha1 better algorithm, to assert the user overall,more compact overall,stronger in sha1 better algorithm, function consider binary type in good hashing security, not overall,larger in sha1 better algorithm,","764215,12332127,18105107,29112766,9935465,3505368,41795843,3759576,18154912,855978,19874443,","an md5 hash is a form of validation;if a single bit in the file is different the provided hash will not match the one generated from the file you downloaded,the resulting page identifiers will also be shorter than md5 hash and will only contain digits so the resulting page identifiers will be easy to include in url query strings etc,you can for example store a hash stored with something stronger than md5,md5 hash is not recommended to be safe no more;if you prefer md5 hashing still then as i see in your code in line where md5 hashed theres seems no problem,finally sha1 hash are safer than md5 i tell you that because you used the md5 tag in your post,after you authenticate a user you start a session and in that session you store md5 username+time and also store a cookie on users browser of md5 username+time then on subsequent page requests you could check your session hash matches the cookie hash to assert the user is who they say they are;md5 is not a good hash for this type of thing but hashing in general can help in situations such as these,i would like to apply a hash code solution on my webpage which is more compact than md5 and sha-1 because i want to use them as keys in a json hash table,valid choices for hashing include sha1 or md5 although sha1 is preferable because it produces a larger hash and is considered cryptographically stronger than md5,md5 hash is not the best choice if security really matters;however if you will use any hash function consider binary type for it instead md5 will produce 16-byte hash so binary 16 would be enough instead of char 32 for 32 characters representing hex digits,it doesn t matter when you digest the salt prefix postfix infix all produce different hash but achieve the same purpose of defeating rainbow tables or other pre-hashed dictionary attacks;i think that the comment has to do specifically with a vulnerability in md5 not hashing in general,the difference between crc32 and md5 is that md5 generates a larger hash that s harder to predict,"
"hash","md5","better in sha1 better algorithm,worse in proven breakable worse, isn overall, is not in good hashing security,more reliable in proven breakable worse,more robust in sha1 better algorithm,better in sha1 better algorithm, maps not overall,much more secure in sha1 better algorithm,larger overall,general overall,","6046512,32569597,25536294,21646234,116739,10798653,3295996,6200589,134346,38041702,42140940,","however if you want to use the sha256 hash alogorithm which is better than the md5 then here s a code sample,mac os x llvm gcc based crypt fuction is borked junk hardwired to only use des a proven breakable hash worse than md5,so the mismatch of hash types is quite obvious;setting security sethash md5 isn t going to do anything since the hashtype set in auth config will take priority,md5 is not a good hashing algorithm for passwords as it s designed for speed that s the opposite of what you want you want the password generation to be relatively slow so use a more secure algorithm and hash multiple times;you can generate a hash using sha-256,sha-256 uses 64 characters in the database but with an index on the column that isn t a problem and it is a proven hash and more reliable than md5 and sha-1,all hash functions have that problem but some are more robust than md5,or you could employ a request signing scheme in which certain request fields are hash together with the password of the user basically sending the password without sending the password similar to digest authentication but you can use something better than md5,however this is a simple hash good enough for hash maps not for security;if you are looking for cryptographic hash then the issue is completely different and md5 is loosy and you ll need a library for for example a sha-2 hash,using a non-reversible hash such as md5 is much more secure and you can store the hash value as clear text,hashing is one way you can prove this to yourself by taking an md5 or shasum of a large file since the file s size is larger than the hash output by pigeonhole principle hash can t be restored.,edit the answer to that post has also a very good workaround for applying crypto hash functions with older versions of hive using apache commons static methods and reflect,"
"hash","md5"," functions produces larger in output functions cryptographic, would not in sha1 better algorithm,unique as  overall, does not in sha1 better algorithm, not overall, is no smaller overall, is more in sha1 better algorithm,higher strength in second time sink,","28166852,997309,55178439,10115216,24225289,1293769,46296527,1786428,","sha-1 and sha-2 hash functions produces larger outputs so it is unlikely to be that functions;you can t decrypt it as by design md5 if non-invertible function,it is useful to check the integrity of a downloaded package such as ubuntu because generating the md5 hash will be identical if the package is exactly the same as the authenticated source;if there are errors in the download from network interference etc. or if you downloaded a malicious package that was made to look like ubuntu but contained other code whatever then the md5 would not match and you would know something was different and you should download again from a different source,note but checksums are not as unique as hash like md5 sha ..,md5 does not encrypt a password it hash it;a hash is a one-way function that takes a given input and creates a predictable output that is a given input will always have the same output ideally such that even very similar inputs will yield vastly different outputs,md5 is a hash not a compression thing;md5 is not reversable,a md5 hash is no smaller than the uuid so it doesn t help with storage,this is because attackers make malware that will have the same crc hash;in this case md5 is more secure -- crc was not made for security,it is likely that you could do an sha hash of 100kb in well less than 10 second though and though sha-1 is still theoretically flawed it is of higher strength than md5,"
"debugging","profiling","easier overall,client.assemblyattributes.cs newer overall,probably better overall, is much easier overall,more strict overall,more sql overall, is spending more overall, but not overall,better overall, requires seeing more overall, is harder overall,","3957154,11270565,12081242,22381797,22792924,7207166,10114896,42926161,40565757,2094577,53242516,","of course you can only debugging the code to see the generated sql but it s easier with a profiling tool and you ll see how long time the query takes,corecompile input file c buildagent temp buildtmp.netframework version v4.0 profiling client.assemblyattributes.cs is newer than output file bin debugging dllname.xml,time spent learning a profiling is probably better spent than time writing and debugging micro-benchmarks,the reason is that maybe like in my case the xcode bot was attempting to archive with the development profiling and development profiling is much easier to debugging than 4f96f173-7ebe-4892-b283-52489de2f409,note that this usually happens under the release profiling which is more strict by default about errors than say debugging,another thing to try is the excellent entity framework profiling - has saved us many headaches in debugging performance issues like these - gives you a lot more than sql profiling and you can see what queries objects are created per objectcontext and method source line etc -,if so you can use the profiling to find where the profiling is spending more time;if not you can run the debugger in monodevelop put some breakpoints in the code and press the debugging button,visual studio 2012 profiling remotely debugged process;visual studio allows remote debugging but not remote profiling,this performance difference issue when doing profiling is better than debugging also has troubled me for a few hours i even tried to move code from web api to console application to test and console application has similar performance as good as when profiling web api,rom your description your description appears that the problem happens in your extension so set up the profiling according to the documentation and look in the error console;other than that remote debugging requires seeing more of your code,tooling is worse the code is longer debugging and profiling is harder,"
"tinyint","varchar","more overall,less efficient overall,better overall, and is much better overall,","27714778,36011861,10645605,17655235,","as varchar n takes spaces much more than tinyint and your main table is table2 so the second scenario is more optimize,using varchar is less efficient than using tinyint like aju john suggested in his comment but unless i m dealing with a really performance-critical or a huge table - i find it easier to deal with,i think tinyint is better than varchar in this case,i always use tinyint;it takes up less space than a varchar and is much better for speed and indexing,"
"bmp","png","slower in slower faster,lower resolution overall,format much more overall,smaller current overall,likely caused by the  overall,smaller than  overall,better result than  overall,faster in slower faster,","20219087,13790064,11533065,44526267,11281905,5756128,20395323,32483365,","jpg png is slower to draw than a bmp,is not simple scale or crop but to be able to rotate scale and crop every single sprite and draw it in the exactly order to compose the final hight resolution image in a backbuffer then save it in bmp and make some copies in lower resolution as png,note that the png format is much more complex than bmp since it allows compression etc,rebuild a new png format in-memory data which is much more smaller than current bmp data and send the new png format data by socket to remote server,the code i have lying around for loading .bmp s uses gl_bgr instead of gl_rgb so i think that will solve your problem with bmp s;the problem with your png image is more likely caused by the png being 32-bits per pixel,not only do png files have natural support for transparency png files are also smaller than bmp files,plain 24-bit rgb truecolor png images are compressed and should produce better result than bmp in any case,i have a sample wpf app here and wondering why bmp is loading faster than png,"
"isotope","masonry","more in complete version better,better in complete version better,easier overall,","17770459,12500664,4917802,","they do this using a jquery plugin called isotope which is a more complete version of masonry you pay for,according to this answer the isotope plugin might work better than masonry for media-queried resizes,in its stead you can use isotope - which does filtering a lot easier than masonry,"
"pexpect","telnetlib","much more flexible overall,","36830610,","i got it to work by using a child process from pexpect which is much more flexible than telnetlib,"
"grouping","sorting","more efficient way overall, is easier overall, is more overall,faster than  overall, method requires much less overall,less overall,faster overall, is more correctly overall,more overall,simpler in application simpler fields,looser overall,","38325709,53545862,6426536,48065753,18055600,31703782,45599311,49301000,35825126,10211855,16425985,","of course ordering could be forced obj.gettype .tostring and using lexical order but since a strict order is not necessary only grouping i was wondering if there s a more efficient way than sorting,the first solution likely will be faster with smaller list of dicts since the sorting is easier but you would need to test that it is not required in the solution that i gave that the keys be grouping the keys is required for groupby to work,for more day-to-day end users having canned reports that might be modified by the end user via a simple user interface that allows canned reports to modify restrictions grouping sorting is more useful,grouping is essentially mapping and should run faster than sorting, similar approach would be to to simply concatenate the arrays to strings banana banana-shake so that you didn t need a custom compare function any more and then split them after sorting to get back the original values;however your grouping method is superior to these approaches as your grouping method requires much less comparisons,your desired output does not reflect what you have as input if you want to grouping common sublists you can use itertools.groupby considering you want sorted output using a dict and then sorting makes less sense than just creating the grouping from the sorted list using groupby,the reason being that sorting less elements which the grouping generally produces is going to be faster than sorting all input documents,what you describe in your question as grouping is more correctly described as sorting,if the database is sophisticated enough adding an explicit order by clause will hint that sorting is more optimal for the grouping operation as well as the sorting can then be re-used in the query execution pipeline,instead knowing that we want to grouping by these columns we can make the application code to do so simpler by sorting by these fields,grouping has looser constraints than sorting so in theory it could be marginally faster than sorting but unless you re dealing with a lot of data you re unlikely to see a speed difference,"
"grouping","sorting"," superclass together overall,much better in application simpler fields, has the greater overall,cheaper than  overall,more than one  overall,more than one  overall,","2393155,40069452,54478022,48065753,57073230,52334302,","i also propose manual sorting since i think manual sorting makes much more sense to grouping superclass together and make sure superclass are defined before subclassing -- otherwise your module won t even be importable,this way only a single int column needs to be sorted grouping which performs much better than sorting two text columns,the second sorting condition i added breaks the tie should two aggregate_id grouping happen to have the same maximum update value;in this case the sort falls back to whichever grouping has the greater detection time to decide which comes first,but my confusion is since i don t care about the order of name i should be able to do some kind of grouping which should be cheaper than sorting first and do sorting only on the numeric field,and in case of how many times it is executed then bubble sort is executed up to o n 2 distribution sort grouping more than one sorting alghorithm but there is no sorting faster than ps if this comes from high school professor there is good chance he does not have full understanding of complexity theory either,will work as long as all your users are on the planet after making all the calculations use an array sorting method to find the x closest nodes to each node and there you have it you may or may not want to prevent a user being grouping on more than one grouping but that s just business logic actual code would be a little too much to provide at this time without seeing some of your progress first but this is basically what you need to do algoritmically,"
"vb6","vba"," not overall, which is almost overall,learning  to maintain overall,less overall, has the clipboard overall,more classes overall,more functionality overall,","975194,43946342,9770974,3846475,639916,41372065,5060785,","this knowledgebase article explains how to do it from vba it s exactly the same in vb6;the references command is on the project menu in vb6 not the tools menu i think that s the only difference,you can often get better results by searching vb6 which is almost the exact same thing as vba;vba just has a few less functions,also vba does not have forms right so you will need to understand how the vb6 gui forms archetecture works;whether you are learning vb6 to maintain existing vb6 or migrate it to .net or other more modern language you should try to come up to speed on standard object-oriented methodology,vba being a whole lot less than vb6,vb6 has the clipboard object that allows you to get the clipboard data in different formats;vba doesn t have this object,fortunately the vbide for vb6 has more classes and methods for working with vb projects than the vba version of vbide,vba is in fact a superset of vb6 not a subset -- vba has more functionality built in than vb6 itself,"
"puma","unicorn","more overall,worse overall,better overall, does not have threads as overall,","19808270,34256764,39574901,24489745,","it uses less resources than unicorn but has more requirements to your code it better be thread-safe because puma is a threaded server,puma will tend to optimize itself by spawning more threads and its performance should range from no worse than unicorn in the pure cpu case to being vastly better than unicorn in the case of an app that sleeps a lot,for example it looks like unicorn is a better choice on heroku than puma,puma can spawn many workers and each worker can use many threads to process the request;unicorn does not have threads as far as i know it just has the worker model,"
"http","spdy"," and check this article overall,more overall,more overall,better plain overall, but not overall,","35967374,19428874,8830628,27117137,23056738,","http 2 is even better than spdy and check this article for a use case regarding spdy,spdy is a more efficient protocol than http,anything that you do with http is more or less equivalent with spdy and neither is really anything but an implementation detail of your soa work,in fact with optimizations like spdy you might find that a tls-secured connection actually performs better than plain http,after 1 http request follows 1 response;you cannot send content type text csv and content type text html at the same time maybe yes with spdy but not with pure http,"
"rpm","yum"," will show package overall,more than one  overall,easier in easier groups machines, doesn overall,available as  overall, does not overall, says nothing overall, makes it easier in easier groups machines,proficient with  overall, which is a more overall,","28535295,24264078,6616998,25995517,55624081,35963680,34160974,50442898,56759697,8201051,","both commands works in the same way only difference is yum list installed output maybe bigger than rpm -qa because yum will show package dependencies also in package dependencies output,i strongly suggest either of the first two options over the latter option as the latter option does not always play nicely with the way rpm and yum handle transactions when more than one rpm are involved,yum makes it easier to maintain groups of machines without having to manually update each one using rpm,it might be possible to play with marking them as config files and then modifying them so that rpm won t remove them but it might still rename them and i don t know if this will actually work;if you are manually copying the files to some directory in post for example then yum rpm doesn t know anything about them and can t remove them,sometimes we would like to use some niche open-source products which are not available as rpm in the redhat official yum repositories or the versions available are very old ones,rpm does not have dependency solver;yum dnf has dependency solver,but the rpm does not install unless i manually erase the gpp-1.10 pkg;until erased yum says nothing to do for some reason,yum makes it easier to maintain groups of machines without having to manually update each one using rpm,i can think of three possible solutions find via yum convert whl file to rpm build from source i can t seem to locate the package in yum but maybe someone more proficient with yum repositories knows how to find it,to expand on the udo s answer there is the program rpm which manipulates specifically the packages it is asked to manipulate and there is yum which is a more intelligent management system that can find dependencies and download .rpm files even if .rpm files re not in the system,"
"free","malloc","more overall, does not in constructor destructor standard,more than one  overall, ated memory in not track in,n more in memory size null,much easier overall, is more than just in memory size null, uses sbrk in system calls d,bigger boundaries overall,more in likely so-called metadata, do not in constructor destructor standard,","29995966,21170970,35448417,38540476,2336345,26310115,32436990,6388011,4055322,1464476,2328556,","remember malloc is quite expensive action and free costs even much more than malloc,free is a c standard library function and does not call the destructor of an object;and malloc does not call the constructor.,1 okay so it is technically possible to allocate it as one big blob and then wire up the 20-element array to point into the desired offsets into the blob this convolutes free ing though and usually isn t necessary for most use-cases the performance gains would be negligible and you d still need to malloc a separate array of pointers that address into the 2d blob  you typically only see 2d-blob allocation when data is massively 2d such as image data and access syntax is eschewed in favor of syntax because it s no less efficient than what the compiler would do and doesn t require more than one malloc free pair per blob of data,the successive free call therefore tries to free messageptr not the memory allocated by malloc;not only this the malloc ated memory will be lost since you have lost all reference the pointer to it,suppose char p malloc n assigns more than n say n bytes of memory are allocated and free p is used to free the memory allocated to p,malloc is much easier to implement if there is no free,here s why checking for null in free is more than just a nice idea;when you use malloc you don t have to remember how much memory you asked for the heap manager does that for you,you can think of the memory free but not released back to the operating system as your own personal cache of memory;this is all assuming of course that malloc uses sbrk at all,presumably the mac malloc aligns to bigger boundaries and so it s spotting the pointer you re passing to free can t be correct since it has the wrong alignment,most likely malloc allocates more memory and puts so-called guard values that happen to contain null bytes or it puts some metadata to be used by free later and this metadata happens to contain a null byte right at that position,applying new delete to a class type t will call the constructor and destructor respectively of t malloc free do not call the constructor and destructor of t;malloc will return null in case of memory exhaustion new will throw an exception,"
"free","malloc"," will not overall, it doesn overall,bigger in memory size null,general in system calls d, doesn in constructor destructor standard,better than   in constructor destructor standard, but not overall, but is less in extensible prone, doesn in memory size null,more in memory size null,earlier in memory size null,","43818348,40682640,1920516,56662732,34052819,57433736,7811818,724365,5486739,37316237,8518769,","that means it is indeed allowed to call mprotect on malloc ed memory regions but free will not reset it as there is no way to know the old protection flags so i have to reset the flags before calling free;in fact this is exactly the problem i met when the program runs for a while it randomly crashes in malloc that is because malloc is writing it s housekeeping data into the previously allocated memory and that memory was set to prot_none by an earlier mprotect after i set the memory to writable before calling free the program never crash again,otherwise or if free ptr has already been called before undefined behavior occurs;since your str is a pointer obtained by adding a number to a pointer obtained from malloc it doesn t meet this requirement and undefined behaviour results,here is a solution which encapsulates the call to malloc allocates a bigger buffer for alignment purpose and stores the original allocated address just before the aligned buffer for a later call to free,because system calls are costly your standard library will try to avoid system calls and prefer marking free -d memory zones as reusable in future malloc -s,and also remember to call the destructor yourself before calling free since free doesn t call destructors either;malloc allocates a memory block but does not actually construct an object there so it will contain garbage,this is better than malloc free at least because new will automatically call a constructor and delete will call a destructor,the reason malloc works is probably because or pulls in a declaration for malloc but not free;thus malloc is unmangled but free is mangled,this avoids malloc free but is less extensible and more prone to buffer overflow issues so i rarely ever use bytedata,as far as malloc realloc and free are concerned memory is just a flat array of bytes;if you store pointers in memory that s you choice but malloc doesn t care,malloc finds 10 more free bytes and marks them as used,how does free know how much memory to be free d which was earlier allocated by malloc or calloc,"
"free","malloc"," to do so in delete different new, doesn overall, it was a bit in delete different new, does not in memory size null,new with  in memory size null, ages ago overall, it isn in not track in, will not overall,more memory than the  in memory size null, actually allocates a bit in memory size null, but is less in extensible prone,","22810472,3505879,7995775,50871808,2570702,14067742,24092276,34383266,34165853,21275158,7725180,","yes it is undefined to free an object that you allocated with new primarily because free does not invoke class destructors;similarly for delete ing something you created with malloc to do so would be to mix two different allocation construction idioms and although you can make it work it would be spectacularly difficult to do so robustly,additionally c++ s new and delete could actually request a larger amount of memory from malloc and use the extra for book keeping like storing the address of the destructor function and so the pointer you passed to free would not actually be one that was malloc ed;c++ wants to call a destructor on the object when you use delete but passing it to free doesn t allow this to happen,his means using new gc instead of new gc_malloc instead of malloc and don t bother about free -ing or delete -ing memory objects;a couple of years ago i measured new gc_malloc versus malloc it was a bit slower perhaps 25µs for new gc_malloc versus 22µs for system malloc,where i work malloc free is slower and more inefficient than new delete for two reasons free does not know the size of the object whereas delete often has this size at compile-time,although some compiler allocate memory for new using malloc free doesn t call destructors;so if you mix new with free you ll get memory leaks and a lot of hard to deal with problems,the rest of the statement just says why it s invalid it s not from the stack it s not something you ve got from malloc and not been free recently;the recently is mentioned because valgrind keeps track of free memory for a limited number of frees so it can t say for sure that it wasn t free a million frees back - in this case it wasn t but if you see a message like that it may be that it has become invalid because it was free ages ago,finally like i stated in a comment to another answer the object will often have a hidden header that stores the size of the object so that a subsequent free can know how large a block is being returned;while your code will keep track of the total amount of memory request via malloc it isn t really keeping track of the total amount of memory used by the objects that are returned,malloc will not free the allocated memory;you need to use free to free the allocated chunk,if you are working on an embedded system that has limited memory good chances are that the malloc returned a null because you were attempting to request more memory than the malloc had free in one contiguous block,and your only initializing first variable of struct but while you are trying to free the memory you are unallocating memory which is not yet allocated astruct 1 and so on till 100;when you use a malloc a malloc actually allocates a bit more memory than you you specified,this avoids malloc free but is less extensible and more prone to buffer overflow issues so i rarely ever use this,"
"free","malloc"," which is quite overall,safer overall, to implement t overall,quicker in delete different new,worse than using  overall, is no longer overall,more in delete different new,allows  to know in memory size null,safer than  overall, you re actually in memory size null,more in memory size null,","3854434,28527213,4907141,23442314,17442783,32032054,38607797,43297625,42438617,4665278,1111015,","somebody say that this behavior can mask double-frees since free null doesn t produce errors but imho this is better than the alternative;one important detail you re not checking the return value of malloc which is quite bad,in addition to the previous answers the strncpy char pointer which seems unsafe for my opinion and the malloc which is safer but you need to remember to free it outside of the function and its inconsistent with the hierarchy of the program you can do the following,the standard doesn t specify anything deeper than malloc and free which leaves c libraries free to implement t to work in t target environments,in this context free store is different and incompatible with heap because the new delete free store library is simpler and quicker than the malloc free realloc calloc heap library and thus provides huge memory usage gains to the c++ embedded programmer in a context where you have only 512 bytes of ram,alloca is also non-standard unsafe and non-portable basically worse than using malloc and free in c++,once the pointed-to next node is free any malloc is no longer valid,if your .so are statically linked to c++ runtimes - you should free objects in the same module where they were allocated since new delete is something more than malloc free and need some extra info to work properly,that allows free to know the size of your memory even though free does not get the size parameter malloc got;the same data also allows malloc to know how much space is available and where,the c++-way here is more readable in my opinion and new and delete are safer than malloc and free,so with that malloc function no your free does not free the memory;first of all you re losing the address of the memory so every time you call this malloc you re actually pushing the process break a little further and never reuse freed memory locations,now if you free some values on the heap and allocate more with malloc you may be given back some of that dirtied memory that you scribbed on earlier,"
"free","malloc"," one is very overall, is not in memory size null,use  not to deallocate in memory size null, is not overall, doesn overall, doesn overall,slower than  in memory size null, memory not in memory size null, is usually overall, is far less in likely so-called metadata, does not always in memory size null,","11218233,25480887,10434147,37803160,20650238,2958685,2163475,28556145,45681753,13792801,16902445,","i don t know if getline will try to call free or not when it dynamically grows the buffer;if it does then the malloc one is very preferable.,this makes it safe to free them all since free null is valid and does nothing;the memory returned by malloc is not initialized,then you are correct in thinking that without a destructor the memory allocated by malloc will not be free;if you follow the rules above then you need to use free not to deallocate it,well calling free is not mandatory as your os will reclaim the space allocated by malloc when you program ends but as long as your program runs you may be ocupying more space than you need to - and that s bad for your program for other programs for the os and the universe as a whole;malloc x creates space of size x bytes on the heap for you to play with,due to the a++ a pointer that malloc returned is no longer what malloc returned and thus free doesn t know what to do with a pointer that malloc returned,free accepts null pointers;malloc doesn t need a cast in c,garbage-collection may be slower than malloc and free for programs that allocate at once all the memory garbage-collection need and work with that,in the freememory you are trying to free memory not allocated with appropriate function malloc calloc or realloc;malloc used once to allocate memory for struct _employee but pointer stored in array the parameter of freememory are obviously not are pointers to _employee,edit don t forget that after malloc you use free and not delete to free the memory;i have to note that the usage of malloc is usually discouraged in favor of new in c++,the most likely explanation is that there s a memory bug in your program writing to free memory buffer overrun etc;an outright bug in malloc is far less likely,when all malloc entries in a virtual page are free that virtual page is then released back to the os this also implies that free does not always release memory back to the os since the virtual page may still have other malloc entries in it;if there is not enough space within a given virtual page to support another malloc of a specified size another virtual page is requested from the os,"
"free","malloc","more memory in memory size null, but does something more overall, to deallocate memory in memory size null, cannot in not track in, and is no more in memory size null, does not overall, memory is bigger in memory size null,","42588652,7444128,29922290,9189168,6263705,236111,51507215,","one interesting experiment you can try is to try and malloc more memory after you free d that pointer,free is the opposite of malloc;new calls malloc but does a bit more;as delete calls free but does something more,use malloc or calloc to allocate memory realloc to allocate or extend a buffer that s been allocated with malloc calloc or realloc and free to deallocate memory that was allocated with malloc calloc or realloc;malloc does not initialize the dynamically-allocated memory,malloc keeps track of the memory that is free or not;in general malloc cannot return this arbitrary region of the heap to the os because the heap is a single contiguous region from the os point of view,such an operation in reality would lead to a memory leak as the memory address which was stored in a initially is not free and is no more pointed by anyone;let the first malloc in main fetch the address 0x1234abcd,on linux with glibc the memory is actually always returned to the os above a certain size glibc malloc uses mmap for big allocations controlled by mmap_threshold and in that case free calls munmap which frees automatically the reserved memory;below that threshold it uses brk and free does not return the memory in that case,thus if your memory size is not large some malloc may fail even if the total free memory is bigger that requested,"
"atoi","strtol","better overall,better in better error checking,better error in better error checking, exists a simpler in representation atoi the,much more overall, which is a better in better error checking,better in better error checking, is more overall,more powerful overall,better in better error checking,better in better error checking,","27962961,2461383,10380364,53082631,4631310,11083338,12420547,25722763,36078280,2428697,13328049,","you can also use strtol which is obviously better than atoi,all have more or less cumbersome and non-obvious error checking involving errno strtol is way much better than atoi in any case so avoid using atoi,use strtol it does better error reporting than atoi,have a look at strtol and its brethren;with atoi exists a simpler function which however is not as robust and versatile,i understand that strtol and strtof are preferred to atoi atof since the former detect errors and also strtol is much more flexible than atoi when it comes to non-base-10,there is no standard method of checking whether the conversion actually succeeded or not when using atoi;since you are writing c++ you could get the same result with better error checking by using a std istringstream std stoi c++11 or strtol which is a better interface when dealing with arbitrary numbers,strtol is better than atoi with better error handling,the atoi and atol just convert what they can;strtol is more advanced and handles bad parsing better by showing you where your input stops being parsable,i start off calling gets to fill the buffer then using strtol to convert the human-readable text in buffer to an actual computer int value - note that strtol is more powerful than atoi and uses long int instead of int as its declared type,i would recommend strtol which provides better error handling than atoi or sscanf,besides strtol is a better option than atoi as strtol can handle failures better,"
"atoi","strtol"," which is the safer overall,better overall, is a better overall, already provides a way overall,better in better error checking,much better in better error checking, is not overall, function is more in representation atoi the,better in better error checking,much better in better error checking, or better in better error checking,","52067144,36374418,1704423,53385856,41887116,46208863,56268217,40307693,31982135,2238063,15955572,","use strtol which is the safer version of atoi,for instance strtol is better than atoi and you should be checking each time whether strtok returns null,atoi is a basic one with very limited error handling capability;strtol is a better one,or 3 tokenizing the string with strtok and then using strtol as atoi should not be used as it provides absolutely zero error checking;there really isn t any need to use both strtok and strtol as strtol already provides a way to advance past the digits converted,you can also look into strtol which is better than using atoi in terms of error checking,if you want to accept a number rather than a digit and only a number strtol works much better than atoi as it allows you to check for failures,you ll also notice that strtol has a lot of error-checking involved while atoi does not;this is precisely why using atoi is not recommended,but in your case the number is given in octal representation so you cannot use atoi;the strtol function is more general because you can specify the base 8 in your case,you need to error check strtol and ensure there are as many passed before using them -- strtol is better than atoi as helps detect errors,the c function strtol is much better make it a habit to prefer that one to atoi,o convert from a string to an int you need to use atoi or better strtol;strtol is better because it does not result in undefined behaviour if the input string is not convertable to a number and it allows you to detect cases like 42xyz and react correctly,"
"atoi","strtol"," to convert the user overall,","56354107,","a possible way to achieve what you want is to use atoi or better strtol to convert the user input to integer type and then use the modulo operaptor to extract each digit and add them;atoi does not do what you think it does,"
"multiplication","multiplying","slightly more overall, is much faster in division faster better,slightly harder just in power easier number,first probably simpler in representable float mathematical, is closer in representable float mathematical,easier overall, has higher precedence in higher precedence error, is much faster overall,easier in power easier number,longer in power easier number, is faster in division faster better,","79771,54297525,2198143,14028965,24101261,29875861,57500076,22488897,9815550,9519571,57032106,","multiplication is slightly more complex as it needs an integer multiplying followed by a scale back such as 0.72 2 becomes 72 200 becomes 14400 becomes 144 scaleback becomes 1.44,this seems a better solution as multiplication is much faster than division better accuracy as multiplying with a power-of-2 number is exact so i d like to create a small utility function which has a logic like this by i mean exponentiation this function should return a normalized scaler scalerreciprocal both are power-of-2 numbers where scaler is near to value and scalerreciprocal is the reciprocal of scaler,multiplication is slightly harder just multiplying two scaled numbers and then divide by your scale factor,multiplying first is probably simpler than using floating point if you only want an integer result and if you know that the multiplication will never overflow,when multiplying 4.35f or 4.35 by 100 which is exactly representable as float and as double too one of two possibilities happens;the mathematical result of the multiplication is closer to 435 than to any other floating-point number,matrix multiplication is the easier one there are several matrix implementations with a multiplying method in packages org.apache.spark.mllib.linalg and org.apache.spark.mllib.linalg.distributed,you re getting that error because you re trying to multiplying a timestamp by 1440 rather than the difference between the date and timestamp - multiplication has higher precedence than subtraction,then to estimate the next 64 bit multiplying the highest 64 bit of x with this number and shift everything into the right place;multiplication is much faster than division,multiplication is the easier of the tasks just remember to multiplying each block of one number with the other and carry the zeros,if i make a mistake and multiplying a number by 1.0 instead of 1 and i do not use any compiler optimization then my multiplication will last much longer than multiplying a number by 1,this explains why you are not seeing multiplication as faster than division when timing with instead let s write setup code which performs multiplication and division on random numbers yields on my machine multiplying is faster 99.00 of the time,"
"multiplication","multiplying"," it is much simpler overall, is lower overall, is always slower in power easier number,efficient than  overall,slower if after  overall,easier in power easier number,more than 1  overall, is cheaper in power easier number, is often in division faster better, has higher precedence in higher precedence error, has a higher in higher precedence error,","34925635,50007788,51393562,48856657,48222143,27922709,55379435,55875924,1168616,31321946,23612128,","division though fractional whole numbers doesnt matter fractional math is done with whole number math logic blocks anyway just like we did in grade school line up the decimal point then do the add or subtract likewise multiplying and divide we used basic multiplying and divide with a little decimal adjustment;division though is an iterative process in logic the implementations you see on educational sites verilog vhdl are simply doing the same thing we did with log division in grade school but like multiplication it is much simpler than grade school you pull down bits from the numerator in the long division until the number being checked against the denominator is equal to or larger basically the number can either go in only zero times or one times into the number under test unlike decimal where it can be between 0 to 9 times,for each primes if you multiplying it by the other primes in a descending order the first palindrome you find will be the highest possible for that primes so its not necessary to iterate the rest if the multiplication is lower than highest palindrome found checking if it is palindrome is not necessary not sure how many of these tips you can apply to your code in a functional way but here is a javascript non functional approach that makes full use of these ideas and takes less than 1 second additional tip it could be further optimized by calculating with binary search the first prime2 that will cause prime1 to have 9 digits instead of 10 that way skipping a lot of iterations over primes2,a faster convert function multiplication is always slower that sum and shift therefore change multiplying with shift,depending on the scale a tree-fold rather than a strict left-fold can improve the performance of doing lots of multiplication because multiplying numbers of approximately the same magnitude is more efficient than multiplying one large and one small number,it means that there might be two potential source of the performance difference you rounding up to more than 9 significant digits so the loop will be run more times if you multiplying first div96by32 might work slower if after multiplication the mantissa has more non-zero 32-bit words,implementing multiplication is easier if you remember an shl operation performs the same operation as multiplying the specified operand by two,in practice this method is not fundamentally different from your multiplying by a large prime approach but in this case the factor is chosen more carefully and the fact that sometimes more than 1 multiplication is required adding to the apparent randomness,multiplication is cheaper so you can save computing power by multiplying all the values to the right of the first one and then raising the first to that power,for division by a constant the compiler can often convert the operation to a multiplying by a magic number followed by a shift;this can be a major clock-cycle saver since multiplication is often much faster than a division operation,so for example if i want to multiplying 4 by 5 and then add 7 to the result i can write;this is valid under normal arithmetic operator precedence rules because multiplication has higher precedence than addition +,since multiplication has a higher precedence than subtraction you should subtract 0 to your digit character before multiplying it,"
"multiplication","multiplying","faster in power easier number, oprator has higher precedence in higher precedence error, is more or less overall, which is slightly slower overall, you actually overall, is usually a little faster overall,","12021618,13048327,51041732,43958549,56977614,55549079,","mathematically left shifting is the same as multiplying a number by a power of 2 but as the operation is done only by shifting it is much faster than doing multiplication,if you want to add a to b before multiplying you ll need to use parentheses;that s because the multiplication oprator has higher precedence than addition,when you multiplying two n-dimensional matrices with numpy i suppose it automatically multiplies the two last dimensions together and keeps the first ones;the n-dimensional matrix multiplication is more or less like the 2d multiplication,the autojit compiler realizes you re multiplying by all 0s and removes the matrix multiplication completely and simply returns a matrix of all 0s in the 1s it skips the actual multiplication part and just does the summation part of a matrix multiplication which is slightly slower than just returning all 0s finally the final one actually forces the autojit compiler to have to do a matrix multiplication since it can t assume the answer,we still loop through days but we do all 100 simulations at once by generating an array of random numbers and making use of numpy s element-wise multiplication which is much faster than using a loop you will need to add the following import import numpy as np and then replace your nested loop with this single loop edit to add because you are using a very simple formula which only involves basic multiplication you actually can get rid of both loops by generating a random matrix of numbers using numpy s cumulative product function column-wise and multiplying it by a dataframe where each value begins at 100,i don t know swift but simply do something like this pseudo-code as i indicated if the bytes coming out of the device are big-endian you obviously do you can perhaps speed this up a bit by multiplying by the reverse because multiplication is usually a little faster than division,"
"allocator","boost"," variant is faster in inefficient copying vectors, has a more in inefficient copying vectors,intrusive_ptr better in effect transform_iterator back_inserter, generally requires much more overall, transform_iterator cannot in effect transform_iterator back_inserter, icl doesn overall,higher in effect transform_iterator back_inserter,string_ref much faster overall,more in inefficient copying vectors, wouldn t overall, overhead is also overall,","2237837,9796712,6603712,3217975,52500865,31287845,26396293,23439161,23855901,416917,13888402,","using boost variant is faster than a union and leads imo to the most elegant code;i d guess that the extremely poor performance of the class hierarchy approach is due to the need to use dynamic memory allocator and dynamic dispatch,some people will suggest using std vector s of std vectors but this is inefficient due to the memory allocator and copying that has to occur when the vectors resize;boost has a more efficient version of vectors of vectors in its multi_array lib,boost intrusive_ptr performs better than shared_ptr because it doesn t need a second allocator to hold the reference count,better is to use shared_ptr or many of the other popular smart pointers available in boost and or tr1 and or c++0x;performance-wise objects allocated on the stack can be done so very quickly the stack size is increased per-function-call so all the required memory has been allocated up-front by a simple move of a pointer. contrarily dynamic allocator generally requires much more time,you can almost achieve the desired effect of 1 memory allocator using boost transform_iterator instead of std transform with std back_inserter;the problem though is that because boost transform_iterator cannot return a reference to an element it is tagged as std input_iterator_tag,the problem is that clearly boost icl doesn t currently support stateful allocator;this means that not all constructors take an allocator instance to pass in the required state,to be able to use std allocate_shared with boost fast_pool_allocator as the allocator method using g++ 4.8 or higher with boost 1.56.0,just to goof off a version using boost string_ref is much faster still due the reduced allocator,the latter doesn t do any dynamic memory allocator and is more than 10 times faster than std to_string on boost karma benchmarks,replacing your containers with boost containers is not a good idea;the work to make appropriate custom allocator wouldn t be that bad but you d be violating the spirit of your allocate at startup rule,boost any internally holds a pointer to an object which it allocates with new;one of the things that makes std vector significantly faster than say std list is that vector keeps all of its objects in a contiguous storage in a single allocator which aside from the obvious reduction in memory allocator overhead is also a lot more cache-friendly,"
"xquery","xslt","simpler overall,much better in class problems xml, is a better overall,significantly more overall,better in amenable static analysis,streaming better in class problems xml, 2.0 not overall,more overall, has a more overall, is better in class problems xml,better overall,","10261092,5165338,6088194,5849881,18728322,5689727,16745577,1908876,48028075,19961442,8375240,","if your output is going to be similar to your input with small changes then the xslt solution is often a lot simpler than the xquery solution,as dimitre says xslt is much better at this class of problem than xquery,xquery can produce a modified copy of your document but it s not particularly easy because you need to explicitly copy all the parts that you don t want to change;xslt is a better bet for producing a modified copy of the document,xslt is significantly more appropriate to use than xquery for such kind of tasks,xquery works better than xslt for this because it s more amenable to static analysis as it lacks the polymorphism of xslt s template rules,saxon-ee supports streaming of large xml documents using xslt or xquery streaming is better supported in xslt than in xquery,xslt 2.0 and xquery 1.0 utilize xpath 2.0 which is what you are recognizing as xquery in the code above;the above code is xslt 2.0 not xquery,unless you are using the static typing feature xquery is no more strongly typed than xslt,as for creating a sequence of attributes you can t do that in xpath which can t create new nodes you can however do that in xslt then you can use it elsewhere as in and will get xquery has a more compact syntax and in contrast to xpath allows expressions to create new nodes,but typically xslt is better for transforming xml into different xml formats on disk;xquery is better for more complex xml queries applications and when the xml is in a database,my usual rule-of-thumb is that xquery is better than xslt for simple tasks whereas xslt is better for complex tasks,"
"xquery","xslt","more overall,more in amenable static analysis,much better in class problems xml, does this better overall,better overall,better overall,","7610958,40861476,25684670,9015987,16190637,1907942,","this kind of processing is most easily done with xslt which is more expressive than xquery,xquery is more amenable to static analysis than xslt because it lacks the very dynamic template despatch mechanism,i hate to answer a request for a solution in language a by suggesting a solution in language b but what you are doing here falls into the class of problems which xslt handles much better than xquery,setting a namespace as the default namespace might be useful though sadly in xquery this also makes a namespace the default namespace for the output - xslt does this better,generally i know it sounds banal xslt is better at transformation generating a new document from each source document while xquery is better at query extracting a small amount of information from each source document,as for whether to use xslt or xquery the proof is in the pudding xslt is better at transforms and xquery is better at queries,"
"cpu","processors"," android studio overall, has the neon overall,much more in system threads resources,more time in system threads resources,better in better thread longer, is not in system threads resources, not so overall,more than one  in computer memory logical,more in system threads resources,general overall,indeed slower in modern world performs,","56154713,5509684,2443473,25954165,142240,27578327,32737403,54463366,5924309,31327579,1371975,","for more visit android your cpu does not support vt-x;a solution from android studio documentation if you have an amd processors in your computer you need the following setup requirements to be in place amd processors recommended amd ryzen processors android studio 3.2 beta or higher download via android studio preview page android emulator v27.3.8+ download via android studio sdk manager x86 android virtual device avd create avd windows 10 with april 2018 update enable via windows features windows hypervisor platform,the iphone 3gs and iphone 4 armv7 cpu does not have a pipelined vfp unit and is thus actually slightly slower at some floating point sequences than the iphone 3g;but the armv7 processors is faster at vectorizable short floating point because the armv7 processors has the neon parallel vector unit instead,as soon as you go beyond single processors it s much more effective to add another cpu or two to system than to struggle with gpu calculations,the difference you notice is very small but i think the multi-thread processors is spending more time because the concurrency for the cpu resources between the threads,some fancy compilers understand the un interrelatedness of instructions to a limited extent and will automatically interleave instruction flows probably over a longer window than the cpu sees to better utilise the processors,in the described scenario given that process is a time-consuming task and given that the cpu has more than one core multi-threading will indeed improve the performance;the processors is not the one who allocates the threads,2 by using intel atom x86 and x86_64 an intel cpu which is use hardware acceleration of android emulation;3 by using mips another form of embedded processors not so popular,cache consistency on a computer with more than one cpu each processors typically has its own memory cache,the operating system will give your program all the resources it needs the reason your process is not consuming all the cpu is probably because it s waiting for the io sub system more than the processors,with the larger cache sizes of modern cpu s it is less necessary to need to go out to ram as often;when the cache needs to go out to ram the cache uses an asynchronous processors dma to grab more information allowing the cpu to switch to a different process entirely,the cpu is indeed slower on sparc 1.2ghz and as answered by one of the sun s engineers t2 is usualy 3 times slower for single-threaded application than modern intel processors,"
"cpu","processors","smaller in size single copy,more than one  overall, doesn overall, not overall, is a lot faster in computer memory logical, doesn in size single copy, typically has a register in program register machine,smarter overall, consumes more in computer memory logical,cache much more important overall,earlier than other  overall,","32394808,51645895,699484,24789557,37448602,22386937,24064278,24367125,48455236,30826090,50616894,","in general when used on single processors single core machine this should be sufficient assuming int size same or smaller than cpu word like 32bit int on 32bit cpu,as detailed in documentation here the 1 cpu in is equivalent to 1 aws vcpu 1 gcp core 1 azure vcore 1 hyperthread on a bare-metal intel processors with hyperthreading so you can request a core using cpu 1 or cpu 1000m but if you want to be more precise - you can allocate like 250m of a cpu cpu 250m lastly if you need more than one cpu you could do cpu 2,keep in mind that doing this calculation directly affects how much the cpu is in use;cpu usage cannot be directly measured because unlike an x86 the arm processors doesn t have a register for it,but the cpu doesn t know anything about objects and their constructors for the processors it s just a pair of assignments that can be reordered if the cpu s memory model allows it;of course compiler and jvm may instruct the cpu not to reorder these assignments by placing memory barriers in the generated code but doing so for all objects will ruin performance of the cpus that may heavily rely on such an aggressive optimizations,modern systems have extremely complex memory architectures with multiple layers of memory and caches either private and shared across cores and cpu;quite obviously accessing a data in the l2 cache in the current processors is a lot faster than having to go all the way to a memory stick from another socket,if your cpu doesn t have specialized copy instructions transfer using the word size of the processors;if the processors has 32-bit words transfer 4 bytes at a time with 1 word rather than using 4 8-bit copies,the cpu does not interpret machine code - it executes it directly - that s why it is called machine code;a processors typically has a register called a program counter pc this starts on reset at a location normally known as the reset vector which is either fixed for the processors or is loaded from a fixed location and is incremented to the next instruction after each non-branching sequential instruction is executed,so whether or not method 5 or 6 is faster depends on the cpu i can only surmise this is because the branch prediction in the command processors of the cpu is smarter on the new processors but i m not really sure,2 errors when i do either this is what happens in my computer 1 cmd processors consumes more than 60 of cpu and memory and runs for a while. 2 it then gives out error messages,notice also the the cpu cache is much more important than processors registers today,in the case that a store eventually does become globally visible you have an interesting time-travel type effect the load on the local cpu has potentially seen the store much earlier than other processors and in particular perhaps the store sees the store out of order with respect to other stores on the system,"
"cpu","processors"," is growing faster in ram data memory, is not overall,more overall, performs better overall, no not overall,more overall, deliver leading security overall, but not overall, is kept busy;multiple in system threads resources, is executing more formally in program register machine,usage higher overall,","2846994,20648208,360494,42913978,5709287,6297409,6863108,18609264,26431615,32596207,18330416,","since the 80s there is a difference in access time between the cpu and the memory;the speed of the processors is growing faster than the speed of the memory,vimage optimizes image processing by using the cpu s vector processors;if a vector processors is not available vimage uses the next best,passive loadbalancing if a physical cpu is running more than one task the scheduler will attempt to run any new tasks on a second physical processors,there s a reason why processors reviews use large benchmark suites;it could be said that the desktop cpu performs better on average but execution times between two small but non-trivial pieces of codes does not have to favor the desktop cpu,it would be good to know what physical cpu this is but i m assuming from your phrasing about logical processors that there is 1 physical socket 2 cpu cores and hyperthreading is enabled giving you 4 logical processors;the short answer is for this complicated definition of processors no not all processors are created equal,if the processors is loaded enough then my thread works fine but when the processors is more or less free i rich quickly my cpu limitation 50 and finally the pool terminates and needs to be recycled,as long as the connection between the browser and the cpu isn t interviened which i believe there is more risk of with a browser than a desktop application;as an alternative for the future new intel business processors deliver leading security manageability and performance,physically the controller resides in the southbridge component which controls all the peripeherals of the motherboard and sends data it reads from the drive to the northbridge component which interfaces to the cpu s graphic controller and ram;this description illustrates a design which applies to many processors but not all amd s opteron being one,consuming cpu time doesn t necessarily require executing something complex just that the cpu is kept busy;multiple web services calls may take time to execute but won t necessarily tie up the cpu as the processors may do other things while it waits for the i o involved with the service calls to complete,whenever a processors cpu runs a program a processors cpu stores the line number of the code a processors cpu is executing more formally address pointing to that instruction .the computer stores a processors cpu in a register kind of variable called as stack pointer,i find using system monitor that consistently 100 of one cpu is used when i run the program directly in terminal whereas when i run it in bash in a loop a maximum of 60 of cpu usage is recorded and seems to be linked to the longer completion time although the average cpu usage is higher over the 4 processors,"
"cpu","processors"," is an earlier overall, performs much in modern world performs, to run code;a overall, and have no time in ram data memory,changing  affinity to allow overall, setup matters less overall, that takes any longer overall, does not in ram data memory,more in computer memory logical, like hyperthreading technology overall, itself doesn overall,","15967940,13720564,25564681,20817372,1717875,56699527,660920,12319839,23935539,10143559,17126482,","your cpu doesn t support the rdtscp instruction;it s a core i7 instruction and your processors is an earlier generation merom-l,then bigger miss then more performance penalty will be introduced by processors;actually in the modern world cpu performs much faster then memory,the idle state is a hardware state the cpu is not running code just waiting for some event to occur scheduling timer or i o interrupt that signals the processors to run code;a 8086 processors can achieve the idle state by executing the hlt halt instruction,register are the fastest memory a processors has register resides inside the cpu a processors run at the speed of a processors and have no time penalty to access a processors,so you may be starving ssis of cpu not memory;you could try changing processors affinity to allow sql server to use only 16 cores and let the memory find it s own level,but here s my 2 cents anyway confluent s documentation might shed some light cpus most kafka deployments tend to be rather light on cpu requirements;as such the exact processors setup matters less than the other resources,negating a number is a very simple operation in terms of cpu hardware;i m not aware of a processors that takes any longer to do negation than to do any bitwise operation - and that includes some 30 year old processors,when processing audio the cpu feeds the sound device with samples by putting them either in a hardware buffer on the sound card or in the ram;the sound processors does not get its data directly from the cpu instead it reads the samples from one of these two buffers,multicore refers to a computer or processors that has more than one logical cpu core and that can execute multiple instructions at the same time.,the rule of thumb is that if a cpu is given more than double the count of actively running threads as a cpu has execution units these are the physical cores on a cpu chip and logical processors like hyperthreading technology that splits one core into two then the os will spend more time scheduling threads and switching between them cache-thrashing than it will spend actually running the threads,yes if the cpu is the same the instructions would be the same but things such as executable file format hardware peripherals memory layout and any os rom services will be different so the program probably won t run;the processors itself doesn t directly deal with displaying things,"
"cpu","processors","far more in ram data memory, information using sudo dmidecode overall, doesn overall,more tasks in cores contention tasks,more overall, it is more overall, wouldn overall, affinity meaning only overall,vastly more overall,more overall, won t in system threads resources,","10332847,18261464,13658118,23300674,7801957,50560909,29381726,8547903,8859053,11718006,4759517,","having data structures that start on 4 byte word alignment on cpus with 4 byte buses and processors is far more efficient when moving data around memory and between ram and the cpu,the cpu doesn t report that;you can find processors information using sudo dmidecode --type processors on linux,cpu usually supports some sort of methods to disable interrupts for example x86 processors provide two special instructions - cli disable interrupts and sti enable interrups;if cpu doesn t provide such facilities interrupts usually can be disabled on side of interrupt controller too i belive that it is a case for different risc processors,given that you re seeing extra tasks causing a slowdown you likely either have resource contention via locking or your tasks are cpu bound and having more tasks than processors cores will cause slowdowns,to the operating system a single thread which i assume is what you mean by java process essentially cannot use cpu on more than one processors which may or may not mean a physical core-- see below simultaneously,in today s modern servers with pretty fast multi-core processors it is more common for a given server request to be limited by i o than by cpu,that means that a cpu has to physically have the register space and fpu front end to be able to use them;in other words avx given to a non-avx processors wouldn t run,with a query that doesn t end up blocking on resources on a 4-core machine you theoretically get a 4-times speed up 4 hyper-threaded might give you more or even less but probably not 8-times since hyperthreading s doubling of some parts of the cpu doesn t give a clear two-times increase;with the same query on a single-core or with processors affinity meaning only one core is available a webserver in web-garden mode then there s no speed-up,not only is it more expensive in terms of developer costs designing a cpu is vastly more difficult than writing user-space assembly code but it would increase the transistor count of the processors,if a task is cpu bound calcuating something making it multi threaded will only improve performance if you have more than one processors to run the calculations,whether or not all cores are in the single cpu or not makes little difference it does make some difference in caching though;anyway the processors won t do any multithreading by its own,"
"cpu","processors","more overall, doesn overall,more in computer memory logical, is an armv6 overall,more than one  overall, is better in better thread longer,more overall, do not actually overall, provide aes-ni instruction overall,more instance than  overall, which doesn in size single copy,","15190198,29904392,11835474,44096737,54386391,8614181,18559464,6764627,54386306,53688452,57364462,","binding threads to cores prevents the operating system from moving around threads between different processors cores which speeds up the executing especially on numa systems machines with more than one cpu sockets and separate memory controller in each socket where data locality is very important,here is a guide to check for this feature using coreinfo how to check if your cpu supports second level address translation;if your processors doesn t get a new processors pc,multicore refers to a computer or processors that has more than one logical cpu core and that can physically execute multiple instructions at the same time,the calls will be emitted by llvm in case the cpu does not support atomic commands such as cmpxchg;my processors is an armv6 and does support atomic commands,if more than one cpu is accessing the same memory it must still be assured that both processors see the same memory content at all times,pinning the processors is not recommended since it wastes cpu resources;the processors is better to block a thread when a thread is not required to do any work and awake once a thread is required to act upon signals,many current processors chips incorporate more than one cpu and a cpu may itself be able to interleave a couple of threads,the cpu certainly puts an upper limit and that is essentially the only limit on 64-bit systems;although note that current x86_64 processors do not actually let you use the entire 64-bit space,the possible reasons for using chacha20-poly1305 which is a stream cipher based authenticated encryption algorithm over aes-gcm which is an authenticated block cipher algorithm are chacha20-poly1305 is almost 3 times faster than aes when the cpu does not provide dedicated aes instructions;intel processors provide aes-ni instruction set 1 chacha20-poly1305 does not need the nonce to be unpredictable random unlike the iv of aes-gcm,my theory is that opening more instance than cpu could handle would make processors less efficient,for example you lose the ability to cull out-of-frustum objects on the cpu which is highly recommended best practice even on a more modern gpu;mali-400 is an old design with a very single vertex processors which doesn t scale with increasing core count so it can definitely struggle with vertex complexity where complexity is,"
"cpu","processors","cache physically closer in ram data memory, maintaining good performance;if overall,more in cores contention tasks,cheaper mainstream overall, is not as fast overall, manufacturers moved to more in quad threadpoolexecutor callables,more in quad threadpoolexecutor callables,","42436018,32251550,22638062,4026163,46272291,313130,6769912,","the point of this cache is to store data that the cpu is using quite regularly to speed up transfer time since the cpu cache is physically closer to the processors then ram is,if you have a good cpu i5 or i7 then you can go up to five or six hours with your cpu maintaining good performance;if you have a lower end processors i do not recommend allowing it to go at 100 for more than a couple of hours,if that processors has more than one cpu can the interrupts run on different cpu cores at the same time,one addition for embedded cpu architecture they have to be usually cheaper than mainstream processors so that they do not raise the product s life considerably,more pertinently they probably don t share it deliberately because they don t want you to benchmark it and complain when your cpu is not as fast as the core should be;they don t want you to see the processors model and mistakenly think it will do a lot more than they will have it do for you because of some benchmarks for said processors you are aware of,with only one processors you d have to render pixel-by-pixel on a super-fast processors to achieve the same effect;the reason cpu manufacturers moved to more than one core was because manufacturing a single core at higher speeds was getting more difficult and expensive and that a single core would suck up more power and produce more heat than two cores at half the speed basically,i have a quad core processors and the threadpoolexecutor is set to 4 core threads but when i submit my callables hundred or so to the threadpoolexecutor java never uses more than 25 cpu,"
"primefaces","richfaces","more overall,more overall, does not overall, does not overall, does not overall,better overall, which is better overall,","3403729,4121333,4121333,4121333,10801252,13392568,24220424,","richfaces is more developed and tested generally since primefaces came out later than rf,primefaces has more components but lack combobox available in richfaces,richfaces does not have a bunch of common components like the star rating component captcha component and password strength;primefaces is easier to skin since it is based on themeroller,primefaces is tiny and can be implemented in the code easily - not too many dependencies;richfaces does not support jsf 2.0 till now nov 2010 - primefaces has a stable release that supports jsf 2.0,standard jsf and primefaces does not support request based el evaluation in attributes;richfaces is the only who supports that,i haven t tried any of these in a jsf 2 application but i would recommend using the primefaces one at least the documentation is better than richfaces,richfaces was biggest piece of crap software ever written don t know how it is now on version 4;we have primefaces which is better but still you will run into bugs or lack of features especially with more exotic components,"
"ant","maven","easier in gradle support easier,more in better dependencies files,more luck with  overall,general in gradle support easier, is more overall,better in better dependencies files,better in better dependencies files, is more in dependency likely future,less mature than  in gradle support easier,older in number ground sophisticated, is a little more overall,","20756425,4970153,10685259,57558828,33673358,20756425,13064741,2898174,39601072,28907043,5005518,","you can take a look on gradle which for me could provide more freedom than maven but is easier to use than ant,also maven projects come with a different directory structure and seems to be doing much more than what ant does in the spring source tutorial,you may have more luck with maven projects but the ant infrastructure re not directly supported by liferay ide however you can use the ant infrastructure with plain eclipse,if you are not using a dependency management tool i would recommend using maven gradle or if you are using ant,since ant is more of a scripting tool and doesn t impose structure or lifecycle like maven and gradlew you simply manually invoke ant when you want to use it,maven is better for managing dependencies but ant is ok with them too if you use ant+ivy and build artefacts,if you want to stick to lower level basic file operations ant is better bet than maven,ant is more likely to be used in their future employment;you may consider maven if you want to go through the problem of complex and or remote dependency in projects,gradle support is much newer and somewhat less mature than ant or maven but gradle support works,ant is older and while it is still used in a number of projects it is rapidly losing ground to maven,really you can do almost anything you wish to put time into using ant;maven is a little more strucutred and newer and you can see a discussion of the differences here,"
"ant","maven","good as  in gradle support easier, is not overall,more overall,script more .class in better dependencies files,faster than either  in clearer slower faster, is the more overall,clearer in clearer slower faster, build using the workspace in build characteristics result,general overall,more overall,much more in dependency likely future,","56897091,43033605,2230024,28597911,53571533,7986180,989209,24164935,57194762,2435935,2850172,","unless you are using netbeans 11.1 which is currently in beta i would not use this option as gradle support in older versions is not as good as ant or maven support,maven gradle ant jenkins are few others;which means maven is not dependent on sonarscanner and you can simply use maven instead of sonarscanner,the quotation you brought up just claim that maven does more than ant so that it s not fair to compare the two,1 - why ant script generates more .class files than maven,however it s my experience that gradle is not slower or faster than either maven or ant,with ant you can do exactly what you want;maven is the more up-to-date way to build and package jars and do much more other stuff,i find the ant one much clearer than the maven one,if you are running the maven build from the command line or in eclipse but using an alternate jre in the jre tab of the run configuration ant will not recognize the property;if you try to run the maven build using the workspace jre however it may recognize it because the property is defined in the eclipse workspace configuration,i have done the following before running the command as per instructions from sudo apt install default-jdk sudo apt install maven sudo apt install ant sudo apt install postgresql postgresql-contrib cd etc postgresql 10 main sudo cp postgresql.conf postgresql.conf.bak edit postgresql.conf to uncomment the line listen_addresses localhost sudo cp pg_hba.conf pg_hba.conf.bak edit pg_hba.conf and add the following line before any other uncommented line host dspace dspace 127.0.0.1 255.255.255.255 md5 sudo systemctl restart postgresql sudo groupadd tomcat sudo useradd -m -s bin false -g tomcat -d opt tomcat tomcat cd opt sudo wget sudo mkdir tomcat sudo tar xvfz apache-tomcat-9.0.22.tar.gz -c opt tomcat --strip-components 1 sudo rm opt apache-tomcat-9.0.22.tar.gz sudo chown -r tomcat tomcat opt tomcat sudo chmod -r g+r opt tomcat conf sudo chmod g+x opt tomcat conf create etc systemd system tomcat.service with suggested contents sudo systemctl daemon-reload sudo systemctl start tomcat sudo systemctl status tomcat sudo cp opt tomcat conf server.xml opt tomcat conf server.xml.bak edit opt tomcat conf server.xml to add the following line before the end of the block uriencoding utf-8 sudo wget sudo tar xvfz dspace-6.3-release.tar.gz sudo rm opt dspace-6.3-release.tar.gz sudo mv opt dspace-6.3-release opt dspace-source sudo chown -r tomcat tomcat opt dspace-source sudo -u postgres createuser --username postgres --no-superuser --pwprompt dspace sudo -u postgres createdb --username postgres --owner dspace --encoding unicode dspace sudo -u postgres psql --username postgres dspace -c create extension pgcrypto,but maven and ant are so different that there is no real point at comparing them and maven is still much more than ant + dependency management,ant is much more powerful than maven in many respects but maven excels in dependency management and ease of deployment,"
"ant","maven","standardized than  overall, like riding a bus overall,more in gradle support easier,more powerful in complex powerful,definitely better in better dependencies files, did not overall, that does these packaging in better dependencies files, makes  easier in gradle support easier,slower overall,younger tool overall, is the much more in better dependencies files,","53318144,14955658,4970153,31543494,29234035,22096779,9193258,26913507,4373083,1732320,1590087,","maven is way more standardized than ant and this means that you will not always be able to do exactly the same in maven and ant,edit an important aspect of the ant-maven comparison is that maven has a convention describing where the files should lie where the dependencies are found where to put the resulting artifact while ant does not;so you can think of using maven like riding a bus - you select the stop where you enter and the one where you leave,but as i checked-out some example apps from spring source repo and it seems that maven is more preferred and powerful than ant,i know maven is more powerful than ant,maven is definitely better than ant and well used for big projects,eventually ant was looking for just 1 content;but maven did not know anything about project.properties which was the problem,if you want the jar to be created everytime you build try using maven or ant that does these packaging tasks for you automatically;maven is more powerful and you would love to see that it mainly does dependency management and your library is the dependency for your project,i recommend you to compile and generate a jar you must supply eclipse s libraries too with maven ant i ve used maven because you will have to package several libraries and maven makes maven easier,also note that while maven is no slower than ant for multi-module projects of this sort importing your maven project into an ide will generally result in far faster builds than maven can perform itself,first maven is a much younger tool than ant or make so you have to expect that its going to take time to get to the maturity level of those applications,maven does address some things that aren t addressed implicitly by ant but with some up front planning ant is the much more flexible better documented and the less buggy tool,"
"ant","maven","better in better dependencies files,more overall, is that it is more overall,more overall,jar bigger then in better dependencies files,usage lower overall,earlier in better dependencies files,better in better dependencies files, and then overall,higher overall,older in gradle support easier,","13496917,10834500,3540407,10787570,18609318,1077627,23112507,47381263,1077520,31279766,26651455,","trust me i did this before for another job where the system architect decided that maven was better than ant and all of our projects must be converted from ant to maven,maven is more like a replacement for ant,in my opinion ant is a little easier to work with when you have to make custom builds;the problem with ant is that it is more difficult to make into a sharable module that you can share and configure like you can with a maven plugin,maven is more or less an update concept of ant,the size of maven jar is bigger then ant jar i don t know why help me please,maven usage is lower compared to ant but just how much lower is not really known,can maven examine and resolve internal dependencies of non-mavenized jar library defined earlier by ant,maven is a better alternative as build system compared to ant,learning ant as a build system may be a better first step as it will help you understand the requirements and difficulties in a build;also if maven is not acting the way you want you can always revert to ant and then integrate in the maven bigger picture,maven is higher level than make makefile the classic c c++ build tool or ant build.xml,first of all ant is older than maven and therefore does not include core support for dependency management,"
"ant","maven","more in gradle support easier, is broader in number ground sophisticated, uses build.xml in better dependencies files,more overall,definitely better integration in better dependencies files,more in better dependencies files, etc is clearer overall,better in better dependencies files,better in better dependencies files,more freedom jacoco overall, offers even overall,","24474576,943494,15122181,413752,8742257,6989690,47300757,304069,18851107,45308097,2140717,","ant pre-dates more modern tools like maven and gradle that have this feature baked in,having said all of that i do think that maven is a very interesting and useful system i m not calling a while a tool like ant is maven is broader than that for software development configuration and build management,ant uses build.xml files to define where to find the source code and which steps to take to build your project;maven is more than just a build tool it is a project management tool,i d definitely use ant or maven - my preference is ant because it s more flexible and i think it would suit your development style more than maven as well,maven definitely has better integration with flex than ant,maven can t be compared with ant as in maven vs ant maven is more than a build script than ant is,often running the equivalent commands in terminal be they ant maven etc is clearer when failures occur because you ll see the full error output,however some things ant handles better than maven and if there s not a maven plugin for it it may be difficult to handle in maven,while you can munge together a solution with ant maven is a better solution for managing dependencies than ant,in jacoco 0.7.9 there are jacoco ant tasks and in 0.8.0 there will be jacoco command line interface - both provide more freedom than jacoco maven plugin in specification of what should be included into report in particular can be used for arbitrary third-party jar files,also ant is platform-independent so you can collaborate with those who use other operating systems;maven offers even more compelling features,"
"ant","maven"," is more in gradle support easier, is newer more overall,easier with  in gradle support easier,really better in better dependencies files,more in gradle support easier,better in better dependencies files,less reliable in gradle support easier,better tool in better dependencies files, it is much more in complex powerful,better off with   in better dependencies files, is not overall,","6087022,13892105,51096472,599070,5913844,13922438,1010593,1190551,16594231,25420923,2432278,","i feel like using scripts ant maven is more natural for this task whereas java needs to work hard to read all the files folders to heap and write folders to heap to another folder, ant is kind of out of date today you should check out maven especially if you are new to ant;1 ant is newer more robust integration and building tool,before i used maven but i need a report customization and find it easier with ant,and it turns out maven really does a better job of managing dependencies for ant build.xml files than does ivy,maven is more of a meta-program that can use ant scripts and run your junit tests,i also find ant with ivy to be better documented than maven although that s not too difficult,troubleshooting the build maven is less reliable than ant especially the non-core plugins,while i personally think maven is a better tool than ant in the long run our experience will make maven3 a better tool than maven2,ant build.xml;for maven it is much more complex,if you need to run all 3 in 1 click then you are better off with maven ant or something similar that has a good integration with eclipse,so people have to judge between how easy they can adjust their builds for how maven works and how long they can wait for a bug fix and how hard it will become to maintain a complex ant build script;the problem with maven is not when it works it s when it breaks,"
"ant","maven","more overall,general in gradle support easier,more overall,more complex in complex powerful,more overall, well i overall,simpler in better dependencies files,simpler in better dependencies files,less verbose overall,better in better dependencies files, gradle introduce a dependency overall,","39645836,10232,41064536,2435935,183290,2435935,31721251,13443342,2812325,1347793,38235016,","did maven has achieved more then ant or ant is still alive,the flex-mojos maven plugins do a great job for we and i would highly recommend using maven over ant,i guess that the maven version contains much more than the ant version,does this make ant more complex than maven,as you ve already said ant is more a replacement for gnu make while maven is primary a buildout dependency-management application,you already know the answer to the very oriented body of your question there is no ant equivalent to this minimal pom.xml because unlike maven ant does not define a set of standards patterns conventions in other words a lingua franca or shared language for project management that would make it possible so ant can t beat maven on this sample;somehow yes and i believe that most complains about maven s complexity are due to ignorance i know ant well i know how to do things in ant ant is simple i don t know maven i don t get it i don t know how to do things i feel lost i feel constrained i don t want to learn it maven is complex. and fud,ant is simpler and older and the default just called a java project maven is newer more powerful and flexible,so i suggest you build a p2 repository can be done in ant but seems simpler in maven tycho and split the projects to several repositories,alternate build tools ant lots of configuration gradle conventions again less verbose than maven,this question is similar in nature to whether ant is better than maven or c# better than java,maven and gradle have support for dependency management but ant does not;your project should switch to maven gradle introduce a dependency manager such as ivy or hand-roll a half-baked custom solution you can call whatever ant target you want,"
"ant","maven"," scripts; is far more overall,more sophisticated in number ground sophisticated,pretty easier in better dependencies files, gives you more in gradle support easier, version contains many more in build characteristics result,","2695303,22056239,16960689,14955658,2729988,","all of this is well-intentioned but can make it hard to figure out someone s ant scripts;maven is far more fully features,a hood technique is to tag milestone releases of your projects giving then a version number and then manage dependencies with maven or a similar tool a bit more sophisticated than ant,i think updating dependencies with maven is pretty easier than dealing with ant but of course you could select the other way if you feel more conifrtable,maven knows how to fetch the libraries and where to find the source classes on it s own;while ant gives you more flexibility it also forces you to constantly reinvent the wheel,i then constructed an ant build file with the characteristics that you describe and got exactly the same result that you did;i noticed that the maven version contains many more dependent libraries,"
"add","subtract"," sequence was slower overall, is better in attempt check better,greater 2pi in greater difference negative, x+1 possibly in attempt check better,greater in greater difference negative,less n overall,greater than 28  in greater difference negative,smaller than then  in smaller numbers left,less in smaller numbers left, m lcm the count in smaller numbers left,greater overall,","22076715,57386889,13225126,57386889,36934027,32158492,53599235,6265840,34110561,51691147,25599703,","shift and add subtract sequence was slower than the lea method above,attempt 1 check whether adding or subtract is better with one bit of information start x1 add x+1 0 divide x+1 2 operations subtract x0 divide x 2 operations we reach an impass if x or x+1 were even the optimal move would be to divide,just take the difference and if it s negative add 2pi and then if it s greater than 2pi subtract 2pi,attempt 2 check whether adding or subtract is better with two bits of information start x01 add x10 divide x1 add x+1 0 divide x+1 4 operations subtract x0 divide x 4 operations subtract x00 divide x0 divide x 3 operations add x+1 possibly not optimal 4 operations conclusion for x01 subtract will result in at least as few operations as adding 3 and 4 operations versus 4 and 4 operations to reach x and x+1,provided a string i want to convert the chars from the string to an int add a value from another method getkey to this number and if the result is greater than 26 subtract 26,to find all the pairs of integers x and y that sum to n when cubed set x to the largest integer less than the cube root of n set y to 0 then repeatedly add 1 to y if the sum of the cubes is less than n subtract 1 from x if the sum of the cubes is greater than n and output the pair otherwise stopping when x and y cross,to achieve expected result use below option do another check for end date checking if greater than 28 add and subtract days based on that condition codepen -,quick idea - go through the roman number from right to left if value of more to the left is smaller than then subtract the left from the result if larger then add the left,i need some help though i need each number in that list to be subtract from 27 then if the result is less than 33 add 94 then print the numbers as they were just with that sum completed how do i do this,for a given number m we can find out how many magical numbers are smaller than m add m a the count of numbers divisible by a that are smaller than m and m b the count of numbers divisible by b that are smaller than m subtract m lcm the count of numbers divisible by both a and b that are smaller than m to avoid double-counting them,look at it this way based on your logic while x is greater than 100 add 5 while it s greater than 500 subtract 5 .,"
"add","subtract","general overall,closer overall,more -- than just  overall,general overall,higher latency than an  overall,general in greater difference negative,","54399937,40771438,56924669,54792056,1147960,48318668,","formulas ceil take integer part of x and add 1 if decimal value is greater than 0 floor take integer part of x and subtract 1 if decimal value is less than 0 examples ceil floor i have checked all the corner cases including negative number with mysql ceil and floor functions,we observe that for 7 nodes 1 2 3 5 6 8 9 we are getting further by 1 add 7 9-2 to the score for other 2 4 7 we are getting closer by 1 subtract 2,c s special ++ and -- operators do more -- significantly more -- than just add or subtract 1,i saw that in dart there is a class duration but it cant be used add subtract years or month,this garentees that while a can be issued to the pipeline every cycle the pipeline will have a higher latency than an add subtract circuit,similar to sam westrick s definition number of times you can subtract n2 from n1 without going negative you could also do integer division with addition and greater-than using the definition number of times you can add n2 to itself before it is greater than n1 . addition might seem like a conceptually simpler thing than subtraction,"
"racket","scheme"," is much more overall, does not overall, feels much overall,compatible with  overall, goes much farther overall, doesn overall,compatible with  overall, isn overall,more specifically overall, do not;for overall,more object-oriented overall,","33678546,31668202,5372482,51680898,10639406,4260831,54165633,27203539,4472281,3345417,5372482,","all of racket s languages can interoperate so your language of choice is up to you though the rnrs languages tend to go unused in the racket community #lang racket is much more useful for writing programs than any of the scheme implementations but programs can be useful if you want to write programs that run on different scheme implementations,edit under linux the installer for petite chez scheme does not include swl you have to download it directly;racket is kind of an extended scheme but you can also choose to use standard scheme by specifying,mit scheme is known for strong interactivitiy support while plt racket feels much more static,so while racket has a sicp language where sicp code should work racket s default language is not compatible with scheme,at a high-level it s very close to scheme but scheme goes much farther in that you can quickly slap up a language that is a restriction of the racket language to just lambda expressions and function applications,if you install a scheme implementation i am using mit scheme edited to add that this also works with racket using mzscheme it may come with a symlink named scheme - this is what emacs looks for i think;if it doesn t mit scheme doesn t seem to on os x you can edit your emacs configuration in emacs type m-x customize-group then type scheme,racket is historically a standard scheme implementation made for education purposes and is pretty much still compatible with scheme for the most part today but they have made a split and does not consider themselves to follow a scheme standard,scheme isn t following a scheme standard at all but is the legacy name of a language that was r5rs compatible once but has changed it s name to #;racket and has immutable pairs as standard,i would suggest you start with scheme and more specifically with racket formerly plt scheme,racket also includes a lot of standard libraries web server that other scheme do not;for one big example racket lists are immutable by default whereas scheme s are mutable,there are various object systems you can load as libraries when you want to do oop but integration with existing code heavily depends on the scheme dialect and its surrounding culture chicken scheme seems to be more object-oriented than racket for instance,"
"racket","scheme"," probably has more overall, isn overall, doesn overall,more advanced other overall,closer overall, yet; has support overall,","17137973,10363117,4305402,3215428,4355037,38878354,","see page 25 section 6.3.2 in the scheme r5rs specification for a full list of list accessing functions;racket probably has more,scheme is defined by a standard;racket isn t,both ocaml and racket plt scheme have opengl bindings;it looks like racket doesn t have sdl bindings however which may or may not be important to you,on the side of using macros racket has always been more advanced than other scheme and lisp implementations,racket is closer to scheme than to common lisp but you could dip your toes into the lisp family without the speed bump of the emacs style of development,latest version of scheme r7rs isn t available in racket yet;racket has support for many surface languages,"
"kerberos","ntlm","more secure in secure delegation domain,faster overall, is however more in secure delegation domain,pick between  overall,better performance in better chatty ways,slower overall, is much more widely overall, is less in secure delegation domain,better in better chatty ways,less in secure delegation domain, does not overall,","17356806,592640,6745058,1566777,5244202,592640,14650512,49535320,453751,2319001,54730004,","but as i understand it ntlm disallows the more secure kerberos domain credentials if they re available,kerberos is complex to set up and even though it generally is considered faster than ntlm this is only true when you reach a certain limit of simultanious users on your site,kerberos is however more secure and can handle delegation where the web server can access other resources a file server using the client s identity,yes negotiate will pick between kerberos and ntlm but this is a one time choice,i understand that kerberos has better performance than ntlm,for a low traffic site the huge tokens that kerberos send across the network actually makes it slower than ntlm,however kerberos is much more widely supported;as for how you can use ntlm kerberos with http in the framework you are using,ntlm is less secure and negotiate lets the client and server use kerberos if both of the client and server support it - if not both of them fallback to ntlm,in fact in some ways ntlm is better than kerberos,but since ntlm is less secure than kerberos why isn t it the other way around,when you use kerberos and enable delegation which the domain admin needs to do you can have the original integrated auth credentials flow over the linked server link to the next connection;ntlm does not do this,"
"kerberos","ntlm","selects between  overall,better option in better chatty ways,chatty than  in better chatty ways,","55348376,34309390,22089087,","in a way negotiate is like kerberos but with a default backup of ntlm currently the negotiate security package selects between kerberos and ntlm,kerberos could be considered as a better option than ntlm,kerberos is better when it comes to performance;mainly because it is a lot less chatty than ntlm,"
"disk","ram","lower than the  overall, is even better though often overall,slower than  overall, which is a much overall,access much slower in slower faster io,slower overall,slower in slower faster io, doesn overall, is much slower in slower page system,faster in slower faster io,higher overall,","9429223,8931800,44384670,13976274,10949313,44453468,16181839,3457180,56990602,25408923,28897569,","at any rate disk speeds are orders of magnitude lower than the ram speed and i wouldn t be too much concerned about the mode here unless of course the mode here turns out that caching is different in the two modes,using a ram disk is even better though often more of a challenge to get a ram disk to implement for you,since your disk even if the operating system s an ssd is several orders of magnitude slower than ram the systems gets unresponsive,when an index is too large to fit into ram mongodb must read the;index from disk which is a much slower operation than reading from,disk access is much slower than ram,pros of objects faster disk read is slower than ram lesser dependencies of the system s state,or if there is too much intermediate output to be shuffled your job will become slow as you will need disk based shuffle in such a case which will be slower than ram based shuffle,if it s close to 100 an ssd or hybrid disk won t help you at all you simply need a faster cpu;hard disk speed is a large factor when initially loading the project but for every compilation run after that a machine with sufficient ram doesn t need to touch the hard disk at all - all your source files will be cached by the os,it has nothing in common with selenium you need to get a snapshot of what s going on with your operating system when you launch the browser for example using windows performance monitor blind shot chrome browser is very memory intensive you can check how much ram it consumes using windows task manager and if your machine is short on ram it starts intensively using page file to store some memory pages to disk and since disk is much slower comparing to the ram - you re getting inconsistent results,for this reason it seemed natural to me to initially load the file into memory and interpret it later at my leisure since reading from ram is supposed to be much faster than from disk,also ram bandwidth is much higher than disk or ssd or network bandwidth and the ram latency is much lower too,"
"disk","ram"," is faster in slower page system,more memory available in larger available files, but not overall,much slower overall,not larger in larger available files,slower than  in slower faster io,access much slower in access map-reduce though,smaller overall,vastly slower overall,more expensive overall,slower in slower faster io,","52746923,28231912,1004225,14873969,1896687,6922874,13396579,30568341,20283122,21370836,15467085,","normally this would speed up subsequent reads of the same file because ram is faster than disk but because my dataset is so large the file is removed from ram by the time i loop back around to it so there is no added benefit,the os heap uses the cpu s virtual memory hardware and is free from fragmentation issues and can even effectively use the disk s swap space allowing you to allocate more memory than available ram,the ssd is still slower than ram by orders of magnitude but the ssd s quite reasonable to have a 50gb hash table on disk but not in ram unless you pay big money for big iron,reloading pages data or program code from disk which is much slower does not usually happen very often as long as the program is actually running and as long as the machine is not desperately low on ram,edit true the file on disk is not larger than ram but the in-memory representation can easily become much larger than available ram,t won t do anything to change the fact that disk io is orders of magnitude slower than ram,however if this is indeed the case - and the data does not fit ram and you cannot use map-reduce i suspect sorting and iterating - though will be o nlogn will be more efficient using external sort - because the number of disk accesses will be minimized and disk access is much slower then ram access,in this case the latter is likely to cause trouble because the insertion of a name hits a random node in the tree i.e the name insertion doesn t follow a pattern and your ram is smaller than the index chances are high that the destination must be fetched from disk,remember the tuples are saved into the disk which is vastly slower to access than things in ram,but ram is volatile the data in ram is erased when the computer loses power and ram is far more expensive than disk per unit of storage,this is relatively slow since reading from the hard disk is slower than reading from ram,"
"disk","ram","less overall,slower in slower faster io,slower overall, is also significantly more overall,slower overall,greater than available  overall,slower in slower faster io,smaller than  overall,faster overall,slower than  overall, starts happening and basically overall,","33856372,6282736,30592276,23432451,45615135,7381501,11832363,52789751,18437355,48044897,21009952,","if the amount of ram is less you ll have a lot of swapping to disk which is a lot more time consuming,that means that without caching a hit against disk will be 200 times slower than accessing ram,the first load involves reading alot from the hard disk which is slow even ssd is slower than ram subsequent loads should be faster though 3 seconds on the ssd seem to be odd,as you stated in-memory databases are usually an order of magnitude faster than disk resident databases for obvious reasons;however ram is also significantly more expensive than hard drives and consequently not viable for large databases,if lob-storage isn t in ram at the time of a query execution then we need to read it from a disk which is of course much slower than from ram,also keep in mind that once ram is exhausted your program will start running in virtual memory on disk which will probably cause far more disk i o activity than your program so if you re concerned about disk i o your best bet is probably to make sure that the batch of data you re working on in memory doesn t get much greater than available ram,the initial read has to access the disk which is a lot slower than accessing ram,when smaller than ram the first run is likely to fetch stuff from disk,since using ram is faster than using disk zram allows linux to make more use of ram when swapping paging is required especially on older computers with less ram installed,this is a performance killer for any server because disk even ssd is orders of magnitude slower than ram,if the size of the data is large millions of rows of data tens of megabytes then disk is always better;the problem is that if you shove a ton of information into a variable you will fill up your ram and once your ram is full things slow down paging memory to disk starts happening and basically everything stops working including any commands that you currently running robocopy,"
"disk","ram","faster overall, has limited storage overall,slower in slower faster io, memory is lower then overall, is no longer overall,slower overall,slower overall,faster in slower faster io, became way faster overall,slower overall,much faster in slower faster io,","20704516,52538218,35918591,12819707,53457510,1829072,7504168,31332082,48068768,18457435,26475024,","so my question is how to move this database into ram where i can access it via sqlite3_open or if my idea is bullshit and leaving the database on disk is faster than moving it into ram via mapping,since ram access is faster than disk access storing and fetching data in redis is much faster than sql but the catch is ram has limited storage,caching and buffering are quite important since disk are just so much slower than ram and ram is much slower than the cpu,you told you have 670g free memory but that is hard disk space;the running program requires memory in ram and probably your ram memory is lower then what you require,however you need to consider that at some point in time ram is no longer sufficient and swapping to hard disk occurs,as disk i o is orders of magnitude slower than ram i o this can cause a very significant difference in query execution times,in other words the operating system is using some of your hard disk space to satisfy your 13 mb allocation request at great expense of speed since the hard disk is much much slower than ram,ram is a lot faster than disk,in client-server model and in computing in general we try to stay away from i o if possible and i o is everything that is not cpu including registers and ram;with the advent to fast ssds disk became way faster and it is the network that is usually the slowest part in any application that uses it,once you re out of ram and the system starts swapping - disk access is thousands times slower than ram so any potential benefits of 64-bit code are flying out of window,the ram is much faster than the hard disk,"
"disk","ram","hdd as  overall,slower in slower faster io,smaller in data quantities ephemeral,access much more fast in access, more is better in data quantities ephemeral,faster overall,more documents than available  in larger available files,larger available in larger available files,always faster in slower faster io, takes more overall, means less in access,","47871508,9163426,21370836,41195702,46713578,6433744,35854309,14280134,5113033,56334498,2465473,","you need to check your size on disk is not full - this s often happen due to creation of some virtual devices that are unused the ram size of emulator is not minimum as your requirements one more thing that i personally use use pen drive as a ram updated use hdd as ram right click on this pc or my computer and select properties,so even ignoring practical considerations like disk is slower than ram it will be slower,so even if your ram is much smaller than your disk you could assume you can read data that s already in ram 90 of the time or more,and ram access is much more fast than disk access,data disk throughput use ssd for best results;ram more is better,just wanted to weigh in my two cents what serialworm and thephpdeveloper said share the fact that memory ram is much faster than any disk io bound operation you come up with,if you have more documents than available ram only the most-frequently accessed documents will be stored in ram for quick retrieval with all others being evicted to disk,it is needed for a lookup of repetitions in disk files much larger than available ram,for the stand of file operations writing to memory ram is always faster than writing to the file on the disk directly,i have a high-end debian desktop with a amd 2970wx 64gb ram 1tbyte ssd system disk multi-terabyte 7200rpm sata data disk and just running wc on a 25gbyte file some archive sitting on a hard disk takes more than 10 minutes measured with time and wc is doing some really simple textual processing by reading that file sequentially so should run faster than grep but to my surprise does not,more ram means less disk access,"
"disk","ram","slower in slower faster io, it has less in 4gb enough cpu,faster in slower faster io,slower overall,slower in slower faster io,slower overall,much faster in slower faster io,io extremely slower then in slower faster io, is faster in slower faster io,storage cheaper in storage persistence cheaper, access is faster as in access map-reduce though,","30478460,53801443,16427006,21370836,9456024,36956097,40904075,10243816,52833224,18718208,32830364,","disk is 100x slower than ram,with 3000 to 5000 i could buy in 2019 a desktop with 128gigabytes of ram and 10terabytes of ssd or disk it has less than a few terabytes of disk,ram is always faster than disk,a disk seek takes about 10 000 000 nanoseconds of course some disk are faster but the best of them are still thousands of times slower than ram,if the worker processes do other things than just calulations read from or write to disk they will have to wait a lot since a disk is a lot slower than ram,using multiprocessing is probably not going to speed up reading data from disk since disk access is much slower than ram access or calculations,the idea is to get the library and application loaded from ram into ram which is much faster than loading from disk,when the data is in memory - you can do anything much faster on it since disk io is extremely slower then ram so sorting it and reading it multiple times is expected to be much slower then manipulating the data on memory,because accessing the data from ram is faster than accessing from disk,always favor disk persistence disk storage is cheaper than ram,finally if both local and ec2 are pulling data from ram there s a good chance that your local cpu core is just faster than the ec2 cpu core and that your ram access is faster as well,"
"disk","ram","access much more overall,slower in slower faster io,slower in slower faster io,larger in sequential random larger,much slower in slower faster io, which is obviously faster overall,slower in slower faster io,slower overall,slower in slower faster io,slower than  overall,slower in slower faster io,","3211989,8741427,17869058,8654903,23316098,27296030,40588290,18708182,16312981,54034874,42893433,","i m sure there are other holes like that too - but the code above will work on any system which supports paging and where disk access is much more expensive than ram access,remember disk is 1000s of times slower than ram,run the following to sort the data on disk this is slower than pulling it into ram sorting and then writing to disk,so while the writes my be sequential on disk for datasets larger than ram these random reads will quickly become the bottle neck,changing it will require a reboot. that will slow things down a bit as the swap file on disk is much slower than ram,the first time you execute the query postgres will load the data from the disk which is slow even with a good hard drive;the second time you run your query the data will load the previously loaded data from the ram which is obviously faster,if too much memory is consumed it might swap to disk which is slower than ram,there are libraries that allow on-disk data structures comes to mind and another one whose name i can t recall at the moment but disk accesses are orders of magnitude slower than ram,disk i o is about 100 000 times slower than ram,remember that a disk even an ssd one is many thousands times slower than ram and it does io operations sequentially,as you run queries it has to fetch data from disk which is much slower than ram,"
"disk","ram","much faster in slower faster io, however even in larger available files, the working set in larger available files,slower in slower faster io, is way faster in slower faster io,slower in slower faster io,slower overall,less overall,more space in data quantities ephemeral,bigger than  in larger available files,much faster in slower faster io,","7548458,48065652,1181840,41349090,55830413,47602416,13148304,42524485,41373116,52789751,472333,","ram is much faster then disk io,i would even say that intuitively files can usually be much larger than ram but smaller than a single disk however even this is not always true,if the working set is smaller than your ram the working set should not be disk bound,disk even ssd are orders of magnitude slower than ram,this is simply done because your ram is way faster than your hard disk,which will lead to performance issue all programs will be work slower because read info from disk is slower than from ram,disk files are of course an order of magnitude slower than ram and thrashing your virtual memory system could actually be worse than that depending on your access patterns,and at worst impossible at all ram size is usually much less than disk size,data in ram can take a lot more space than on disk,whem bigger than ram all runs may need to hit the disk,ram is much faster than disk,"
"disk","ram"," may cause a number overall, which is significantly slower overall, memory is more overall,slower cpu in slower faster io, is not overall,greater than the  overall, it is much faster in slower faster io,much slower in slower faster io, if not overall, is much faster overall,way slower in slower faster io,","217514,36175976,55102187,32743075,51354904,57396179,39234785,10243816,32006614,32908530,42698909,","for example the popular recursive quicksort algorithm provides quite reasonable performance with adequate ram but due to the recursive way that adequate ram copies portions of the array adequate ram becomes much less practical when the array does not fit in ram because adequate ram may cause a number of slow copy or move operations to and from disk,owever there is the issue of the index size and whether all the indexes you d be creating will fit into ram alongside the database working set;if all the indexes you d be creating dont then they dont will hold the index on disk which is significantly slower to read,checked the registry nothing remains in it about the old version. the ram and disk memory is more than enough,query speed is mainly limited by disk i o speed which is at least 1000 times slower than cpu ram speed,at first you have to pick up a place for storing your information hard disk your own server other service firebase ... ram is not useable in this case;supposing you like hard disk option let s select a method for storing create a file json text file ... save your information every time user enter a new one open it to get information back,some potential issues i can foresee is the smalldataarray being greater than the ram especially in quad precision and so unable to write to disk,hen skipping the rows the rows must fetch the rows -- if the rows are on disk this take time;if the rows are cached in ram it is much faster,only one disk read - since the disk is much slower then ram -,if you had swap space configured you d likely start swapping to disk if not and you were running on linux then you may hit the kernel s oom-killer which would start terminating processes to obtain memory - and as it often kills the biggest process you d probably find the couchbase server memcached process being killed;the couchbase server ram quota specified the maximum amount of ram to be used by the data service for caching documents,accessing the main memory ram is much faster than reading values from the hard disk,anyway you will have a huge performance loss due to the fact that your disk is way slower than your ram,"
"disk","ram"," is better in data quantities ephemeral,slower overall, image is faster overall,slower in slower page system,less overall,space cheaper then in cheaper space usage, is not big enough overall, will speed up everything overall,bigger in larger available files,larger available overall,much faster in slower faster io,","51826055,5353574,688636,17899469,35076340,16308547,1106609,21954754,26281309,7314774,21918245,","it could store in ram or on disk but i think in ram is better because the data sounds pretty volatile,because disk access is orders of magnitude slower than ram access,get a ram disk that supports automatically loading saving disk images so you don t have to use boot scripts to do this;sequential read write of a single disk image is faster than syncing a lot of small files,make sure you have enough ram so that your data set fits with ram atleast your index should fit inside the ram coz each time a data fetched from disk is 10 times slower than ram,any db tech is fine with me as long it does not need lots of ram and uses less disk,this approach will radically reduce heap space usage - disk space is cheaper then ram too,network latency nearly everything that affects application speed said for disk is equivalent for network i o;ram if your ram is not big enough to store your complete application image you need to store it on an external disk,and write through to disk is not contradicting;keeping heavily used data in ram will speed up everything,the table on disk is bigger than your ram,alternatively you can use an ssd with file storage in varnish to reduce disk io bottlenecks when using an object cache larger than available ram,obviously reading a block from cache is much more efficient than reading it off the disk since ram is much faster than disk,"
"disk","ram","faster in slower faster io,more enough in 4gb enough cpu, is significantly faster overall,slower overall,io way slower in slower faster io,slower than  overall,faster in slower faster io,more expensive in storage persistence cheaper, was more expensive and partly in expensive crash scarce,faster in slower faster io,significantly slower in slower page system,","19457886,45343069,47641503,9057271,21040937,52537393,3520281,5049314,6563020,1204121,28817103,","ram is a lot faster than disk so reads and writes are temporarily stored until the data is requested by the code or the disk is able to receive it,for this reason i deployed a 2 cores 4gb ram and more than enough disk to run through the getting started example of the enterprise integrator,using openssl rand filbytes out file output description a directory in dev shm since ram is significantly faster than disk storage,that s basically possible but it would take hours as hard disk access is so much slower than accessing ram caches,but you should consider that disk io is way slower than ram,the problem is that the more paging to disk that is required the more you are going to degrade the performance of your applications since disk io is orders of magnitude slower than ram access,no trivial support for cache accessing ram is faster than accessing disk,basically ram is more expensive than the disk storage,relational database systems do relational database systems own disk swapping of course partly because relational database systems were designed when ram was more expensive and partly because relational database systems still sometimes deal with unusually large volumes of data,loosely speaking ram is 1000 or more times faster than disk and cpu is faster still,this is slow because your hard disk is significantly slower than ram and at 7gb there will be a lot of data being read from your hard disk put into ram then moved back to your page file the file on disk your operating system uses to store data that has been copied out of ram,"
"disk","ram"," when not overall, which is slower overall, is faster in slower faster io, is better in sequential random larger,slower system in slower page system,typically more expensive in data quantities ephemeral,bigger overall,more expensive in expensive crash scarce,much faster overall,general in data quantities ephemeral,faster in slower faster io,","5269203,48835605,55447092,2777409,10350034,14328119,1156709,44453468,28297171,57286176,46452799,","this use of virtual memory lets parts of programs and their data be stored on disk when not in use then brought back into ram when needed;when it is brought back into ram being virtual means that it can be put any place there s space for it rather than having to go back exactly where it came from,accessing hard-disk takes you from the realm of caches and ram to that of disk which is slower by orders of magnitudes,ram is faster than disk,even flash drives which beat the hell out of disk for random writes are still significantly better at sequential writes;actually even most ram is better at sequential writes because there are fewer control signals involved,there exist battery-backed packages of ram modules which can act as an ultra-fast hdd substitute but if they attach via sata scsi or other typical disk interface the still are slower than system ram,however they may be cheaper to operate depending on how much data you are expecting to store with each session key holding large quantities of data in ram is typically more expensive than storing on disk,disk is bigger than ram,you lose two benefits of data base consistency data persists after a crash and you need more ram which is more expensive and scarce than disk flash,reading audio files from ram is much faster than reading audio files from hard disk,if you can t change anything on the server side then on you can just use the .ephemeral which apple documents here use this code .ephemeral is used for data that will only be stored in ram and nothing will be stored on disk,i understand that ram is typically thousands of times faster than disk but i o speed is not the only code running,"
"disk","ram"," access is always faster overall, which is much slower overall,more enough in 4gb enough cpu,larger in larger available files,larger than  in sequential random larger,always faster overall,slower than  overall,slower in slower faster io,more important overall,larger available in larger available files,memory faster in slower faster io,","55952797,52387276,7201179,38472387,53289841,21009952,52148211,24217960,18394009,969709,31322336,","a ram access is always faster than a real disk access.,you re probably loading all of the data in your ram thus allocating all memory available forcing your system to rely on swap memory writing temporary data to the disk which is much slower,this particular jboss runs in a vm with 4gb of ram and 2 cpu s and more than enough disk space it has never has less than 5gb free at any time,this is well within disk space but far larger than ram,the dask dataframe is stored mostly on disk rather than in ram- allowing you to work with larger than ram data- and can help you do computations with clever parallelization,saving information to a variable and therefore to ram is always faster than direct to disk,however you need to keep an eye on jvm heap usage and garbage collection activity this can be done using jvisualvm as if you set too high heap which won t be really used by jmeter the jvm will spend much more time while doing the garbage collection and if there will be page file extensive usage your jmeter engine will not perform well as disk io is much slower than ram operations,ssd disk are good but they are still much slower than ram,if the hot data won t fit in ram on either machine then disk i o performance becomes more important than ram mostly random read i o and the fsync flush rate,in future these lists may be read from disk and larger than available ram,we all know that the access to ram memory is faster than access to hard disk,"
"disk","ram"," are not overall,predominant as  overall,slower in slower faster io,larger than  in larger available files,faster in slower faster io,alway faster in slower faster io,bigger than  overall,slower overall,slower in slower faster io,usually faster overall,larger in larger available files,","26899062,23432451,8741427,56545570,10429777,565933,53585376,27458977,13921423,1192201,24240326,","but hard disk are not volatile so they can store your files for ever until you delete;ram is a volatile storage device that is it cannot hold files after the pc has shutdown,of course this ends up undermining some of the benefits of in-memory databases by re-introducing disk i o as a performance bottleneck;in any case in-memory databases will likely become predominant as ram becomes cheaper,as disk is 1000s of times slower than ram as the memory usage increases your machine grinds more and more closer to a halt,my data set can be larger than ram and local disk,ram is 100 thousand times faster than disk for database access from,buy as much memory as you can afford ram is alway faster than io from disk,limited ram where your working set is significantly bigger than ram and slow disk can contribute to slow queries,however be aware of this to cache pixels to disk is several orders of magnitude slower than using ram,as disk is 1000s of times slower than ram this problem can grind the machine down to a practical halt,but becuase reading from ram is usually faster than from other kind of memory storage divice os copy the program from disk on ram and start executing program from there,it uses swap space on disk to allow for processes much larger than ram,"
"disk","ram","larger than  overall, i o is more overall, which is slower overall, but is much bigger overall, being faster than  usually in slower faster io,faster in slower faster io,slower overall,slower in slower faster io,faster overall, seeks - faster overall,slower overall,","19951422,402774,55718607,14578193,47734655,30961271,1356125,20095562,35685853,8562600,848572,","and the whole purpose of maglev is to have a ruby implementation which can deal with heaps that are orders of magnitude larger than ram by storing heaps that are orders of magnitude larger than ram in a distributed cluster on disk,disk i o is more important than cpu power but with more cores you can more easily scale to more vms;plenty of ram assigned to each vm avoids swapping and aids caching meaning less disk i o,json can take a while to serialize especially if you are using the r or python api because it is a separate process and needs to go back and forth between the native jvm executors on the worker nodes to serialize deserialize objects if you performed any wide transformations like aggregation or join prior to df.collect you most likely triggered a shuffle which will result in 200 default partitions being written to disk hence when you call collect it has to retrieve that data from disk which is slower than retrieving from ram although your data set is small you may need to increase the default executor ram executor cores slots # of executors and re-config the # of partitions to get more parallelism check number of partitions df.rdd.getnumpartitions check shuffle partitions spark.conf.get spark.sql.shuffle.partitions check other configs like executor ram cores and instances spark.sparkcontext.getconf .getall spark is a tough beast to tackle .,this is usually a hard disk;basically a backing store some other device that ua usually slower than ram but is much bigger in capacity,ram being faster than disk usually vector is the better approach,1 yes there is a obvious benefit reading from ram is faster than reading from disk,another option is to spend a bit of cash on a 15000 rpm disk or a ssd solid state disk although that ll be slower than a ram disk,disk io will be slower than ram,i would say that probably yes as long as we have enough of ram which is faster that virtual memory in case we need to access something from the disk which is extremely slow... but also i know that some applications just require having paging,less disk seeks - faster queries;if data sits in ram - no disk seeks are required at all required data is served right from memory,memory is a bottleneck to performance ram runs slower than the cpu and if you re paging to disk than it s really slow,"
"disk","ram","slower overall,slower in slower faster io,slower memory overall, to lessen  access overall, has more in 4gb enough cpu,slower in slower faster io,cheaper in cheaper space usage, stored is certainly slower overall,","36959985,14571358,28661841,44391017,52526547,1706506,516941,51922463,","as sven marnach wrote in the comments your problem is most likely i o bound since disk access is orders of magnitude slower than ram access,i think it is because the disk is slower than ram,because hard disk have a much slower memory than ram virtual private server performance may slow down considerably,the table you update on disk is not the table which is used by your virtual users;at the beginning of the test the parameter table is loaded into ram to lessen disk access,it has created a temp file though in the specified temp folder i have windows 10 32gb ram about half look free after the imagemagick command above is run inter core i7 cpu and disk has more that 200gb free,disk io - even ssd - is many orders of magnitude slower than the ram that the hashing is going though,disk space is probably always going to be cheaper than ram,they re all in the realms of speed of memory ram - nanoseconds;database disk stored is certainly slower because the medium access time is slower - milliseconds sessions are good practice,"
"mvp","presenter"," is less overall,pattern more overall,cleaner overall, have no knowledge overall, is more passive and just overall, must not overall,closer overall, is not better overall, is well overall, actually there in design iproducteditorview parameter,prefer mvvm over  in web pages interfaces,","3034603,17432822,34595834,4676970,1317710,40826134,39627887,31877618,52029984,56319145,52000660,","for example in an mvp -based web application;now another class such as a presenter is less likely to depend on the standardnavigator implementation and more likely to depend on the inavigator interface since the standardnavigator implementation would need to be cast to an interface to make use of the redirect method,also this way mvp pattern becomes more testable as you can mock the view with fake object and test your presenter in unit tests,since the flow of logic is still basically a loop but the api boundaries of the presenter is a cleaner boundary than view in mvp which helps decouple v and m in mvp than is possible in mvc,from my experience a strongly enforced mvp pattern has been much better for data centric complex lob applications;mvp offers greater seperation as your presenter have no knowledge of web centric concepts,to summarise the differences controllers in the mvc have more control over the ui and handle events while a presenter in the mvp is more passive and just presents information through the ui,here is a mvp sample using dagger rxjava and retrofit which might help you to learn more about mvp in android;context is a part of android view layer in mvp so presenter must not have any idea about it and you should not pass it to presenter,the use of a presenter is closer to mvp and gets away from the dependency injection model advocated by apple mvc,mvp is not better than mvvm they just fit in different ecosystems;presenter and viewmodel are as their names imply ui beasts,one guy from here told me that mvvm is better than mvp he told me many pros and many conts but i d like to know if my mvp is well done or if you know any other way maybe to convert to mvvm. it s a sign in with google mvp. i won t put all of the code but i ll put the folders and little explanation -model user simple user pojo class -presenter i have an object of my igoogleloginview where i call the methods but here i have the logic creategoogleclient signin onactivityresult onstop onstart ondestroy all of those methods are from my interface inside the same package igooglesignin igooglesignin - all of the methods from the presenter class -view googlesignin gotomainactivity and then i have my mainactivity where i call the methods.,as for your existing architecture mvp it is better to put the logic in the presenter actually there should not be any logic on the view in your case custom view aside from taking input eg click of a button and providing output,i prefer mvvm over mvp for the below reasons reactive mvvm is short term for model view viewmodel where view model is a similar layer of abstraction over presenter which provide data to view,"
"mvp","presenter"," object is more overall, responds to more indirectly overall, does not in design iproducteditorview parameter, could not overall, not overall,more in web pages interfaces,","45122640,22601570,963879,21288636,26076620,2411687,","you might be using mvp mvvm mvi etc;gui architectures in which case some other presenter object is more likely to manage the button than is the activity,or example in mvp you can either directly bind the model to the view or properties of the model individually that you map on binding;you can also call methods on the presenter directly or raise events that the presenter responds to more indirectly,this is a problem because you use mvp for decoupling view and presenter but this design makes them tightly coupled;your presenter does not need to know about your view so you can remove iproducteditorview parameter from the presenter constructor,mvp could not be simpler which is why i start with it;the viewmodel is replacement for the unidirectional presenter it s a state-tracking bidirectional equivalent,in the classic mvc pattern both the view and controller access the model whereas only the presenter has access in mvp;also note that in mvp not all business logic is necessarily located in the presenter,i have been searching on the net and i made a couple of examples of both of them but i m even more confused because in some sample web pages mvp uses more than 2 interfaces to communicate the presenter with the view layer some ones even have completely blank interfaces only declarated but in other ones it only takes two interfaces to transport data from presenter to view,"
"division","multiplying","more than  in step reciprocal quaternion,slower than  in decimal multiplication floor, is a lot more in step reciprocal quaternion, is faster in decimal multiplication floor,slower in decimal multiplication floor,slower than  in decimal multiplication floor,faster in decimal multiplication floor, is often in decimal multiplication floor, is less overall, is stronger slower more overall,slower in decimal multiplication floor,","55764240,15712237,7147370,57032106,38342154,55571752,6056262,24249023,41192405,47150236,15712237,","division dividing a quaternion named p by a quaternion named q is nothing more than multiplying p by the reciprocal of q,using a division is a lot slower than multiplying by decimal number 5 s just an approximation,i suppose one minor thing you can do unless you re already doing it is that in the 3rd step multiplying by the reciprocal of the sum instead of dividing by the sum;division is a lot more expensive than multiplication,this explains why you are not seeing multiplication as faster than division when timing with instead let s write setup code which performs multiplication and division on random numbers yields on my machine multiplying is faster 99.00 of the time,according to agner s instruction tables a single fp division is slower than a single reciprocal op and a single multiplying op,essentially you will and up with multiplying m with a number between 0 up to but not including 1 and finally we are taking the floor of the result of that to get an integer. this is slower than division method but it doesn t matter what you choose for m value of m is not critical,these can be compared to multiplying by 2 x left-shift or divinding by 2 x right-shift but it should be noted that a binary shift is much faster than a division operation,if your magnitude of your numbers will be roughly within the range 1e+ -12 start by finding the smallest power of ten which is smaller than your number multiplying that by 100 000 multiplying your number by that round to the nearest unit and divide by that power of ten;because division is often much slower than multiplication if performance is critical you might keep a table with powers of ten and their reciprocals,the value of i is less than 2 64 so the error after multiplying is less than 1 5;after the division by 4 the error is less than 1 5 2 minus,repeated add sub is normally a terrible way to multiplying divide but you want to loop that many times anyway so you use that as the loop counter;this is called a strength reduction optimization because division is stronger slower more expensive than subtraction,i presume that you know that using a division is a lot slower than multiplying by decimal number 5 is always slower than 0.2,"
"division","multiplying","much more accurate overall,more in step reciprocal quaternion, is much more overall,better overall, circuit is a far overall, is significantly slower in decimal multiplication floor,much more concise in decimal multiplication floor, was clearer overall,better overall,slower in decimal multiplication floor,faster in minor enhancements basic,","38342154,13148266,22522575,10420573,506252,55100623,11588693,55340748,7945958,4301621,35264675,","perhaps it s the case that division is much more accurate than reciprocal plus multiplying,division of quaternion a by quaternion b is nothing more than multiplying a by the multiplicative inverse of b,this would change a multiplying and three divisions into four multiplies and one division;division is much more expensive than multiplication,but in normal case it takes n 2 comparison so you half the height of tree but you multiplying each merge with two so total running time is not better than division to 2 part,think of the hardware involved and you ll understand a lot better why it takes so much longer to divide than multiplying;both operations are done down at the floating point unit fpu level and even in the world of integral alus the division circuit is a far busier place than a multiplication circuit,also 64-bit multiplying was slightly slower on some cpus and 64-bit division is significantly slower than 32-bit even on current intel cpus,but determining the digit and the carry by division is much more concise and for the larger factors also much more efficient when multiplying a digit by 100 the result is on average 450 requiring 45 subtractions but two divisions are sufficient for all factors,you can reason directly by multiplying one side of the inequality by the unity h x h x and then multiplying through by h x;but i thought canceling by division was clearer,while working with integer division it s better to multiplying first and divide later to minimize the rounding error,sure that s probably compiled or jit d away but you should avoid division in performance critical code it s far slower than multiplying,note that i ve incorporated dshin s comment that multiplying is faster than division however the performance improvement is about 90 removing the binary search 10 multiplication vs,"
"division","multiplying"," is slower in minor enhancements basic,more overall, is slower in step reciprocal quaternion,","51885475,35845566,56985581,","there could be a few minor enhancements to be made such as multiplying by 0.5 instead of dividing by 2.0 as division is slower but the basic premise should hopefully help,i read about python following pemdas that is precedence of multiplying is more than division,use the product computed in the previous step multiplying it by the number entering the window and divide by the number exiting the window;division is slower but it will pay off for larger values of c,"
"itext","pdfbox","longer overall, has more overall,less overall,slower than  overall,better overall,","26335506,4784852,10496107,55947276,584198,","i have noticed that content extraction is faster in itext but searching words using regex in the content extracted by itext takes longer time than pdfbox,pdfbox contains tools for text extraction;itext has more low-level support for text manipulation but you d have to write a considerable amount of code to get text extraction,on the downside pdfbox is less mature than itext so it has less features and documentation available,pdfbox is a lot slower than itext when it comes to this,start with pdfbox as it s text extraction abilities are better than itext s,"
"elixir","erlang"," you shouldn overall, could not overall, there is nothing overall, are not overall, is nice enough overall,steeper learning overall, is less overall,less farmiliar overall,more verbose overall, is a dynamically overall,","37965050,45001263,45399326,20561973,36054455,37966947,54946919,33960957,20304284,53468886,","erlang since elixir is built on top of erlang and compiles down to erlang and the beam vm you have access to every erlang module and package;so if you are worried about using elixir and missing out on all of the features of erlang you shouldn t be,you ll need to add the ebin folder of your elixir installation to erlang s load path using -pa or other similar flags like -pz to make erlang load elixir s core libraries as that folder contains the compiled .beam files of elixir core including elixir.io.beam;the error means that erlang could not find a module named elixir.io,in general keyword in elixir can not be translated to erlang proplists since keys in erlang can be of any type but in elixir they have to be atoms;if you have to have more general way of passing erlang proplists from elixir to erlang there is nothing stopping you from constructing them manually in elixir,if you search the erlang questions mailing list you can find plenty of discussion on the topic;note that protocols in elixir are not implemented like that though,if i m not mistaken erlang doesn t know how to handle utf8 in char lists as well so it might be implemented this way for historical reasons;however elixir is nice enough to convert utf8 characters in a char list literal to the appropriate code points so you can later convert it to a binary and get the proper utf8 characters,erlang has a steeper learning curve compared to elixir,a number in erlang and hence elixir is less than any other type hence nils for x1 and y1 are simply fine,i m doing a comparison of erlang haskell elixir and es6 and i m less farmiliar with erlang and elixir but i want to represent all of these languages fairly so is this good erlang code,i agree some mechanisms in elixir are slightly more verbose than erlang function definitions being my personal pet peeve and vice-versa,erlang and hence elixir is a dynamically typed language and you might want to have different clauses for different types passed an error message if the types do not conform or just sink the input out or whatever;erlang does not take out the liberty to deal with not proper input in the way you need that is why guards you use is the way to go,"
"enumerate","xrange","microscopically faster overall,also more overall,more overall, is more concise and faster overall,slower overall,","1352521,7975182,9594107,20500913,1352497,","if you measure properly you ll see there s essentially no difference enumerate is microscopically faster than xrange in this example but well within noise,enumerate is also more appropriate than xrange,enumerate is more pythonic but xrange is fine here too,or use xrange len letters to generate sequence of indices;but enumerate is more concise and faster,why is enumerate slower than xrange + lst i,"
"angular","ionic","better go with  in familiar typescript regular, tools including life cycle overall,slower plain overall,higher than   overall,new working with  in new hard information,less intimidating overall,familiar with  in familiar typescript regular, 4 is the newer overall, adaptation makes it easier overall,compatible with  overall,familiar with  in familiar typescript regular,","51094156,57020468,31505527,55396235,48870474,37466169,51380786,45846839,52553216,55909919,54962735,","if you are familiar with angular typescript you better go with ionic,ionic 4 employs more of the angular tools including life cycle events routing and loading,i ve only done one experiment with angular material myself and i wasn t experiencing any real performance issues at the time but i did read some people writing about it being slower than plain ionic,last one working ionic angular 4.0.0-beta.17 not working anything higher than ionic angular 4.0.0-beta.17,hope you can help me with this since i m pretty new working with ionic and angular i m just trying to share data between pages using push method,the docs for ionic 2 are clear user-friendly and much less intimidating than angular 2 docs,i m not familiar with ionic 2 or angular however in regular html5 web apps we have just used which downloads the file,ionic 4 is the newer version of ionic the open source sdk that enables developers to build performant high-quality mobile apps using familiar web technologies html css and javascript;ionic 4 is paired with angular,i personally prefer to go with ionic 4 since the angular adaptation makes it easier for me to work around,using npm audit there are 10 vulnerabilities requiring fixes but both fixes require angular 7 and i know ionic 3 is only compatible with angular 5,im not familiar with ionic and your angular setup but there is a similar position fixed bug in safari browsers,"
"angular","ionic"," which gives a more overall, tags serves more overall,familiar with  overall,higher overall, 3 then overall,recommend starting with  overall, to manipulate your dom overall, would not overall,using  router to navigate overall, for now overall, becomes a more overall,","56261712,51784271,53037722,37372204,55619166,55438173,39365109,33432289,57069443,54378174,22179064,","there is a more updated fork of this plugin available at you could probably use ionic native for angular which gives a more angular like api otherwise you should be able to work with filechooser directly as this is the js object that is added to the global scope make sure to wait for the deviceready event before trying to access it,the ion- prefixed directives ion-list ion-item etc are angular directives that let you have extended features swipe to delete reorder etc . link as mentioned here main difference is that ionic tags serves more functionalities and other than that,i was wondering if i could get some help from someone more familiar with ionic and angular,i think your code has no problem except that angular has deprecated legacy promise methods success and error of http after version 1.4.4 you should use standard then method instead if your ionic depends on higher angular version,ionic 3 install the cordova and ionic native plugins you must append version 4 to the package name after the character version 5.x is not compatible with ionic 3 then add to provider source ionic 4 for angular the import path should end with ngx then add to provider source,ionic vs cordova first of all i would recommend starting with ionic because its better to have with some ui widgets to start with rather than a blank screen cordova and to start from scratch hybrid apps these are not native apps but rather use webviews browser built-in your app to display your views coded in html and fetch user input and then use javascript or angular etc to process these,ionic is faster than jquerymobile;if you want dom manipulation you can use custom directives in angular to manipulate your dom in much easier way than jquery,it s better to use ionic and ionic material together as ionic material is a material extension to ionic;ionic material extends the actual ionic namespace in js renders material styles on the ionic elements following ionic s conventions and will aim to follow the releases of ionic and support material theming ink and motion for any new ionic release which explains why ionic material + angular would not be a great choice either,i recommend you to use ionic and doesn t care about things like this;in order to help you with this you should tell us if you re using angular router to navigate,ps we didn t start with ionic or whatever because we have no idea about it and we are fairly comfortable with angular for now,about the backend angular provides http and resource for ajax requests take a look at the angular documentation i assume ionic didn t change the way angular makes these calls;my conclusion i think when ionic becomes a more mature framework which won t take long with better documentation it will be the best ui framework,"
"angular","ionic"," is a less overall,new with  in new hard information,compliant with  overall,","47728251,55841860,51862508,","it s just how the angular community works right now - anything that the angular team decides to endorse is becoming the safe standard whether it s good or not. that s why i think currently ionic is a less safe choice not because their ui or cli are not good but because the majority of the community will prefer to use angular material and angularcli no matter the actual differences. i believe that s why ionic is pushing to become relevant outside of the angular community it has become a hard environment to innovate outside of the core team but i believe the angular team is working to change that,i am pretty new with ionic and angular so it s been a bit hard to find the information that i need to get going with this,2 errors i tried to do that the event of my stencil component s is not compliant with angular two-way binding contain a property detail where there is my value use in ionic app html code typescript code,"
"perforce","svn","more intuitive overall, doesn overall,much better in faster check-outs network,better in faster check-outs network,more comfortable overall,more overall,faster in faster check-outs network,less examples overall,much nicer overall, then just overall,","26851,964014,2794083,44752,26851,1768520,2794083,15797795,3969786,2476569,","if anything i find working with svn easier and more intuitive than perforce,it s been some years since i last time used perforce but i remember it certainly didn t like anyone else messing with its workspaces;svn doesn t like it either,svn works much better when disconnected from the network - with perforce you have to tell the server when you ve done a checkout,make sure you re using a source control system that supports branching and merging i ve had experience with perforce and svn and while perforce is better svn is free,perforce does have support for many other oses but our non-windows devs feel more comfortable with svn too,perforce offers more advanced branching and merging capabilities than svn,perforce is soooo much faster than svn because all the check-outs are stored on the server so it doesn t have to check every file on an update,if you want to a little bit further you can imagine use more powerfull scm clearcase perforce ... but maven integration is fewer not well documented and community provide less examples than svn or git,if you have the money ericgorr is right perforce is much nicer to work with then svn,and if you just want something faster or more scalable than svn then just buy a commercial product - i ve used both perforce and rational clearcase and perforce and rational clearcase scale up to huge projects without any problems,"
"boost","stl","more modern overall,much richer in standard template richer,better in learning parts better, does not overall,better solution overall,better in learning parts better,better in learning parts better,less standard in standard template richer,greater mastery overall, and doesn overall, makes it much neater;i never overall,","1577097,4778449,8231535,47658741,3024677,548848,277306,548827,8222708,261277,3365484,","while boost is more modern c++ it also harder to use for non trivial tasks - and without a modern c++ experience and deep stl knowledge it is difficult to use correctly,as for not reinventing the wheel like many have said already make sure to first explore the possibilities in the standard template library stl which is much richer than you might think and then look at boost www.boost.org which has libraries for a lot of diverse purposes and they are extremely high quality and some are just works of art like spirit proto lambda and mpl,c++ s stl is getting better and better yet it s not nearly as huge as boost and is not intended to,so either use boost as suggested by other in this thread or write your own function which you essentially already did -;indeed the stl does not have a simple std contains function,boost s io stream might be a better solution than stl s own stream,once you re past the basics with c++ learning how to use parts of stl and then how it works will be better than tackling boost,i ve used both for the same project boost is better integrated with the stl and especially c++ exceptions,boost libraries are generally less mature and less standard than stl,i don t want to use boost because that s a library i haven t used extensively yet and i m doing this partly as an exercise to gain greater mastery over the stl,i recommend nicola bonelli s answer to be accepted. that s part of the stl and doesn t require using boost if your code doesn t use boost thanks to the commenters for pointing this out;boost has a utility called next and its inverse prior for just that purpose,boost makes it much neater;i never use the stl,"
"boost","stl"," is a tool overall, which makes life easier overall, doesn overall,","570707,50113786,6742554,","clarification boost did not boost up my ood skills;stl boost is a tool for the job,boost python is a richer library with size cost where as pybind11 is header only and it supports stl which makes life easier to pass on basic data structure without writing any code,you can use boost to do it though requires a recent c++ compiler such as msvc 2010 or gcc 4.5;sadly the stl doesn t allow you to split a string on a separator,"
"delphi","pascal"," are not overall,code more advanced overall,easier overall,","48383795,10146882,26573650,","the turbo pascal string is mostly statically allocated and the string types that are new in delphi are dynamic;delphi strings have a null at the end but are not null-terminated the strings can contain null characters and turbo pascal are not,from my pov the object pascal paradigm used with fpc object pascal dialect which can coexist with the default object pascal code is more advanced lightweight and integrated than the interface-based plumbing of delphi xe2 compiler with on-the-fly marshalling using rtti,in free and turbo pascal file handling is much more easier than in delphi in pascal we have assign instead of assignfile and close instead of closefile,"
"mantis","redmine","less overall,better overall,","30459072,10654685,","i ve read that mantis contains less project management tools than redmine,i have been trying out redmine to see if i like it better than mantis,"
"accelerometer","gyroscope","better option overall, is more overall,more in function sensor accurate, is usually much more overall,sensor more accurate in function sensor accurate,more overall, not overall,","28369430,20693849,16772950,43261384,16760584,39996315,22053405,","i want to use accelerometer but i learnt that gyroscope is better option to calculate the accelerometer,the gyroscope is more sensitive more precise and produces events faster than the accelerometer but it seems to be an overkill for this usecase,also i have tried the accelerometer sensor and it worked but i m asking to check whether the gyroscope is more accurate to this function or the accelerometer,the only reliable reading an accelerometer can give you is the direction to the center of the earth thanks to gravity;gyroscope is usually much more precise sensor but it only detects relative rotation not linear movement,so after googling this i found that using gyroscope sensor is more accurate than the accelerometer,gyroscope consumes more power than accelerometer based on my analysis its 4-6 times higher,however if you are trying to determine device tilt attitude you want to use the gyroscope not the accelerometer;the accelerometer tells how fast the device is moving in each direction,"
"lemmatization","stemming","simpler overall,better results overall, handles matching car overall, is usually more sophisticated especially overall,relatively more overall,","32527650,41796551,33300859,5605064,22454669,","you can also user a stemming which is a simpler version of lemmatization,lemmatization should lead to better results than stemming source,stemming handles matching car to cars;lemmatization implies a broader scope of fuzzy word matching that is,lemmatization deals only with inflectional variance whereas stemming may also deal with derivational variance;in terms of implementation lemmatization is usually more sophisticated especially for morphologically complex languages and usually requires some sort of lexica,lemmatization is relatively more complicated as compare to stemming that is why it s harder to find some ready and free solution,"
"sctp","tcp","better in different better,more options than  overall,higher latency overall,better in different better,","32988229,1420987,33657007,32986662,","sctp is not better than tcp in any way but it does something different,sctp requires more design within the application to get the best use of it;there are more options than tcp the sockets-like api came later and it is young,as i understand websockets are on top of tcp and have higher latency than sctp that underlies webrtc when for example sending binary data between server and browser that also could be 2 peers in webrtc,how sctp is better then tcp,"
"cappuccino","sproutcore","better data in use bindings data,probably more freeform overall,better in use bindings data,","4289512,3286643,4289512,","sproutcore has a better data store api than the one cappuccino has,like bruz says cappuccino is probably more freeform at the data layer than sproutcore,sproutcore makes use of bindings better than cappuccino currently does,"
"floating-point","integer"," code has higher latency overall,faster than  in faster arithmetic slower,faster in faster arithmetic slower,higher granularity than  in operations 32-bit 64-bit,variables much more strange overall,larger range in types largest numbers,math faster in faster arithmetic slower,arithmetic much faster in faster arithmetic slower,less exact in issues rounding wrong,slower in faster arithmetic slower,arithmetic usually faster in faster arithmetic slower,","2679922,25898877,38034768,49808204,24415732,34164029,5069628,9236157,28182934,18883825,288727,","a further increase from 16 to 32 integer registers would help some but not as much;avx512 does increase to 32 vector registers though because floating-point code has higher latency and often needs more constants. see comment,integer math is often much faster than floating-point so such a function could be a major performance win,it s even possible that you could implement pong using only integer arithmetic which is likely to be faster than floating-point -- but the difference is unlikely to be critical,it s perfectly possible to perform integer operations that have higher granularity than floating-point,floating-point representation in memory can t add third link - because floating-point variables is much more strange than integer ones,floating-point types have a larger range than the integer types so,generally integer math is faster than floating-point math,but another added benefit of this approach is that it could make your program run faster since fixed-point integer arithmetic is much faster than floating-point arithmetic,however due to unpredictable floating-point precision issues it is sometimes little less than exact integer and in this case it is rounded down too much,as a rule of thumb floating-point is about 2x slower than integer on,which uses all integer arithmetic is usually faster than its floating-point equivalent likely significantly faster in the case of a floating-point type equivalent to t-sql s decimal type,"
"floating-point","integer","division typically faster in faster arithmetic slower,general in types largest numbers,faster in faster arithmetic slower,slower overall,faster than  overall,slower in faster arithmetic slower,coarser in faster arithmetic slower, mantissa is often more overall,simpler in faster arithmetic slower,more complicated in faster arithmetic slower,smaller than an  in types largest numbers,","25024535,53028382,1900794,5069643,53004659,514184,44682117,3692789,141340,12982942,47398841,","floating-point division is typically faster than integer division on the cpu,use a signed 16-bit integer type to store your results i actually prefer using floating-point types it simplifies a lot of things,depending on context floating-point code may be as fast as or faster than integer code or it may be four times slower,on somewhat limited processors like those in high-end cell phones floating-point may be somewhat slower than integer but it s generally within an order of magnitude or better so long as there is hardware floating-point available,note that i first made a mistake in the earlier versions i forgot to convert the random index to an integer and that made string key lookup a lot faster than floating-point key lookup.,they take up more space and floating-point math is slower than integer math,you re performing integer division which is coarser than floating-point division,therefore apis such as opengl and direct3d often use floating-point types which are capable of holding these values;however manipulating the integer mantissa is often more efficient so these apis allow specifying coordinates in texture space color space etc this way as well,but integer arithmetic arguably is inherently simpler than floating-point,floating-point arithmetics is by far more complicated than integer arithmetics,even when the inputs to xcorr are integer values the results which you would expect to be integer values as well can end up having floating-point errors that cause integer values to be slightly larger or smaller than an integer value,"
"floating-point","integer"," representation is of lower in types largest numbers,slower in faster arithmetic slower,bigger numbers in types largest numbers,more overall,higher rank in value rank compiler,faster in faster arithmetic slower, that is less in types largest numbers,faster in faster arithmetic slower, arithmetic requires as overall, math is the more overall, so is_ in issues rounding wrong,","2640105,26227821,14995249,32813065,10221622,30311819,55499865,3042066,49808204,8153127,49745753,","because floating-point representation is of lower precision;while long type can represent all integer numbers in the range from its minimum to maximum double type can only represent some of it,historically floating-point could be much slower than integer,nsdecimalnumber and the floating-point types may be able to store bigger numbers than the integer types though with decreasing precision,floating-point solves the more general problem of representing some real numbers that aren t integers and some real numbers that are larger than the maximum integer up to which all integers are representable here 16777216 all with a nearly uniform relative accuracy at least 1 2 precision,because the floating-point value is of a higher rank than an integer it will promote the integer to a float,there are lots of cpu gpu combinations where a 32b integer multiply is faster than a 32b floating-point multiply on cpu and vice-versa on gpu,floating-point remainder for the float and double operands the result of x y for the finite x and y is the value z such that the sign of z if non-zero is the same as the sign of x;the absolute value of z is the value produced by |x| - n |y| where n is the largest possible integer that is less than or equal to |x| |y| and |x| and |y| are the absolute values of x and y respectively,today s floating-point units are pretty fast and may actually divide faster than an integer unit,however especially if your hardware is physically constrained this can be a hugely benneficial approach as floating-point arithmetic requires as much as 100 times more silicon and power than integer arithmetic,this way you ll get the exact result using integer math alone;using floating-point math is the more general solution but you need to be aware of rounding issues,explaining floating-point rounding issues is a big enough job that it takes up a whole paper that s so important that it s been included by reference in multiple language specifications but the short version is that sqrt could very easily give you a number that s a tiny big bigger or smaller than an integer so is_integer will give you the wrong answer,"
"floating-point","integer","expensive than  in faster arithmetic slower,longer latencies than  in faster arithmetic slower, code is simpler in faster arithmetic slower,value slightly less overall,divide faster in faster arithmetic slower,slower in faster arithmetic slower,better results overall,longer in faster arithmetic slower,greater in types largest numbers,chooses  over  in value rank compiler,slower in faster arithmetic slower,","50101584,17975280,22825251,46047474,2149859,5069643,16154311,4178780,798046,21271895,26227821,","floating-point arithmetic is often more expensive than integer arithmetic in terms of processor cycles and or the space required for it in the silicon of processors and or the energy required for it,floating-point instructions typically have longer latencies than integer instructions but you do not have actual measurements or processor specifications and you do not know trigonometric functions in your application are pipelined so that throughput may be comparable,as soon as you need more integer operations you probably need a lot more so the slight speed advantage is more than eaten up by the additional operations;the floating-point code is simpler which means it is faster to write the code which means that if it is speed critical you can spend more time optimising the code,since the floating-point value is slightly less than the integer you rounded to thanks to .nextdown the integer part is going to be one less than that integer,floating-point divide is faster than integer fewer bits to divide assuming your cpu has floating-point unit,historically floating-point could be much slower than integer arithmetic,i did this very successfully with scipy.ndimage in the floating-point domain way better results than integer image processing like this,an individual floating-point division instruction will take longer than an integer one,i know c++ have functions that return largest or smallest integer that is greater or lower than a like ceil or floor.is there a function that implement digit limitation of floating-point variable,the compiler always chooses floating-point over integer calculation if a floating-point value like a double is involved,floating-point may be somewhat slower than integer but it s generally,"
"floating-point","integer"," that is smaller in types largest numbers,fewer operations with 64-bit  in operations 32-bit 64-bit,general in faster arithmetic slower,","21471411,16641682,3676955,","note that your program may report some numbers as prime if some numbers largest prime factor is very close to some numbers square root if the number is the square of a prime because the conversion of number to a floating-point value may round it down so trsq may end up being less than the square root even less than the largest integer that is smaller than the square root,another feature that many modern processors have allows the processor to execute several arithmetic operations simultaneously often four 32-bit integer operations or four 32-bit floating-point operations sometimes more operations with narrower integers sometimes fewer operations with 64-bit floating-point,as you see integer multiplication floating-point multiplication and floating-point addition all took about the same time;array indexing took a little longer and you re doing it three times and integer addition was a little faster,"
"selenium","splinter","easier in simpler easier,simpler webdriver overall,simpler in simpler easier,","37666683,9028251,38908865,","splinter makes it easier to use selenium plus,consider taking a look at splinter which is a simpler webdriver api than selenium,splinter simpler than selenium,"
"prng","random","more overall,less in non-random behaviour reflection,faster than invoking  overall,behaviour more in non-random behaviour reflection,higher quality than what  overall, produces more overall, was slower overall, is better algorithms overall, is called more than once overall, i got better overall,","16537763,7480271,53221705,41412930,14984025,38838903,3168292,51458654,34614141,41291369,","but a large period prng takes up more memory for maintaining the internal state and also takes more time for generating a random number due to complex transitions and post processing,then you use it as seed in random which is less good that one is a non-cryptographic prng and its output may exhibit some structure which will not register in a statistical measurement tool but might be exploited by an intelligent attacker,it can be simply done by generating random numbers from the range 0 ... 127 and then doing some arithmetic this will likely be faster than invoking prng multiple times,the non-random random behaviour is more a reflection on the quality of the rand prng â it is often not very good,a prng entropy source has much higher quality than what random uses but its entropy source s not guaranteed to be cryptographically secure,hat is where prngs are used to stretch the real entropy to produce more pseudo random numbers from the smaller amount of entropy provided by the trng;the real entropy is used to seed the prng and the prng produces more numbers based on that seed,he f#.net journal articles numerical libraries special functions interpolation and random numbers 16th march 2008 and numerical libraries linear algebra and spectral methods 16th april 2008 tested quite a bit of functionality and nmath was actually the slowest of all the commercial libraries;all the commercial libraries prng was slower than all others and 50 slower than the free math.net library some basic functionality was missing the ability to calculate gamma -0.5 and other basic functionality the gamma-related functions all the commercial libraries did provide was broken,rogram output 9 8 4 5 1 10 7 3 6 2 the library s prng is not very random but for many cases that is not important;if program output 9 8 4 5 1 10 7 3 6 2 the library s prng is better algorithms are available,now the probability that a random value is rejected is guaranteed to be smaller than 50 resulting in a very efficient algorithm just like your bit masking approach;for small bounds the probability that prng is called more than once is extremely small,it s well known that rand is often not a high quality prng pseudo-random number generator but i m a little surprised by this apparently systematic behaviour with seeds that differ by 1 each time;on my mac when i changed srand to srandom and rand to random i got better as in more unpredictable results,"
"fgetc","fread","simpler overall,much slower overall,slower overall,flexible than  overall,","30387301,32013831,14003138,55024991,","you might have even noticed the fgetc version is simpler than the fread version,doing things like 1000 successive fgetc is much slower than doing one single fread of 1000 bytes,the fgetc loop variant was consistently 45x slower than the fread loop,write is a little more flexible than fgetc - it can write more than one byte at a time similar to fread,"
"ffmpeg","libav"," have supported version overall,more recent overall,more compatible overall,","18218032,40391836,24395383,","ffmpeg does not yet support hls version 4;as of 2013-07-29 commit c44191039944526dd7eb6e536990b555837961f5 libav ffmpeg have supported version 3 of the protocol,not to be confused ffmpeg is more recent and libav was used in some distributions of linux,ubuntu 12.04 ships with the ffmpeg fork libav in version 0.8 which is more compatible with ffmpeg 1.0+ or even later ffmpeg versions iirc,"
"i2c","spi","faster overall,more complex slow in communication complex uart, is almost always overall,higher than  overall,faster overall,more in communication complex uart,way more overall,better overall,faster in research faster last, makes more sense; is almost overall,slower in research faster last,","21848629,42320267,19872867,52678463,43422643,37053641,41093145,40757207,46550567,19872867,42818805,","i wanted to know that what makes spi faster than i2c,i know i2c is more complex slow than spi uart etc. but it s a constrain,spi is almost always simpler to use so that s a plus if there s not too many devices;i2c uses fewer pins,i have multiple interrupt like spi i2c timer and the requirement is the spi protocol is the highest priority. however according the datasheet the priority of i2c eusci_b0 higher than spi eusci_a1 and cant change the interrupt priority,post explaining why spi is faster than i2c,communication via i2c is more complex that with uart or spi solution,first bit banging i2c is way more complicated than bit banging spi,how is spi better than i2c at these temperatures,i have done some research about them and it seems to be that spi is faster than i2c but the last one ensures more control and error detection over the first one,if there are a lot of them and the required data rate is low again i2c makes more sense;spi is almost always simpler to use so that s a plus if there s not too many devices,on top of that the i2c bus is slower than spi because there are control data exchanged,"
"i2c","spi","comfortable with   overall,not less limited overall,","57357943,40765245,","i recently started learning iio-subsystem and now quite comfortable with spi i2c based sensors within iio subsystem using regmap apis as well,spi is not less limited than i2c in this case,"
"mbunit","nunit"," has more overall,more overall,more overall,more overall,","3979,3681067,2089364,3678783,","i used to use nunit but i switched to mbunit since nunit has more features,nunit is more popular because it was there first therefore more articles about it on the web and better tooling and because most programmers don t care about or need the advanced features that mbunit offers,nunit is more widespread mbunit has the most features but mstest has more manpower behind it,from what i have read on here i here that nunit is more popular over mbunit,"
"imageview","textview","less overall,bigger than  overall,greater in smaller bigger larger,smaller width in smaller bigger larger, and call your.performclick overall,smaller in smaller bigger larger,higher overall,always higher in smaller bigger larger,smaller height in smaller bigger larger,bigger in smaller bigger larger,higher in smaller bigger larger,","17632868,50857616,32205371,21051095,14920022,44346113,20312493,29874834,44346113,7799656,44346113,","i want to have another textview underneath these two but i am having a problem when the textview length is less than the imageview,how can i align textview to end of imageview that textview would be centered vertically using constraintlayout i managed to align it to the end of imageview but it s not centered vertically because imageview is a bit bigger than textview i know how to do it using relativelayout but wanna use best practices and use only constraintlayout here is an example how it should look cloud icon and text last backup,for api 18 and earlier the margin is being applied after the alignment so if the margin in the imageview is greater than 0 you will get your textview moved,however the imageview has a smaller width than the textview,you can t do that if the textview_selector.xml set for textview not for your imageview;set that selector to your imageview and call yourimageview.performclick in your textview onclicklistener,i also tried to do spacing from imageview to textview but then if the textview is smaller than the imageview the spacing will be wrong again,in this case if textview is higher then imageview imageview aspect ratio should be intact with max width possible and if imageview i higher then textview again width should be maximal and parent should have height as imageview,i want to set imageview and textview on same line in linearlayout but the imageview is always higher than the textview,in the image the square represents a fixed size imageview the rectangle is a textview which can 1 line smaller height than imageview or multiline height is larger than imageview,i have an imageview that is bigger than a textview and i want to center the textview on top of the imageview both vertically and horizontally,i tried constraints spacing each imageview with xdp it is ok if all textview are not higher than the imageview but if a textview is higher than a imageview it will overlap,"
"imageview","textview","lower in lower higher,larger in smaller bigger larger, is not strictly overall,bigger in smaller bigger larger,text longer in text dynamic cell,taller in bottom taller subtitle,higher in lower higher, it is a better overall,taller in bottom taller subtitle,more priority overall, but not overall,","34694309,27327422,12307701,36741971,37507337,6869485,37540133,49224628,44232501,37294062,23228485,","make textview lower than imageview,this should work as long as the imageview is larger than the textview in all dimensions,you can also display one textview and display a small next arrow like so;a linearlayout and separate imageview is not strictly necessary,add layout_weight properties to sub views as well.assign 3 to imaveview and 1 for textview now imageview always will be bigger than textview 3times and textview can not invade imagevies s space,when the textview text is longer than cardview length it shows against the imageview and i don t want that,the imageview is bit taller than the textview and so what i do is add padding margins to the bottom of textview to get it align and look like the imageview and textview horizontal centers are aligned,i try to align my imageview and my textview but the imageview is higher than my textview,set this for your imageview do not use layout_editor_absolutey and layout_editor_absolutex at all because they don t compiled into your app they are in tools namespace you should change your xml like this if you have just two textview and one imageview at all put your constraintview in a scrollview but if you have lots of textview and imageview it is a better idea to use cardview,i have a button that is bottom constrained to the bottom of the imageview however should be top constrained to the bottom of the subtitle textview if subtitletextview is taller than imageview,how could i make the imageview have more priority than textview,i think that if you were to set on the textview s you d get what you were looking for;it looks like it s laying them out to the right of the imageview but not giving them any width at layout time since they don t ask for any and they don t have any content,"
"imageview","textview"," itself not overall,bigger in smaller bigger larger, using android centerinparent overall, has more in text dynamic cell,","10369039,24720677,16285442,49973609,","so it would be logical that you touching somewhere on your activity such as your textview would trigger this image rotation thing;my best guess seeing your code would be this it would be best to implement a touch click event listener for your imageview itself not for your whole activity,assuming the imageview is always bigger than the textview it ll drive the parent height,then center your textview using android centerinparent true;imageview is not a view group so you can t do that exactly,i have an imageview and a textview and i need dynamic cell size for both depending if the textview has more text inside of it,"
"mysql","postgresql"," doesn in reserved word forgiving, is more in query stricter sql, is more overall, contains a better overall,stricter in query stricter sql,easier in easier replication worse,more restrictive in robust mature complicated, is technically more in serious web popular,better in better data database, performs better overall,higher pdo overall,","5568590,17516253,31168935,49831460,7214433,2193943,1717152,48410134,1016512,3114467,12656101,","postgresql uses the reserved word serial as a part of the ddl in that process;mysql doesn t,postgresql is a more mature database postgresql has a longer history postgresql is more ansi sql compliant postgresql query optimizer is significantly better;mysql has different storage engines like myisam innodb in-memory all of them are incompatible in a sense that an sql query which runs on one engine may produce a syntax error when executed on another engine,between the two i think postgresql is more suitable for a central data warehouse;mysql doesn t allow certain sql methods such as common table expressions and window functions,so in your case the insert statement should look like this further reading mysql docs and example similar article for postgresql contains a better explanation of potential problems,but in mysql it is ok and in postgresql it is wrong and ask for the other fields besides site_id either in a group by clause or in a aggregation function i know that postgresql is stricter on sql than mysql so i must select the site_id in the query object of msg_published but in pure sqlalchemy i can do like this,mysql is easier than postgresql but it doesn t really matter either way,postgresql is a bit more restrictive than mysql,although many short articles on the web comparing mysql and postgresql generally tend to claim that postgresql is technically more advanced acid json support etc.. these two problems seem to be serious drawback to me,honestly though postgresql scales much better than mysql,using stored procedures helped out regarding structuring the project but performance actually got a little worse this was at the time stored procedures was a new feature in mysql;postgresql performs better with complex queries in my experience but writing real graph queries for it isn t really possible read here and here for why this is so,drupal 7 mysql 5.0.15 or higher with pdo postgresql 8.3 or higher,"
"mysql","postgresql","better handling in better data database,much faster overall, is also free; is easier in easier replication worse,better in better databases reputation, is more in robust mature complicated,better in better data database,way more in robust mature complicated, is more overall,specifically easier in easier replication worse,much smarter overall, database does require more overall,","442181,9062987,4779070,7587624,46327107,33402831,1630987,399039,36525504,10337106,22729760,","i also personally believe that postgresql has better handling of complex queries that include sub-selects and the like which most mysql users used to avoid,with django it is easy to use postgresql instead of mysql so i tried it with the same query and same data in db postgresql is much faster that mysql x10 more faster while using inner join analyse shows it uses indexes unlike mysql,postgresql is also free;mysql is easier to learn due to it s friendly sql not standard compliance,if you want to use sql i strongly reccommend postgresql it seems to deal with large databases and frequent writes a lot better than mysql,if you are using postgresql you can do the following;mysql is more complicated since is lacks a distinct on clause,i was told that postgresql is a better choice than mysql for displaying hierarchical data so i installed postgresql and i m ready to go,explain in postgresql is way more useful than in mysql,postgresql is more oracle-like in syntax and in some of the features indeed it has been occasionally called poor man s oracle;mysql i am not sure what to compare to,postgresql specifically has gotten easier to manage while mysql has lost some of the simplicity that gave it an advantage without picking up enough features that really matter,if you want open source postgresql is much smarter than mysql as well,cleardb mysql database does require more effort than using the default postgresql database though but this guide might help,"
"mysql","postgresql","fewer security in fewer security roles,more mature overall, is much more in better databases reputation,bigger repertoar overall,slower in slower faster powerful,way better in better data database,implementation less sql in sql server error,less in serious web popular,more in slower faster powerful,stricter in sql server error, s; is generally in better data database,","1270393,15937377,846356,1926885,1841772,7999353,8373087,14197739,957655,22927557,9879578,","traditionally postgresql has had fewer security issues than mysql but they are both doing very well on that,i know postgresql could be considered more mature than mysql with regards to locking thanks to mvcc - can i use row-locking or some other feature in postgresql instead of the token field,in mysql you connect to all databases not just one at a time;postgres postgresql is much more standards compliant but postgres postgresql s uglier and more complicated especially from a ui perspective,b use indexes - postgresql has bigger repertoar of indexes then mysql so use it - there are gist gin indexes,i ve found that postgresql is in my expirience is slower as mysql,postgresql is way better than mysql in nearly every respect,i totally understand the error and assume that the mysql implementation is less sql conform than the postgresql implementation,while postgresql is less popular than mysql most of the serious web hosting supports it,in general postgresql knows more tricks for how to optimize complicated queries than the mysql optimizer does but it also relies heavily on your having given the optimizer enough data to work with,postgresql is stricter about conversions than mysql is and generally will throw an error rather than try to convert a string to an integer if it doesn t look like one,i really like postgresql s indexes which are far better than mysql s;postgresql is generally much better in features,"
"mysql","postgresql","more in reserved word forgiving, features more in detail here overall,older than  overall,familiar with  in better data database,better in better data database,more in sql server error,more in serious web popular, generally has better resilience in weak better power,more overall,more sql in compliant comfortable looks,better in better data database,","45400424,52393988,2277052,49499722,8137020,10775695,1270393,1937370,24603749,1769414,4394622,","it seems mysql is more forgiving with this than postgresql but in every case you use a reserved word you should escape it,an example of what s possible in postgresql 11 but not mysql 8 is this with t v as values 1 1 3 5 5 5 6 select v array_agg v over o groups between 1 preceding and 1 following exclude current row as current array_agg v over o groups between 1 preceding and 1 following exclude group as group array_agg v over o groups between 1 preceding and 1 following exclude ties as ties array_agg v over o groups between 1 preceding and 1 following exclude no others as no_others from t window o as order by v yielding i ve blogged about these postgresql features more in detail here,postgresql is older than mysql so it might have influenced them,i m not familiar with mysql spatial i use postgresql with postgis,and a wiki for why postgresql is better than mysql,mysql is more permissive and allows the non-standard use of distinct but postgresql throws an error,postgresql supports some more security features than mysql for example integration with gssapi or kerberos for logins last i checked mysql didn t have these,in the event of having to kill the server forcefully either by kill -9 or due to power outage postgresql generally has better resilience to table corruption;full support for acid compliance and other relational db features that support for again imho and experiance are weak or lacking in mysql,if you are looking for location based queries in relational databases postgresql is more matured compared to mysql,postgresql is more sql compliant than mysql,generally speaking i find postgresql lends itself to 24 7 operations better than mysql,"
"mysql","postgresql","more overall,picky pickier in slower faster powerful,more resistant in sql server error, not in sql server error,earlier than  in easier replication worse, instead so overall,slower in slower faster powerful, is not in sql server error,more widely in easier replication worse,more in sql server error,mature than  in robust mature complicated,","32821716,9759899,2221787,57342597,48411622,27303513,2797107,50859647,1311750,10320676,51812695,","why postgresql is more capable than others mysql etc. on django,postgresql is picky pickier than mysql -- all fields in the select list when using distinct must be present in the order_by and group_by clauses,the question is is mysql somehow more resistant than postgresql to sql injection attack under the perl dbi and why might this be the case,08 00 00 time is how you do a type cast in postgresql not mysql;mysql uses the sql standard cast,about replication mysql has had a replication solution much earlier than postgresql,if you want to use sqlite you need to store your application in your openshift_data_dir but we recommend using mysql or postgresql instead so your application can be scaled if you want your application to,do you find rails with postgresql is slower than mysql knowing that it produce more query on the background,postgresql pl postgresql oracle s pl sql mysql s procedural dialect are somewhat similar to that but far from being compliant with the sql psm standard;i think the closest to the sql psm standard is db2 and maybe hsqldb sql in postgresql is not the same thing as sql per sql standard,mysql is more widely supported and a little easier to use but postgresql has some very cool features and functionality that s worth taking a gander at,poor performance from mysql has more to do with it not being smart and automatically creating an optimized plan like sql server postgresql or oracle would,and do investigate postgresql its geospatial features are more mature than mysql s,"
"mysql","postgresql","more overall, does not overall,better than  in better data database,much more in robust mature complicated,prefer  over  in better data database,better option in better data database,faster in slower faster powerful,stricter in query stricter sql,much more in slower faster powerful, has the larger overall,harder in easier replication worse,","13155314,49281693,51316895,4394406,2605897,34851450,953814,3512286,1338597,1634671,25197871,","as for reliability i think that postgresql is more reliable especially when compared to mysql using myisam - innodb is a lot better here,see if mysql does not have rules about which ips can talk to it i known that postgresql have this feature do not know if mysql does;check that mysql does not have something like drop iddle connections.,and postgresql does a lot of things better than mysql,i feel postgresql is much more mature and robust than mysql,i personally prefer postgresql over mysql and find postgresql over mysql very scalable even with millions or even billions of rows when setup correctly,in your case postgresql may be a better option than mysql because your query is going to likely be against secondary indexes,mysql s version is apparently marginally faster than postgresql but lacks some of the more advanced spatial features therefore it s pretty much limited to finding records that match a certain range of coordinates,i am aware that postgresql interpretation of the sql standard is stricter than mysql and that consequently this type of query won t work...and have read a number of posts on stackoverflow and elsewhere on the subject - but none of them seem to be the definitive answer on this subject,postgresql is much more complete and solid and will much better support complex queries and their optimization while mysql may shine in terms of retrieval speed for extremely simple queries,option 2 postgresql;mysql has the larger user base but fewer features,does this affect postgresql harder than mysql,"
"mysql","postgresql","more overall,better in better data database,generally better in better data database,better in better data database,much more similar in slower faster powerful,more in serious web popular,more in sql server error,more in compliant comfortable looks,slower in slower faster powerful, s especially in better databases reputation, doesn in better data database,","1140923,8748476,1795117,26188200,18931156,46270070,27439,259460,27443,14509574,665042,","mysql is more than capable of serving your needs as well as alex s suggestion of postgresql,postgresql has better support but the support by mysql depends on the used storage engine,now i m not sure if i d say postgresql is generally better than mysql -- there are certainly things that mysql does much better and so it certainly has its uses -- but these are a few things i absolutely love about it,i found that postgresql 9.3 has better capabilities for json than the mysql versions i am using,postgresql is much more similar to oracle than mysql is,while mysql is more famous for serious enterprise-quality database where preserving your data is critical i recommend postgresql,from how i understand it postgresql is a more correct database implementation while mysql is less correct less compliant but faster,postgresql is more compliant but if you re comfortable with mysql and you re using an orm you should probably use that,in most regards postgresql is slower than mysql especially when it comes to fine tuning in the end,for most platforms postgresql s just as easy to install as mysql but postgresql is a better database and postgresql s especially an improvement on mysql when postgresql comes to handling large amounts of data which you are doing,postgresql has better foreign key support better referential integrity transactions views subselects etc;but that doesn t mean mysql doesn t have its place,"
"mysql","postgresql","more popular in serious web popular, not in sql server error,prefer  over  overall, cannot resolve it; overall,better in better data database, gives better performance in better data database,much better support in better data database,more simple in license bsd situation,worse than  in easier replication worse,better in better data database, is not scalable enough in better data database,","11642511,412091,439818,41389428,16679371,53681827,11365372,1546912,52179,74931,9918572,","it s a shame postgresql isn t more popular than mysql since it supports exactly this feature out-of-the-box you d only have to share one sequence object between tables.,postgresql s a lot more enterprise-ready than mysql not to mention that postgresql follows the sql standard a lot better,i have never used postgresql myself but i think postgresql s mostly a matter of taste whether you prefer postgresql over mysql,the specific reason why the error occurs is because the column sort_order is not in the result set so postgresql cannot resolve it;mysql is much more flexible and incorrect about how it handles aggregation so it allows constructs such as this order by which would cause an error in any other database,i d also recommend postgresql over mysql if you are going to have nested comments as postgresql does hierarchical querying better than mysql,on top of that postgresql gives better performance with ef than mysql,the only reason i was considering postgresql was that some research suggested postgresql has much better support for changing schemas along the way than mysql,postgresql license bsd is undoubtedly more simple than mysql s,i just checked the timings using mysql 5 and mysql 5 are slightly worse than postgresql,with correct locking for safety reasons and heavy concurrent use postgresql performed better than mysql,whoever tells you postgresql is not fast enough for writing does not know it;whoever tells you that mysql is not scalable enough does not know it facebook runs on mysql,"
"mysql","postgresql","more in slower faster powerful,more capable in serious web popular,safer bet in sql server error, may put lock overall,general overall, server alone in better databases reputation,larger overall,better in better databases reputation,more advanced in slower faster powerful,more in serious web popular,more overall,","953814,12873077,18648399,30417929,39058033,57007389,3316770,2324076,22307627,4394406,15270642,","postgresql has more established support for this but mysql has played catch up in the last year or so and has a working method of this in the latest versions,i suggest postgresql it s more capable has more features and better support for complex queries and datatypes than mysql and has a lot of tuning options,if i were able to upgrade the server s versioning of mysql to 5.5 would innodb be a safer bet than postgresql,according to the above mario s post postgresql does not allow using the same connection for more than one thread;i am wondering if the mysql is the same plus mysql may put lock while accessing it,i d personally recommend using postgresql as postgresql has a cidr data-type and powerful functions to go with postgresql but there s an interesting discussion on doing similar things in mysql too,when i rewrite left join by subquery it took only 111ms attached plans while googling i have found this blog post in most cases joins are also a better solution than subqueries postgresql will even internally rewrite a subquery creating a join whenever possible but this of course increases the time it takes to come up with the query plan many so question like this a left outer join can be faster than an equivalent subquery because the server might be able to optimize it better a fact that is not specific to mysql server alone,while mysql has a larger user base postgresql is gaining more an more popularity ever since implementing several crucial features that were missing in earlier versions,given postgresql s reputation for doing things better than mysql or at least as good as i daresay that postgresql would demonstrate similar performance if properly used,postgresql is worth learning and much more advanced than mysql,though mysql is more popular than postgresql but instagram is using postgresql maybe due to these reasons,following the first answer it appears that postgresql is more compliant to sql standard than mysql so it needs a group by clause for each selected column you want to display with your aggregated function,"
"mysql","postgresql"," query has more priority in slower faster powerful,choose between  in amazon different compability,fewer surprises than  in fewer security roles,more logical in easier replication worse, has more in serious web popular,more in slower faster powerful,amazon rds with   in better data database,better in better data database,better in better data database, has imo more overall, has a tendency to silently in serious web popular,","51754025,57688824,4074875,3553940,9879578,259517,51868956,21073787,21215884,1658649,8749161,","i need this query to both work postgresql and mysql due to requirements or i will need two queries if one query can t achieve this postgresql query has more priority so far i ve tried to use .extra syntax but it didn t work so asking it here,while creating aws aurora instance you must have chosen between amazon aurora with mysql compatibility amazon aurora with postgresql compatibility based on this you can choose between mysql on postgresql connector for debezium,i would recommend postgresql for a beginner as postgresql has far fewer surprises than mysql,you might look at postgresql as i find it a bit easier to manage and maintain as i feel some aspects are more logical than mysql, one last thing to say mysql is a little bit easier to start than postgresql;postgresql is a little bit more complex because postgresql has more features,i cannot propose any db not knowing your specific needs but if you want to use a free software which excludes oracle and you re not already experienced with mysql you should try postgresql which is more powerful than mysql,you should put the data somewhere else amazon rds with mysql postgresql whatever you like and then make sure the lambda is connected to the same vpc amazon internal router with the rds database,an efficiently configured mysql is better than a badly configured postgresql and vice-versa,what i am hearing from our group leader is that postgresql is better than mysql when working with coordinates,however mysql has imo more user-friendly admin tools see mysql gui tools that you may like if you aren t familiar with the command line;if you are interested by a comparison between mysql and postgresql check mysql vs postgresql from wikivs the open comparison website,however postgresql s sql features are far more advanced than mysql s and mysql has a tendency to silently ignore things you tell it to do - especially in a default installation and if you rely on a foreign key to be created that might be a very unpleasant surprise,"
"mysql","postgresql","better in better data database,general in slower faster powerful, is actually overall, still doesn in slower faster powerful,more standard overall,more robust in robust mature complicated, is far more in better databases reputation,better in better data database,far more powerful in slower faster powerful,more strict in sql server error, is more in license bsd situation,","2756807,54134942,1630987,8182996,2489451,3563373,52053410,4394406,8052193,2685687,5562628,","in my cases postgresql was better than mysql mysql do not completely support unicode,now for pre-production testings is mandatory use servers running together such as activemq rabbitmq mysql postgresql due this the pc goes slower even worst if sonarqube and jmeter are running,pl postgresql is actually a nice language + you can use many other languages;mysql doesn t have sequences so if you need them you have to roll your own,postgresql is a lot more advanced when postgresql comes to sql features;things that mysql still doesn t have and postgresql has,for example pyqt is already available for python 3 as well as 2 and you can use postgresql a splendid open-source sql database engine much more standard than mysql via py-postgresql,there is also postgresql its a bit more robust than mysql and is free just the same,from what i read where postgresql does better than mysql multi-application databases advanced data modelling what advance data modelling means is that postgresql is far more mature at doing complex data modelling than mysql is,postgresql is better than mysql in many ways,postgresql is far more powerful and scalable and doesn t have mysql s silly limitations and gotchas,postgresql is a little more strict than mysql about type conversion and does not let you implicitly cast convert between numbers and strings,that is another i reason i avoid mysql - the license situation is simply too unclear and vague for me;postgresql is more complicated to setup in an environment where you need a true cluster with load balancing read and write statements,"
"mysql","postgresql","more sane overall, subquery returns more overall,more than  overall,better in better data database,more in better data database,such as  in sql server error, does not in sql server error, doesn in slower faster powerful, doesn overall, it handles fks just overall, is definitely overall,","761581,56518028,48854160,10079942,8928034,57025808,24600299,26520278,38656012,7238724,1546912,","since postgresql is a lot more sane than mysql there are not that many tricks to report on,postgresql error more than one row returned by a subquery used as an expression mysql subquery returns more than 1 row,i am a microsoft fan but mssql server costs much more than postgresql or mysql,postgresql with the optional hstore module might be better than mysql at this.,i am using postgresql btw which can do more than mysql incase that changes things,there are client-server databases that you can use locally such as mysql faircom sql server postgresql and many others,some other databases oracle db2 postgresql have the function translate for it;unfortunately mysql does not support it,if you can not switch to postgresql try using innodb;1235 this version of mysql doesn t yet support limit,postgresql doesn t know float but float4 and float8 which are synonyms for real and double respectively;mysql technically knows float but that doesn t mean the db-migrate abstraction will accept is as input when run .,all that said postgresql is much more acid compliant than mysql it handles fks just as you re needing and i would generally recommend it based on what you ve expressed in your post,note i m not saying mysql s license won t work for you but postgresql is definitely simpler,"
"mysql","postgresql"," types point lseg box overall,slower in slower faster powerful, is probably a little more in slower faster powerful, does not in better data database,general in easier replication worse,more in robust mature complicated, timestamp gets harder overall,much better performance in better data database,less time overall,stricter in sql server error, doesn overall,","999255,36927941,13557296,19936672,49789336,4135236,4279554,201950,5598904,4078839,14017089,","also look at the native postgresql types point lseg box path polygon and circle if you want to understand more about how postgis works and why its 2d feature implementers have a shorter row to hoe in general;mysql does not implement a lot of spatial queries -- i don t know for sure about contains but there are ways of approximating it and other functions,unfortunately in postgresql select count is often slower than mysql to which it often get s compared to,mysql is probably more often offered by shared hosting providers postgresql is probably a little more powerful,postgresql also supports nested tables see for example and a number of array-related features;mysql does not support nested tables but there s a set data type to consider,mysql postgresql tends to be easier to use and you ll find a ton of libraries uis etc to help you get started mysql postgresql offer real transactions mvcc - casssandra has lightweight transactions limited to operations on a single key with much weaker isolation atomicity guarantees,i hear postgresql is more robust and doesn t crash like mysql does in these situations,ostgresql timestamp is not unix timestamp and mysql s unix timestamp is not postgresql s or oracles timestamp;postgresql timestamp gets harder to port if you use database timestamps,postgresql gets much better performance and this is coming from a former mysql partisan,may be postgresql takes less time than mysql,postgresql is stricter to the sql standard than mysql is,postgresql has a native boolean type where true is represented by the string literal t and false by f there are other literals but those are the most common ones;mysql doesn t have a native boolean type and uses c-style integers for booleans instead,"
"mysql","postgresql","mariadb slightly worse in easier replication worse,better in better data database,more security in fewer security roles,better in better data database,more compliant in compliant comfortable looks,fewer experienced in better databases reputation,more mature in robust mature complicated, - so in sql server error,slower in slower faster powerful,better option in better data database, has full support in slower faster powerful,","40083672,1373566,6475228,7997125,6751367,1374694,2452926,1840723,30010645,8262827,2941964,","mysql mariadb is slightly worse than postgresql,some people were saying that postgresql is better for security purposes whereas mysql is becoming more feature rich.,i d have said that postgresql is more security aware than mysql supporting roles more authentication methods ... but that the database itself has generally a very limited impact on the security of an application,i would need a scalable database so probably postgresql would be better than mysql,looks like postgresql is a little more compliant than mysql so try this instead,and postgresql has fewer experienced administrators than the big databases and mysql which i believe contributes to the reputation,postgresql will always be a little bit more mature than mysql,i d recommend postgresql s sql is much more ansi-compliant than mysql - so later using oracle sql server sqlite etc should be very straight-forward,for these ultra simple queries postgresql can be slower than mysql - postgresql has richer planner that works better on more complex queries but on trivial queries is slower,is postgresql a better option than mysql for partitioning tables by date,postgresql has full support for utf8;mysql doesn t support the complete utf8-encoding,"
"mysql","postgresql","better overall, is much more in robust mature complicated,complex than  overall,worse in better data database,better db in better databases reputation,smarter in better data database,slower in slower faster powerful, and is extremely overall, is much better in better data database, is better in nearly overall,more in better data database,","32748006,4421747,47470338,2694299,26232186,3008890,34148481,56246832,3082138,8000320,1162378,","in fact this presentation which still refers to an older incomplete version of mysql s opengis support suggests that with proper indexing some mysql geospatial actions may actually perform better than postgresql postgis though i m sure that s up for debate,postgresql is much more mature for this;if your app has lots of stored procs this pretty much rules out mysql,triggers in postgresql have a syntax a bit more complex than mysql because mysql execute procedures as the action,in other words is mysql better or worse than postgresql to handle unicode etc,anyways mysql is bad and on longterm especially if you are enough good for java i suggest you to use some better db with postgresql you were really satisfied i think,ps you can also migrate to postgresql it s smarter than mysql when choosing right indexes,in my personal openion mysql is slower than postgresql and mongo db,i m sorry to say this and maybe i m not solving your problem exactly but postgresql is 10 years older than mysql and is extremely advanced compared to mysql and there s many ways to achieve this easily,sql support - postgresql is much better for complex sql-queries for example with lots of joins and aggregates;mysql s stored procedures didn t feel very mature,bonus tip avoid mysql;postgresql is better in nearly every respect,postgresql requires more tuning than mysql to achieve optimal performance,"
"mysql","postgresql","better in better data database,faster in slower faster powerful,better choice than   in better data database,better in better data database,different database than  in amazon different compability,more proven in better data database,faster overall,choose  over  overall,much faster than  in slower faster powerful,better overall,better imo in better data database,","13155314,205364,4427769,2447484,53484107,32023224,22418916,47959495,1338712,1144979,4916041,","then postgresql is much better than mysql,mysql i am told can be optimized to do faster reads than postgresql but both are pretty ridiculously fast in terms of # transactions sec they support and it doesn t sound like that s your problem,mongodb is fun to toy with and i ve built a few apps using mongodb myself for that reason but mongodb s almost never a better choice than postgresql mysql sql server etc,is postgresql better than mysql for such purpose,the aurora it is completely different database than mysql but when it comes to the compability with mysql or postgresql amazon team did a lot so that there would not be a lot of differences,either postgresql more proven than mysql for such huge data,i wonder why postgresql s single insert statement is completely faster than mysql s when autocommit is turned on,i would choose postgresql over mysql,postgresql has a richer set of abilities and a better optimizer;its ability to do hash joins often makes it much faster than mysql for joins,fyi postgresql scales better than mysql on multi-processor overlapping requests from a review i was reading a few months back sorry no link,postgresql is also a lot better imo than mysql and is the recommended database to use with django according to a lot of the people close to django,"
"mysql","postgresql"," fulltext is a lot in slower faster powerful, you get a top-notch overall, select rows with more in slower faster powerful, is more in better data database, and finally overall, doesn overall, documentation scrupulously overall,better in better data database,more in slower faster powerful, is more overall,general overall,","1634229,12258446,51187068,332410,56779783,46212392,1331967,8749161,13155314,1630987,1316464,","postgresql fulltext;is about 10-100x faster than mysql fulltext is a lot more powerful gist is fast on inserts updates no problem with locks in other words it s a totally decent solution,mysql i wouldn t recommend because you are really tying everything to one app only that can write to the db sql mode soup means relations are basically a private api instead of the public api they are in postgresql firebird and ingres and this means less flexibility down the road;however with postgresql you get a top-notch extensible development platform in a box,my problem is similar with mysql select rows with more than one occurrence but i am using postgresql,my opinion is that mysql is more of a programmers dbms;whereas postgresql and others like it are database administrator s dbms,it may be an option to migrate to 4.0 ee first migrate the db to postgresql which is closer to oracle than mysql and finally upgrade to alfresco community 4.2.f latest versioning with web services api,usually mssql oracle postgresql etc. this problem can be solved with the help of dense_rank function;but since mysql doesn t have window functions you can do something like this,postgresql s traditionally had a more complete feature set in terms of acid compliance support for advanced queries etc than mysql postgresql has solid windows odbc drivers and postgresql documentation scrupulously points out any areas in which postgresql deviates from the sql standard,there are workloads where postgresql is better and there are workloads where mysql is better,the query optimizer in postgresql is more advanced than in mysql and copes with complicated statements much better especially when it comes to sub-selects,postgresql is more developer friendly and makes it easy to do the right thing regarding data integrity by default;if you give mysql an incorrect type it will implicitly convert it even if the conversion is incorrect,otherwise i recommend using postgresql over mysql since afaik is more standards compliant and has a nicer license,"
"mysql","postgresql"," is faster in slower faster powerful, has their own implementations just overall, does not really overall,different than  overall,more overall, has the richer overall, supports more in slower faster powerful, is very in robust mature complicated, please see the load overall,faster in slower faster powerful,more overall,","4920030,2797218,945533,57260542,3776914,44438750,4059538,3422364,13837311,30113392,586834,","postgresql is more suitable for large-scaled websites althought they both work on large-scale;anyway mysql is faster,i don t think that postgresql will affect your performance despite postgresql uses more resource since postgresql a full rdbm implementation in the long term postgresql will be the best option if you are looking for performance;number of sql queries is relative you must remember that to bring same results mysql and postgresql has their own implementations just to give an example mysql dont implement the full ps sql,you have to use an other database like postgresql to get real blob support sorry;i do not know which client api you use but when trying to use blobs from own java and objective-c clients it seems mysql does not really support streaming of blobs,mysql answers will be quite different than postgresql answers,does that mean perhaps rails team favor postgresql slightly more than mysql,unfortunately the mysql version isn t fully supported by django and you will need third party libraries;of the two postgresql has the richer set of functionality and provides better indexing options,postgresql supports more advanced queries it performs better on complicated queries but is harder to manage;mysql is fast easy to manage but you can run into it s limitations on advanced queries stored procedures and the like,mysql is very popular;postgresql i think has lot more features personal preference,for postgresql i do not know;for mysql please see the load xml syntax docs,according to my own experience postgresql run much faster than mysql especially handling big tables 1.4 gb lineitem table in my case,i had mentioned that in our rails application all select queries dropped below 100ms after switching to postgresql whereas some of the complex joins generated by activerecord would occasionally take as much as 15s or more with mysql 5.1 because of nested loops with inner table scans even when indices were available,"
"mysql","postgresql"," issue rather in better databases reputation,fulltext 10-100x faster in slower faster powerful, does not overall,much better in better data database, is bit cheaper overall,less troublesome in table ancedotal servers,better in better data database, is more in slower faster powerful,more flexible indexing in table ancedotal servers,slower in slower faster powerful,much better in better data database,","48931169,5926886,23896165,45528333,52053410,4394396,2000205,2015064,12495775,1162395,14322003,","i think this issue is more common with postgresql databases than mysql databases because with postgresql there is more likely to be lagging connections this is a postgresql issue rather than gcp issue,i did some benchmarking 3 years ago may be stale... which showed that on large datasets basically postgresql fulltext is 10-100x faster than mysql and xapian 10-100x faster than postgresql but not integrated,postgresql is also open source like mysql but far more featureful and supports check constraints;one big difference is that mysql does not support check constraints which is an important part of maintaining database integrity,postgresql is much better suited for data warehousing compared to mysql,now coming to second question based on pricing as you can see from mysql pricing page and postgresql pricing page mysql is bit cheaper than postgrsql reading on the answer you can make informed decision what would be best for you,this is ancedotal but the postgresql servers i ve managed have always been much less troublesome than mysql which likes to randomly crash once in a while occasionally corrupting a table on the way down,also you might take the time to compare postgresql and see if there is something about it that meets your needs as well or better than mysql,the reason is primarily optimizer maturity postgresql can better handle the kinds of queries you re likely to run;mysql is more likely to get confused on five-way joins go bottom up when you run a subselect etc,not only does postgresql have a far more flexible indexing than mysql but the table approaches are very different also meaning the appropriate indexing strategies are as different as the tactics are,postgresql is already slower than mysql up to a certain point it is actually faster when you have a ridiculously large database,some say mysql is much better for bigger projects while others think just go with postgresql,"
"mysql","postgresql","easier in better databases reputation, as well overall, or mariadb now in slower faster powerful, has more in slower faster powerful,good planner as  overall,performance probably better in better databases reputation,such as  overall, has been easier overall,sqlite with  in better data database, handles collations much better so overall,faster overall,","3162419,22810211,48577020,18337020,21661920,846356,54845678,5941723,55177451,1052708,3994593,","postgresql supports recursive queries in the form of recursive common table expressions which make querying heirarchical data easier than in mysql and also give better performance,summa summarum postgresql probably is more ready for primetime than sql-server;- postgresql has has had paging for almost 10 years now mysql as well as sql-standard syntax compliant sql server just got that feature with sql-2012 only standard-compliant,but sometimes postgresql is faster than mysql or mariadb now i think i m doing wrong things.,however postgresql has more advanced features like partial or functional indices if you need mysql and postgresql,current versions of mysql has not good planner as postgresql has there is progress - so complex queries are usually much better on postgresql - and really simple queries are better on mysql;complexity of postgresql configuration is myth,as i said postgresql is far superior and i hate mucking with mysql s bizarre bugs and i think that overall postgresql performance is probably better than mysql for any even slightly complicated query,you will find postgresql is much more enterprise-oriented than other products such as mysql,i agree with denis postgresql is a good choice and is definitely a lot stronger on the sql feature side than any other open source dbms and rivals the big ones as well;mysql has been easier to setup in a high-availability master slave environment but the game has changed with 9.0 and will change even more with 9.1 and synchronous replication which is more or less the same as oracle offers with it s data guard,so i ended up using a relational database anyway since only there i could use compound keys without any hacks . i performed a benchmark to compare sqlite with postgresql and mysql - 500 000 inserts of 60 kb blobs and then 50 000 selects by the whole key,for that to work the database postgresql must have the correct encoding set see the first two references;note mysql handles collations much better so if you aren t too far along and you require multiple collations then it may be a good idea to switch,some recent tests we did showed that postgresql does perform faster than mysql and we believe the table partitioning feature in postgresql will be very important with a table in our database we foresee to grow into 100 million rows and more in production,"
"mysql","postgresql"," is much more popular so in better databases reputation,same as  in better data database,better in better data database, and not in slower faster powerful, but not overall,not less in robust mature complicated,general overall,faster in easier replication worse,slower in slower faster powerful, with returning clause in sql server error, is generally in easier replication worse,","1373617,56076285,14759864,30999205,8736369,1922486,52741626,1373625,30008388,30824969,1545598,","the simple answer is that postgresql is better for both features and security;however mysql is much more popular so is more likely to be directly supported by a random application especially web applications,after creating the above file i ran pgloader database_data.load command to load the data in tech_login_development_mysql database there is a log table is my project whose created_at and updated_at fields in the newly created postgresql database is not same as mysql database,i just recently switched databases to postgresql which has given me a slew of problems although i must admit i like it a lot better than mysql,if you want to use a external database i would recommend you to import the shape file in postgresql and not in mysql since it has a more powerful support for spatial data;mysql supports several geometry functions for editing and querying spatial data,quoting is different mysql uses backticks for quoting identifiers whereas postgresql uses double quotes;like is case insensitive in mysql but not in postgresql,mysql is not less complicated that postgresql -- they re the same thing,we recommend using mysql or postgresql,postgresql is faster than mysql s innodb,i did a simple performance test and i noticed postgresql is slower than mysql,if you care about performance more than using eloquent then with raw queries in postgresql with returning clause or sql server with output clause you can return updated records in one go;mysql unfortunately doesn t have such support on a statement level,mysql has some advantages like easy replication but postgresql is generally nicer to work with,"
"mysql","postgresql","better in better data database, is just in slower faster powerful, makes more sense in better data database, v11 is better in better data database, it isn overall,safe with  in sql server error,faster in slower faster powerful, is a more in better data database,cleaner in easier replication worse, enums cannot overall, s better overall,","47067702,44831306,53582346,55551621,11622152,11397945,30008388,14767734,3485121,9045335,195398,","postgresql seem to better than mysql in terms of speed,other dbms for example postgresql wouldn t allow this query to execute at all;saying that how it works internally within mysql is just that you get a unique record for each posts.id but random values from potentially different rows for all non-aggregated and non-grouped column,there are applications where maintaining the necessary data integrity using an external database sqlite mysql postgresql makes more sense,also not sure which of mysql 5.7 and postgresql v11 is better for this kind of use case - this question arises as this use-case is primary and we can choose between either of the databases,sqlite is for local site and postgresql is very populare but i think it s just a choice;if you want to use mysql it isn t a problem,you d be pretty safe with postgresql or sql server for instance;mysql - requires a bit more thought,mysql run 4x faster than postgresql,in my experience postgresql is a more solid database and performs better with many simultaneous users especially when writing to the database although harder to learn its more strict;mysql is easier to learn and may be sufficient depending on the total number of request traffic,however people often argue that postgresql is cleaner and easier to use that mysql,other dbmss like postgresql have non-redundant enums so the following might not really apply to your situation;mysql enums cannot be applied in every situation anyway,at the load levels where postgresql s better locks overtake mysql s other parts of your platform could be the bottlenecks;postgresql does comply better with standards so it can be easier to replace later,"
"mysql","postgresql"," does not in sql server error,general overall,better in weak better power, as well in slower faster powerful,better in better data database,better in better data database,","16359199,9652662,4343455,4059538,21216225,1467079,","postgresql does not comply with the standard here and folds everything to lowercase;for mysql there is no definite answer on how it does this,depending on what query you make like queries that match at the beginning of the column may use indices in postgresql i know for sure that they do;in mysql i m not sure,this is a weak point that iirc postgresql can handle better but with mysql you have to work around that by reverting the changes yourself in case of rollbacks,they are sufficiently similar that i d recommend starting with mysql but learning postgresql as well,the other question is that what i am hearing from our group leader is that postgresql is better than mysql when working with coordinates,if you need spatial data capabilities postgis with postgresql is better than mysql,"
"button","checkbox"," do not in group better radio, group is easier in group better radio,more than one  overall, is more in group better radio, are not readonly in group better radio,better in multiple confused better,general overall, i know i in multiple confused better, are not in group better radio, should be a verticallayoutcontainer in select asp.net event,more overall,","42030060,34920385,53759936,7022434,19591104,25790632,51738569,21483676,857827,28611304,43656806,","when there is more than 1 checkbox s checkbox do not work as a group like radio button such that only one 1 radio button can be selected when there is more than 1,the checkbox list use the ng-required directive with the someselected function defined in the controller which checks if at least one item is selected;the option button group is easier and use the ng-required directive with the condition,please find fiddle link for the reference code check more than one checkbox and click on the button you can see only the last one is getting displayed whereas in the console i can see the other codes,edit since you say you can t add a none option and want to use a checkbox even though i strongly disagree on checkbox where a radio button is more appropriate - a common ui error .,first all radio button should have the same so if you select one the other will be deselected;if the checkbox are not readonly you can code the selection logic in an init function,you ll want to have multiple radio button each with it s own value to set some property to although this is weird for a yes no you are better off with a checkbox but if you had multiple values this is how radio button work,for example if there is a pressable interface which represents everything that can be pressed button checkbox should implement it,checkbox allow multiple selections while radio button do not;users should expect them to work like that and would get confused if checkbox worked like radio button i know i would get confused.,i assume your select all and deselect all button are using javascript to set or unset all the other checkbox in the form;unset checkbox are not passed on,if the list gets too long you want a vertical scroll bar for the checkbox but the select all button should not scroll;the panel containing all of the checkbox should be a verticallayoutcontainer,but works fine if more than 6 checkbox are selected then reduced to 6 with more than one button click,"
"button","checkbox","first question with  in first comments using,more in group better radio,values greater overall, is not overall, is center horizontally overall,better radio in group better radio,disabled.after in able next column, are not checked then overall,more overall, group seems more in group better radio, is way in group better radio,","49789035,40849763,37568827,52795413,51814296,42163991,42948202,34646065,26847897,54687394,37589637,","it saves only one question at a time or some times null or on keyword against checkbox or radio answers.only first question with checkbox or radio button options can be selected,one more thing i also want select only one radio button when i have selected more than 2 checkbox,i have created a php form with multiple checkbox and there is a script that controls if the sum sum of all checkbox values is greater than a specific number.if yes a sweetalert box appears.the problem is that when i click the ok button on the sweetalert i want to remove the last checked attribute and to remove that value from the sum to execute the script again and display the right results.i cannot figure this out..any ideas will be helpful,the following code will work in cart page for your privacy policy checkbox using mandatory jquery code to make it work as proceed to checkout button is just linked to checkout page without submitting any data;the code will disable the button on start it use sweet alert 2 js message to display an error message on button click when the checkbox is not checked the complete code code goes in function.php file of your active child theme or active theme,demo i did a custom radio button but the dom structure make it hard to align middle for the custom checkbox i need the label which has different rows to be at the right and the checkbox is center horizontally,using checkbox to check uncheck is better than radio button,initially the value of button is disable because value of button is true so the button is disabled.after selecting the checkbox now the button should be able to click,i want that when one or all the checkbox are checked submit button is not disabled;but if the checkbox are not checked then the submit is disabled,obviously for something like show hide functionality a checkbox makes more sense than a button,from the usecase it looks like you want the user to select the address type either residential or commercial so a raido button group seems more suitable;i have edited the html to create the radio button instead of checkbox,a radio button is way of determining a choice between elements similar to an input text;checkbox are not radio buttons,"
"button","checkbox"," seems a better in group better radio, allows answer in first comments using,more overall,more useful overall,more overall, are circular but only in group better radio, doesn overall,more column in able next column,more suitable rather in group better radio, given a class overall,better off with  in group better radio,","45490250,49596443,27614816,20774279,15964168,37833560,39569659,23243749,15388986,49489106,6731351,","adio button is more use when you want a choice between several value;in you case case s more like a boolean so checkbox seems a better choice,so you need an additional line to declare an object of type for it to be a memeber is implicitly static and final if my limited understanding is correct - here s some code that may help you understand how to construct in a few ways get and set using enums - working example here s a working example that - utilises enum s utilises both arraylist and cursor arraylist used for asking question and for list of questions 2nd cursor used for list of questions 1st list traverses questions via previous and next button allows answer no answer checking to be one of edit text radio button or checkbox according to to the question displaying the appropriate view,checkall and uncheckall on single button is worked if there is more than one checkbox present.but it will not worked for single checkbox,i saw he used checkbox which to seems much more useful than button since you can take multiple objects out at a time,when i reload my html pages its button select and checkbox are goes added more and more.because all the html data fetch from a data base and after added new things in the page i store all the html of the page in data base and when i display this page agiain becuase of jquery mobile its button select and checkbox are increased check here visit jsfiddle.net sharma9853 gbe6p 7,if you re sure you want checkbox and not radio button;checkbox are generally square and several can be checked radio button are circular but only one out of a group can be selected,by default it behaves like so even you make the revealbutton visible always the problem now is this button doesn t work properly;the official recommended method is to create a similar ui for example a checkbox to let a user switch the reveal mode,currently i am able to generate checkbox in column what i need is one more column with button same as checkbox in each row,if you want the users to select only one option perhaps a dropdown or radio button list is more suitable rather than checkbox,i would like to have it so that if i select more than one checkbox then the entire column of the action button given a class of .table-btn disabled,well actually you would be better off with checkbox control and you can make checkbox control look just like regular button,"
"button","checkbox","better in multiple confused better, labelled none in options certain expectations,more than one  in group better radio,more in able next column,choose  over  overall,more in group better radio,bigger overall,larger overall, is not in first comments using, not in group better radio, not in options certain expectations,","42560476,5202741,53408418,3591491,7672369,11662905,44543142,43553253,38761932,50253027,27511742,","so i think a custom checkbox is the better choice here but if you really want to use a button i would use a hidden checkbox field and apply onclick functions to the button to change the value of a hidden checkbox field,but in reality if you need them to be able to deselect then you need a checkbox and not a radio button;you have two options create another radio button labelled none or something appropriate to your form that s part of the same group then they can select that instead,more than one checkbox in a group can be checked at any given time what you re looking for is the radio-button element itself because semantically it stands for a radio button is one of a group of controls representing mutually-exclusive choices unfortunately angular material does not include a mat-radio-list component,i can verify that by unchecking the checkbox it does flip the value because i enable disable the next button depending on the myproperty value - i have more than one checkbox btw,i choose button over checkbox in this case because i think the grouping are handier and faster to work with,unlike radio button the user can select more than one checkbox at once or select none of them at all,in both cases plus minus same except in case where is the small checkbox the button is bigger on width maybe it is related with space for checkbox,what i d like to do is make the button images slightly larger than the checkbox and have the text checkbox and both button on the same line,from your comments i have updated to first check that the radio button is checked first using;jquery #yesprefcreditlimit .is checked which says if the checkbox is not checked,use checkbox not radio button;radio button only allow single choice,if you want the user to only make one choice from several options use radio button not checkbox;users have certain expectations when it comes to the interface and using checkbox would be counter-intuitive,"
"button","checkbox","more in select asp.net event,more in select asp.net event,more in group better radio, cannot overall, not in group better radio,more than one  overall,more overall, doesn overall,little value as  in group better radio, not in group better radio,more in able next column,","4808581,31329662,42227507,10615995,50253027,57488168,38006288,29277834,52707563,20808306,45560870,","how to restrict user to select not more than 10 asp.net checkbox on button click event if user select more than 10 checkbox then alert box will pop up that you can not select more than 10 checkbox,i have an asp page where the status of a list is approved by selecting checkbox and then pressing approve button however when i select more than one checkbox it throws the error index was out of range,i have group of checkbox each of this checkbox associated to a li tag which contain a query result i have also button so i want when i check more than one checkbox and after that click on button i want to display text of li tag for each selected checkobox,get button back and hide it by adding display none style add client side onclientclick event call this button s click event using javascript;yes because of event bubbling controls like checkbox cannot fire itemcommand event,note that this also covers the no checked answers option while it is possible to start with no radio button selected once a user selects any radio button they will be unable to un-select;use checkbox not radio button,if i check more than one checkbox and click the add button the checked item is not added into the table,i have a form with multiple checkbox and a button by default the button will be disable but after checking at least 2 or more than 2 checkbox the button should become active,in the new chrome versions the pause on exception button doesn t toggle anymore between 3 states disabled pause on exceptions pause on uncaught exceptions but only between two states disabled and pause on exception;in order to be able to also break on caught exceptions they introduced this checkbox this is useful if you have a global exception handler in gwt but still want to break when the exception is thrown,if you were looking for a checkbox radio button that is checked by default button being selected by default provides very little value as button are used to submit or post,i know this to be true because i remember when car radios had real radio button and i was reliably yelled at whenever i tried to make the row of button straight so that no button was pressed;you re talking about a checkbox not a radio button,as image shows below i want to make next button enabled only if more than one checkbox get checked,"
"button","checkbox","better idea in group better radio, that was intuitive enough overall, and use it instead overall, length is greater in group better radio,better fit in group better radio, is greater in group better radio, is selected the radio in group better radio, is not overall,better in group better radio,","21157659,12452222,32199066,51149103,38365898,25316061,49348853,40661596,14786816,","if it s only one colour you want selecting you can use a radio button instead which would be a better idea than a checkbox,in ios the next and previous button do not work for checkbox;the reason may just be that there isn t really a keyboard way of handling checkbox that was intuitive enough,when you click a button inside a label the label s activation behavior must be to do nothing because the button is an interactive content it means that we should think that user want to activate the button instead of the enclosing label so it won t change the checkbox s state by clicking the button;i think you could use non-interactive content instead of the button not sure if it s good for a11y tho or just unhide the checkbox and use it instead of the button,here jquery script code if checkbox length is greater than 1 then submt button will be enabled,checkbox may be a better fit than button here,1 find which radio button is selected;2 if the checkbox are selected previously and the count of selected checkbox is greater,this part works fine the problem is when more than one checkbox is selected the radio button act like they are all under the same name even though they aren t...when one is selected all the others become deselected when i only want the other ones under that name to become deselected,the result is that when the device rotates the top-level view resizes but each button s position relative to the top-left corner of the top-level view stays the same;turning on the upside down checkbox is not sufficient to allow upside-down orientation on iphones only on ipads,actually i found out why using two radio button with same names and different values true and false is better than checkbox,"
"jar","war","more caused by org.apache.commons.jelly.jellytagexception  in web-inf lib tomcat,general in project runnable executable,easier overall,better way than  in better spring boot, it is much easier overall,general overall, it is a little in project runnable executable, is higher overall, contains more in better spring boot,file not longer in web-inf lib tomcat,more in web-inf lib tomcat,","50858185,52734153,39713145,50506960,10188935,52817749,35398669,5856222,39285351,43719081,7628932,","120 more caused by org.apache.commons.jelly.jellytagexception jar file var cache jenkins war web-inf lib jenkins-core-2.89.4.jar,when i was making a war file of my project from window it is add one more directory with folder name in which i am making war using jar -cvf mywarname.war which cause forbiden issue due different directory structure,so in my practice using war app easier than jar to manage and re-configure,here is my pom.xml edited also i have found that spring boot can be compiled to jar as better way than war,you could also use the maven dependency plugin s unpack goal to extract classes from the dependency war to;but i do recommend if at all possible to refactor your controllers into a separate jar it is much easier and more maintainable,package will add packaged jar or war to your target folder we can check it when we empty the target folder using mvn clean and then run mvn package . install will do all the things that package does additionally it will add packaged jar or war in local repository as well,this will result in a runnable jar with only the specified class of your project;since your project is a war it is a little more complicated,like remove the jar s from your war in case the version of your jar is same or lower than that of jboss s or replace the jar s of jboss and remove that jar s from your war in case the version of your jar is higher than that of jboss s,you linked to a question about a war vs a jar while spring boot s jar is indeed a jar spring boot s jar contains more than what you usually put inside a jar,i tried to do it in server s pom.xml file and i set the impl module to provided scope so the jar file is not longer contained in war but now the tests are failing as after unpacking the war file used for running the tests does not contain the impl jar file which is required for that,i recently got this error with tomcat 7.0.21 on windows 7 when a war contained jars which had duplicate class definitions a class was defined in more than 1 jar within the war s web-inf lib directory,"
"jar","war"," contains older version overall,really thinner overall,more packaging overall, but are standalone java overall,more overall,more common in project runnable executable, or exploded deployment in web-inf lib tomcat,better embedded in better spring boot, is not correctly overall,general overall,","47083949,41872831,10373580,37258522,36870332,28736014,16778792,37590402,16873994,25629565,","weblogic 12c has additional libraries bundled and therefore an exception might be thrown because the jar contains older version of the same library and some classes methods might not be present in that jar;the workaround is to deploy ear that packages your war file and specifying the preferred libraries,if your client jar is really thinner for example only the interfaces having the war s reference the client jar effectively disallows them to access to ejb implementations which is always a good idea,i set up a maven multi module project one with packaging war two more with packaging jar,the reason that you can not run a guide with run on server is because of spring-boot adheres to the slogan of make jar not war;the guide sample code are not things you deploy on a server i.e war but are standalone java apps jar which contain their own embedded servlet container if they need one,keeping your business logic in ejb jar will pay off when you ll need more than one war in your ear different security realms etc.,5 a war file can be runnable executable though this is more common with jar files,tomcat will not find the class if the poi jar isn t in the web-inf lib of your deployment;check your war or exploded deployment to see if it s there,is embedded jar better than embedded war file,if build-project which is of type war defines utility jar as a dependency at provided scope it needs to be bundled with ear;if your utility jar is not correctly being shared across multiple war files you could troubleshoot it by adding utility at compile scope to build-project,also here any component class registered in war s faces-config.xml has higher precedence over the one in jar s faces-config.xml in this particular case primefaces one,"
"modulo","multiplying","less expensive overall,greater than the  overall,","17397158,46939903,","also integer multiplying is less expensive so you may just do the divide first and calculate the modulo 10,more generally you can always just try multiplying the base by the base a number of times no greater than the modulo and you are bound to find a cycle,"
"gecko","webkit"," rendering engine overall,slower then overall,less overall,probably better overall,more overall,stricter overall,inconsistent rendering between  overall,better overall,smaller overall,less memory overall,quicker overall,","15421342,4107879,20306381,6186726,11043798,1078466,52222915,18404543,184381,16970359,14398961,","webkit .net is a wrapper for the webkit engine used by google-chrome and apple safari;geckofx is a wrapper for the gecko rendering engine used by mozilla firefox,anyway i just prefer not to use it because gecko is slower then webkit,it s called web audio api and is currently a draft but is well supported by webkit you ll need to use prefixes and a little less by gecko,so the easiest way is to use one either webkit or gecko webkit has probably better support for svg these days plus i can t find package of gecko right now,gecko seems to like anti-aliasing more than webkit,webkit is stricter than gecko,i m facing weirdly inconsistent rendering between webkit and gecko,somehow webkit seems to do better than gecko on the html5 single-page spec scripts and i can t figure out why at this point,my understanding is webkit is pretty good smaller than gecko,gecko is often considered to consume less memory than webkit but this depends a lot on how the browser is implemented,i m developing with canvas too and have found that webkit based browsers in general handle canvas operations quicker than gecko in most cases,"
"gecko","webkit"," will not overall,","18404543,","the upshot is that in some cases webkit ends up recomputing style on a lot more elements than gecko does as far as i can tell but in others it ends up recomputing style on many fewer elements;for example given a selector like .foo span and a div that changes class from foo to bar gecko will restyle all later siblings of the div while webkit will not do any work at all if there are no span kids since it would never have marked the parent as being affected by the + in that case,"
"spdy","ssl","general overall,handshake more overall,more overall,connection worse overall,","22277079,14248891,9803224,11399413,","well if you re willing to buy an ssl certificate or already have one i would recommend using spdy an ssl certificate s available in the nginx-extras package to enable an ssl certificate you just add an ssl certificate to the listen line,i m using keep-alive so i think the ssl handshake is more or less totally out but i hope to move to spdy soon so i don t really know how that helps after the initial handshake,plus once spdy becomes more common ssl slow down won t really account for anything much,if we assume that you live in a poor coverage area and your phone and your tower are constantly dropping the signal then re-establishing a spdy connection is no worse off than re-establishing a tcp connection modulo ssl handshake,"
"auto","decltype","significantly more versatile overall,more concise overall, is an inseparable construct almost overall, is not overall,","21369252,12084124,39740390,27652808,","decltype is significantly more versatile that auto and can always be used in place of it,in the cases where auto can be used it is more concise than decltype as you don t need to provide the expression from which the type will be inferred,other than that auto cannot be used as a standalone entity inside decltype x because that would prevent x from being a valid expression;decltype auto is an inseparable construct almost as if it were a keyword like decltype_auto,note decltype auto is primarily useful for deducing the return type of forwarding functions and similar wrappers as shown above where you want the type to exactly track some expression you re invoking;however decltype auto is not intended to be a widely used feature beyond that,"
"laravel","yii","much more better overall,larger overall,","44506698,38303403,","according to my experience yii is easy to use but it has some pros and cons too every framework has but i used laravel on some projects and i think laravel is much more better than yii,developers community it might seem that laravel has a larger community of developers but during my career with yii i ve found no bottleneck of having a question without an answer,"
"jtable","jxtable","more overall,better overall,","3231289,5796139,","use swing-x components there is a jxtable which is more powerful than jtable,also is jxtable better than jtable for such an application,"
"coalesce","isnull"," may just overall,better in better sql server, is more in query plans names,better choice overall,better in better sql server, is better choice;below in better sql server,less costly overall,prefer  over  overall,more portable overall,more overall, is more in faster first values,","10668170,29170770,50162657,5024166,18274020,17718956,13366686,28963454,5537484,16697158,39309543,","coalesce is more correct of course you could just embed isnull over and over and over.. but put that under a performance microscope and coalesce may just win,in most cases it has reviled indexes that needed to be add and in most cases the indexes improved the queries the most but after thet have been added the isnull and dynamic still perform better than the coalesce,if all names match if they don t then use isnull or coalesce but isnull is more efficient,sql server is probably smart enough to translate isnull into the equivalent sarg expression but if you are bent on using a function then coalesce is a better choice because it is part of the sql standard allows for multiple values instead of just two with isnull and avoids using quite possibly the most confusing function name microsoft ever devised in isnull,on the subject of performance on sql server isnull often performs better than coalesce but the latter is ansi compliant if that is important to you,in such cases coalesce is better choice;below is an example showing the difference between isnull and coalesce,i ve been told that coalesce is less costly than isnull but research doesn t indicate that,however i prefer isnull over coalesce since the latter has an issue if it contains a sub-query,- coalesce should be more portable than isnull,you can also use coalesce which is the more general form of isnull and is actually part of the sql standard,however coalesce evaluates the first argument twice;so for an expensive operation such as a user-defined function isnull is more efficient,"
"coalesce","isnull","faster in faster first values,more standard overall,faster in faster first values, is a better in better sql server,more cross-compatible overall,faster in faster first values, is more in better sql server,little difference between  in faster first values,more values in faster first values,better in better sql server,better overall,","2287656,7305245,18828697,48300857,1037788,6152611,50473331,49036508,41830025,6186100,38028245,","isnull will be faster i think because it has lesser function code implementation for itself making it faster than coalesce,coalesce is the more standard alternative of isnull,isnull can only have one input however it s been shown to be slightly faster than coalesce,the generic sql solution uses coalesce in sql server isnull is a better choice for performance reasons,coalesce is more cross-compatible than isnull or nvl it works on mssql oracle mysql derby et al.,isnull is marginally faster than coalesce,isnull and coalesce do just about the same thing but isnull is t-sql while coalesce is more generic,this question suggests that there is very little difference between isnull and coalesce,i tend to use coalesce only when i need to get the first non-null value from a set of 3 or more values as isnull only supports two parameters,isnull is better then coalesce because of how datatypes are handled,using coalesce is better option than isnull or case..when for this problem since the input values for the coalesce expression can be evaluated multiple times,"
"coalesce","isnull","faster in faster first values, although provides more functionality overall,faster in faster first values,better option in better sql server,prefer  over  overall,better in better sql server,better in better sql server, is more flexible; will only in important flexible isnull, is faster in faster first values, is more in important flexible isnull,better in better sql server,","25415130,8810612,18828904,44962212,53119273,31327603,46306155,1088743,14169451,1088743,39027665,","this is pretty much the ifloop answer but isnull is slightly faster than coalesce,use coalesce instead;it functions much like isnull although provides more functionality,isnull is faster than coalesce,edit based on the tests done by multiple people and by theory isnull seems to be a better option over coalesce,one of the many reasons why you should generally prefer coalesce over isnull 1,you will find that coalesce works better than isnull here,is the performance of coalesce field constant better than isnull,but more important is the fact that coalesce is more flexible;isnull will only work with two parameters,regarding isnull vs coalesce -;it seems that isnull is faster in this instance,the coalesce is ansi standard and isnull is not;but more important is the fact that coalesce is more flexible,an example of why coalesce is better than isnull,"
"coalesce","isnull","more overall,faster in faster first values, does not overall,more portable code in faster first values,more efficient in query plans names, but looks better in better sql server, is better in better sql server, infers data type overall, is faster than  even in faster first values, allows more in faster first values, has better performance in better sql server,","15523066,13174599,10669778,30082807,2287673,22878576,55065451,50719663,53131425,11349814,50290899,","in this case i would use coalesce which provides more levels than isnull rather than the case stement,in some circumstances isnull is faster than case or coalesce,coalesce function works a bit different coalesce will take any number of parameters and return the first non- null value i prefer coalesce over isnull cause;meets ansi standarts while isnull does not,coalesce will go through the listed values and choose the first one that isn t null it s more portable code than isnull or ivnl etc,to prefer isnull over coalesce when given the choice is that isnull tends to produce query plans that are more efficient than coalesce,note that in this case the isnull would do the same as coalesce but looks better for this application in case you add some new language,you also are missing the on clause on your first join a full join is not necessary as a left join to your catalog table covers what you need coalesce is sql iso standard but as you are comparing only two values and you are using sql server isnull is better to perform that operation use two left joins or you can use union all,dbfiddle demo isnull infers data type from first argument coalesce infers from wider one data type precedence,also note that isnull is faster than coalesce even if the difference is pretty negligible,the function isnull is kind of equivalent but coalesce allows more arguments and is standard sql.,the most natural function to use is coalesce because it is the ansi-standard function for this purpose;under some circumstances in sql server isnull has better performance but this is not one of those circumstances,"
"coalesce","isnull","better in better sql server,quicker in faster first values,better in better sql server, is the better in faster first values,faster in faster first values,","40413888,2287642,35307117,51142142,18085646,","isnull performs better than the generic coalesce and better than having another and,i understand the difference between these functions but my question is when checking for a single null value would isnull be any quicker than using coalesce,but depending on the answers to those questions i m guessing that coalesce might do the trick for you better than isnull,coalesce -- although standard -- has the downfall that it evaluates the first argument twice;so when the first argument is non-trivial isnull is the better approach in sql server,you can use isnull also in place of coalesce as isnull is comparatively faster than coalesce,"
"hide","show","general overall,more in term appropriate run-time,more details overall, is called less overall, doesn in better approach mess,faster overall, logic becomes more in method logic complicated,  instead overall, what is happening clearer overall,more button overall,more columns overall,","48202875,28083229,12538109,21408336,20923774,15525831,22942136,55183032,48162235,46596817,39886197,","when window is scroll down we will detect that if scroll space from top is bigger than 100px the image will be faded out hide and as opposite scroll the image will be faded in show,term hide is more appropriate for run-time dynamic show hide,ironically enough its object model show more details than it hide - good for learning but bad for abstraction,it doesn t show at all if hide is called less than 0.5s after show and it show for at least 0.5s this prevents very fast flickering stuff that you might see with naive implementations,in my experience hide and show is a better approach because show doesn t mess with the dom,i suppose it is because you hide them faster than you show them so for a slight second the overal page height is shorter than it should,if your hide show logic becomes more complicated you can move your hide show logic into a method on your controller,you should use .css on jquery object change to though i prefer using show hide instead of setting the style property. you can check the event.target.nodename to show hide the element,ith these two points we can use d3.geodistance which calculates the distance between two points in radians hence the use of math.pi 2 - which gives us points outside of 90 degrees for these we hide for the rest we show;this can be incorporated a little nicer into your code but i keep this separate here to show what is happening clearer,show less will be hide and load more button will be show .thanks,the issue is that if i scroll to the right and click on say column index 20 really any index that hide indices on the left it will freeze all columns below that index which causes the table to snap back to index 0 and not allowing me to scroll because more columns than show are frozen,"
"hide","show","longer description. overall,less than session  overall,general than   in method logic complicated,such as   overall,more with it  overall, call is less overall,higher overall, s better in better approach mess,more than just  overall, was no longer overall,better in none buttons display,","2374164,48148816,32617630,53311492,14033683,41628869,28936603,55539068,34560228,4054355,26862127,","it displays things normally on the screen of course but when you go to print preview in ie7 the only thing that show is longer description. so in other words it hide that first column and it also hide the second column,lowering the zindex of publisher to less than session hide the thumbnail and increasing it show the thumbnail,this method is more general than show hide as this method can be extended to any style rule,in my experience sometimes we don t use tabbarvc because it s not flexible such as hide show or change items number,you can of course improve it for instance using a proper callback rather than an inline event and do more with it show hide the play button but this is it in it simplest form,which means if the time between show call and hide call is less than that minimum time then there wont be any dialog to be show to the user,you can hide show the status bar on android 4.1 api level 16 and higher by using hide show functionality,i m not sure to understand the thing what do you want to do but if you want show and hide the content show s better like,this code does more than just show or hide fabs on scroll this code actually handles a pretty perfect action bar disappears on scrolling effect,finally i d hide the show more option as the show was no longer needed,also as mentioned elsewhere show is a better option to css visibility as hide sets display none and not visibility,"
"hide","show","transparent as  in part particular element,more overall,higher overall,  here overall, is more in term appropriate run-time,more than 0  overall, is more in none buttons display,more flexibility as the  overall, isn in part particular element,fancier effects than  overall,general overall,","51100991,31707816,34135102,52998125,1121319,51261447,48177296,51793524,54689980,49678608,57080981,","following are the problems v19 1 part of the toolbar hide behind the status bar in each fragment although i ve used in the drawerlayout of activity_main.xml 2 status bar is not transparent as show in above image although i ve used in v19 styles.xml 3 navigation drawer is not drawing behind the status bar v21+ 1 status bar is not transparent although i ve used the following in v21 styles.xml,if you re simply looking to hide the value from your cross table you shouldn t need anything more than the show hide items menu,to hide action bar invoke hide method using getsupportactionbar getactionbar in case of minsdkversion is 11 or higher as show below,i need more than a show hide here i think something that can initiate the template dynamically,term hide is more appropriate for;run-time dynamic show hide,just check for the length of your value then if it is more than 0 hide everything and only show the s you have filtered with .filter,to show and hide buttons i would use display like described here javascript style.display none or jquery hide is more efficient,something like this then using it like so extending collection makes more sense in this case which gives more flexibility as the hide show could be used with arrayslices then,the part that show hide isn t in any particular element. which means there s no way to implement a media-query override that says if the screen is big enough show this all the time;so it doesn t replace all your show hide navigation widgets,i modified your jsfiddle with some javascript i added the jquery library with and if you d like to play around with the code here s the jsfiddle if you d like some fancier effects than show or hide feel free to change those around,have tried hide show with css and variations of displaying results in javascript and html with no ultimate fix,"
"hide","show","indicate gray out  overall,more overall,","55925317,30593267,","for regular slicers there is an option to either hide items with no data or visually indicate gray out show last items with no data,can anyone help me to hide it onload and show when is more than delta and hide on scroll up when is before delta,"
"dir","folder","more overall, is no longer overall, which is higher overall, will generate no output overall, here is more overall, structure is a bit overall, is for java overall, is more overall, does not overall, is marginally more overall, is not overall,","38641779,54894124,49429359,26730671,51632297,1528864,48944854,47889067,52451714,21126781,4958579,","it iterates through the most recent folder returned by the dir command each returned on one line and after it encounters the 4th it stops if there are more than 4 folder,for exemple run your docker image with a volume maaping between you host folder and a temp folder docker run -v host empty dir some-temp-folder -it myimage copy your container folder with code files content into some-temp-folder to fill you host folder with you container folder run you container with a volum mapping on host empty dir but now container folder is no longer empty run -v host empty dir container folder with code files -it myimage note that steps 1 2 may be replaced by copying files from docker container to host,wondering if it is possible within node to require context for a folder which is higher in the folder structure would it be possible to get to the parent2 dir from child 1 see file structure below with code like require.context .. parent2 true for a structure like root -parent1 --child1 -parent2,if the dir command generates data the folder is not empty the set p will read information from the pipe and errorlevel will not be set;but if the folder being processed is empty the dir will generate no output the set will fail to retrieve data errorlevel will be set and the rd command will be executed,here is a basic way to loop through all excel files within a given folder here is more on the dir function,note this copies files into a flat folder structure;if you want to maintain the dir structure the dir structure is a bit more involved,so if you want to leave your resources where they are you will want to add src main java to the resource folder not src main java com voidustries poly;you are putting xml files in a folder called src main java hint java dir is for java files,i could create shortcuts through a python script if this is still a concern however there is no real need and running all the exe s in that one folder is more than fine by me i have found,fourth solution the folder path is used on command dir and passed together with the file name to the subroutine which works mainly just with name of file without path this works also with a unc path;the subroutine renamefile is never executed if the specified folder does not exist or is currently not accessible,folder objects and their properties for example files are more flexible and complete but may be harder to understand;note dir and dir look the same but dir is marginally more efficient,edit suposing you have saved your class in an accesible dir in a folder called applet your code should look something like this;the web-inf classes dir is not visible from the client explorer java environment which is where the applet is run so when it s trying to load your class it can t found it,"
"dir","folder"," and passed together overall,using  b to list in project grayed subdirectories,more overall, is somewhat overall,less overall,slower overall, ad named bin overall, and reecting the output in project grayed subdirectories,","52451714,34707006,12606175,22095998,4225772,6112701,54504102,5889110,","the subroutine renamefile is never executed if the specified folder does not exist or is currently not accessible;fourth solution the folder path is used on command dir and passed together with the file name to the subroutine which works mainly just with name of file without path this works also with a unc path,selecting a random file from a given folder is not too difficult;first you need the count of files using dir b to list them and find c to count them,any ideas how to use the command dir to more than 1 folder in one command,wordpress allow you to put uploads in year- and month-based folder;custom upload dir is somewhat smarter,calculates the size of the first 5 folder in your home dir should work on all platforms fails with an arrayindexoutofboundexception if there are less than five folder in your home dir,things like cmd c dir don t know why - and yes i do cmd c dir in cases i am in some kind of shared network folder with thousands of files and ls is significantly slower than dir,use a for f loop to catch the output of a command note if there are more than one folder bin within the tree this will give you the last finding only dir s b ad d f0 f1 f2 bin will recursively s list all folder ad named bin below d f0 f1 f2 in b are format b,you will then see files and folder not referenced by the project as grayed out;if you wanted to automate this you could produce a list of all files in the project directory and subdirectories from the os using dir and redirecting the output to a file for instance and parse the project file to get a list of all files referenced by it,"
"linkedhashmap","treemap"," is more overall,more overall, as stated by ejp overall, is faster as seen here overall,faster than  overall, is more overall, or changing your map overall, additionally provides iteration in slower predictable iteration,slower in slower predictable iteration,","54264093,9061732,21198223,22710394,50551555,54263969,55198754,23795519,27675156,","to make things a bit more efficient with linkedhashmap you should provide an good estimator of the required size at construction so that there is not need for dynamic resizing so instead of linkedhashmap new you say;i m my opinion the use of a treemap is more neat.,a treemap costs a lot more than a linkedhashmap and adds nothing to your project if you only need to keep original insertion order,linkedhashmap is faster for insertion because linkedhashmap won t have to unnecessarily compare values while inserting like treemap as stated by ejp,however a linkedhashmap is faster as seen here treemap has o log n performance for containskey get put and remove according to the javadocs while linkedhashmap is o 1 for each.,hashmap order not maintains faster than linkedhashmap used for store heap of objects linkedhashmap linkedhashmap insertion order will be maintained slower than hashmap and faster than treemap if you want to maintain an insertion order use this,stream linkedhashmap code style;using treemap is more cleaner since you can achieve it in one line,so using a treemap would not be the best choice in your case;you could either use a linkedhashmap or changing your map to a list,linkedhashmap additionally provides iteration with a predictable order with only a minimal performance hit;treemap has a higher complexity and thus lower performance but it constantly keeps all entries sorted based on their keys,it sounds like you need a treemap which has iteration which is not much slower than linkedhashmap and does what you really want,"
"submission","submit","loader image overall,choose between  in right acceptable button,added entries after  overall, method does trigger in method trigger handlers,general overall, is a better overall,better overall,more overall, takes more overall, which has more overall,more in right acceptable button,","39402164,50241289,47984893,31377553,48887925,56586832,25568874,15577279,50820873,8550609,41151330,","i am wondering if i can submit the paramaters to via ajax show a loader image after submission it but once the file is complete return the file for download,the type of your button is a submit so when you click on your button is triggering the onsubmit event which is not existing in your case so you ll have to choose between submission your form or using a click event on a normal button below is a clear example on how the two approaches work,here is what i ended up hacking together a spreadsheet document document with three sheets main the main data temp only used for the automatically added entries after submission links keeping the editing links and response ids corresponding to each line of the main sheet then a function that can be triggered from the menus creates a prefilled answer per line of main submit and puts the url and id in the links sheet,if you use jquery s submit method instead you use jquery s submit method does trigger handlers before submission the form.,you can either decorate your productviewmodel with the bind attribute as follows this will of course mean all the properties in the tblproduct will be bound when submission if you do not want all of the properties to be bound on submit for tblproduct then you can decorate the tblproduct with the bind attribute as follows more reading at msdn,i would certainly use trigger like this from jquery docs on submit forms can be submitted either by clicking an explicit or or by pressing enter when certain form elements have focus;it seems to me that manually triggering form submission is a better workable approach,you should treat submit as what happens after you ve done any pre-processing so binding to click events then conditionally raising the submit is better than always submission but conditionally stopping the submit,so i have a form that submit to a database and i want to prevent more than one submission if the user clicks the submit button more than once...with a little research this seems easily enough done however none of the solutions i found worked for me as i am using the jquery validationengine plugin to validate the form client side,i have a submit button on my forms and the formrequest function the question i have is how do i add a pop-up message after the user clicks submit that will display a success message if the submission takes less than 5 seconds display a fail message if the submission takes more than 5 seconds have the dialogue box disappear immediately after displaying the message and allow the post to either submit successfully or go back to the blank form if it is a failure and have the user re-try the submission,nowdays we do not submit mails to port 25;port 587 is used for submission which has more relaxed rules although it may be necessary to authenticate yourself if your ip address is not white listed,i suspect this will submit the right form data but may not be acceptable if your button is calling js and is doing more than submission form data,"
"submission","submit"," there is controller action overall, which is better anyways;you in method trigger handlers, button makes codeigniter actually in right acceptable button, is slower overall,","51026041,2619383,25614535,44963455,","i have a form which sends patch request with multiple submission buttons button_tag save type submit class btn btn-secondary name commit value save button_tag submit type submit class btn btn-secondary name button value submit there is controller action which check clicked button value,you can have the form validate on submit which is better anyways;you can return false from your validation function to prevent the form from submission,using a submit button makes codeigniter actually perform the problem standard form submission function which causes a reload or a redirect,nothing special but i don t know how many people have learned that you can use and with for two different actions in a single form with a single submit button;in the code above it seems to call the before the or perhaps it s just because the post submission is slower than calling a javascript function...,"
"alarmmanager","timertask","much better overall,more efficient overall, is for longer overall, consumes lesser overall, uses lesser battery overall, might be better overall,","17273214,17065295,3838911,12974736,13465710,7348124,","hence the alarmmanager which runs much better than a java timertask thingy,alarmmanager should also be more efficient than timertask,timer and timertask are fine for things with short periods that are only relevant while an activity is on-screen;alarmmanager is for longer periods such as time and date to be notified because it allows your service to get out of memory,alarmmanager consumes lesser battery power than timertask or thread,timertask;both of these are made for repeative execution on certain interval however alarmmanager uses lesser battery,alarmmanager is better if you need to receive the intent even when your activity is not active;timer and timertask might be better if you only need the periodic behavior while your activity is running,"
"uinavigationcontroller","uiviewcontroller","more overall,more overall,better rotations overall, - not overall, that contains the playersviewcontroller overall, -based project overall, doesn in framespace smaller uinavigationbar, s not overall,smaller in framespace smaller uinavigationbar,","9389913,16219524,13298254,31682396,18933086,14540687,41420099,1681855,7813178,","the app uses a uinavigationcontroller that never has more than 1 uiviewcontroller on it s stack,the uinavigationcontroller is more of a behind-the-scenes organizer for holding and displaying other uiviewcontroller s,the uinavigationcontroller class seems to handle better rotations than uiviewcontroller,2 rootviewcontroller is defined as a uiviewcontroller - not the uinavigationcontroller subclass;we need to cast it to a uinavigationcontroller to be able to operate on it as one,the message you are seeing is because at index 0 there is a uiviewcontroller instance corresponding with the label view controller - gestures in your storyboard and you are trying to send it a message that uiviewcontroller does not respond to in the belief that it is a uinavigationcontroller;the uinavigationcontroller that contains the playersviewcontroller controller is at index 1 in the tab bar controller s viewcontrollers array,i ve performed extensive logging and breakpoint setting investigating if didreceivememorywarning unloads the view when a uiviewcontroller is not the visibleviewcontroller;the results a uiviewcontroller s view does not get unloaded tested using arc in a uinavigationcontroller -based project,but initially it needs a uiviewcontroller to set as a root view controller in both uiwindow and uinavigationcontroller s root view controller they need a starting point;both works differently like uiwindow root view controller can be change at any time but uinavigationcontroller doesn t allow us to change root view controller,therefore your approach is exactly right use uinavigationcontroller s subclass of uiviewcontroller in place of standard view controllers when you want to achieve navigation within a given tab;the uitabbarcontroller represents its sub-tabs as an array of uiviewcontroller s not an array of uinavigationcontroller s,the framespace for the uiviewcontroller becomes smaller because uinavigationcontroller manipulates the view by adding a uinavigationbar which is 44.0f in size,"
"lighttpd","nginx","faster overall,even more ridiculously overall,faster overall,easier overall,better overall,","3436202,803700,3475275,3726590,2711216,","shortly put running some benchmarks on a page doing some database operations and serving static dynamic content has shown that plain cherrypy was twice as fast than nginx and memcached and about half faster than lighttpd,it s simple and nginx is even more ridiculously fast and lightweight than lighttpd,nginx seems to be the webserver getting the majority of the buzz lately it may be able to serve faster than lighttpd maybe not,i actually found the setup of nginx much easier than lighttpd not to mention that you can install a macport of nginx port install nginx +ssl that does not contain the ssl-breaking bug that lighttpd suffers from here,according to this benchmark nginx is a little bit better than lighttpd for serving bigger static contents so nginx would be a good choice for static flv video contents,"
"cos","sin","cheaper overall,slower in machine performance std,better job with  overall,faster overall,faster in machine performance std,faster than ug  in machine performance std,better overall,bigger overall,","18793975,6977614,56628417,16432028,18602020,51727716,14175641,29446008,","if i want to calculate a sin and a cos is it cheaper to calculate a sin and a cos or calculate a sin then a sqrt 1-sin 2 to get the cos,after reading a question related with the performance of sin cos why is std sin and std cos slower than sin and cos,in my experience implementations do a better job with sin and cos than with tan and this also applies to the inverses. neither the c standard nor the ieee754 floating point specifications a common scheme adopted by floating point in c require these functions to be as precise as the data types they use cf,so if we have a vector and want to calculate a component-wise function over it say sin and cos it is faster to use vvsincos from accelerate because it will pipeline the calculations of sin and cos for all the entries in the vector which is faster than using a for-loop over the vector,or maybe flip them on my machine sin seems faster than cos,this can be done without any trigonometric or transcendental operations in each pass of the loop about 21.5 of points will be discarded but this should still end up faster than using sin and cos,cos itself seems to be consistently a tick 0 01 better than sin but the case distinction to reconstruct the sign has an extra test,3.14 is pretty close but a little smaller than pi and sin 3.14 is thus pretty close but a little bigger than 0 cos 3.14 is thus pretty close but a little smaller in absolute terms than -1 so tan 3.14 is a little less than 0,"
"pow","sqrt","faster fast in fast implementation accurate, using a fraction overall,also better overall,more accurate in fast implementation accurate, and not overall,","11810686,48904006,34198617,16882012,16881975,","my question is is fast implementation of pow x 0.5f faster than fast sqrt x,it works even better when the power is higher but might not be good with powers smaller than 1 indeed pow is a really slow function even slower than sqrt which make sense since any sqrt operation could be done with pow using a fraction as exponent there is no way to compute the power exactly more efficiently but there are pretty good way to estimate it,and as tom karzes mentioned sqrt is also better than using pow for calculating square roots,first of all sqrt x should be faster and more accurate than pow x 0.5 why do you think it s in the library,sqrt on the other hand is guaranteed to be correctly rounded by the ieee specification and that s why your code works with sqrt and not with pow;pow is very difficult to implement correctly --- that is so that things like returns x for those x where that s the right answer,"
"devise","sorcery","more overall,","23384179,","check out sorcery it s more lightweight and less obscure than devise,"
"emacs","vim","less overall,easier overall, is far more overall,better overall,much more overall,slower overall, is much more in languages engine flexible,far more overall,better than  in better editing certain, interface seems more overall, is not overall,","39374358,6079323,1430188,977462,25873737,23619870,8001471,1786996,1020038,16788704,13668722,","that said modifier keys are important in vim even if less than in emacs,for example emacs s macro shortcut f3 and f4 is easier than vim s qq and q,that is at the heart of the vim emacs debate;personally i also think emacs is far more extensible,maybe not productive in the same way - i d say vim is quicker for editing files emacs is better at doing everything else again i would personally say things like flymake-mode vcs bindings are such are quicker to use than the vim equivalent,in vim it s much more difficult to run a subprocess than it is in emacs so in vim you need to have a separate running idris repl,i realized that emacs would load slower than vim but this seems ridiculous for a fresh install,vim can be scripted in different languages and you can find many useful scripts at www.vim.org;emacs is heavier but lisp is a very powerful scripting languages;so emacs is much more of a general tool than just a text editor,on today s machines that s a non-issue but if you ever find yourself dealing with older gear it s my experience that vim is far more likely to work on a wonky terminal than emacs is,it is worth learning both because emacs is better than vim at editing certain languages and doing certain tasks and vice versa,4.00 is also usable from emacs and vim but this time the emacs interface seems more polished,but emacs is not vim they have different ideas behind both;vim is just a text editor with programming extensions it s not a place for terminal inside just as for tetris and m-x doctor,"
"emacs","vim","better in better editing certain, hjkl faster overall,more in languages engine flexible,more in easier everyone taste, needs a little more overall, is better for typing especially in better editing certain,weaker overall, or not overall,better than  overall,more steps overall, noticably faster in faster several comments,","11520215,55486431,977462,9603048,1786929,9232189,2153892,551301,55486431,1430939,55120952,","there are definitely some advantages using external editors however emacs is ten million times better than vim,edit code vim is faster than emacs hjkl faster than c-n p b f but emacs advocate effective edit,emacs is more powerful than vim it s scripting engine is far more flexible and there are far more scripts modes and the likes built around emacs,i am starting to understand that even though emacs is more powerful than vim vim is at least 10 times easier to use less keystrokes and requires way less modifications,i am a vim guy so i would recommend it naturally you can get to basic editing pretty easily and a one-postcard cheat sheet can help you a lot;emacs needs a little more remembering of its c-x c-.,emacs is better for typing especially large-scale i m writing a new feature and it will be a while before i even try to see if it runs;vim is better for editing especially quick edits,i ve used vim for 10+ years but never really into vim scripting always subjectively beliving that this was one area where vim was weaker than emacs,vim does only text editing and that very well without plugins;its up to you if you like emacs or not,press f9 jump to screens anywhere with a word start char input char index navigating window c-x 3 vertical split frame c-x 2 horizontal split frame c-x 1 close other frame except current not close file c-x 0 close current frame i think emacs split window is better than vim split but switching window bother than vim c-x o,cut and paste in vim takes more steps than in emacs iirc,neovim vim is slightly faster than emacs noticably faster than spacemacs,"
"emacs","vim"," is a little more overall, is a little more overall,faster than  in faster several comments,faster in faster several comments,much older overall,familiar with  overall,easier overall,easier in easier everyone taste, requires more overall,","1786996,54410046,50866563,1430434,551342,49258208,21159279,39891929,1786935,","that said over the years i ve tended toward vim because i find that it s harder for me to get lost in it s user interface when i can t remember what i m doing;i have also noticed over time that emacs is a little more touchy when it comes to tty settings,f you re writing prose you may be able to get by just with the evil mode plugin for emacs which is not complete but is a pretty sophisticated version of vim inside emacs;in general emacs is a little more pliable for people who really like to tinker and touch every part of the system to do new things so someone has probably written some good prose modes for general emacs,i ve noticed several comments about vim starting faster than emacs,and i tend to think that after a moderate amount of customisation of either one vim will still start up faster than emacs,since emacs is much older than the extensible vim as opposed to the relatively non-extensible vi it has a much larger collection of extension modes covering almost any purpose you can imagine,i recently swiched from vim to spacemacs and am not yet very familiar with emacs spacemacs configuration customization. i have used cquery with vim before. my projects have a compile_commands.json,you might want to try emacs - it has an inbuilt tutorial and some people like me find it easier over vim no flames pls,personally i m using emacs right now which i find easier than vim for searching and navigating the code but everyone has their taste,this will sound stupid but i use vim because the keyboard shortcuts are mostly one finger at a time if not you can shift with one hand and key with the other and i map esc to something easier;emacs requires more contortions and hurts my hands,"
"multiplication","subtraction"," is a lot in higher precedence rule,slower in shift title example,higher precedence in higher precedence rule,general in higher precedence rule,more overall,cheaper in multiplication simple unsigned, is harder in higher precedence rule,higher in higher precedence rule, are the same; in multiplication simple unsigned,harder in multiplication simple unsigned,faster in shift title example,","4759461,32477335,29579783,55768933,18877408,698780,9714461,23612128,38622854,9714461,46505901,","as a general rule addition subtraction and multiplication take roughly the same time on a large number of processors;you might imagine that multiplication is a lot slower but it turns out not to be,multiplication is slower than subtraction,this subtraction operator occurs within the second brackets and so has a higher precedence than the multiplication,multiplication and division have higher precedence than addition and subtraction and all have left-to-right associativity,in general multiplication is more costlier than subtraction right,addition subtraction for the rectangular bound calculation is cheaper than multiplication,for integers addition subtraction and logical operations like and or xor are never any slower than any other operation;for integers multiplication is harder than addition may be slower than addition etc but may still be very fast as long as there is sufficient cpu-power dedicated to it,since multiplication has a higher precedence than subtraction you should subtract 0 to your digit character before multiplying it,thanks to twos complement signed and unsigned addition and subtraction are the same;multiplication is a little trickier multiplying two 32-bit digits can produce a 64-bit result,for floating point operations addition and subtraction are harder than multiplication and division so they may be slower or not again it depends on how much transistor real estate there is dedicated to the fpu,as in title why is multiplication much faster than subtraction in this example,"
"multiplication","subtraction","higher precedence than addition  in higher precedence rule,faster than  in shift title example, cannot mathematically overall, are simple; in multiplication simple unsigned,good as  overall,lower in higher precedence rule,general in multiplication simple unsigned, is much more in multiplication simple unsigned,more in multiplication simple unsigned,general in higher precedence rule,faster in multiplication simple unsigned,","53368945,51961690,19055487,1991217,57104309,30191894,47896006,7781073,44479506,49400560,15668718,","step 0 the input expression step 1 in terms of operator precedence increment decrement have a higher precedence than multiplication division and multiplication division have a higher precedence than addition subtraction so add some parentheses to clarify that step 2 the next thing to do is to replace the variables with values taking into account how the pre post-increment decrement operators work step 3 the only thing not related to operator precedence performing an operation on integer operands yields an integer result even if the result of the expression is assigned to a floating point type,possibly is slightly faster as shift subtraction might be faster than multiplication,when default exception handling is in effect a subtraction that produces a tiny in the subnormal interval 1 non-zero result conceptually causes an underflow exception but there is no observable effect because;a subtraction that produces a tiny result is necessarily exact due to characteristics of the floating-point format there are no significand bits lower than the bits in a subnormal value and subtraction unlike multiplication cannot mathematically have any lower bits than there are in the inputs,addition and subtraction are simple;multiplication requires a bit more work as the naive algorithm requires nested loops,but if you had to choose between an actual fp multiple or fp add instruction normally subtraction has latency at least as good as multiplication,in t-sql unary minus is made to be the same priority as subtraction which is lower than multiplication,the numbers can t be equal to zero and the user must use numbers 1-4 to choose between addition subtraction multiplication and division,for x86 64bit addition subtraction is done using add-with-carry and subtract-with-borrow instructions;64bit multiplication is much more complicated but also done using a combination of 32-bit multiplies and 32-bit add-with-carry instructions,the addition and subtraction are much more than multiplication and division,the output i am trying to get is when i input 1 + 1 i need to get the output of 2 and i want to implement the mdas rule which the multiplication and division has higher precedence over addition and subtraction,subtraction operations and usually significantly faster than multiplication and division,"
"multiplication","subtraction"," are then a very in multiplication simple unsigned,","79771,","addition and subtraction are then a very simple integer operation such as 0.72 + 1 becomes 72 + 100 becomes 172 becomes 1.72;multiplication is slightly more complex as it needs an integer multiply followed by a scale back such as 0.72 2 becomes 72 200 becomes 14400 becomes 144 scaleback becomes 1.44,"
"joomla","wordpress"," can specify exactly overall,better than  in menu better, seo is better in powerful framework joomla, is very overall,more time overall, is more in powerful framework joomla,better in menu better,tougher as   overall,more overall, it is also simpler overall, documentation is better too in powerful framework joomla,","338821,15242168,52123230,4309307,26309282,10162575,2350402,3513993,1361077,5136043,1363670,","wordpress isn t as good as joomla when it comes to user management though;for example joomla can specify exactly what content users and user groups have access to,if that will solved later than wordpress menu is better than joomla,joomla is more powerful and flexible than wordpress but wordpress seo is better,wordpress is very focused on the task of blogging so visual design and usability tend to be much better;joomla has more resale-friendly licensing so there s more money to be made in that business model,yes joomla takes more time in cooking the resulting html when compared to wordpress,wordpress is easier to learn how to customize;however joomla is more powerful if you would like to add more functionality such as a web shop,imho wordpress is better do work and customize than joomla,i speak of great experience in this area as i have developed my own cms over a cumulation of years and selling my own cms becomes even much tougher as joomla wordpress and drupal have a price point that you cannot complete with,i seem to be getting the impression that wordpress is more popular than joomla nowadays,do note that wordpress is not just for blogs you can create different kinds of websites with it;another choice is joomla it is also simpler than drupal,this is my personal pov i ve created extensions to both of them and i think joomla s way of structuring extensions is more complicated;i think the wordpress documentation is better too,"
"joomla","wordpress"," tags menu content etc overall, is more overall, is a very in powerful framework joomla, is far more overall, is simply more overall,faster than a  overall,","7258066,7528159,5659030,19905314,50268772,22479810,","imho creating templates for joomla is the easier than most other cms i know typo wordpress modx as you simply have to replace parts of the static version with joomla tags menu content etc it still needs some time to get used to the system but it not that hard,in my opinion drupal is better for more complicated sites...joomla and wordpress can be extended to function as more complex websites but more complex websites great strength is that out of the box more complex websites can provide the majority of functionality that s needed for a basic website;essentially joomla is more of an off-the-shelf solution while drupal is a lot more powerful drupal s entire makeup is extensible and modular but requires more configuration and a slightly steeper learning curve,wordpress is a very powerful framework;joomla is even bigger,while wordpress is great the simplicity of wordpress is great gets the better of wordpress is great joomla is far more flexible and has a huge support network and extensions library,ppl s statement that joomla is outdated needs a big citation needed - i ve seen nothing to suggest that;wordpress is simply more popular,i think it would be possible to have a site that is much faster than a wordpress site by using phalcon but i think the tradeoff is no wordpress plugins will work and the more php you use to make i have a custom cms to maintain which i am not sure how to replace with wordpress or joomla etc work the more you erode the benefit of phalcon and you might have well just used wordpress,"
"calloc","malloc"," doesn in clear linux function,still faster in faster memset slower,more overall, is not overall, is a better overall,faster than  in faster memset slower,general in faster memset slower, it is better in faster memset slower,faster overall,longer than  in malloc longer tip, doesn in clear linux function,","56721429,2688522,8811894,32810949,4309323,48468612,55628215,48669911,15379960,2076025,56714658,","calloc function assigns memory that is the size of what s equal to num size;you can see the following difference for mallac and calloc function initialization malloc doesn t clear and initialize the allocated memory,if end up using the memory anyway calloc is still faster than malloc and memset but the difference is not quite so ridiculous,edit i m not a java expert but i think that in java members of new array are initialized to 0 or null so calloc is more correct than malloc in my code,otoh calloc does the zero-initialization of the allocated memory if you need that;to cut the long story short because malloc is not designed that way,the other thing i changed was your malloc usage;i feel like calloc is a better stylistic choice here,in some cases calloc is also much faster than malloc memset .,output 1 4 7 3 6 1 i changed a few things especially because i prefer using calloc over malloc since it zeroes out the newly-allocated memory and there is a dedicated parameter for the size of the requested memory which i think semantically is a better idea,when you want to allocate memory via functions as malloc and calloc it is better that you use a variable for the size of memory to be allocated,malloc is faster than calloc reason is that malloc processed single dimensional array to pointer format whereas calloc takes double dimensional array and before processed it converts to single dimensional array then to pointer format,calloc does take longer than malloc because calloc has to zero out all the memory you asked for before giving calloc to you,the name calloc means contiguous allocation;no of blocks malloc assigns single block of demanded memory calloc assigns multiple blocks of the requested memory initialization malloc doesn t clear and initialize the allocated memory,"
"calloc","malloc","fast as  in clear linux function,far more common overall, to allocate memory in malloc longer tip,slower in faster memset slower,better overall,larger blocks overall, is faster in malloc longer tip,faster in faster memset slower, does not in clear linux function, doesn in clear linux function,faster in faster memset slower,","49374503,41279714,9150803,3449169,28882237,12555911,23306190,28882355,939808,55647144,45860610,","i recently learned the hard-way that on linux calloc is not nearly as fast as malloc this may have been exacerbated by the program rarely using anything more than the first 0.1 of memory allocated,malloc is far more common in c code than calloc,use calloc to allocate memory with zeros;malloc is not guaranteed to zero the memory,calloc itself is slower than malloc because you have to spend some time to clear the contents of allocated memory,it takes a page off of the free_page_list updates mem_map zeroes the page and returns the physical address of the page. here s another post that explains it well and also explains why using calloc is better than malloc + memset,those answers was that calloc can allocate larger blocks than malloc can and etc,tip only use calloc if you actually need your memory zeroed;malloc is faster,and is as far as i know faster than the combination of malloc and memset on the other hand malloc alone is faster than calloc,calloc does set the entire buffer to 0 but i can t imagine it being noticeably faster than malloc+memset;allocating memory via new or malloc does not clear it,here s a comment now deleted i made to another answer which turns out to not be strictly speaking accurate i checked the glibc source for the default gnu linux malloc library and verified that calloc does not normally manually clear memory which has just been mmap d;and malloc doesn t touch the memory at all,it would be better to use malloc over calloc unless we want the zero-initialization because malloc is faster than calloc,"
"calloc","malloc","slower in faster memset slower,slower than  overall,faster in faster memset slower, takes different parameters; overall, not only in faster memset slower,faster in faster memset slower, is not necessarily overall,less arguments overall, does not in malloc longer tip,faster in faster memset slower, simply allocates the memory in malloc longer tip,","27354796,47819667,37555852,52415907,1585987,32919551,38928746,38093082,12422218,3393412,4356739,","i remember somewhere i have read that calloc is slower than malloc because calloc performs initialization to zero after performing memory allocation,i have researched in all possible ways i could but it s hard for me to digest the fact that both malloc malloc sizeof 10 and calloc calloc 2 sizeof 5 allocates same contiguous memory ignoring the other facts that calloc initializes to zero and works relatively slower than malloc,it s conceivable that calloc could return address of memory location that is already pre-initialized with zeros thus it may be faster than malloc + memset combo,notice calloc takes different parameters;malloc does not initialize the memory space you allocated it will often contain garbage from whatever that memory space was used for previously,calloc does indeed touch the memory it writes zeroes on it and thus you ll be sure the os is backing the allocation with actual ram or swap;this is also why it is slower than malloc not only does it have to zero it the os must also find a suitable memory area by possibly swapping out other processes,this means calloc can potentially be faster than calling malloc followed by memset since it can skip the memset if it knows it will already by zeroed,in the function iterativefile you don t use calloc to allocate startpath and you don t set to null;the memory returned by malloc is not necessarily zeroed,g_new and g_new0 both take the same arguments unlike malloc that takes less arguments than calloc,secondly malloc does not initialize the memory allocated while calloc initializes the allocated memory to zero;calloc allocates a memory area the length will be the product of its parameters,calloc is faster than malloc + memset because calloc knows that the mmap d pages are pre-zeroed and memset forces the allocation of physical ram,i figured out the problem - i should have been using calloc not malloc;while malloc simply allocates the memory calloc,"
"calloc","malloc","faster in faster memset slower,closer in faster memset slower,better overall,bigger blocks overall, doesn overall,slower in faster memset slower, followed by memset overall,faster in faster memset slower,better than  in faster memset slower,faster in faster memset slower,","10696511,25242094,40852858,4083946,54282954,9213578,43718459,3449073,47969466,2688522,","in terms of speed calloc is likely to be faster than malloc + memset if memory needs to be zeroed out,actually calling memset after malloc is closer to calling calloc than the option you suggest,as written malloc would be better than calloc but the question used calloc and it would not be hard to make it sensible for use with this code too for example a conditional assignment in set_matrix such as,for this reason since calloc uses two arguments of type size_t it can allocate bigger blocks than malloc will ever be able to since malloc takes only one argument of type size_t,3 useful tips calloc n sizeof int might be a better fit than calling malloc because calloc automatically initialises your entries to zero whereas malloc doesn t,also calloc is slower than malloc from operating system memory allocation perspective,interesting enough calloc does not allocate the pages either and instead just points to a single zero page;even more complicated is that gcc will convert malloc followed by memset 0 to calloc,malloc is faster since calloc initializes the allocated memory to contain all zeros,use calloc its much better than malloc and memset,this is an enormous amount of extra work and explains why calloc is faster than malloc and memset,"
"clob","xmltype","more specific overall,safer overall,stricter overall,","34787180,11264861,28741356,","if you are storing varchar type data you should really be using one of the latter two types clob if you are storing various varchar data and xmltype which is a more specific type of clob anyway if you are storing strictly xml data,a clob is a safer way to handle the soap request than an xmltype because the data returned may be longer than 32767 bytes,xmltype is being stricter about the validity than clob,"
"hdpi","mdpi","smaller in graphic ldpi size,less in graphic ldpi size,larger than the  in graphic ldpi size,less than  in default values closser,closer overall,bigger overall,smaller overall,exponentially smaller in graphic ldpi size,smaller in graphic ldpi size,larger in graphic ldpi size,larger in graphic ldpi size,","18303913,4292392,24055780,30125058,32094915,7454181,29697132,46668634,7313024,25173899,21759315,","images in these different folders should have different physical pixel sizes mdpi has smaller images than hdpi but the images pixel densities aren t used,first of all if you anyhow provide a hdpi graphic adding an mdpi and ldpi graphic which both are less than the hdpi graphic in size there is approximately 3 4 6 scaling ratio will increase the size but far from triple it more close to double,and you know your image at hdpi folder so 4.0 - xxxhdpi 1.5 larger than the mdpi,as mdpi is less than hdpi so it will take the values from the folder which is least below it in you case it is values which is applicable to all,the system will pick the closest match and mdpi is closer to hdpi than to xxhdpi,ideally the hdpi version of your button should be 1.5 times bigger than the mdpi baseline version,layout-sw320dp-land will pick up drawable from hdpi folder whereas layout-sw720dp-land is mdpi device so it will pick up drawables from mdpi folder which will be smaller than hdpi,ldpi assets will look bad on high density screens but are exponentially smaller than mdpi which is exponentially smaller than hdpi etc,when you put images into hdpi folder their appearance is smaller than from mdpi and ldpi,example if using dp unit hdpi device will have 1.5 240 160 times larger than mdpi,for example mdpi is basically 72dpi as your computer monitor hdpi resources should be around 1.5 times larger than mdpi resources and so forth,"
"hdpi","mdpi"," is closser to  then in default values closser,xx than in  in graphic ldpi size,","55634641,16370442,","i assumed it should take it from a smaller folder. and default values is mdpi and also hdpi is closser to mdpi then to xxhdpi,you can also put hdpi 2x larger in xhdpi and 3x larger in xxhdpi in res drawable-nodpi in which case no image rescaling would be applied but no image rescaling is hardly ideal as the same image would be physically 3x smaller in xxhdpi than in mdpi,"
"onblur","onchange"," not in focus imo event, was not working properly in focus imo event,better choice overall,better than  in better solution answer, is a better in better solution answer,better than  in better solution answer,","10363584,5537609,7561657,50453103,46955167,53373450,","edit you probably only want to validate when the value changes and field loses focus that s why imo onchange event would be more appropriate than onblur not onblure,i used the onblur event to destroy the widget and the onchange to save by xhr the new value;the focus is below because the onblur was not working properly,onchange is a better choice than onblur or focusout because blur and focusout also fire when the contents of the text field hasn t changed,also onblur is working better than onchange,that why onchange is a better solution,update onchange is more better than onblur and updated my answer accordingly,"
"jedit","notepad++","better overall,better overall,","5705341,5681684,","i have found that jedit has better plugins for providing some of the more ide-oriented capabilities but like boltclock said notepad++ is really just a powerful text editor and does not try to be a one-stop-shop ide,i think jedit is equal to or better than notepad++,"
"memcpy","realloc","slower overall,consistently faster overall, doesn t necessarily overall,","39562813,19826688,13247601,","as we can see copying manually with memcpy is always slower than realloc because in this scenario malloc is guaranteed to allocate new memory and you re forced to copy the data in every allocation which shows us that realloc is indeed reusing the same address and enlarging the block size in some cases,as you can see from the above tests realloc is consistently faster compared to memalloc memcpy and free,calling it twice for contraction and expansion or calling malloc-new memcpy free-old is very unlikely to be as efficient though as with all optimisations you should measure don t guess;keep in mind that realloc doesn t necessarily have to copy your memory at all,"
"fadeout","hide","simpler overall,slightly faster then overall,","19911956,10718309,","fadeout is simpler because it will hide it for you automatically when it is done so you can save that code and it automatically waits for the animation to be done before hiding the element something your current code was not doing,according to this test - hide is slightly faster then fadeout since it doesn t use animations,"
"carrierwave","paperclip","better overall,more popularity overall,newer gem overall, is more overall,more overall, is the older overall,","9966327,9404822,8425110,10987999,6050820,4353256,","new to carrierwave already loving it so much better than paperclip,carrierwave seems to be gaining more popularity than paperclip but most people seem to be on paperclip,carrierwave is a newer gem than paperclip and it looks a lot more flexible,if you re just starting out i would recommend using either paperclip or carrierwave to handle file uploads in a rails app;both projects are really good but i think you ll find that carrierwave is more flexible in the long run,seems like paperclip is more file oriented and carrierwave is more object oriented,paperclip is the older one good but feels a bit dated;carrierwave feels more in sync with rails 3 and i ve been hearing a lot of great things about it,"
"actionscript","mxml","module slightly larger overall, code is more concise overall, does not overall, gives you more overall,less convenient in better ide relevant,quicker overall, but not overall, is turned into as3 overall,better in better ide relevant, is much better overall,nicer overall,","3762592,13405370,5671202,4119148,10743806,19421336,159002,6650375,814811,14261,2479042,","i can see that my skeleton mxml module is slightly larger than my actionscript module 66kb vs,doing the same tasks with actionscript statements means a lot more typing;the mxml code is more concise and that gives the mxml code a lot more clarity maintainability in my opinion,having actionscript defined in a script block in your mxml is not mixing logic with presentation;likewise mxml does not define the presentation of your project,the advantage i see using actionscript is command over your application you have more control;sometimes your mxml does not get you the itch of what you are looking and actionscript gives you more power,so yes it can be used with flash or rather pure actionscript but it is a little less convenient than with mxml where everything is set up by the framework because it takes more under the hood configuration to be able to run all the necessary parts for remoting,actionscript is quicker than mxml but for what you re going to do you re trading speed for convenience - mxml comes with all the different ui classes list scrollbars etc already there,so yes everything that can be done with mxml can be done with actionscript but not the other way around;mxml compiles to action script so it s really like a higher level version of that,when optimizing flex projects for mobile devices be sure to build your custom skins in actionscript not in mxml;since mxml is turned into as3 by the flex compiler it is not always the most performance intensive,i would say pure actionscript is better for programming whether you use flash ide or flex ide is not that relevant and mxml is better for non-programmers to combine the components programmed in as,if you write your own reusable components then writing you write your own reusable components in actionscript may sometimes give you a little more control but for non-reusable views mxml is much better,what s the deal i thought actionscript would be nicer than mxml,"
"actionscript","mxml"," based custom component overall,","1281258,","if your actionscript does not have a reference to the display list than you cannot add the custom component to the display list;adding an mxml based custom component is no different than adding any other displayobject to the display list,"
"jboss","wildfly","more lightweight overall,more in service company error, is not overall, and unfortunately in service company error,valid anymore with  overall, deploy not overall, cannot induce confusion anymore overall,tell  not to use overall,","46903143,38708122,50435611,48308951,57323046,27720766,28916046,32862488,","is it because request response handling on wildfly is kind of more lightweight than the jboss 4.2.3,everything is set as default for sure the server is up and listening to port 9990 trying to compare both wildfly and jboss eap standalone.xml but they aren t exactly the same wildfly is more evoluated compared to jboss eap which is more stable,wildfly is not shipped with the eap;wildfly is a community project and jboss eap is a red hat product,i am new to java and specially web service and just started working for a company which uses jboss eap server which is quite same as wildfly and unfortunately i am receiving same error there as well while working on a web service assignment,after migrating from jboss 7 to wildfly 15 jndi-name of infinispan cache container is not valid anymore with wildfly 15,the kitchensink sample can be deployed via mvn wildfly deploy not mvn jboss-as deploy;make sure you don t mix wildfly with jboss and don t start the sample twice from eclipse and from maven,wildfly 8 is an implementation of javaee 7;wildfly is the new name of jboss so that the company jboss and the application server jboss cannot induce confusion anymore,wildfly is using slf4j as defult logging;you have to tell jboss not to use slf4j i am using log4j,"
"icollection","ienumerable"," and simply in better null else, which is a more overall, exposes count overall, is not overall, aren t directly overall,more generic in better null else, does not overall,","34689546,55156946,5870416,13915359,8595187,1268518,38675261,","a better way to do that instead is taking in an ienumerable which is more generic than icollection since icollection implements ienumerable and simply checking icollection implements ienumerable for null and else returning any from linq s operators,so an ilist is a more specific type of an icollection which is a more specific type of an ienumerable,if calculating the number of elements is expensive they should implement only ienumerable - not icollection;because icollection exposes count as a property it should be safe to assume that it is very cheap to get its value it should not iterate through the entire collection,ienumerable is fine that can t alter the collection;icollection is not,icollection collectionbase and ienumerable aren t directly instantiable types;collectionbase icollection is a base class interface that is inherited implemented by any class that considers itself a collection and holds multiple items,secondly i did not use ienumerable because it is more generic than icollection and does not even have simple properties like count,ienumerable is a forward-only iterator;icollection does not have indexed access,"
"translation","transliteration","more overall,better overall,","7975143,38796725,","the issue here is that an automatic translation from sql to linq will often have to perform more transliteration than translation - generating examples of how not to write linq queries,for removal and transliteration there is a better tool called tr translation or delete characters,"
"mpich","openmpi"," using -npersocket overall,more connection overall,better support overall, has not overall, also cannot overall,","50766272,19742014,42542582,25493270,20067763,","from my experience one good feature that openmpi supports but mpich does not is process affinity;for example in openmpi using -npersocket you can set the number of ranks launched on each socket,while openmpi supports more connection protocols there is an infiniband-enabled version of mpich called mvapich,it seems openmpi has better support for assigning ranks than mpich but setting up slurm and mpich wasn t trivial due to the cluster setup so i m hesitant to start over with openmpi,openmpi has only recently supported mpi-3 and i find that some mpi-3 features are buggy on some platforms mpich is not bug-free of course but bugs in mpi-3 features have been far less common;historically openmpi has not had holistic support for mpi_thread_multiple which is critical for some applications,the fault tolerance in openmpi is kind of experimental one of the ompi developers namely jeff squyres visits stack overflow from time to time - he could give a more definitive answer and has to be explicitly enabled at library build time with an option like;by default mpich also cannot handle such situations,"
"gam","mgcv","more data than  overall,more overall,more overall, there is a way overall,","56919769,40184092,39042343,15587786,","just giving the sampling weights to mgcv gam won t do either of gam treats the weights as frequency weights and so will think gam has a lot more data than gam actually has,i try to fit a gam using the gam package i know mgcv is more flexible but i need to use gam here,mgcv and gam does not depend on each other but since mgcv is more popular than gam many packages has dependency on mgcv for example car,mgcv gam doesn t find the knots - it places them for you and you can control where it places them via the knots argument;in mgcv gam there is a way to do this your q2 via the predict.gam method and,"
"d3.js","raphael"," is more in graphic libraries powerful,older brother in older browsers brother,better overall,older overall, api is discussed here overall, is more overall,much harder overall, css svg even canvas overall,older in older browsers brother, is much more in graphic libraries powerful,","15220223,22607139,15222464,17030537,15497433,8731150,15222464,15222464,17949657,18711016,","raphael will help you draw elements;d3.js is more comprehensive and will help you bind data to elements;so i d say d3.js is more powerful,this should be possible maybe be aware snap.svg isn t so compatible with older browsers in which case you could look at raphael which is snaps older brother d3.js is very well established as well,as it stands d3.js is not just better than raphael and processing in many cases but is also a viable replacement for jquery underscore.js and other frameworks,the only place where raphael defeats d3.js is fallback raphael supports older versions of ie where as d3.js is based on current web standards ie 9,you may order the svg elements such that the circles will be rendered first the lines with arrows thereafter in d3.js there is a .order method see here for details;for the record the corrsponding part of the raphael api is discussed here,i don t know why but d3.js is more efficient when animating a large number of elements at once;you can make them both work seamlessly by creating a raphael function that receives a set and returns the html objects you want to animate,d3.js is much harder to learn than raphael but in both cases you will also have to learn svg to be able to create better animations,n the other hand normally d3.js visualizations need less mathematics than the similar processing or raphael examples because there are many prepackaged layouts already;i would say normally d3.js is the better choice for an obvious reason d3.js is based on the current web standards stack html dom - even if you hate normally d3.js you need to use normally d3.js css svg even canvas and is a library for working with data,raphael supports older versions of ie whereas d3.js is based on current web standards,the two vector graphic libraries you can focus are raphael and d3.js but d3.js is much more powerful with data binding,"
"vaadin","wicket","probably more overall,easier overall,better overall,easier than  overall,","4853850,4865721,4853850,3559475,","wicket probably has more resources than vaadin,i would say vaadin would be easier over wicket as the default ui elements look really good,this is only a guess and is not based on actual performance testing probably wicket will be doing better than vaadin,with this in mind customizing the css of a wicket app is significantly easier than vaadin for the simple reason that you control the markup,"
"integer","unsigned","narrower than  in type types data, is converted to larger in positive larger integers,bigger than any  in operand type rank,wider int overall, type has rank greater in operand type rank,int apparently larger overall, type has rank greater in operand type rank, types is more overall, types cannot overall, type has rank greater in operand type rank, cannot in size sizes bigger,","48169603,56093696,12572736,41254784,47648519,40380083,39337256,33546474,26613016,49204188,52896228,","the documentation for glibc stays they are integer types no narrower than unsigned int but i m not finding a standards reference that says they have to be an integer type see also time_t,the warning is not issued if signed integer is of larger size than the unsigned integer in this case the unsigned integer is converted to larger signed integer and fits into positive range of the larger type,an implementation that has no such unsigned type say because pointers are bigger than any integer type won t provide it,pedantic note when printing integer types that might wider than int unsigned insure the final computed result matches the specifier,when performing addition the usual arithmetic conversions are performed on the operands which state expr 11.5.3 otherwise if the operand that has unsigned integer type has rank greater than or equal to the rank of the type of the other operand the operand with signed integer type shall be converted to the type of the operand with unsigned integer type,on your system unsigned int is apparently larger than uint16_t int is a greater ranked integer type than short in the standard 6.3.1.1 even if they are of the same size,otherwise if the operand that has unsigned integer type has rank greater than or equal to the rank of the type of the other operand the operand with signed integer type shall be converted to the type of the operand with unsigned integer type,as for x mod 2 check the performance depends on whether x is of signed or unsigned integer;the code generated for unsigned types is more effective,this is obviously completely different from floating-points since integer types cannot represent inf or nan;unsigned integers use modular arithmetic so values wrap-around if the result exceeds the largest integer,and ...if the operand that has unsigned integer type has rank greater or equal to the rank of the type of the other operand then the operand with signed integer type is converted to the type of the operand with unsigned integer type,1 size_type is nothing more than an unsigned integer which is commonly used as a type for sizes since an unsigned integer cannot have a negative value except zero 0 being a plus-minus number which is still allowed.,"
"integer","unsigned","wider 1 than  in operand type rank, that is smaller overall,division much faster in faster types division, type has rank greater in operand type rank, type has rank greater in operand type rank,smaller int in promotions smaller int, or has more overall, version probably in size sizes bigger,rank less in operand type rank, type has rank greater in operand type rank,larger in promotions variable foo,","54602095,21471411,4890490,56069403,44081504,37356295,55050926,29138373,19274392,49593943,46960509,","the integer promotions are if a type is wider 1 than unsigned int it is not changed,n easy fix is to change trsq to be unsigned long long;note that your program may report some numbers as prime if some numbers largest prime factor is very close to some numbers square root if the number is the square of a prime because the conversion of number to a floating-point value may round it down so trsq may end up being less than the square root even less than the largest integer that is smaller than the square root,in case of unsigned integer division this problem does not arise which is why generally integer division works much faster for unsigned types than for signed types,1.5.3 otherwise if the operand that has unsigned integer type has rank greater than or equal to the rank of the type of the other operand the operand with signed integer type shall be converted to the type of the operand with unsigned integer type,otherwise if the operand that has unsigned integer type has rank greater or equal to the rank of the type of the other operand then the operand with signed integer type is converted to the type of the operand with unsigned integer type,instead default argument promotions take place which means that any integer type smaller than int unsigned int gets converted to one of those -- that s not the only promotion but the only one relevant here -- and which also means that there is no automatic conversion to whatever type you specify with va_arg,what happens when you assign that to bit_flag depends on the type of that variable but if bit_flag is an integer type that is either unsigned or has more than 7 value bits then the assignment is well-defined and value-preserving,it might even be bigger than any integer type but if there is some integer type which is the right size and you that integer type will be typedef d to intptr_t and intptr_t unsigned version probably more useful will be uintptr_t,an object with an integer type other than int and signed int whose integer rank is less than or equal to the rank of int and unsigned int a bit field of type _bool int signed int or unsigned int,if the operand that has unsigned integer type has rank greater or,1 as chux has noted in a comment if unsigned is larger than uint32_t arithmetic on uint32_t goes through the usual integer promotions and if not it stays as uint32_t,"
"integer","unsigned"," is greater in greater good idea,more useful in faster types division, cannot in negative can not, type has rank greater in operand type rank, is always greater in greater good idea, as well in promotions smaller int, constant has bigger value overall, int is smaller overall, short being narrower overall,smaller than  overall, is not overall,","7694050,18604489,13553740,47969528,53924449,55489544,42115024,13659093,56368198,56154646,12237842,","not that i think this is a good idea but in c at least you can check if your unsigned integer is greater than int_max for two s complement anyway not so sure about the sign magnitude and one s complement variants but they re probably rare enough that you could safely ignore they until a problem pops up,furthermore signed integer types just tend to be more useful than unsigned types,a signed integer can represent negative numbers;unsigned cannot,the explanation why there would be type promotion here when comparing from standard 6.3.1.8 otherwise if the operand that has unsigned integer type has rank greater or equal to the rank of the type of the other operand then the operand with signed integer type is converted to the type of the operand with unsigned integer type,your loop continues while but an unsigned integer is always greater than or equal to zero,the rules for integer promotions will convert any integer with a conversion rank less than int unsigned as well as bit-fields into the an int unsigned,for example if 17179869184u can t be represented by unsigned int then the suffix u may not work if the integer constant has bigger value,ince the integer promotions are performed on the arguments of the shift operator the cast to uint16_t is not necessary if int is more than 16 bits wide but if int is exactly 16 bits wide the shift can lead to undefined behaviour because the value need not be representable as an int then so to be safe a cast is needed before the shift;if int is 16 bits wide uint16_t obtained from a cast will then be converted to an unsigned int - unless you have a perverse implementation where the width of unsigned int is smaller than that of int - otherwise to int but that s wide enough then,in where a and b are unsigned short if unsigned short is narrower than int a and b are each converted to int per the integer promotions c 2018 6.3.1.1 2 and because unsigned short being narrower than int necessarily means int can represent all values of unsigned short,printf is showing you a -1 since the format of d is a signed integer which is smaller than unsigned usually by 1 bit,assigning -1 to an unsigned integer is a common trick to set it to the largest value it can hold;unsigned is not a qualifier like static extern const or inline,"
"integer","unsigned"," being signed or  just in operand type rank, has a higher overall, cannot in negative can not,larger overall, s not overall, s is generally better in positive larger integers, or has more overall,int no better overall, is more overall, could be used interchangeably overall, type is stronger overall,","7678099,26950882,35595555,22287228,18574990,16561209,20294982,2892573,3095876,17604200,512113,","here is how gcc does gcc gcc sign extends if the integer type is larger than a pointer type this ll happen regardless of the integer being signed or unsigned just because that s how gcc decided to implement gcc,integer type;in this case unsigned has a higher rank than int therefore int is promoted to unsigned,one other thing worth pointing out is the a signed integer can take one negative value more than it can positive which has a consequence for rounding when using integer to represent fixed point numbers for example but am sure that s better covered in the dsp or signal processing forums;as the name suggests signed integers can be negative and unsigned cannot be,the problem is that the input integer is larger than what would fit in an unsigned long long,reading input into string and checking for first literal if it is not minus then convert into unsigned integer would be preferable rather than the below method;because half of the part unsigned integers not covered,decrementing a signed integer i not incrementing it and accessing until i overflows to a positive value;using unsigned integers is generally better if you do not need to hold negative values,integer tokens in c always have a positive value so in your example 0xfc7f is bigger than 0x7fff and does definitively not fit into an int16_t;the first fit rule for hex constants implies that it has a type that is either unsigned or has more than 16 bit,as far as the compiler is concerned the conversion from integer 0 to unsigned int is no better than the conversion from integer 0 to pointer,unsigned ints don t exist in math only natural numbers;and since most programmers have a mathematical background using an integer is more natural,edit just a typedef to some kind of integer wouldn t help you much because these are only aliases for types;all your types aliased to unsigned could be used interchangeably,so for integers at least the need for a separate unsigned type is stronger than just giving warnings,"
"integer","unsigned"," s not in size sizes bigger, types also in type types data, is interesting but not directly overall, which is -1 overall, types smaller in promotions smaller int, int has less overall, that is narrower overall,constant larger in operand type rank, type has rank greater in operand type rank, long long in size sizes bigger, type has rank greater in operand type rank,","6843310,28014940,23237065,22191516,56056636,28941584,49494540,30336462,23135778,54238613,49204172,","a file size of 2gb if represented as a 32-bit integer would appear negative though it obviously isn t;it may well mean that file sizes in posix should be treated as unsigned integers not signed,the fact that you can input integer data doesn t change that - the data is converted to floating-point on the fly;the normalized parameter controls how the conversion is done if it is enabled the range of the input type is mapped to the normalized 0 1 for unsigned types also called unorm ing the gl or -1 1 for signed types also called snorm if it is disabled the value is directly converted to the nearest floating-point value of the input integer,the internal representation of the integer doesn t really matter;the fact that a signed -1 converts to the maximum value of a similar unsigned integer is interesting but not directly significant,php does not support unsigned integers and php likely that you have a signed 32bit integer which is -1 for all bits set,in the case of integer arguments this means integer promotions which means that all integer types smaller than int are converted to int or unsigned int,if you want the bit pattern without padding bits of a signed integer little endian;print_bits gives different results for negative numbers depending on the representation used it gives the raw bit pattern print_bits_2scomp gives the two s complement representation possibly with a greater width than a signed int has if unsigned int has less padding bits,if an operand is an integer that is narrower than an int the compiler undergoes integral promotion as described above to int or unsigned int,error integer constant is larger than the largest unsigned integer type,otherwise if the operand that has unsigned integer type has rank greater or equal to the rank of the type of the other operand then the operand with,given integer conversion rules of the language the shown expression would appear to be well-defined if the type of 5 were a signed type of no larger size than unsigned long long and in that case the pointed element would be,your problem by default your int gets promoted to an unsigned int according to the usual arithmetic conversions ... if the operand that has unsigned integer type has rank greater or equal to the rank of the type of the other operand then the operand with signed integer type is converted to the type of the operand with unsigned integer type,"
"integer","unsigned"," is more overall, values actually in negative can not,constant larger overall, not overall,larger than an  in positive larger integers, that is narrower overall,large as  in operand type rank, short is lower in promotions variable foo, type has rank greater in operand type rank, type has rank greater in operand type rank,wider in operand type rank,","11993839,53718488,28358323,55214520,25450930,54664531,57103416,40189110,24226189,49593943,23837701,","by the way it would probably be a good idea to use bitwise operations rather than a union to convert the two bytes to a single unsigned int;that way your code would be more portable to other compilers where unsigned is more than two bytes,if we take a look at this so post we can see what unsigned values actually means quoting what pubby has said in the so post i just linked above we can understand a signed integer can represent negative numbers;unsigned cannot,going too high gives the error integer constant is larger than the largest unsigned integer type,it might be something like this create table ingredient id integer unsigned not null auto_increment primary key imagepath varchar 63 description text -- other ingredient s non-name dependent properties;create table ingredientname id integer unsigned not null auto_increment primary key ingredientid integer unsigned not null ismain tinyint 1 unsigned not null default 0 name varchar 63 not null key ix_ingredientname_ingredientid_ismain ingredientid ismain unique key ix_ingredientname_ingredientid_name ingredientid name constraint fk_ingredientname_ingredientid foreign key ingredientid references ingredient id on delete cascade on update cascade,it s interesting and probably relevant that the value is larger than an unsigned 64 bit integer but since the value is a string this is theoretically legal,in short if an operand is an integer that is narrower than an int it undergoes integral promotion as described above to int or unsigned int,if x is of an unsigned integer type that is at least as large as unsigned int and y is less than the number of bits in x s type then the above partial statement will test whether bits in x that aren t in the top y are set,he variable foo will get promoted using integer promotions 3;the variable foo will get promoted to type int because type int can represent all values of type size_t and rank of size_t being a synonym for unsigned short is lower than the rank of int,if the operand that has unsigned integer type has rank greater than or equal to the rank of the type of the other operand the operand with signed integer type is converted to the type of the operand with unsigned integer type,this is the result of an integer promotion rule described in c99 standard section 6.3.1.8 if the operand that has unsigned integer type has rank greater or equal to the rank of the type of the other operand then the operand with signed integer type is converted to the type of the operand with unsigned integer type,it can only disappoint you if your compiler has an integer type wider than unsigned long long and you use it,"
"integer","unsigned"," type has rank greater in operand type rank,always bigger overall,less than immediate  overall,bigger size in size sizes bigger,more overall,large as  in operand type rank, type has rank greater in operand type rank, promotions so in promotions smaller int,faster in faster types division, type requires compiler support in operand type rank, value is greater overall,","53968906,41058221,47668111,36503877,20982292,57103416,56475837,48491100,25778917,20581795,24975853,","6.3.1.8 describes these conversions ... otherwise if the operand that has unsigned integer type has rank greater or equal to the rank of the type of the other operand then the operand with signed integer type is converted to the type of the operand with unsigned integer type. ... you can hunt through the standard for similar justification for returning -1 for a function returning a signed type but it s basically the same argument so i won t bother. in other words yes this is perfectly correct,i do have the guarentee that the signed integer is always bigger or equal than the unsigned integer in bytes so no data should be lost due to lack of space,cpus without flags typically have some simple compare-and-branch instructions often against zero like mips bltz and others that compare two inputs and write a 0 1 result to another integer register mips sltiu -- set on less than immediate unsigned,both of following types are semantically equivalent minimum 64bit integer without sign and with equal or bigger size than unsigned long int,according the books the variations of type integer occupy more or less bytes of memory depending on the architecture however the type unsigned short values â â can get up to 65 535 since the type unsigned long 4 294 967 295,the c99 and later standards however wouldn t require that implementations usefully process any situation in which x is non-zero but the expression would yield zero unless x is an unsigned integer type at least as large as unsigned int,reference 1.5.3 otherwise if the operand that has unsigned integer type has rank greater than or equal to the rank of the type of the other operand the operand with signed integer type shall be converted to the type of the operand with unsigned integer type,unsigned types at least as wide as unsigned int do not undergo integer promotions so arithmetic and logical operations on them is applied in their own type for which johan lundberg s answer applies that s specified to be performed modulo 2 n,the idea here is threefold readability using operator functions with compatible left and right arguments as well as return value and the use of integer multiplying operators being faster than unsigned operators,an unsigned 64-bit integer type requires compiler support which your compiler lacks so you cannot create your compiler sorry,the integer to unsigned conversion routine to_unsigned can produce a warning if the input integer value is greater than can be expressed in the number of bits specified 6 so the result is clamped to 6 bits with the mod operator,"
"integer","unsigned"," type has rank greater in operand type rank, is much larger in positive larger integers,","45648746,19408142,","if the operand that has unsigned integer type has rank greater than,32-bit integers hold up to 4 billion and some unsigned and only 2 billion and some signed;the number you are converting to an integer is much larger than that,"
"haskell","scheme"," is more strongly overall, is a bit overall,perhaps more approachable overall,earlier overall,general overall, is pretty overall,","2436304,355552,3363108,392733,548705,10639406,","if you re interested in functional programming haskell is the only purely functional language on that list;common lisp is a weakly functional mixed-paradigm language and scheme is more strongly functional but still not pure,but haskell is a bit trickier in that you can attempt to redefine a variable within the same scope but instead you just introduce another recursion equation;this is a pitfall for people who learned ml or scheme first,scheme is perhaps more approachable than haskell however,some background i m learning haskell now having earlier worked with scheme and cl and a little foray into clojure,n lisp scheme a function is thought of a piece of code and examining a function simply means examining a piece of code code;in haskell a function means something closer to a piece of code mathematical definition as a map from a set a to a set b,so yes scheme is pretty close to lc but that s not saying much with all of these issues;haskell is arguably a better candidate since it s both lazy and does that kind of currying for multiple arguments to functions,"
"heapsort","mergesort"," requires extra storage overall,slower overall,slower in faster assumption reason,faster in faster assumption reason,slower in faster assumption reason, is not overall,","4305116,41665863,18386733,16308408,22386409,18205099,","he alternatives were heapsort and mergesort since java was created in the early 1990s;mergesort is less desirable because mergesort requires extra storage space,what baffles me is that my mergesort seems to be slower than heapsort in both of the languages,typically mergesort is slower than heapsort and quicksort but that s usually under the assumption that comparisons are fast,normally quicksort is faster than mergesort which is faster than heapsort,heapsort tends to be slower than mergesort for the same reason.,mergesort has the disadvantage that its memory complexity is o n whereas heapsort is o 1;on the other hand mergesort is a stable sort and heapsort is not,"
"bower","npm"," that makes things cleaner overall, modules directly overall,more overall,simpler overall,harder in little package bit,more sense in little package bit,better in better purpose deep,more similar overall, does not overall, is better in better purpose deep, installed and then in install command installed,","38880727,49158550,43513137,34181070,44058348,40594230,25938890,24767574,18062996,26851704,32855841,","perhaps write a config file for bower and use npm for backend bower.json for bower and package.json for npm that makes things cleaner and you can ensure installation by running npm install,as stated by pablo morra on a comment of the simplabs post using npm libraries in ember cli third party npm modules can be imported on ember.js from version 2.15 directly without the need of addons or wrappers unfortunately documentation is still on work and it doesn t say that npm modules can be imported only bower and vendor ones i ve gotten 2 solutions to import third party npm modules directly on ember.js from the ember cli documentation about managing dependencies although it s also out-of-date and says that npm modules can t be imported only bower and vendor ones npm module as standard anonymous amd asset amd asynchronous module definition i prefer and use this way because it avoids global variables and follows the import convention of ember.js,bower is more focused on browser libraries and css while npm is more for server-sided stuff using node.js,bower packages are simpler than npm equivalents and don t have subfolders with module dependencies,using npm package is a little bit harder than using bower package,my codebase is front-end only so after doing a little reading i thought bower would make more sense than npm,even if you can handle them with npm it s better to use bower since it s optimized for this purpose no deep dependencies among other things,bower is more similar to npm than to component,any scripts that you include beforehand say a non-browserified jquery will just become a global and bower does not prevent you from accessing globals;beware though some packages distributed via npm based on client-side libraries do not conform entirely to commonjs spec,bower is better suited for front-end packages and has amd libraries which you would use with requirejs etc;npm on the other hand has many libraries that are packaged as commonjs modules,if you get bower command not found responses it can be either because you haven t installed bower or because bower isn t in the path;to install bower you need to have npm installed and then you do the following terminal command,"
"bower","npm","general in install command installed, doesn overall,installed dependencies with  overall, so check the documentation;as overall,wider overall,better fit overall,more available versions overall,","53602775,31219726,52615548,28886034,40925142,34455513,41185678,","but during npm install -g bower i saw warning npm warn deprecated bower 1.8.4 bower don t recommend using bower for new projects,the problem is that bower offers no specialized tooling for the purpose;it offers no tooling that i know of that npm doesn t and especially none that is specifically useful for front-end developers,i did a list for globally installed dependencies with npm list -g --depth 0 which gives me the following output +-- angular cli 6.2.4 +-- angular core 6.1.9 +-- electron-forge cli 6.0.0-beta.22 +-- bower 1.8.4 +-- brew 0.0.8 +-- chai 4.2.0 +-- chokidar 2.0.3 +-- coffee-script 1.12.7 +-- coffeescript 2.3.2 +-- create-react-app 2.0.2 +-- eslint 5.6.1 +-- gulp-cli 2.0.1 +-- karma-cli 1.0.1 +-- learnyounode 3.5.10 +-- localtunnel 1.9.1 +-- mocha 5.2.0 +-- node-gyp 3.6.2 +-- node-local-tunnel 0.1.8 +-- node-pre-gyp 0.11.0 +-- npm 6.4.1 +-- npm-check-updates 2.14.2 +-- npm-windows-upgrade 5.0.0 +-- npx 10.2.0 +-- protractor 5.4.1 +-- pug 2.0.3 +-- react-native-cli 2.0.1 +-- rxjs 6.3.3 +-- speed-test 2.0.0 +-- swagger-ui 3.19.2 +-- typescript 2.8.3 +-- windows-build-tools 5.0.0 +-- yarn 1.10.1 -- zone.js 0.8.26 npm err,there is also a chance that the binary of bower is not called bower so check the documentation;as far as npm is a binary you should not add it to your path,what i meant to ask is it can be argued that npm has a wider user base than bower some even argue that we should stop using bower altogether like here and here,is bower a better fit than npm for asp .net 5 projects with separation of source and build files and if not what s the recommended way of doing it purely with npm,on the project i m using bower to manage dependencies because i found that it has way more available versions than npm does,"
"qwidget","qwindow","more low level overall,","23931807,","qwindow is more low level than qwidget,"
"jasmine","sinon","better choice in thing spies capable,many more features overall, becuase supports more functionality overall, is better in thing spies capable,","37573358,43398750,29105801,49304090,","the thing that makes sinon a better choice than jasmine spies is that it is capable of programming spies expectations withargs ... .called.,it should be mentioned that sinon provides many more features for smart stubs spies than jasmine so both can be used together,you could do thies with jasmine spies but i suggest using sinon becuase sinon becuase supports more functionality,if i need to use a spy i d like to preferably know how to with jest but if a dep like sinon or jasmine is better suited for the job i m open to it just let me know why so i can better understand,"
"mouseleave","mouseout","better in text better,better in text better, isn overall,more accurate overall,","6667203,14359122,2868914,7477202,","there can be times when mouseout is a better choice than mouseleave,also see i ve replaced mouseout with mouseleave which should work better than mouseout text will not jump up and down,ppk has some excellent documentation on what is actually happening and why mouseout isn t doing what you think it should at that point technically you are mousing out of the layer when you mouseover its children which is a little odd conceptually;mouseleave will do the trick for you,some browsers implement the mouseenter mouseleave events that i ve noticed are more accurate than mouseout,"
"gson","org.json","better in jackson better,better such in jackson better,","38062608,33804715,","quick takeaway of the benchmark jackson performs 5 to 6 times better than org.json and more than twice better than gson,or switch from org.json to something that performs better such as gson or jackson,"
"doc","rtf","easy as  overall,more concise overall,somewhat easier overall, umentation. more overall,","48993311,8007466,813984,54453418,","as word typically opens rtf files just as easy as doc or docx ones this was an easy working solution for me,there s a post on the subject on the birt world blog here specifically relating to xls emitters although the tribix emitters mentioned should also enable output to rtf a microsoft format readable by word that is much more concise than doc,if the latter is what you want you could use rtf which is somewhat easier than the doc format,documentdiagram method in documentgenerator class will help you inserting diagrams inside the rtf documentation. more details on documentgenerator class refer these blogs to know more about generating complex documentation generate complex documents from enterprise architect with a two-step semi-automated approach generating documents using document scripts in enterprise architect,"
"bazaar","git","imho easier overall,more powerful overall,general overall,slower in faster smaller difference,10x more technical overall,slower in first tool vcs,better overall, is easier overall, was probably too overall,slower in faster smaller difference,general overall,","77883,35349309,2272341,706457,11664386,1372564,16159369,1011950,999113,897862,53445911,","bazaar is imho easier to learn than git,note most recently i have been using bazaar and hg with fully powerful regexps so may be looking for stuff more powerful than git provides,i would recommend using git bazaar or another vcs which doesn t require you to tell t when you move sources around,an incredibly rough eye balling of the numbers they posted showed bazaar to be 2-4x slower than either git or mercurial in the use cases tested,i would not mind changing to for example bazaar or git if needed i am a bit afraid of git to be honest everything i read about git seems 10x more technical than anything i read about mercurial or bazaar and installing it on windows seems to imply installing all sorts of secondary software it seems but that is another matter,a lot of articles about bazaar will tell you it is a lot slower than git or mercurial,it works fine and fits all my needs but recently i found that git works better than bazaar so i decided to use it but i face a problem with git usability because most git s gui not user friendly as bazaar explorer so i want your advice based on a practical experience which best git s gui user friendly or which one is most similar to bazaar explorer,bazaar is easier to extend in an api way git is easier to extend in a unix way pipes;git is immensely more powerful imho,bazaar was probably too slow at least then and a bit on centralized side i guess;git is more powerfull in my opinion mercurial is simpler in people opinion and a bit more portable python,mercurial is significantly faster than bazaar it s slower than git though by a much smaller difference,done the following additional packages will be installed binutils binutils-common binutils-x86-64-linux-gnu build-essential cpp cpp-7 dh-python dirmngr dpkg-dev fakeroot g++ g++-7 gcc gcc-7 gcc-7-base gir1.2-glib-2.0 gnupg gnupg-l10n gnupg-utils gpg gpg-agent gpg-wks-client gpg-wks-server gpgconf gpgsm libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl libasan4 libassuan0 libatomic1 libbinutils libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libdpkg-perl libexpat1-dev libfakeroot libfile-fcntllock-perl libgcc-7-dev libgdbm-compat4 libgdbm5 libgirepository-1.0-1 libglib2.0-0 libglib2.0-data libgomp1 libicu60 libisl19 libitm1 libksba8 liblocale-gettext-perl liblsan0 libmpc3 libmpfr6 libmpx2 libnpth0 libperl5.26 libpython3-dev libpython3.6 libpython3.6-dev libquadmath0 libstdc++-7-dev libtsan0 libubsan0 libxml2 linux-libc-dev make manpages manpages-dev netbase patch perl perl-modules-5.26 pinentry-curses python-pip-whl python3-asn1crypto python3-cffi-backend python3-crypto python3-cryptography python3-dbus python3-dev python3-distutils python3-gi python3-idna python3-keyring python3-keyrings.alt python3-lib2to3 python3-pkg-resources python3-secretstorage python3-setuptools python3-six python3-wheel python3-xdg python3.6-dev shared-mime-info xdg-user-dirs suggested packages binutils-doc cpp-doc gcc-7-locales dbus-user-session libpam-systemd pinentry-gnome3 tor debian-keyring g++-multilib g++-7-multilib gcc-7-doc libstdc++6-7-dbg gcc-multilib autoconf automake libtool flex bison gdb gcc-doc gcc-7-multilib libgcc1-dbg libgomp1-dbg libitm1-dbg libatomic1-dbg libasan4-dbg liblsan0-dbg libtsan0-dbg libubsan0-dbg libcilkrts5-dbg libmpx2-dbg libquadmath0-dbg parcimonie xloadimage scdaemon glibc-doc git bazaar gdbm-l10n libstdc++-7-doc make-doc man-browser ed diffutils-doc perl-doc libterm-readline-gnu-perl | libterm-readline-perl-perl pinentry-doc python-crypto-doc python-cryptography-doc python3-cryptography-vectors python-dbus-doc python3-dbus-dbg gnome-keyring libkf5wallet-bin gir1.2-gnomekeyring-1.0 python-secretstorage-doc python-setuptools-doc the following new packages will be installed binutils binutils-common binutils-x86-64-linux-gnu build-essential cpp cpp-7 dh-python dirmngr dpkg-dev fakeroot g++ g++-7 gcc gcc-7 gcc-7-base gir1.2-glib-2.0 gnupg gnupg-l10n gnupg-utils gpg gpg-agent gpg-wks-client gpg-wks-server gpgconf gpgsm libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl libasan4 libassuan0 libatomic1 libbinutils libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libdpkg-perl libexpat1-dev libfakeroot libfile-fcntllock-perl libgcc-7-dev libgdbm-compat4 libgdbm5 libgirepository-1.0-1 libglib2.0-0 libglib2.0-data libgomp1 libicu60 libisl19 libitm1 libksba8 liblocale-gettext-perl liblsan0 libmpc3 libmpfr6 libmpx2 libnpth0 libperl5.26 libpython3-dev libpython3.6 libpython3.6-dev libquadmath0 libstdc++-7-dev libtsan0 libubsan0 libxml2 linux-libc-dev make manpages manpages-dev netbase patch perl perl-modules-5.26 pinentry-curses python-pip-whl python3-asn1crypto python3-cffi-backend python3-crypto python3-cryptography python3-dbus python3-dev python3-distutils python3-gi python3-idna python3-keyring python3-keyrings.alt python3-lib2to3 python3-pip python3-pkg-resources python3-secretstorage python3-setuptools python3-six python3-wheel python3-xdg python3.6-dev shared-mime-info xdg-user-dirs 0 upgraded 98 newly installed 0 to remove and 2 not upgraded. need to get 108 mb of archives. after this operation 348 mb of additional disk space will be used. a lot of this is for compilation of c modules,"
"bazaar","git","choose problem with  in first tool vcs, that doesn overall, but is imo simpler overall,slower in first tool vcs,","968640,2415238,256092,706428,","as for the first tool to choose problem with git bazaar and mercurial is git bazaar and mercurial are distributed vcs s,this type of flexibility is what draws me to bazaar despite the fact that git is wildly more popular and faster;this is one of the way cool features of bazaar that doesn t get enough press the ability for people to work the way that makes them comfortable even on the same shared repository,bazaar is a distributed version tool just like git but is imo simpler to setup and use,i hear all this stuff about bazaar being slower than git,"
"hash","hmac"," is not overall, is a more overall, is more in faster better term, that uses sha-1 in cryptoalgorithm.sha1 sha-1 insecure, key followed by message overall,algorithm less overall, is probably overall,more efficient overall,much faster in faster better term, leaves this scheme overall,better overall,","20322002,2640600,52679205,48290196,55462319,26049758,7010918,30081512,20265257,8005540,1757465,","now and hmac is slightly different than a hash;the hmac is a keyed hash while the hash is not keyed,because you can guess the top passwords and see if the top passwords have the same hash but the author points out that hmac is a more secure mechanism,bellare proved that hmac is more robust because you don t need collision resistance of the hash function for it to be secure,cryptoalgorithm.sha1 as a hash isn t supported because the sha-1 hash function is insecure;you can create an hmac that uses sha-1 with crypto.createhmac which some oauth 1 sources need,the hmac algorithm is more than just hash key followed by message,the structure of the hmac algorithm makes it less susceptible to attacks on properties of the underlying hash algorithm,unless you need the signatures to be verifiable by someone who doesn t know the secret key hmac is probably a better choice than public-key systems for reasons of efficiency,in some cases on embedded platforms where a fast hash function may not be available these may be more efficient than hmac,after looking it s seems that hmac is much faster and better in term of security even if the underlying hash function sha1 is broken which is not the case when using rsa-sha1,assuming that the is stored alongside the final -- since the hash wouldn t be testable without it -- this scheme is quite weak;using a salt does protect against rainbow tables but a non-iterated hmac leaves this scheme weak to brute-force attacks,hmac is better than a plain hash because it is not vulnerable to hash length extension attacks,"
"hash","hmac"," is more than enough in cryptoalgorithm.sha1 sha-1 insecure,","41433351,","but as you need to hash these passwords too you need to choose a value that won t overload your system or arguably even worse test your users patience;use either bcrypt on its own or pbkdf2 in combination with your sha-512 although sha-1 hmac is more than enough,"
"strcmp","strncmp","safer overall, and not overall,safer in note words slower,safer version in note words slower,safer than  in note words slower,more secure overall,safer in note words slower,safer in note words slower,slower overall,better than using  overall,safer in note words slower,","24359293,4070043,29420623,39110395,5123066,30190628,30190966,4069941,39154525,50152075,33513971,","the only case where strncmp would be safer than strcmp is when you re comparing two character arrays as strings you re certain that both arrays are at least n bytes long the 3rd argument passed to strncmp and you re not certain that both arrays contain strings contain a 0 null character terminator,make certain you use strncmp and not strcmp;strcmp is profoundly unsafe,note strncmp is safer than strcmp,also have a look at strncmp which is safer version of strcmp,the strncmp method is much safer than strcmp,strncmp is more secure than strcmp,in few words strncmp is safer then strcmp but it is slower too,strncmp is a little bit safer than strcmp because you specify how many comparisons will be made at most,is strcmp slower than strncmp as one can give pre-calculated string length to it but strcmp does not receive such information,i have 2 strings to compare and i thought using strncmp would be better than using strcmp because i know one of the strings length,you should use strncmp to compare your strings it s safer than strcmp,"
"short","unsigned"," long long overall,general overall, is smaller in narrower wider size,narrower in narrower wider size,bigger in narrower wider size,larger size than   overall, is a narrower in narrower wider size,wider than   in narrower wider size,narrower in narrower wider size,larger than an  in narrower wider size,slower overall,","54263267,660864,55349844,38251710,4337684,49529554,52555440,34981236,39964651,54451217,4996740,","when in a math operation you put together a signed type with a unsigned type and the unsigned type doesn t have a lower rank the signed typed will get converted to the unsigned type see 6.3.1.8 usual arithmetic conversions i m linking to the c standard but rules for int arithmetic are foundational and need to be common to both languages;in other words assuming that size_t isn t unsigned chars or unsigned short it s usually unsigned long and the c standard recommends it shouldn t be unsigned long long unless necessary size_t 0 - 1 gets implicitly translated to size_t 0 - size_t 1 which is a positive number equal to size_max -1 cannot be represented in an unsigned type so it gets converted converted by formally repeatedly adding or subtracting one more than the maximum value that can be represented in the new type until the value is in the range of the new type 6.3.1.3p,when you convert a signed type to an unsigned type if the number is negative the result of the conversion must be the 2 s complement representation of the integer;in short there is not a faster way than -a,because the size of unsigned short is smaller than the size of int when comparing a value of 0xffff to eof the unsigned short 0xffff will be converted to an int with a value of 0x0000ffff extra digits shown for clarity,alignment may fail as unsigned short can be narrower than unsigned int,an unsigned char is an unsigned value which is typically smaller than and is guaranteed not to be bigger than a short,however c++ standard s only and just the same in c... requirement for unsigned char is being able to minimally hold the values from 0 to 255 and actually not having larger size than unsigned short,indeed a + b is an int type since an unsigned short is a narrower type,so if you tell printf to expect an unsigned short for example then printf will actually expect either an int if int is wider than unsigned short or unsigned int if int and short are the same size,the rule for performing arithmetic operations in c c++ is that if the type such as short is narrower than int then it gets widened to int if all values can fit or unsigned int otherwise,if we use as a mask to get all of the bits that are larger than an unsigned short with we get a 0 if uint_with_unknown_content fits into an unsigned short,btw on some processors unsigned short is much slower than unsigned int because the c standard requires that operations on unsigned types wrap,"
"short","unsigned"," operand is always in narrower wider size,general overall, which is much more overall, is actually er in x convert short, is not overall,wider in narrower wider size, example using your test overall,undefined as   overall,smaller than int  in narrower wider size,smaller in narrower wider size,better than  overall,","46073296,50599798,33510358,34981236,9003139,26403076,52574432,55589024,56604510,19336046,47968544,","but in case short is a smaller type than int the unsigned short operand is always converted to signed int regardless of the unsigned short operand the short was signed or unsigned,we re using one _mm_add_epi16 two _mm_unpackxx_epi16 and two _mm_sub_ps for 8 values the _mm_castsi128_ps are no-ops and the _mm_set would be cached in registers usage demonstration if your input would have been unsigned short the _mm_add_epi16 would not be necessary and the in the _mm_sub_ps would need to be removed of course,if the mathematical result of the addition is outside the range of float which is unlikely or if the mathematical result of the addition s outside the range of unsigned short which is much more likely then the program will have undefined behavior,that will first convert x from short to unsigned short;then because of the default argument promotions and assuming short is actually shorter than int to int,you cast it to char that is an unsigned value discarding the negative sign if there is one a narrowing conversion;then converting to short is not bringing back the negative sign if there was one,if size_type is defined as unsigned short and int is wider than unsigned short the lhs of your addition will be converted to the rhs and you rely on the addition being performed in the lhs s type,for example using your logic with a slight change in variable names to make it clear whether you are operating on the least or most significant bits in an element you could do the following note you need to look carefully at your logic for determining cont_idx and test whether the number of bits you are replacing will span more than sizeof unsigned bytes effecting more than two elements or whether the bits to be replaced all fall within a single element -- that is left to you putting that in a short example using your test case you could do example use output,and shifting unsigned short more that 16 is also undefined as unsigned short is 16 bits,thus if the execution environment represents short as something smaller than int unsigned short becomes int,as unsigned short int is in some implementations smaller than unsigned int,second one is better because unsigned char to unsigned char exact match is better than unsigned char to short conversion,"
"short","unsigned","smaller in x convert short, cannot overall, is more in narrower wider size, is lower in lower rank prvalue, has a lower in lower rank prvalue,declare packetsize as   overall,","46776297,31652667,29327795,40189110,46284103,52722010,","but i wrote this code to check if we compared an signed int x 0xdeadbeef and unsigned short y 0xffff then after converting the unsigned short to int we should have 0x0000ffff in y at the comparison which should be smaller than the unsigned value of x,what happens now is quite subtle acknowledge pascalcuoq beacuse all values of unsigned short are representable in int a + b will be computed as an int;only if short and int are the same width or in some other way some values of unsigned short cannot be represented as int will the sum will be computed as unsigned int .,this is pretty simple for types sizeof int but unsigned chars short is more complicated and requires compiler specific packing,the variable foo will get promoted to type int because type int can represent all values of type size_t and rank of size_t being a synonym for unsigned short is lower than the rank of int,otherwise the source prvalue can be converted to a prvalue of type unsigned int;and indeed short has a lower rank than int,ascii code of character d is 100... try casting it to an integer-type alternatively since you appear to want to store the number as a 2-byte type you could avoid such casting and simply declare packetsize as unsigned short,"
"cufft","fftw","slower in experiments discussion batched,slower in experiments discussion batched,slower cpu overall,","18069017,18069017,6684918,","in the experiments and discussion below i find that cufft is slower than fftw for batched 2d ffts,however for a variety of fft problem sizes i ve found that cufft is slower than fftw with openmp,i am working on a code which needs to be time efficient and thus using cufft for this purpose but when i try to compute fft of a very large data in parallel it is slower than cpu fftw and the reason i find after finding the time for every line of code using high precision timing code is that cudamalloc taking around 0.983 sec while the time for rest of the lines of code is around 0.00xx sec which is expected ..,"
"ironpython","ironruby","much more overall,team smaller overall, to write pocs overall,more overall,","1670651,2778756,3508954,1972630,","however as things stand right now ironpython is much more mature and has much better performance than ironruby so you may prefer to use that,also i believe the ironruby team is smaller than the ironpython team,since developing in ironruby ironpython is faster you can use developing in ironruby ironpython to write pocs and later implement the real application in c# or whatever you re using,ironpython has had more time to focus on performance improvements but ironruby has made significant performance improvements as of late,"
"codeigniter","yii"," fan not overall, is more overall,higher learning overall,faster in benchmark idea is,complex than  overall,structured than  overall, is; is much faster in benchmark idea is, way; is more overall,better than latest  overall,prefer  over  overall, is a lot faster in benchmark idea is,","16202056,11765559,4515976,8396521,24941354,6385419,3572910,3983743,8396571,20328742,17993556,","but i am a big codeigniter fan not yii;yii is also cool though,codeigniter is a product of ellislab team and ellislab team main product is expression engine which developed with codeigniter;after some time of using ci i found that using yii is more convenient,yii has a higher learning curve than say codeigniter and cakephp,i have found one benchmark in which yii is faster than codeigniter and another benchmark in which codeigniter is faster than yii,i am currently looking at yii and while yii more complex than codeigniter the documentation is much more informative and therefore understandable than cakephp s,2 yii a bit more structured than codeigniter but you have a basic app up within 10 mins and if your familiar with mvc design mvc a great way to develop portals incredibly quickly,no idea what codeigniter is;yii is much faster to develop just fyi,you may find it getting in your way a bit more too as you try to do everything the yii way;codeigniter is more flexible - gives you the foundation you need w o getting in your way,here is codeigniter 2.0 and yii 1.1.6;so latest codeigniter 2.0 is better than latest codeigniter 2.0 according to this benchmarks,so i always prefer yii over codeigniter,regarding performance from my expericnece yii is a lot faster because it comes with way less overhead,"
"codeigniter","yii","more overall,","6385419,","personally i d say yii might have the edge as it encourages convention more than codeigniter would,"
"git-svn","svn2git"," which is much more in one-time use-case conversions, which is much more in one-time use-case conversions,worse in worse kde superior,easier in worse kde superior, is worse in tags correct nirvdrum,worse in worse kde superior,better results overall, is easier in worse kde superior, which is much more in one-time use-case conversions, which is much more in one-time use-case conversions, which is much more in one-time use-case conversions,","39141667,43472081,40002621,39634362,48925696,43615825,840730,45463852,42979940,41145820,39528229,","it is a great tool if you want to use git as frontend for an existing svn server but for one-time conversions you should not use git-svn but svn2git which is much more suited for this use-case,it is a great tool if you want to use git as a frontend for an existing svn server but for one-time conversions you should not use git-svn but svn2git which is much more suited for this use-case,there are many reasons why git-svn is worse and the kde svn2git is superior,even though git-svn is easier to start with here are some further reasons why using the kde svn2git instead of git-svn is superior besides its flexibility,with the proper svn2git tags are where they belong if you changed layout in svn you can easily configure this with svn2git with git-svn you will loose history eventually with svn2git you can also split one svn repository into multiple git repositories easily or combine multiple svn repositories in the same svn root into one git repository easily the conversion is a gazillion times faster with the correct svn2git than with git-svn you see there are many reasons why git-svn is worse and the kde svn2git is superior,you see there are many reasons why git-svn is worse and the kde svn2git is superior,using svn2git might produce better results than git-svn if you have tags or branches.,even though git-svn is easier to start with here are some further reasons why using the kde svn2git instead of git-svn is superior besides git-svn flexibility,it is a great tool if you want to use git as frontend for an existing svn server but for one-time conversions you should not use git-svn but the right svn2git which is much more suited for this use-case,it is a great tool if you want to use git as frontend for an existing svn server but for one-time conversions you should not use git-svn but svn2git which is much more suited for this use-case and also makes proper git tags from the svn tags,that git-svn is a great tool if you want to use git as frontend for an existing svn server but for one-time conversions you should not use git-svn but svn2git which is much more suited for this use-case,"
"git-svn","svn2git","cleaner by  in tags correct nirvdrum, which is much more in one-time use-case conversions, is a much overall,lightweight tags with  in tags correct nirvdrum, which is much more in one-time use-case conversions, will reproduce the svn overall, which is much more in one-time use-case conversions,","48454696,46000009,55475018,48925696,38276189,1310736,41035985,","even though git-svn or the nirvdrum svn2git is easier to start with here are some further reasons why using the kde svn2git instead of git-svn is superior besides its flexibility the history is rebuilt much better and cleaner by svn2git if the correct one is used this is especially the case for more complex histories with branches and merges and so on the tags are real tags and not branches in git with git-svn the tags contain an extra empty commit which also makes them not part of the branches so a normal fetch will not get them until you give --tags to the command as by default only tags pointing to fetched branches are fetched also,it is a great tool if you want to use git as frontend for an existing svn server but for one-time conversions you should not use git-svn or tools based on it but svn2git which is much more suited for this use-case,git-svn is better suited if you re going to push pull commits between git and svn;for a straight export svn2git is a much better solution and way faster,even though git-svn or the nirvdrum svn2git is easier to start with here are some further reasons why using the kde svn2git instead of git-svn is superior besides its flexibility the history is rebuilt much better and cleaner by svn2git if the correct one is used this is especially the case for more complex histories with branches and merges and so on the tags are real tags and not branches in git you can generate annotated tags instead of lightweight tags with git-svn the tags contain an extra empty commit which also makes them not part of the branches so a normal fetch will not get them until you give --tags to the command as by default only tags pointing to fetched branches are fetched also,it is great if you want to use git as frontend for an existing svn server but for a one-time conversion you should not use git-svn but svn2git which is much more suited for this use-case,a simple git-svn will reproduce the svn branches actually simple directory with copies in a simple git-svn so i would recommend using svn2git and git2svn,it is a great tool if you want to use git as frontend for an existing svn server but for one-time conversions you should not use git-svn but svn2git which is much more suited for this use-case and should not suffer from oom error,"
"google-chrome","safari","slower in version browser webkit, are officially in version browser webkit, takes the harder overall,better than  in better first great, browser takes up more in fine browser okay,browser version as mine  in version browser webkit, however did the job overall, does not overall,slower in version browser webkit, doesn in background size smaller, it takes less memory overall,","42985566,21003284,48803311,50395019,55588082,50589927,26955647,4181446,10400859,55266076,57543893,","if i try to do a google search for website loading slower in google-chrome than in safari i get a lot of results about how safari is slower than google-chrome,google-chrome is not a supported browser for ssrs report builder;only ie firefox and safari are officially supported,otherwise browsers cannot seek outside the buffered range hence the behaviour on google-chrome;safari takes the harder line of blocking playback altogether,first time i ve seen safari handle something better than google-chrome,it works fine on safari on ipad but since the google-chrome browser takes up more space for the topbar i have to use a media query specifically for google-chrome but i can t seem to make it work,machines have same browser version as mine google-chrome safari browser,google-chrome did not display the html-document generated with xml data;firefox and safari however did the job,safari does not store content in the dom google-chrome does;google-chrome keeps each browser window sandboxed in its own operating system process multi-process model,my ipad 1 safari js benchmarked 38 times slower than google-chrome on my pc,actually it is even worse as google-chrome automatically sets the background size on cover safari doesn t,however it seems 900 mb on safari it takes less memory in other browsers like google-chrome,"
"google-chrome","safari","more overall,main.v8.js overall, is very in syntax forgiving for, isn overall,better in better first great,faster than  in version browser webkit, has less support overall,diff2 faster in version browser webkit,ipad with  in version browser webkit, does not overall,bolder overall,","43278201,4332352,33270067,10436253,8173799,51098443,28139152,3478660,54015519,51274969,24101663,","ios google-chrome is more ios safari than google-chrome itself,i also have .js files being redirected to javascript engines as well for main.js in google-chrome it tries main.v8.js in safari main.nitro.js in firefox main.gecko.js,google-chrome is very forgiving when it comes to css syntax error;safari not so much,that s very interesting as google-chrome isn t anything special on pc;in fact on my windows 7 pc safari is my tertiary browser behind maxthon-3 operates like ie but better and opera 1-key window cycling and page resize beats all,safari comes in at just under 41ms performing better than google-chrome but still not great,so winner is purejs and fastest browser is safari more than 8x faster than google-chrome,important as of writing this safari on os x is the most accessible browser;google-chrome has less support for all the aria features,conclusion diff1 is faster in firefox opera and safari diff2 is faster in ie and google-chrome,doesn t reload on the following media devices android phones iphone ipad with google-chrome incognito browser and safari browser,google-chrome allows mixed content on and;safari does not allow any mixed content,safari renders open sans bolder than in google-chrome so you would need to apply a lighter font weight for safari open sans google web fonts rendering in google-chrome,"
"google-chrome","safari"," doesn in valid generous date, does not in angular works phantomjs,still older in version browser webkit,also earlier version in version browser webkit, i found this explanation overall,better in better first great,better job in api susceptible job, etc. not overall, 49 goes a step overall,slower in version browser webkit,harder overall,","51297124,37205099,19180704,179520,10499458,7138980,35663133,36366808,52061395,11364222,20360593,","google-chrome is being more generous in parsing the date you re giving it which is a string that looks like 7 1 2018;google-chrome accepts that as a valid date safari doesn t,so the short answer is no there s no way of stopping this via angular - and currently there doesn t appear to be a way to tell google-chrome not to do it either;it turns out it s not angular that s doing it it s google-chrome safari does not have the same behaviour haven t tested other browsers,safari still uses the older version while google-chrome uses a more current one,google-chrome also uses an earlier version of webkit than the current safari so pages should be checked in both browsers,from the blog post it seems it is buggy in safari so maybe google-chrome has not implemented it yet;i m unable to activate that experiment myself there s no developer tools experiments in my google-chrome flags but from safari i found this explanation,i think safari renders the moves better than google-chrome,google-chrome also is susceptible to this same issue although it does a significantly better job than safari at getting the realtime api re-connected,for example if you discover that google-chrome takes the 152x152 apple touch icon well now you know what you need;problem you probably want to support all scenarios android google-chrome ios safari etc. not only the new tab of ios google-chrome,google-chrome 49 goes a step further and has support for the unhandledrejection event which lets you add a custom event handler to deal with all unhandled rejected promises;safari also shows a warning and since version 11 also supports the unhandledrejection event,the one used by google-chrome and is slower than mobile safari s nitro javascript engine,dus anyone knows if there is something wrong with my json or if google-chrome is harder to get than safari,"
"google-chrome","safari","smaller in background size smaller, not filling 100 height in height makes child,faster in api susceptible job,such as  overall,more strict in fine browser okay, is better imho in fine browser okay,more overall, it is less overall,more overall, modeuser is higher in fine browser okay, uses the more overall,","4835787,35537510,6688552,50970245,41102298,23841372,41959925,52005331,19977926,55217449,10009533,","safari will display fonts without a specified size significantly smaller than in google-chrome,google-chrome ignoring flex-basis in column layout;google-chrome safari not filling 100 height of flex parent,this seems to me like a bandwidth error or something like that originally i ve got the error when i played with the html 5 audio api and if i loaded the audio file 10-15 times sequentially then i ve got the error but now i ve discovered that i get the error without the audio api too just by reloading the site a lots of times also safari gives me the error much faster than google-chrome wtf,fun fact you don t even have to use safari the button is available in other browsers as well such as google-chrome,okay safari ios is more strict than google-chrome android when it comes to this all fine but it still needs to enable me to allow the connection through,google-chrome javascript console f12 or ctrl + shift + j;mostly the same browser as safari but safari is better imho,from testing this in google-chrome and safari it seems google-chrome is more forgiving in that it parses the style string and puts the right style in place for you but safari does not,this issue is very frequent on safari but on google-chrome it is less frequent,safari is more secure than ie or google-chrome,on the following browser it works fine safari internet explorer but on google-chrome modeuser is higher than tbsearch and btnsearch,safari also uses this;google-chrome uses the more modern rfc 6455,"
"google-chrome","safari"," it didn t play overall,more funny overall,larger overall, it is less overall,newer version in version browser webkit,better in better first great, is better in page default web,more stringent overall,older version in version browser webkit, will not in version browser webkit, would not overall,","8736838,10271789,4465683,24138152,8073648,8566225,49139109,5093790,45875146,45622729,26374568,","i used the same code written in this question which worked in firefox and safari but not in google-chrome;in google-chrome it didn t play again,safari is more funny than google-chrome i can easily make google-chrome cache my page but safari does not,total width of button in safari web inspector 6px larger than in google-chrome web inspector,in google-chrome you can set the default fonts at google-chrome settings under advanced settings;in safari it is less easy but i could find this post,it s not even a webkit issue unless google-chrome s got a newer version than safari,i guess firefox and safari handle huge numbers of elements better than google-chrome,you can imagine the markup looks like so the above produces the results as expected in google-chrome even without using page specifically - seems like google-chrome is better with setting default margins whereas i am struggling to find a solution in safari,i read somewhere that safari has more stringent requirements on cross domain scripting - it and google-chrome share the same code base,it also gets broken even more in ios google-chrome which is an older version of safari,in my case google-chrome tools displayed the file name not just author stylesheet;i m not sure if this is 100 accurate but i believe that safari will not show the name of the css file if it is named default.css,here was my problem if i put the mp4 line first google-chrome would play the video part but there would be no sound;if i put the webm line first google-chrome would play the video and sound correctly but apple safari would not detect the video at all,"
"google-chrome","safari"," giving a phishing overall,way more in version browser webkit,smaller in background size smaller,more overall, may not in version browser webkit,worse than in  overall, error is very in angular works phantomjs, is just really overall,longer than  overall, auto-completes the request overall, does not overall,","45147067,19182824,24042145,15194990,25445352,50470536,45559140,53753388,35580321,28092489,40653657,","google-chrome doesn t have duplicate popup if login credentials are incorrect though;the method of explicitly redirecting document.location with username password in url caused me some problems with safari giving a phishing warning,the browser does not matter though safari gets way more consistently working results than google-chrome,safari 5.1.10 6534.59.10 middle handles a smaller viewport but in sort order with a smaller viewport computes new image sizes even smaller than google-chrome,so if something works on safari it s more likely to work straightaway on google-chrome more than it is on firefox - but that s just a family resemblance at the end of the day you ll still have to test each as every webkit browser product and version still uses a different version of webkit,the tryanchordownload approach should work on modern versions of firefox and google-chrome and is a cleaned-up version of code provided in this forum post;the trysaveasdownload approach should theoretically work on any major modern browser though safari may not respect the specified file name,i ve tried it of different pcs browsers it is freezes only in google-chrome in google-chrome on a mac book it works much better then on pc but worse than in safari as example,works with angular on google-chrome but not phantomjs;your safari error is very illuminating and i am kicking myself for not reading it more closely,+-----------------------------------+ | browser | delete | destructure | +---------+-----------+-------------+ | google-chrome | 3 229 791 | 1 993 256 | | safari | 1 186 679 | 1 872 396 | +---------+-----------+-------------+ the results on ios are less surprising as google-chrome is just really safari under the hood,it takes longer than google-chrome or safari to load but it does load successfully,the point is that safari has a functionality google-chrome doesn t when safari auto-completes the request it sends the request;google-chrome doesn t send the request until the enter button is pressed,it seems google-chrome calculates the row height according to the highest image;safari does not calculate the row height according to the images,"
"google-chrome","safari","consistent with  overall,more overall,more overall, does not offer nightly overall, and seems bigger overall, to view the notebook overall,larger in height makes child, says please enter in valid generous date,stricter than  overall,worse overall, it looks much better overall,","56029986,2305116,19264379,15379023,12785099,44190727,39526896,48798198,55490393,33721616,12315839,","the css code is as follows and the svg filter is could anyone tell me how i get safari looking a bit lighter more consistent with google-chrome,in any case vorbis clearly has the most coverage google-chrome has more of the market than safari now opera is about half of safari and obviously firefox is huge and it doesn t really have the strong opposition that theora has,google-chrome has more of the market than safari not and invitation for flame wars just a personal opinion without any weight -,google-chrome does not offer nightly builds although they do have an open source project called chromium that does offer nightly builds google-chrome is based on chromium but they are not identical;similarly safari does not offer nightly builds either though again they have an open source project based on webkit,google-chrome moves the transformation-center over the z-axis safari leaves this center were it was but moves the object itself over the z-axis;the object therefore is zoomed in safari and seems bigger,i used the same approach as kiem nguyen above but google-chrome wouldn t work for me even after editing my .bash_profile as hanshenry90 suggested;to use safari to view the notebook open your terminal and enter,safari makes child block s height larger than google-chrome,safari says enter a valid value. google-chrome says please enter a valid value,now safari by default is even stricter than google-chrome here and a simple click on the document doesn t work in this browser you need to start the playback from the user-event itself,google-chrome is worse than safari,i think it s a bug in google-chrome s svg rendering implementation;at least in firefox and safari it looks much better,"
"google-chrome","safari"," is getting better too overall,wider in background size smaller, but doesn overall, but not overall,smoother overall, doesn overall,more time than  in version browser webkit, is easier overall,less noticeable overall, are taking care overall, which uses a lesser overall,","8932659,21788551,35805359,40849433,10793683,56032185,51496542,27104041,7138980,31129523,7483347,","i have had issues with dynamically created nodes that use filters and animatemotion has been a bugged in google-chrome for way too long.. we use ff5+ as our interactive interfaces for this reason safari is getting better too,in safari the background of my menu navigation drop-down menu s is much wider than in google-chrome firefox,it s tough to gauge from the parameters you posted but just based on you saying that this works perfectly in safari but doesn t work in google-chrome or firefox it sounds like this could be a cors issue;firefox and google-chrome have different requirements for cross-origin requests than safari,google-chrome has many issues with screen readers that were not primarily written for it such as voiceover safari and nvda firefox;i tested this example myself and the labels were read aloud using voiceover and safari but not with google-chrome - because of these issues,safari 5.1 still renders smoother than google-chrome but it is now clear that google has done much improvement,and here is the screenshot - google-chrome left safari right;this is going to seem dumb - but move the into the filter element instead of the fecomponenttransfer - safari doesn t seem to check for it when it s put into a primitive,but safari takes more time than google-chrome browser,however zooming in on the map in safari hangs the web page;google-chrome is easier on that,the safari controller bar is the quicktime bar which is smaller and dark so the movement is less noticeable than google-chrome which has a big blue progress bar,if the chunked response does not end properly google-chrome cannot read it;but firefox and safari are taking care of this internally,the open source browsers are webkit which is used for safari and the mozilla flavoured firefox;there is also chromium which is the open source version of the google flavoured google-chrome which uses a lesser javascript engine squirrel as opposed to v8 and does not have the multi-separate-thread modularisation of google-chrome,"
"google-chrome","safari"," doesn overall,more than  overall,general in page default web,more overall,directconnect with  overall, are still in version browser webkit,faster as the  in version browser webkit, it s execute javascript in syntax forgiving for,","57500044,52979200,55114452,41814737,57514296,51274969,47179887,47366665,","google-chrome doesn t load the same video multiple times;i only loaded that video once on safari but according to web inspector it loads different sections of the video to reduce the load on your computers memory,these red lines are just lines on the picture it seems safari scales my picture more than google-chrome and because of that all content is moved,i am trying to solve a mystery a page from a react web app can be loaded in safari on ios that page can be loaded in google-chrome on ios if you choose request desktop site that page is blank when attempting to load it in google-chrome on ios if you go with the default request mobile site the page works fine in desktop browsers i have taken steps to enforce a timeout on the server side in case there is a connection that is hanging,a lot of people use iphones ipads and safari is literally the only option even google-chrome is more like safari in disguise... so it s quite a portion of users who are suffering from this issue,you can at the moment only directconnect with google-chrome and firefox. see docs here directconnect basically means you don t need a selenium server to execute the browser commands for you. however if you need to lets say run your tests against internet explorer edge safari you ll need a selenium server for that,it looks like there has been some work at firefox and google-chrome so isn t treated as mixed content;looks like the crowd at webkit safari are still thinking about it,figure out how to install selenium with the google-chrome driver safari driver may be faster as the google-chrome driver safari driver js engine is faster,the do javascript syntax is for safari not google-chrome;for google-chrome it s execute javascript however changing that doesn t fix your code,"
"android-emulator","bluestacks","less memory overall, works better overall,better overall,more possibilities overall,","45340271,35022155,10101698,13373733,","2 if your host machine laptop doesn t have more ram then you might want to find alternative android-emulator like genymotion bluestacks which takes less memory than android-emulator,this android-emulator works better than bluestacks,in xda-forums i read about memu - most powerful android-emulator for pc better than bluestacks,yes some functionality possible only on device but you can try also bluestacks app player which has more possibilities than android-emulator,"
"np-complete","np-hard","harder overall, is easily overall,harder overall, it wouldn overall,","27576988,30543429,25288133,33721207,","if there exists a np-hard problem that is not in np to the best of my knowledge no such problem has been proved to fall in this category at this moment of time such problem is harder than np-complete problems,unfortunately there is little hope to find an algorithm which is much better than brute-force considering that the problem is actually np-hard but not even np-complete;a proof of np-hardness of this problem is that the minimum vertex cover problem well known to be np-hard and not np-complete is easily reducible to it,as i understand it an np-hard problem is not harder than an np-complete problem,the decision variant asks whether there is such a subset of size k and is np-complete making the optimization variant np-hard;the sets here are always exactly 4 elements which is still np-hard it wouldn t be if the sets had exactly 2 elements which is an easy problem,"
"screens","switching","more overall, apps is no longer overall,more than one  overall,bigger overall, is smaller in display-properties x hence,size smaller in liquid media query,smaller in liquid media query,more than one  overall, actually causes much more overall,longer overall,less in liquid media query,","36684777,51117368,55764941,19079736,21043468,47344039,31657992,55489356,21349390,36792786,38781058,","and would the answer be different in a case where screens switching is more frequent like between an overworld screens and a battle screens in a game like final fantasy or pokemon,i m pretty sure stop gets called when you force kill the app switching apps lock the screens phone auto locks or anything really where the app switching apps is no longer visible,i am using libgdx to build my game i am using more than one screens and switching between them is very slow and it crashes the game sometimes without any log in the android device,the point of such an architecture is because i have news with some text between which i want to be able to switching with paging effect but the news text can be bigger than the screens,and then switching them display-properties when the screens is smaller than x px,when the screens size is smaller i have it switching flex-direction to column which works well except i cannot figure out how to make the 2nd row trending down start where the trending up stops where ever that may be,the panels must be side by side for a large screens but they must be responsive and as the screens gets smaller and switching to vertical orientation,by default it does not handle back actions and it resets routes to their default state when you switching away you can perform goback action inside goalstack if you have more than one screens,you can imagine this pointer switching does not require very much time;i hope you see now that directly writing an image to the screens actually causes much more delay and annoying flickering,when the screens resizes the longer elements in my navbar switching to two lines to take up less space,but then i noticed after changing a mobile to landscape it would switching back to the desktop mode due to the width of the media query being less than the screens landscape width,"
"screens","switching","bigger overall,taller overall,better compatability overall, is less in liquid media query,larger in liquid media query, is smaller in display-properties x hence,no longer overall,common between  overall, gets smaller in liquid media query, motion even overall,lower normal overall,","1959842,3909833,8791721,49798648,5442712,40170143,11488206,53464200,54719475,1387232,33295770,","when the switching gets bigger than a couple of screens full split it into functions that handle each state using a state table to look up the function directly,however when i switching to landscape mode the listview is taller than the screens,switching to 2.2 framework which has better compatability with screens resolutions and densities,use media queries like this in your css file media query will switching your css styles in case the width of your screens is less than 767px,i ve already built a view that can take a drawable can use focused pinch-zoom and drag can auto-scale images can switching images dynamically and takes images larger than the screens,hence am switching over to div;preferable use which will align automatically if the screens is smaller,at this point there is nothing i can do since the switching is no longer there... the external screens is just a blank screens for the sake of the exercise,i d rather not refactor the layout to not use screens have a single screens and just switching out the widgets that are not common between screens when changing nor do i want to add if this_widget_on_current_screen style logic to every property s on_change handler,i need to get the images in the second container on the right to switching from vertical on the right to horizontal under the first container and get smaller as the screens gets smaller,by automatically switching between these two modes the codec maintains video quality while maximizing compression;overall the windows media video 9 screens codec delivers better handling of bitmap images and screens motion even on relatively modest cpus,after switching to windows 10 i ve noticed that when selecting a lower than normal screens resolution the panel fit mode will default to maintain aspect ratio meaning black bars appear,"
"screens","switching","larger in liquid media query,smaller in liquid media query, which gets rid overall,","14789361,9591286,53026204,","i am using view flipper to show bunch on images like a slider where i implemented the functionality of swipe left and right to switching images now some of my images are larger than the screens size in terms of height are getting hidden,but when the screens is smaller than 900px we switching from fixed to liquid via media query switching to a liquid width will allow our images to scale down our text to wrap and a whole bunch of other great things,however i can t do so because as soon as i setcontentview to the fragment that contains the textview it immediately sets the fragment to full screens which gets rid of the navigation drawer and keeps me from being able to switching between menus,"
"clang","llvm"," normally shouldn overall, is better overall,general overall, is not overall, is not overall, this is more overall, doesn overall, is not overall, itself doesn overall,more compatible overall, is not overall,","52750230,11219099,1551247,12462053,42593657,53249844,17380264,24836566,23287218,30608407,29409986,","debug information is best effort but if you haven t run any other transforms since clang emitted the ir then it will usually work;this is an intentional design decision llvm normally shouldn t be treating system and non-system functions differently,clang is better;it parses c and c++ to asts can do analyses on the llvm intermediate representation and has some kind of mechanism for spitting out to-be-applied-later patches to source text inspired by a desired tree change,llvm s back-end is faster than gcc s,in fact in the clang compiler it plays a major role but that s entirely internal - what comes out of clang is pure native arm ios machine code;llvm is not a virtual machine in that sense of the word,clang with microsoft codegen is using clang to parse the source into an ast;but then msvc s code generator kicks in so anything related to llvm is not used,knowing that pypy is on osx compiled using gcc and apples llvm this is more akin to the linux world than python which is on osx compiled directly against clang,change the lib target in the xcode to use the new clang llvm add a cflag -fsanitize address;then build if some api such as opengl system video function is reported not supported then you can put it into the app project your clang doesn t support compiling it,clang converts c c++ etc to llvm if llvm performs optimizations on the if and the llvm x86 backend writes out x86 machine code for execution;despite the name llvm is not a virtual machine in the traditional sense - it is a computation model and representation that lends itself well to the task of manipulating code,you will need to modify quite a bit of code for example there are portions of type definitions in ... llvm tools clang lib sema sema.cpp and ... llvm tools clang lib ast type.cpp;if you grep for int128 good choice as clang itself doesn t use that much in itself as opposed to for example size_t you will see that it turns up in a lot of places,good options are llvm libc++ static library fewer features more compatible with clang and gnu stl static library more features i had an issue that required me to turn the clang optimizer to -oz to prevent a segfault,fresh compiler-rt build doesn t contain this library and fresh clang is not using it;and i failed to compile since i didn t sync the other llvm projects before compiling i only put compiler-rt inside llvm projects directory and build from llvm build root directory,"
"clang","llvm","code faster overall,better standard overall, does not yet overall, wouldn overall, has much better overall, does not overall, now supports cuda overall, does not yet overall,","4628751,30585310,34249916,21820642,1463254,14723945,12101094,29104148,","llvm compiles code faster than gcc may create code that runs faster and the clang frontend provides more accurate error messages than gcc â so there are definitely reasons for switching,first fiber won t compile with apple clang because of the use of thread_local which apple does not support according to what i found online they think they can implement it better than standard llvm and don t want to introduce it only to break abi later,from a couple of years the default compiler in os x is clang llvm and not gcc;the linker is telling you that it cannot find openmp libgomp this is due to the fact that clang does not yet include it however the support seems now complete and you can install it by downloading the latest version of llvm,if you want colored error messages from clang be sure to install the ncurses development package libncurses-devel with cygwin setup beforehand;without it clang will fallback to its colorless mode because llvm wouldn t be able to detect whether the terminal supports colors,clang llvm has much better separation between the parser and the other parts of the compiler chain,edit apparently i m mistaken and clang does use some other integer types as well see jens s comment below;yes llvm does not make a distinction between signed and unsigned integer type so both will be lowered to i32,clang though also based on llvm does not support cuda;2016-05-01 update clang now supports cuda,note that clang still uses your system linker ld and this is fine;currently llvm does not yet provide a fully functional alternative to this program but they are working on it,"
"lisp","scheme"," is much easier overall, is considered a better overall,more overall,somewhat simpler not in good ides user-friendly,more user-friendly in good ides user-friendly,many more in fewer conservative libraries, would only overall,approach more in fewer conservative libraries,much more uniform in fewer conservative libraries,older common in fewer conservative libraries, supports a completely overall,","3440111,9614527,3526596,563472,2115070,11683143,41166985,16651843,5372482,13225458,1250008,","fter spending years with scheme and common lisp i ve spent a year programming almost exclusively in c;when i go back to scheme scheme is much easier to express myself because i know the kinds of things the machine is good at,for sure this dual meaning s a complication that makes this dual meaning harder to explain the language to whoever is learning this dual meaning and that s why scheme is considered a better simpler language for teaching but many expert lispers think that it s a good choice that makes the lisp language a better tool for real problems,my impression common lisp is more for getting stuff done scheme is more for education and fun,it s popular it s actively developed it has many libraries offering the features of a modern programming environment and scheme is somewhat simpler not to say better just simpler than common lisp,i ve also found scheme ides much more user-friendly than lisp s plt scheme is a good one,i noted that while in languages like c variable identifiers can only be alphanumberics and underscores common lisp allows many more characters to be used like and at least scheme does,other dialects of lisp such as common lisp and emacs lisp have multiple form evaluation in their cond clauses so not allowing it in scheme would only reduce compatibility adding to someone s workload when they convert code from another dialect to scheme;scheme isn t a functional language let alone a non-strictly evaluated one,i ve noticed that the common lisp approach is more conservative than the approach scheme has,overall common lisp is much more uniform than scheme and more radical language experiments if done at all are usually embedded as a portable library rather than defining a whole new language dialect,scheme is older than common lisp,while common lisp supports a functional programming style that is not its general focus scheme while not purely functional is much closer;common lisp supports a completely imperative style of programming very well,"
"lisp","scheme"," is not overall,fewer batteries in fewer conservative libraries,better other overall, which is now overall,smoother overall, is more pedagogical primarily in fewer conservative libraries,simpler language common in fewer conservative libraries, do not overall,less memory overall,more verbose in fewer conservative libraries,smaller in good ides user-friendly,","3041832,4587918,29022844,5073472,4587918,4028988,5691342,9729770,2741553,19383232,2973943,","but scheme is not common lisp which is what lisp typically means today they are really different languages;if you want to learn common lisp i would start with common lisp and skip scheme,that said it is a scheme which has fewer batteries included as compared to common lisp,t s purpose is to test the thesis developed by steele and sussman in their series of papers about scheme that scheme may be used as the basis for a practical programming language of exceptional expressive power and that implementations of scheme could perform better than other lisp systems and competitively with implementations of programming languages such as c and bliss which are usually considered to be inherently more efficient than lisp on conventional machine architectures,if you are new to lisp and not an emacs user presently then i would strongly recommend the free editions of either allegro lisp or lispworks;or if you are going the scheme route plt scheme which is now called racket,however gambit scheme has smoother access to c c++ code libraries which far outnumber common lisp s libraries,common lisp is the de facto standard for lisp;scheme is more pedagogical primarily due to its association with the famous mit class and book sicp although it is powerful on its own,i don t see why sbcl should be so fast - scheme is a far simpler language than common lisp,strictly speaking funcall would not be needed but there are some lisp list-2 implementations such as common lisp that separate the variable name space of the function name space;list-1 implementations scheme do not make this distinction,for a counterexample i think scheme programs ran faster and used less memory than the lisp programs that preceded them mdash,common lisp has a separate namespace for functions which makes operation like this more verbose than with scheme,scheme is also a good language for that purpose and it is simpler smaller than lisp,"
"lisp","scheme"," does not and  also in name empty object,intentionally more compact in fewer conservative libraries, repl.it runs biwa overall,fewer in fewer conservative libraries, which is significantly cleaner overall, is useful too in good ides user-friendly,better common in fewer conservative libraries, has more hats than defun overall, is much more in fewer conservative libraries, using c overall, that was not also in fewer conservative libraries,","42436563,3041805,28122838,4587918,3528737,3073344,170948,19982505,56299497,4350640,56296876,","lisp then chooses to pun making the empty list and false be the same object scheme does not and treats them as distinct;additionally lisp provides a name for the empty list object while scheme does not and scheme also does not treat the empty list object as self-evaluating while lisp does,scheme is intentionally more compact than common lisp and you ll find that you can learn the language very quickly,note that lisp is not a single language but a large family of somewhat similar languages;you seem to have tried out scheme repl.it runs biwascheme and clojurescript,scheme has fewer libraries than common lisp,the class uses scheme a dialect of lisp which is significantly cleaner and easier to use than common lisp yes this is an opinion deal with this,learning common lisp is not that difficult but getting deeper experience get take some time because the language and its eco-system is surprisingly rich;learning scheme is useful too,here is a solution in scheme because i know that better than common lisp and have an interpreter for checking my work,this macro will make a global defun that works as in common lisp;define in scheme has more hats than defun mostly because of the one-namespace nature of scheme,common lisp is much more extendable i m a big scheme fan and sad to say but none of the reports have any standardized way to do any ffi,for an environment to implement tail call elimination is mandatory in scheme but not in most other lisp dialects such as common lisp;i thought lisp using clisp would recognize the tail recursion,i have heard that common lisp is more broadly featured while scheme is more conservative and austere but in my casual use i haven t been able to find any feature of common lisp that was not also in scheme,"
"lisp","scheme"," was a 1 overall, doesn in name empty object,","37425695,42436563,","scheme looked more like cl than scheme but was a lisp1 so the empty list rules cannot be about lisp1 vs lisp2 since scheme was a lisp1 from the first report that didn t require quoting and has nil as a false value,additionally lisp provides a name for the empty list object while scheme does not and scheme also does not treat the empty list object as self-evaluating while lisp does;both languages provide a canonical true object as well but again lisp provides a name for it while scheme doesn t,"
"bson","mongodb"," does not overall, doesn overall,more overall,tricky as with  overall,even more overall,more overall,more overall,more in documents document nesting,more overall, cannot overall, to produe a syntax overall,","30896153,6674721,30155258,16232926,33718403,10763863,31617963,19052828,4276683,4359477,27960917,","but from the source code here or here for example it appears that mongodb bson s implementation use strcmp to perform binary comparison on element names confirming there is no way to achieve what you want;this might be indeed an issue beyond case sensitivity as using combining characters the same character might have several binary representations -- but mongodb does not perform unicode normalization,mongodb s underlying storage format bson doesn t support high-percision decimals only 64-bit floating point numbers;javascript in mongodb is single-threaded and can be a bit slow,the distinction between int64 and int32 in mongodb is more about bson storage size,the actual checking is done inside mongodb and not mongoengine;the object sent to mongodb should be the same but this is where it gets tricky as with bson order is important and in python with dictionaries its not,the bson becomes even more annoying to work with when exporting data from mongo to another db platform this is the case when dealing with big data that is collected and you want to merge it with some properties from the back office mongodb this means a lot of pain you need to transform the binary objectid to a string in order to join with the id in different platforms that do not use bson representation,a document in mongodb is more or less like a json structure bson to be specific,mongodb supports no more than 100 levels of nesting for bson documents,mongodb supports no more than 100 levels of nesting for bson document.,mongodb stores everything in memory anyway and works in a similar vein being a key-value based system however i believe mongodb is more flexible as it allows for storing bson objects within themselves,of course it works with bson documents but when you want to update some record mongodb returns full bson doc not the part;i am new in mongodb but as i understand mongodb cannot returns some part of document,it allows for mongodb to produe a syntax and object rich document beyond what json can;bson normally takes more space than json due to its object usage and what not as such there is no serious performance gain,"
"bson","mongodb","more in documents document nesting,more in documents document nesting, which is kinda overall,larger overall, and are then overall, is a lot easier cleaner overall,","34703648,33088282,26726875,43101666,14952823,8809847,","basically mongodb supports no more than 100 levels of nesting for bson documents,mongodb supports no more than 100 levels of nesting for bson,mongodb doesn t use json in the first place;it uses bson which is kinda similar to json from the outside but supports more types and more importantly is binary not text,so if your document bson is larger than 16 mb mongodb throws exception,also mongodb does not require json php what-ever that is handling knowledge;all results from mongodb come into the driver as bson and are then de-serialzed to a standard dict within php namely an associative array,you can definitely store the structure in mongodb;typically bson is a lot easier cleaner than using xml to represent complex relationships and you can also read edit it cleanly from the shell,"
"qpushbutton","qtoolbutton","much more overall,smaller overall,","38580502,38580502,","qtoolbutton is much more complex under the hood than qpushbutton,qtoolbutton has smaller default internal margins than qpushbutton,"
"connect","geddy","not better overall,better overall,","10828487,10826795,","geddy is not better than connect express the big thing that differs is the structure of it,so what makes geddy significantly better than connect,"
"thin","unicorn"," you need quite overall,choose between  overall,better choice overall,faster overall,better performance overall, seems better overall,","13154077,29610760,21745505,8022554,15755155,19966067,","for example to implement rolling restarts in mongrel and thin you need quite a lot of steps in your deployment scripts;unicorn doesn t require as many steps but still significantly,i often use example in development to choose between unicorn and thin for my rails server,jruby support - unicorn s a better choice than thin but it doesn t support jruby,i m running unicorn which is about 40 faster than thin on celadon cedar,in production it is much better to use a more sophisticated server like phusion passenger or unicorn since they have better performance than thin mongrel or webrick,if you use thin and your code doesn t clear requests very quickly then you re in trouble - since heroku uses random routing requests will stack up on a blocked dyno even if there are free dynos;using unicorn seems better if you can handle the memory hit because free dynos s less likely that all of your forks will get slow requests at the same time,"
"font-size","width","wider in string wider part,greater in text greater larger,less overall,larger in text greater larger, is smaller in div child better,smaller overall, is less in text greater larger, then adjust the class overall, 100 so in div child better, is more overall,wider in string wider part,","9526369,24730789,31454005,15319528,37041431,39202362,2430207,29606026,20407697,54229374,8490195,","from there on the width slowly increases again as the part of the string still on line 1 linearly gets wider as the font-size increases,when you are using fittext at that time if the font-size is greater than 100px than you have to set the exact width as per your font-size,the first media query says that if the screen width is less than 992px the h2 in the item class should have a font-size of 3rem should be red and should have an underline,as you can see the font-size of the text is bit larger than the width of the noisy lines,he issue appears to be with the font-size or fontfamily that you are using;the issue may also be affected if the parent element s width is smaller than 200px,you can remove the transform s and the margin-left and add a width set as 0 here smaller than the font-size 16px here to the get the effect,you can then check if the width of the text is greater than 800px and reduce the font-size until the width is less than or equal to,for the images you adjust their size using width height not font-size;if you want to adjust the font-size then adjust the class of the captions,you can use javascript to measure the div and set the font-size of a child div but a better idea might be to use an image of text that can resize a better idea by having width 100 so that a better idea always fills the parent div exactly,font-size become equals to 50px when the screen width is more than or equals to 720px,i initialize the loop with this value and increment the font-size with 1 until the string is wider than the width of the containing element,"
"font-size","width"," is smaller overall,","54311269,","so i had a fixed font-size but because on gmail on phones the width is smaller a lot of times the text gets wrapper into 2 lines. what i want to do is to use a width dependent font-size. so what i tried was using vw like this my problem is that even tho my main view has max-width 640px,"
"junit","testng","better results overall, does not overall,more contrived overall,lightweight than  in hey best android,more overall, is better and more in framework frameworks testng,more overall,easier in easier please better, has better test in hey best android, which has bigger feature in framework frameworks testng,general overall,","26423195,17024479,6726758,6217744,1823057,25763284,430883,11842853,2310396,34398541,51810224,","firstly i am not sure how to properly use multi-thread with junit last time i tried i had no success anyway i have had better results with testng,testng from a very early stage was suitable for integration automation testing it has a more powerful version of dataprovider than junit even before junit had dataprovider capabilities;it has much better annotation support than junit and unlike junit testng does not require your before after class methods to be static,here is what it would look like with testng it s a little more contrived with junit since you can t pass parameters directly to test functions,i actually picked junit over testng for android testing i thought android testing was more lightweight than testng for on-device testing,if you have to do a lot of this honestly testng is more flexible but you can absolutely get it done in junit,now concerning testing frameworks there are junit or testng;i don t want to start holy war but imo junit is better and more widespread and hence is more actively developed,testng is more flexible than junit and have multiple advantages like support for parallel testing for example,if you are familiar of using junit it is easier to switch into testng,hey might not be exactly what you want but they are probably the best you get with junit;supposedly testng has better test grouping possibilities but i haven t really looked into it myself yet,in case if you have flexibility to choose another testing framework you can try with testng which has bigger feature set than junit,using a profile is a possibility but it is not mandatory as grouping and excludedgroups are user properties defined in the maven surefire plugin to respectively include and exclude any junit 5 tags and it also works with junit 4 and testng test filtering mechanism,"
"junit","testng"," is older and have more overall, has much better in framework frameworks testng,more overall,better solution overall,better in easier please better, is not overall,better option in hey best android, is not overall,way more overall,much friendlier overall,more overall,","7558548,56301173,3411768,913928,6216983,20056622,6349659,38675102,3412814,811937,10596592,","junit is older and have more extensions dbunit cactus etc;testng has much more annotations,the spock framework which runs on top of junit has much better support for parameterized testing which is what you seem to want than either plain junit or testng,edit it seems junit has more printed books as references compared to testng on amazon,if there is a junit extension or similar framework that provides a better solution than testng then please let me know,isn t testng supposed to a better than junit,junit is creating a new instance of the test class before each test so junit fans like me will never face such problem;testng is not creating a new instance of test class,if you re doing non-unit testing testng might be a better option than junit,junit is not able to run testng tests or features;but at the opposite testng is able to run junit tests,to be perfectly honest i junit is way more popular than testng at least here where i work and live,testng is much friendlier to this paradigm than junit though,testng offers you more options and possibilites how to run your tests and in which order especially something junit can t,"
"junit","testng","more overall, needs or does poorly; even in easier please better,more configurable overall, does not overall,more overall, which allows test in look unit for, is more overall, is more in look unit for, which is just overall,easier in easier please better,","1686834,17024479,6671,27010145,6738770,11662022,49256995,7969462,10387713,24033060,","junit is more direct and simpel to use if you start to learn java testng has more features but maybe is more complex,all of which junit needs or does poorly;testng even has better reporting please google for reportng or my fav testng-xlst,testng strives to be much more configurable than junit but in the end they both work equally well,the junit complies the code and we can even have data pushed into the actual database and rolled back after testing;the testng does not provide this facility,testng has more capabilities and can be helpful with integration tests junit is more focused on unit tests,as far as i know junit does not support the behaviour that you want;you might want to take a look at testng which allows test specific setup with beforemethod and aftermethod annotations,unit and integration tests use junit and can run on the jenkins master for the system tests we use testng and they can run the testfx tests on a jenkins agent. i think testng is more suited for system tests than junit development build project build unit+integration tests was already in place,junit is more aimed at unit testing;for functional testing take a look at testng which offers advanced functional testing features such as,i suspect what you re trying to do will be easy with testng - junit does not have the grouping capability that testng does;i always recommend that people switch to testng which is just like junit 4 except with more functionality,i know this can be achieved with junit but in my experience it is easier with testng,"
"webclient","webrequest","sometimes easier in simpler easier,simpler in simpler easier,more overall,just more functonality overall,shorter overall, which is really overall,easier in simpler easier, to send a request overall,general overall,simplier api overall,","91317,449918,1726749,18410138,7688412,11832917,7779848,27391741,4123564,27555959,","webclient is sometimes easier to use than webrequest,webclient is simpler to use than webrequest,update i ve created a webhelper class that takes the place of webclient but provides more access to the necessary features of the underlying webrequest,webrequest just offers you more functonality than webclient,webclient is a shorter and more concise syntax but behind the scenes it uses a webrequest so in terms of performance it won t be faster it will be equivalent,for older framework version you can use either webclient which is really simple class for downloading uploading data or use httpwebrequest;webrequest is lower level but you can define method get post useragent etc,a webclient is much easier than a webrequest,first of all you cannot do it with webclient it doesn t maintain cookies between redirects;the ugly way is to use webrequest to send a request then check responseuri to determine if you was redirected to authentication page,webclient is simpler and provides a downloadfile method that should do what you want;if you find you need more control over the process you could switch to using webrequest,to get the html instead of hardcoding it as above use the webclient class since it has a simplier api than webrequest,"
"tcp","udp"," which is more overall,better option than  in better protocol simpler,slower in faster slower reliable, is not in care connection packets,more processing in overhead tcp end,less latency overall,better than  overall,slower than  in faster slower reliable,faster better throughput in faster slower reliable,better in better protocol simpler,slower than  in faster slower reliable,","9138399,6104661,8813553,31365356,27732495,20769240,35183791,51086296,638446,16924212,9627280,","f the clients are listening on that address the clients ll get the message although the message s not guaranteed like tcp if the clients re not there the broadcaster doesn t care;it sounds like what you need is udp which is more of a broadcast to anyone that s listening scenario,if you can t afford lost packets then tcp is probably a better option than udp since it provides that guarantee out of the box,tcp is slower assures data arrival udp is faster data corruption may be possible,udp uses packets whereas tcp is a byte streaming protocol;tcp is reliable whereas udp is not,moreover tcp has more processing overhead than udp,udp lends itself to real-time less latency than tcp,it s unlikely in the extreme that you ll be able to implement udp better than tcp does considering that tcp has been developed and optimized by top experts both in network communication and in the details of your platform,but udp may have a considerable packet loss so try webrtc-tcp for your quake idea;here is 2018 update for you your off-the-shelf solutions are red 5 pro wowza kurento unreal media server flashphoner also note that in modern public networks tcp is not much slower than udp,in some applications tcp is faster better throughput than udp,or is there any specific scenario where udp is better than tcp,because of tcp requires connection and provides security it is slower than udp and therefore it should not be preffered during a video streaming,"
"tcp","udp","more appropriate overall,higher in higher degree advanced, gets more in faster slower reliable, doesn in care connection packets,faster in faster slower reliable,slower in faster slower reliable, has is source overall,less in overhead tcp end,way better in better protocol simpler, when dealing with multiplayer in better application thousands, is faster in faster slower reliable,","1946932,5561668,20276248,6187535,9552483,4086350,31849345,39556790,14199053,55001445,42528647,","btw i concur that udp is far more appropriate than tcp in this case,so assess the situation the development cost of a udp transport is higher to significantly higher than tcp and to some degree you are re-inventing tcp,with tcp this isn t too hard with udp gets more complicated because of the fact that tcp packets are stored and put back in order before being delivered to the socket.,note that if tcp loses too many packets the connection dies;thus udp gives you much more control for this application since udp doesn t care about networkt transport layer drops,what s currently baffling me is in my results tcp finishes almost 2x faster than udp,with tcp its slightly slower than udp and has more features,tcp is a slower more reliable protocol than udp is;in comparison udp is much faster and efficient;for example tcp has much more flags window-length syn ack etc - and also starts and ends a connection in a very stable way - the three way handshake - while all udp has is source ip dest ip length source port dest port and checksum,while udp has less network overhead than tcp it generally relies on you the developer to come up with your own mechanisms for flow control fragmentation handling lost packets etc.,tcp is way better then udp for that,first tcp is not the way to go if you plan to have thousands of user to keep in sync you have to go with udp that is a far better protocol than tcp when dealing with multiplayer,tcp is but udp is faster,"
"tcp","udp"," is not supported natively overall,faster in faster slower reliable,much slower in faster slower reliable,faster in faster slower reliable, sockets thus overall,better in better protocol simpler,lower performance in performance tracks redelivers,more accurate overall, not overall,also more work in likely work quantities, licensed per server overall,","33486181,7603776,42298194,16589674,53650076,16924352,33396509,22241001,2623806,2364616,50767537,","file acceleration software goes over udp instead of tcp essentially masking the latency and increasing the speed of the file transfer;one negative to using this approach is that your clients will need to download special software to download their files since udp is not supported natively in the browser but you mentioned they use a download manager already so that may not be an issue,from experience i can tell you udp is about 10-15 faster than tcp on dedicated and udp-tuned networks,tcp has to do a lot of error checking to ensure that your packets don t get dropped and so tcp is much slower than udp,i was expecting that udp would be faster but tcp is on average two times faster than udp,multicastloopback cannot be accessed from tcp sockets and lingerstate cannot be accessed from udp sockets thus tcpclient and udpclient are not out of the box json serializable,does udp always perform better than tcp,if you were attempting to beat the performance of tcp by shifting to udp keep in mind that part of the reason you get lower performance with tcp is because tcp tracks and redelivers the lost packets for you,i m aware of the differences in general the facts like tcp is more accurate while udp is more fast,since you mention wow they use udp not tcp;tcp offers stream guarantee semantics with order guarantee and no duplicates,udp is also more work than tcp if you need reliability which is built in to tcp,ems is a broker that supports standard protocols jms tcp rv has focus on low latency like zeromq or maybe akka point-to-point over tcp or point-to-point over udp or multicast over udp licensed per server messages are sent via topic supports topics with wildcard segments that don t require specific routing rules or explicit subscriptions to receive data from new topics after tibco acquired 29west rv seems to be evolved from the 29west low-latency message bus broker for any fans of low-latency messaging 29west and or rendezous rv i would recommend taking a look at zeromq a.k.a,"
"tcp","udp","more in response requests request, not overall,simpler in better protocol simpler, ip is not much overall,transmission slower in faster slower reliable, is always better in server small lua,slower in faster slower reliable, has more in overhead tcp end,lower overhead in faster packets etc, doesn in data mechanism traffic,slower in faster slower reliable,","14649005,18926665,2576102,3366523,9300555,26975048,31891715,2531997,8783507,5330319,11455687,","also remember that dns requests can use tcp if the request or response would need more than 1 udp packet,fwiw dns is based on udp so talking to a public dns server might be a good starting point;apache is an http server http being a protocol built on tcp not udp,udp is simpler protocol than tcp and you can still simulate features of tcp using udp,if you want a reliable as in other parts of the system will worry about errors and retries stream of bytes between the two pcs then tcp ip is not much more complicated to use than udp,maybe one of you guys already sees a problem in the code snippets or have any other suggestion or hint for me why my udp transmission is slower than tcp,you will get very less performance network improvement with udp in small scale usage;tcp is always better,if the data is critical you should go for tcp which is slower as compared to udp which in fact doesn t guarantee the packets will arrive in order or even if they d arrive or not,in the end at the levels where java will operate there s either udp or tcp;tcp has more overhead by guarantees delivery via retransmission and ordering,udp sockets have much lower overhead than tcp because packets are not acknowledged by the recipient,udp doesn t;tcp - used for traffic that you need all the data for,tcp is slower than udp and you ll have to mitigate that in realtime multiplayer,"
"tcp","udp","harder than  in traversal nat harder,socket even more likely in likely work quantities, is not overall, does not in care connection packets,more in level susceptible interface, does not overall, requires more in server small lua,slower in faster slower reliable, is the default overall,more than a  overall,other over  in delivery connection long-lived,","54385736,6527310,5759317,3248382,5978352,5772144,5970686,36317272,32046992,48128604,10633193,","doing p2p nat traversal over tcp is a bit harder than udp,tcp socket is even more likely than udp socket but both work,udp is not a good choice for persistant connections;tcp is a much better choice,for example an image takes 3 packets to transfer udp does not guarantee that all 3 packes will arive at the destination and if they do it does not guarantee that they ll arrive in the same order you ve sent;now for tcp reestablishing a new connection for every file could be done yes but it is not necessary,2 tcp needs more processing at network interface level where as in udp itâ s not,saying udp sucks would be quite ignorant towards a lot of programs i know that would never work with tcp and require the fast and easy data distribution of udp which cont rary to tcp also supports multicast;most of my traffic counting in gb external these days is udp based because udp does not get stuck on a lost packet and list packets can be easily requieried with a specialized higher level application level protocol,reasons udp is used for dns and dhcp;dns - tcp requires more resources from the server which listens for connections than it does from the client,i suppose this is one of the reasons for the misconception that udp is slower than tcp,tcp not udp is also possible though doesn t guarantee application logic actually processes the notification;however udp is the default for snmp for good reasons as described at this so answer,difference between tcp and socks5 proxy socks5 is a general proxy protocol that can do more than a tcp proxy including one-to-many connections listening ports and udp,udp does not guarantee data delivery could be lost so i would use udp multicast for device discovery and open up a tcp connection for data that requires guaranteed delivery;tcp listening port can be advertised via udp multicast so that everyone can connect with each other over tcp,"
"tcp","udp","faster communication than  overall,slower in faster slower reliable, doesn in things transmit rough,just smaller overhead in overhead tcp end,faster then in faster slower reliable, is slower; is faster in faster slower reliable,extremely faster in faster slower reliable,less in better protocol simpler, data have a sequence in faster slower reliable,much more in traversal nat harder,faster in faster slower reliable,","54483831,2290873,17421510,9409379,25350887,5594195,35262131,39079959,26902428,10861938,8110762,","by either i providing a command line argument to mpirun or by ii building openmpi from source with additional flags related to udp or iii possibly there is another protocol support for faster communication than tcp,tcp is reliable but slower than udp while udp is not safe and i have to implement my own fault-handling codes,if you re going to use udp instead of tcp you have to do everything tcp does that udp doesn t by yourself if you need it;one of the things tcp does is transmit pacing -- slow start exponential backoff and so on,udp just has a smaller overhead than tcp but that comes at the cost of reliability,one often finds the argument that udp is faster then tcp,the speed for tcp in comparison with udp is slower;udp is faster because there is no error-checking for packets,udp is extremely faster than tcp which is suitable to stream a user s voice input,the fact that udp s header size is less than tcp s is because is a simpler protocol that needs less header space that s all there is to it,udp does not care and will deliver the packets in this order to the application;in tcp data have a sequence number so the receivers operating system will detect reordering and forward the data to the application in the correct order,it was introduced since the nat traversal for tcp is much more complicated than udp,i don t think you should make the assumption that udp is faster than tcp,"
"tcp","udp","more overall, is better choice in better protocol simpler,less stringent overall,faster in faster slower reliable, does not in data mechanism traffic, doesn overall,more difficult in faster slower reliable,much easier in traversal nat harder,more in overhead tcp end,much better just in better protocol simpler,slower in faster slower reliable,","31235299,15863773,2588997,24726976,14297743,39323901,27344909,20769240,5520735,17652027,41901995,","as far as tcp goes i think tcp is more generally used protocol for more data-centric requests like chat or things that require packet integrity udp tolerates packet loss to lower latency,if you need your clients to stay connected and ready to receive data from server for example a push service you should implement it using tcp;if you want to implement a simple request-response service then udp is better choice,instead you can use udp and implement your own scheme for verification of data that is less stringent than tcp,only when packets can be discarded unordered can udp be faster than tcp,also sending receiving data over udp is much simpler than over tcp and udp does not require any connections,udp doesn t have error recovery if there s an obstacle it will just collide with it then continue;while tcp makes sure that all packets are sent received perfectly no errors so the car just passes obstacles without colliding,note that udp is more difficult to work with than tcp because packets are not always guaranteed to be delivered,as such traversing a nat through udp is much easier than tcp,certainly tcp has more overhead than udp,instead of implementing all these over udp it is much better just to switch to tcp,4 tcp is a slower than udp,"
"tcp","udp","lighter in lighter,general in better protocol simpler,lighter in lighter, is not in better protocol simpler, has the concept in irrelevant sure concept, since here overall,worse than  in worse clients easy,faster in simple faster set,lower in faster slower reliable,easier structured in faster slower reliable, streaming infrastructure overall,","19535121,6461775,38817314,1539576,54425070,16273431,25587789,47929,24880258,8059790,1618621,","yes udp is much much lighter than tcp,it is possible to achieve this with java when nats are traversable but it requires a sophisticated solution for tcp;for udp it is simpler,if you can do everything with udp it is lighter than tcp,if your hardware device is sending udp it should be it s a lot easier to handle both in your client program and on the device itself receiving that data is trivial;tcp is not much harder,the fact that udp isn t connection-oriented is irrelevant;sure tcp has the concept of sessions but both have port numbers and that s really all the nat needs,but it is designed to work with tcp since here package transmission is ensured alle packages are acutally delivered and in the right order;udp is less relieable so there was micro transport protocol designed for it,if your client connections are more or less permanent and not too many clients are connected at the same time tcp is only slightly worse than udp,udp is faster than tcp and the simple reason is because its nonexistent acknowledge packet ack that permits a continuous packet stream instead of tcp that acknowledges a set of packets calculated by using the tcp window size and round-trip time rtt,one can say udp has a lower overhead than tcp because its packets have a smaller header and therefore take less bandwidth to send the payload the data,udp packets are easier structured than tcp packets but sacrifice security for their size,udp does not guarantee that a given packet was actually received so encoding whatever you transmit as a difference from last time is problematic -- you can t know that your counterpart has the same idea as you about what the last time was;essentially you d have to build some overhead on top of udp to check what packets have been received tagging each packet with a unique id -- everybody who s tried to go this route will agree that more often than not you find yourself more or less duplicating the tcp streaming infrastructure on top of udp.,"
"tcp","udp","less reliable in faster slower reliable,faster in faster slower reliable, is probably better in better application thousands,better in better protocol simpler, has flow control and automatically in faster slower reliable,slower than  overall, based code overall,much slower in faster slower reliable,easier in traversal nat harder,connection faster in faster slower reliable,better in better protocol simpler,","17310250,34408250,8018700,26325721,23399507,54483831,12082396,11890330,7374426,46038937,2935622,","udp is way lighter and faster but somewhat less reliable than tcp,i know udp is faster than tcp for various reason,or example you could have the central core of the application written in c++ set up a tcp server then have toolbar apps written in java send command packets to the core program via a dedicated socket connection;come to think of the core program udp is probably better for that,for some requirements tcp is better for some udp,while you already know that udp is not reliable you maybe missed the other advantages of tcp;relevant for you is that tcp has flow control and automatically scales down if the receiver is unable to cope with the senders speed packet loss,i see that openmpi uses tcp ip as default protocol for communication which is considered slower than udp when synchronous communication is not required,udp doesn t care and thus sends less packets;in my experience udp based code is generally less complex than tcp based code,tcp is much slower than udp but when the two machines are not on the same lan udp is not reliable,keep in mind that implementing udp traversal is easier than tcp,i know that in practice this would only happen with a great amount of connection given that processing time of an udp connection is faster than tcp but it could potentially happen,in general the tcp protocol manages the available network bandwidth better than the udp protocol,"
"tcp","udp","better in better protocol simpler,communication much less in overhead tcp end,ip more reliable overall, is a byte in care connection packets,connectionless more in connectionless client contrivance, is better overall,smaller in smaller bigger latency,faster in faster slower reliable,better in better protocol simpler,faster overall, is more in server small lua,","16924212,12082396,47535739,31365356,1442901,23080381,26496961,47127660,32270156,1514244,51274640,","i used iperf on two linux machines to send data using both udp and tcp i found that tcp performs better than udp average 65 better,udp communication requires much less overhead than tcp due to the number of messages exchanged,tldr tcp ip is more reliable than udp but not a 100 iron-clad guarantee that nothing will ever go wrong,tcp is connection oriented whereas udp is not;udp uses packets whereas tcp is a byte streaming protocol,a heartbeat is by nature a connectionless contrivance so it goes that udp connectionless is more relevant here than tcp connection-oriented,in practice this means that tcp is better suited for continuous transmission of data whereas the more lightweight udp can be used when reliability isn t important,the package is bigger than udp s package but smaller than tcp s package,udp should be much faster than tcp because there are no acknowledge and congestion detection,we know tcp is better suited for this but the hardware development decided it s got to be udp,theoretically udp should be be 30-50 faster than tcp because it s missing the extra trip for the ack and has a smaller header overhead however in reality there are many cases where tcp would outperform udp just because of congestion control,tcp will ensure that your client server has received the whole photo properly;udp is more used for content streaming,"
"tcp","udp","really faster in simple faster set, packet is more overall, can operate at line overall,much faster then in faster slower reliable,using  to save in response requests request, does not in faster slower reliable,significantly easier in faster slower reliable,more in header connection size,less overall,harder in traversal nat harder,faster in faster slower reliable,","12530442,26988312,27965575,28290012,322241,5772144,3568783,19350736,9287416,10491782,638446,","udp is really faster than tcp and the simple reason is because it s non-existent acknowledge packet ack that permits a continuous packet stream instead of tcp that acknowledges a set of packets calculatd by using the tcp window size and round-trip time rtt .,note udp doesn t really have a concept of connect just sending and receiving packets;if making a tcp connection is analogous to making telephone call then sending a udp packet is more like mailing a letter,udp certainly is not much faster;both tcp and udp can operate at line speed minus some rather small overhead,udp is much faster then tcp but tcp has flow control and guaranteed delivery,udp doesn t provide the safety that tcp does but doesn t require a response but such responses are part of their protocols not yours;i would suggest using tcp to save you some headache,most of my traffic counting in gb external these days is udp based because udp does not get stuck on a lost packet and list packets can be easily requieried with a specialized higher level application level protocol;tcp has the really nasty habit of getting stuck up to 3 minutes when a packet is lost,udp is significantly easier do you really need tcp btw,udp is more of a fire and forget whereas tcp maintains a connection state,udp is less reliable on a wide area network but in a closed environment of a vm talking to its host you can safely skip all the tcp reliability stuff,but it sounds like you want to do nat traversal over tcp which is a harder problem than udp,for example i read an experiment in which a stream of 300 byte packets was being sent over ethernet 1500 byte mtu and tcp was 50 faster than udp,"
"tcp","udp","significantly faster in faster slower reliable,better in better protocol simpler, does not in delivery connection long-lived,less in faster slower reliable, is more in suitable series sets,better in better application thousands,more reliable in faster slower reliable,same as   overall,much faster in faster slower reliable,more popular in better protocol simpler,safer in faster slower reliable,","3424324,7329214,40365773,799142,12778772,24411213,1898944,51829565,13413608,7400055,28038806,","udp is significantly faster than tcp and is why it is or was used for video and various things back in the day,this was surprising for me as it is well known fact that udp performs better than tcp,udp does not require a long-lived connection so setting up a udp socket is a little simpler;on the other hand udp messages must fit within a single packet for ipv4 that means the other hand udp messages can only hold 65 507 bytes because the 65 535 byte packet also includes header information and delivery is not guaranteed as it is with tcp,udp has less overhead than tcp and is therefore faster,udp is more suitable for streaming media;but if you are sensitive with your music streaming tcp is more secure,hello to all i am developing an application that needs to send a image via the udp socket.i know that tcp is a better protocol but playing with kryonet in java i have learnt that udp is better for this type of application.i have this small class that i have made,what i have thought of so far is that tcp is going to be more reliable than udp and in rmi corba we want network reliability,is it not same as tcp udp,this is the reason why udp is much faster than tcp,udp is more popular in nat punching because provides much better results than tcp,i know tcp is a safer choice but let s assume i can only use udp and i need to ensure i can send packets over at a high rate with no missing packets what should i do,"
"tcp","udp","better in better protocol simpler, isn in message use message-based,socket much harder in harder port events,slower in faster slower reliable,slower overall,bigger packages overall,faster in faster slower reliable, packet is bigger overall, this is a slightly in harder port events,general in faster packets etc,much better in better protocol simpler,","3918611,55478020,6437427,9300555,37656151,9047258,19019042,36560105,8959673,49031129,4416038,","udp is actually expected to work better than tcp in lossy networks or congested networks,the broadcast is done by keeping a list of all the connected clients and their file descriptors updated and by iterating through it to send each client the message you can t do a broadcast in tcp as you would in udp because tcp is a connected protocol while udp isn t;if it can be of any use here is a repo with a multi-user tcp server i made in c for a school project,when writing your server bear in mind that the sequence of events for a tcp socket is much harder than for a udp socket since as well as the normal socket and bind calls you also have to listen and accept,i assumend that the transmission using udp have to be much faster than using tcp but in fact my tests proved that the udp transmission is about 7 to 8 times slower than using tcp,i was going through internet and so and understood that web sockets are encapsulations to tcp which by itself is slower than udp ofcourse at the cost of reliability but i couldnt find much info if websockets or udp would be ideal to implement such a server,the problem is that tcp creates bigger packages of data while udp uses 8 kb of data blocks,we propose to use udp over tcp since udp is faster than tcp,switching our attention from the terminal driver to network sockets i had thought that under certain circumstances if you do a read on a udp-mode socket and the actual udp packet is bigger than your read request you can lose the rest of the packet there too;although a commenter suggests i may be wrong tcp mode sockets on the other hand are obviously hugely buffered,i wish to discover all systems on the same local network as the computer running my program s primary ip address which are listening to a certain tcp port.;with udp this is a slightly harder problem - you could look for bounces but your packet may be eaten or lost as could the reply,when creating a new socket the desired protocol must be specified tcp udp etc. which may go over ipv4 ipv6 etc,tcp is much better than udp in terms of reliability,"
"tcp","udp"," is a bit more overall, doesn in transmission halved the, or localsend the message overall,faster in faster slower reliable, which is more in care connection packets,better performance in better protocol simpler,faster in faster slower reliable, socket send buffer overall, doesn in care connection packets, socket is more overall,faster in faster slower reliable,","2303046,20493106,56793405,47079177,5114332,16442504,5594213,55808163,40705439,56262907,18107414,","udp servers are the only ones which use signal drive i o and at the very rarely;tcp is a bit more complicated,thus maximum throughput possible as compared to udp based transmission is already halved;the tcp doesn t support multicasting or the new emerging standard of multicasting amt,i am using the nuget syslognet.client and i send the udp message this way again this works just fine with udp but when using tcp or localsend the message does not send the flags facility and severity,udp is faster and requires less bandwidth than tcp,http streaming servers will in most cases use tcp as most cases network transport rtsp servers usually offer rtp over udp which is more suited to multimedia streaming where some errors packet loss can be tolerated with the benefit of lower latency and less network overheads,udp will almost always provide better performance than tcp at the cost of reliability,udp is faster than tcp because packets are sent without guarantee of delivery nor order,as long as the send low-water mark for a udp socket is less than the send buffer size which should always be the default relationship the udp socket is always writable since a connection is not required;a related read from the same book tcp socket send buffer and udp socket pseudo send buffer,note that if tcp loses too many packets the connection dies;thus udp gives you much more control for this application since udp doesn t care about network transport layer drops.,opening a tcp socket is more costly that opening a udp socket because opening and closing a tcp socket creates a tcp session whereas opening and closing a udp socket is a local action only,as a general rule udp is faster than tcp due to less protocol overhead,"
"tcp","udp","faster in faster slower reliable,faster in faster slower reliable,more overhead in overhead tcp end,faster than  in faster slower reliable,header more fields in header connection size,lower in smaller bigger latency, is a much overall, gives better performance in better protocol simpler,faster in faster slower reliable,compare with  in guarantees definite packet,faster in faster slower reliable,","31398082,33911237,11428762,53625776,45039592,10624972,41711221,18346341,5978352,26334792,28701982,","i m trying to avoid tcpclient because udp is faster but would this work in tcp since it s streamed,i read in a case where a stream of 300 byte packets was being sent over ethernet 1500 byte mtu and tcp was 50 faster than udp,if they are connected over the internet you could try to use the examples for tcp but tcp has more overhead than udp,udp snat ports generally exhaust much faster than tcp snat ports due to the difference in the algorithm used,you will notice that the tcp header has more fields than the udp header and many of those fields will be populated by information from the handshake,like matzi suggested udp gives you lower latency and lower packet overhead as the header is smaller than tcp but on the downside the delivery of the packet to the destination is never guaranteed ie,tcp is a much larger pain in the .. with udp you can do this kind of thing quite easily swap macs swap ip swap ports fill in the payload checksum if you want or not and send this kind of thing back,while you could use either tcp or udp i think you ll find udp gives better performance,the reason udp is faster than tcp is because there is no form of flow control or error correction,with udp there is no definite way of knowing whether the packet reached the packet destination or not compare with tcp which sends ack packets to let the sender know the packet packet was received,i though that udp was faster than tcp but do you think that tcp will be faster due to the congestion,"
"tcp","udp","simpler in faster slower reliable, is far better in likely work quantities,higher performance in performance tracks redelivers, does not in data mechanism traffic,less reliable in faster slower reliable,abstract than using  in abstract traffic network,causes  to fail overall, which is less in faster slower reliable,faster than  in faster slower reliable, does not in length field redundant,more reliable in faster slower reliable,","45200423,3918611,8796401,18273098,4884364,16664318,14214764,11128247,55879120,26356487,5431991,","first of udp s datagram is simpler than tcp s one,tcp is far better at transferring large quantities of data but when the network fails the network s more likely that udp will get through,as an additional note my suspicion is that you d need to indulge yourself in some pretty sophisticated benchmarks before you could conclude that udp is actually going to have higher performance than tcp for web services,udp does not have any mechanism to ensure data delivery this make it much faster but unreliable without some hand crafted data protection mechanism;tcp you don t have to worry about packet size if you try to send a bigger chunk then possible wil be automatically and transparently split by the operating system for you,and there are no handshakings required udp are pretty much faster but less reliable than tcp,such as 0mq or rabbitmq provide different ways of interacting with the network which are somewhat more abstract than using tcp or udp directly,udp does not;if you have an error that causes tcp to fail which is very rare you have to start over,tcp is stream-based which means there s no guarantee everything will be sent received all at once;you might consider using udp which is less reliable but everything does get sent at once or coming up with your own delimiting scheme,udp would not be faster than tcp in your case if you would ensure that the data is intact and in order in your code,since udp does have a length field in its header it turns out that the same field is now used twice in order to calculate the udp checksum;it can be argued that the udp length in the udp header is itself redundant for the same reason that tcp does not contain a similar field,tcp is certainly going to be more reliable than udp since udp doesn t guarantee packet delivery which is probably why you application is hanging on the receive,"
"tcp","udp","slower in faster slower reliable,less in overhead tcp end,better in better protocol simpler, which is probably in better protocol simpler, is a message in delivery connection long-lived, | more overall,slower in simple faster set,slower in faster slower reliable, does not overall,better in better protocol simpler,continuous processing than do  overall,","9952101,36116201,28819659,16322346,40365773,49283769,21942178,32354639,23009371,7329214,9047315,","that among other things is why tcp is considered more reliable but slower than udp,generally speaking udp has less overhead than tcp allowing you to receive more data but this is not a strict rule and is almost negligible in this context,it is worth nothing that in a link where udp and tcp are sharing the bandwidth tcp is better behaved than udp in that it will try to limit itself to avoid congestion,your best solution though would probably be to switch from tcp which is ill suited to this task to udp which is probably a better match;udp does not have connection state and it s also unreliable in that no automatic retries are attempted,where tcp is a stream oriented protocol ensuring that all of the data is transmitted in the right order udp is a message oriented protocol;udp does not require a long-lived connection so setting up a udp socket is a little simpler,use sudo lsof -i -n -p | more to view view only tcp connections sudo lsof -i -n -p | grep tcp | more to view view only udp connections sudo lsof -i -n -p | grep udp | more,apart from that tcp packets by themselves are not slower than udp packets and data transfer with a simple tcp connection can be faster than with a simple udp connection because flow control and reliable transfer is already integrated and you don t have to reinvent everything again and often worse,the reason i asking this is because i read tcp is slower than udp because tcp ensures order of packets,tcp udp are elaborate protocols rules that govern how the two applications connect exchange data and terminate the connection;udp does not have the connect terminating phase btw.,but when i used iperf on two linux machines to send data using both udp and tcp i found that tcp performs better than udp for 10mb of data,the tcp connection may be that the kernel heuristics for servering tcp connections is more aggressive than for udp sockets since tcp connections require more state and more continuous processing than do udp sockets,"
"tcp","udp"," is much faster; in faster slower reliable,slower in faster slower reliable,faster in faster slower reliable, and not in care connection packets, does a lot in things transmit rough,less overall, is a better in better protocol simpler,simpler in better protocol simpler,faster in faster packets etc, doesn in message use message-based, you open a listener in listener socket imho,","40063433,19081574,12045089,46618471,41999996,20753875,3745138,24559909,18660445,4359645,10114381,","udp is much faster;tcp is slow as it requires 3 way handshake,in doing so the tradefoff is that tcp becomes slower compared to udp,don t think of it as udp is faster and tcp is slower because that s just wrong,with udp there is no guarantee that the packet will be received on the other side and the sender will not have any confirmation of whether it was received instead on tcp there is such a verification;if you need a connection that ensures that the bytes sent are received on the other side you need to use tcp and not udp,tcp does a lot of interesting things to smooth over the rough spots of network-layer packet-switched communication like reordering packets retransmitting lost packets etc;udp is more unreliable but has less overhead,also if you have very limited memory processing resources it is worth bearing in mind that udp is a less costly protocol as it avoids a lot of the overheads tcp incurs due to its inbuilt connection management,udp is a lightweight protocol that by design doesn t handle things like packet sequencing;tcp is a better choice if you want robust packet delivery and sequencing,c++ is not my first language and this is small part of code i can t figure out i ve chosen udp because it is always much simpler than tcp,use socket for tcp and datagram for udp its a lot faster than tcp but less connection oriented,udp doesn t guarantee message delivery - if there s no place in the buffer the packet is dropped without hesitation;if you need guaranteed delivery use tcp and build a message-based communication scheme on top of tcp,hint udp does not have connections so you don t have a listener socket;for tcp you open a listener socket on which connections can be accepted,"
"tcp","udp"," is better in server small lua, this is a bit overall,better otherwise in better protocol simpler,bigger overhead than  in faster slower reliable,faster in faster slower reliable, has a higher in higher feature initial,less overall, or split the payload in bigger payload iot, server not in server small lua,worse in worse clients easy,faster in faster slower reliable,","19719152,5016045,11338428,24881403,4147351,10698323,29083422,24411343,37317726,5485831,8947847,","a small udp packet with a server s current usage level isn t going to bring down a network even one that s already busy;tcp is better at doing that due to lost packets causing retransmits thus creating even more traffic but even then this is won t be a problem for a couple of reasons,for tcp this is easy once the connection is established the firewall will allow connection in both directions;for udp this is a bit more tricky since there is no concept of a session,short answer if you can tolerance with any packet loss udp is better otherwise tcp,tcp has bigger overhead than udp because it needs to add more data to your payload but you are guaranteed that your data will be received in it s destination in the order you sent it and not corrupted;udp does not have this features so it can t guarantee that,udp protocol is unreliable but much much faster than tcp which is most commonly used for communication,tcp has a higher initial cost for building up the initial connection but the transmission is reliable and for long running transactions may turn out to be the fastest;udp is cheaper to create connections but you may have dropped packets,alternatively it can be used over udp which is less hungry than tcp,with udp you cannot send messages bigger than 64kb;use tcp or split the payload yourself into multiple messages which will be extremely complex because messages can be lost,your lua code is a tcp server not a udp one;also remember that udp is connectionless.,also note that it is easy to implement your own stack on top of udp that performs worse than tcp,in that sense reliable udp cannot be faster than tcp,"
"tcp","udp","slower in faster slower reliable, is more or less in faster slower reliable,more in faster slower reliable, is limited in byte overall, is better overall,easier in better protocol simpler,better in better protocol simpler,helpful than  in care connection packets, is faster than  even in faster slower reliable, has lower overhead in guarantees definite packet,generally faster in faster slower reliable,","7287150,4161458,584131,55878521,17898429,33116934,25287117,24101612,27193974,1099807,5862704,","the problem with using tcp is obviously that it is a lot slower than udp,udp is more or less applicable for reliable applications if you take into account that packets will be lost by design,tcp mounts are more reliable and you know you have a network problem much faster than with udp,how i currently send frames i see two problems with this 1 - using tcp means i am slower than udp protocol however udp is limited in byte size,my conclusion is that iphone powers down the antennae which severely affects udp;tcp is better because the phone knows of the open connection and is faster to power up,but with the udp protocol in particular this is easier than for tcp,since loosing some packets doesn t matter but speed latency is crucial udp is much better than tcp,tcp streaming for audio can be less helpful than udp rtp as you d have to turn off nagling,tcp as you know udp is faster than tcp even if udp may miss some,udp has lower overhead as stated already is good for streaming things like video and audio where it is better to just lose a packet then try to resend and catch up;there are no guarantees on tcp delivery you are simply supposed to be told if the socket disconnected or basically if the data is not going to arrive,udp is generally faster than tcp as it does not have to do the overhead checking of consistency that tcp must deal with,"
"tcp","udp","better in better protocol simpler,generally firewalls better overall,more in suitable series sets,smaller in smaller bigger latency,faster comparatively in faster slower reliable,communication connection less in header connection size, is inherently overall, means a lighter in care connection packets,more in implementation instructions complex, is way faster in faster slower reliable, is more in connectionless client contrivance,","9235592,44155229,36464786,41563268,39323928,2896525,12599696,16527055,1100868,56238877,14519655,","for this particular application sending simple data chunk to the client from an index given by the client tcp will not perform any better than udp,note that there are exceptions to the above - for example tcp generally traverses firewalls better than udp which may be blocked due to security to traffic policy reasons so voip speech may sometimes be sent over tcp for part of its journey at least,i ve considered that udp is more suitable for sending a series of discrete data sets but i need the reliability of tcp,udp gives smaller latency with many many issues to discuss here of course tcp gives bigger latency,if i d directly say that udp is faster comparatively than tcp that it is used for such applications,udp communication is connection less as compared to tcp which need a connection,first off you ll want tcp streams not udp datagrams;datagrams are limited in size and udp is inherently unreliable,if you couldn t care less if a packet or two are lost then udp may be a viable solution;less control than tcp means a lighter protocol but you re never sure that the data reached the other side unless you implement your own control,some protocols are more complex because what s needed are some but not all of the features of tcp but more than what udp provides,my question is this udp is way faster than tcp and a delayed packet has no more value than a lost packet in most cases so udp seemed like the best choice,this is why your client can communicate over tcp;but udp is more difficult to handle because it is connectionless and stateless,"
"tcp","udp"," is more overall,faster in faster slower reliable, sctp is a better in better protocol simpler,considerably simpler in faster slower reliable,better choice in better protocol simpler,higher latencies in faster packets etc,less in header connection size,faster in faster packets etc,higher level in higher degree advanced,bigger payload in bigger payload iot,windowing more overall,","18563740,1581034,26352193,3424324,8587512,5423061,39079959,37632320,16388203,3156544,1930483,","udp suits well for passing short messages but for transferring large amounts of data tcp is more preferable,at my company we have found memory mapped files to be much faster than loopback tcp ip for communication on the same box so i m assuming it would be faster than udp too,so if you want a entire file of 93mb to be reassembled by transferring fragmented packets over the network such that no packets are dropped then tcp sctp is a better choice,a udp stack is considerably simpler than a tcp stack,the key question was related to what kind of situations would udp be the better choice over tcp,tcp is subject to higher latencies than udp as it requires the client to send back packet confirmations,why the header size of udp is less than tcp,i am using udp because it is much faster than tcp but sometimes i need the know for sure if the packet reached to the other side in my program i can not use tcp at all so i am sending ack packets,actually we could say that tcp is a higher level protocol compared to udp because tcp includes some advanced features which might be useful .,tcp sockets- guaranteed delivery bigger payload than udp cumbersome to setup for web based solutions,tcp windowing is more expensive than raw udp but if you use udp to go faster and add a custom loss-recovery or seqno ack resend manager then that may slow you down again,"
"tcp","udp"," being defined before ipv4 in length field redundant, was a better in transmission halved the,better in better protocol simpler,higher throughput in higher feature initial,throughput bigger in smaller bigger latency,less chatty in better protocol simpler,quicker in faster slower reliable,better in better protocol simpler,faster in faster packets etc,simpler in better protocol simpler, to transfer quickly principe in faster slower reliable,","26356487,362013,11406148,36317272,10315535,29499218,28679224,14749840,42887950,4701184,33580686,","it can be argued that the udp length in the udp header is itself redundant for the same reason that tcp does not contain a similar field;i am given to understand that the reason for that is also historic and has to do with udp being defined before ipv4 was finalized,voip is not significantly improved by reliable packet transmission and in fact in some cases things in tcp like retransmission and exponential backoff can actually hurt voip quality;therefore udp was a better choice,udp scales better than tcp because of reduced states that need to be maintained in the operating system,what is that key feature in tcp that makes it have much much higher throughput than udp,i am confused why tcp throughput is bigger than udp,in gaming especially fpss udp tends to be the chosen protocol because it s much less chatty than tcp,udp is quicker than tcp but if you re using quickfix you ll be using tcp,i think udp will perform better than tcp gcdasyncsocket in your case video transfer,in a congested network yes udp will send its packets faster than tcp this is because tcp takes then congestion into account using a mechanism called congestion control,you can use udp as well but if you are dealing with firewalls it is probably going to be simpler with tcp,the dns is normally using udp to transfer quickly principe of udp protocol but not reliably;tcp is slower with acks and payload but reliable,"
"tcp","udp","slower overall, is more in overhead tcp end,knottier in connectionless client contrivance,more than  overall, which is much faster in faster slower reliable,better off as  in better protocol simpler,faster in faster packets etc,other over  in delivery connection long-lived,always faster in faster slower reliable,less in header connection size,slower in faster slower reliable,","13245606,90444,3541790,746606,21195527,57191961,9286808,41660496,2280675,39079245,8143223,","please note however that this architecture implements tcp which is much slower than udp and will not work for any type of fast-paced data intensive games but should accomplish your goals given your description above,udp has less overhead but isn t guaranteed delivery;conversely tcp is more trustworthy,normally a server wouldn t need to know the client s address beforehand but udp s knottier than tcp the more usual stream-oriented approach to socket communication in many ways,a great many applications that use udp use udp because udp need the low-overhead low-latency nature of udp much more than udp need the precise reliability of tcp,personally i prefer the client-server model if you re sending tiny pieces of data using tcp;anything much for example game data video data etc would probably require udp which is much faster but a lot more unreliable which is what skype uses,again this would be better off as udp instead of tcp but someone may be trying to work within strange parameters is bending the rules about what type of traffic to advertise as in order to get around some issue,for instance zeromq can leverage udp multicast to run faster than any tcp protocol but the application programmer doesn t need to learn a new api,note udp does not guarantee data delivery packages can be lost so use udp multicast for device discovery and open up a tcp connection for data that requires guaranteed delivery;tcp listening port can be advertised via udp multicast so that everyone can connect with each other over tcp,also see this other so answer about the misconception that udp is always faster than tcp,getting much more information is needed in the packet header for connection less like udp but why the header size of udp is less than tcp,tcp is a bit slower than udp but more failsafe,"
"tcp","udp","more reliable in faster slower reliable,much more in implementation instructions complex,better in better protocol simpler,more efficient in faster slower reliable, becomes a better in better protocol simpler, is used org.apache.catalina.tribes.transport.replicationtransmitter overall, generates more network in abstract traffic network, is not in faster slower reliable, has a higher in better protocol simpler,more popular in bigger payload iot, does not in faster packets etc,","31179758,9235592,30156943,44406907,9249025,26012210,48344452,31535813,10698323,44773502,28548048,","tcp ip is supposed to be more reliable than udp ip see this comparison,+ consider that the implementation of tcp stack is much more complicated than udp more instructions are executed there,i know tcp is better to send file but i have a homework about sending file via udp protocol,it seems like udp will more efficient than tcp,tcp is probably your best choice;if you are designing a high performance game where you have to allow for dropped packets and handle latency better udp becomes a better option but then you have to take into consideration what happens when you drop packets and have things like out of order packets,udp is used by default for group membership and can be swithed off to tcp for example if you are running in a cloud environment where udp is not suitable;for session replication on the other hand tcp is used org.apache.catalina.tribes.transport.replicationtransmitter,compared with udp tcp generates more network traffic when the cluster size increases,but udp is not reliable the packet might get lost;tcp is reliable is connection oriented but is more complex,udp protocol depends on the type of connection you need and your application s capacity to deal with dropped packets etc;tcp has a higher initial cost for building up the initial connection but the transmission is reliable and for long running transactions may turn out to be the fastest,but there are some cases especially in iot domain udp is more popular than tcp for its bigger transport overheads,udp is a datagram protocol where each packet means a single entity independent of the other packets udp does not detected duplication reordering etc;tcp instead is a stream protocol that is the whole transfer consists of a single unstructured octet stream similar to a large file,"
"tcp","udp","better in better protocol simpler,faster in high faster reliability, datagrams causing ip fragmentation in ip ethernet problems, is more overall,lower overhead in overhead tcp end,faster in faster slower reliable,reliable than  overall, sockets are not necessarily in ip ethernet problems,faster in faster packets etc,better in better protocol simpler,faster reliable communication in faster slower reliable,","35549074,28675216,787579,55097724,584128,33426807,50003363,774698,8930717,43107922,4361953,","if you care a lot about efficiency or really need tens of thousands of connections then implementing your specific protocol in udp will always be better than tcp,there is a perception that udp is faster than tcp but i think it depends on the situation - take a look at this discussion for some further discussion on speed reliability etc between udp and tcp go down through all the high scored answers,i haven t seen this one per se but i have seen similar problems with large udp datagrams causing ip fragmentation which lead to congestion and ultimately dropped ethernet frames;since this is tcp ip i wouldn t expect ip fragmentation to be a large issue since it is a stream-based protocol,udp bob i love u amy received ovi l e tcp is more reliable than udp with even message order guaranteed that s no doubt why udp is more lightweight and efficient,udp has a much lower overhead than tcp,udp is not always faster than tcp,my code as follows sentences why tcp is more reliable than udp words nltk.word_tokenize sentences print words tags for i in range 0 len words tags.append nltk.pos_tag words i print tags however my result is why tcp is more reliable than udp w nnp h nn y nn t nnp c nnp ...,this description is only really valid for tcp ip sockets however;the udp case is simpler and quite different since udp sockets are not necessarily connected,is sending packets via an established tcp connection after all hand shaking has been done a method to be faster than udp,if you re more interested in latency and small amounts of data then something udp based could be better than tcp but you d need to build extra logic for ordering the messages and retrying the lost ones,http is an application layer protocol which could be encapsulated with a protocol that uses udp providing arguably faster reliable communication than tcp,"
"tcp","udp","more overall,more secure in faster packets etc, is more in faster slower reliable,faster in simple faster set,absolutely faster in high faster reliability,faster in faster slower reliable,slower in faster slower reliable,better overall,connection less in header connection size, is not in irrelevant sure concept, is more in listener socket imho,","27706980,22152967,13597029,12855045,21363292,18811601,21729285,17509301,4533720,15336648,12176555,","the downsides are that it is unreliable messages can be dropped or corrupted therefore you may want to add some safety and that some isp may restrict udp more than tcp test yourself to be sure but usually there is no problems,the decision on yours the tcp protocol used for connection oriented network that is more secure than udp,plus tcp is more complicated because you have to handle the protocol stuff;so when you create your system you have a choice - either tcp or udp packets depending on what you are trying to achieve and how you want to go about your system,in a native application i would use udp for the most data player position ... because it s way faster than tcp and it s uncritical when it is lost,if the network between the two point have a very high quality udp is absolutely faster than tcp but in some other case such as the gprs network tcp may been faster and more reliability than udp,because there is no confirmation on udp packets it s slightly faster than tcp,try to increase timeout value tcp is slower than udp,i did some research and found that udp may be more suitable in this case because udp hole punching works much better than tcp hole punching,udp is connection less but at the same level as tcp,you can use udp tcp or sctp session well udp is not much of a session layer on top of it;tcp is the general choice,instead you add a listener which is tcp and a listener which is udp;imho udp is more useful if you use a broadcast or multi-cast address,"
"tcp","udp","more reliable in faster slower reliable,harder in harder port events,more reliable overall,reliable than  in faster slower reliable,simpler in better protocol simpler,less overhead in faster slower reliable, is more overall, is inherently in level susceptible interface, shouldn in ip ethernet problems,","33455053,5869634,4551201,50327136,4220469,1099695,56469732,34582554,49626141,","tcp - more reliable than udp but this comes with some overhead there is a distinct connection a better match for games which require less frequent data transmission such as turn based games as is your game,udp port scanning is possible but it is harder than tcp scanning,you could get them to do a udp multicast within a lan environment to identify the programs using protocol messages then have a stored cache of each other s identity and then use tcp to connect and do main exchanging of messages which is more reliable than udp,it s very easy to spend a lot of time making a reliable layer on top of udp that is both slower and less reliable than tcp,you ve struck lucky with the requirements - because you re going from udp - tcp it s actually a lot simpler than doing udp - udp,most importantly you can easily supplement udp with some reliable delivery hand-shaking that s less overhead than tcp,udp is more of a wide net whereas tcp is specific addressing,udp is inherently susceptible to packet loss and the lost packets cause distortion;tcp is not lossy and if there is some delay client-side buffering in the pipeline you should be able to avoid distortion due to jitter caused by occasional packet loss and retransmission at the tcp level,for udp it s the same only the ip type is 17 instead of 6;tcp udp shouldn t be directly embedded into an ethernet frame,"
"screens","tablet"," is smaller than 600 pixels;typically in size phone mobile,less in size phone mobile,much smaller in tall wider smaller,bigger in big bigger images, width is more in size phone mobile,smaller in size phone mobile,wider in size phone mobile,smaller in size phone mobile, shouldn in larger smallest but,smaller than  in size phone mobile,more overall,","42521852,31435985,24184394,46184983,54079224,24329917,33240193,35498315,25057095,48444052,47318184,","this will change the height of your element with tabmiddle as the id to 1500 pixels high whenever the screens is smaller than 600 pixels;typically a mobile tablet device,on a tablet or phone which may have less than 750px screens width it will go off the viewport,the screens is much smaller so you can not present your app the same way as you can on a tablet,this tablet has a bigger screens yet is still showing only 2 child views in the row,this is a result for a responsive bootstrap4 carousel on mobile and tablet but when the screens width is more than 676px it is no longer a carousel,twitter desktop accomodates screensizes from around 1048px wide when you are on a screens that is smaller then that you are probably on a tablet or smartphone and if you open twitter from a browser on your phone you are instantly redirected to and prompted to download the twitter app,when viewed on a screens wider than a tablet to be responsive according to the smaller screens size like this,based on my understanding i hope that is because the size of the tablet screens which is smaller than the desktop that makes the strokes to get hidden inside the area that was not visible on the tablet however the area is visible in desktop,well sure but that screens is far larger than 600dp in its smallest width;but what ever changes i make for this enormous tablet shouldn t affect normal tablet which would happen if i change stuff in that folder,so i was trying to make my modal only work on screens smaller than tablet,they are for example too big in a phone and too small in a tableth because the screens of tablet has more centimeter than the pone screens,"
"screens","tablet","more in size phone mobile,often lower density overall, it is wider in size phone mobile,less full overall, size is much bigger in size phone mobile, is larger in size phone mobile,wider in size phone mobile,bigger in big bigger images, is less overall,different layout by  overall, i don t in big bigger images,","40697207,18306525,13239133,20360940,23130737,56182759,33235334,18123310,50854957,32750659,19448254,","i have been working on this page that works fine in desktop devices and tablet but not on phones because the width of the page is more than the screens width i need to make this width responsive to have the page with height-only scroll,this is because that tablet often has a lower density of screens pixels,in portrait mode your phone screens is smaller than that so screen_is_narrow is true whereas in landscape mode and on tablet it is wider and thus screen_is_narrow becomes false,as an example my android tablet s stock video player app has a feature that allows it to be less than full screens,on tablet the screens size is much bigger and there is enough space for most of the action items that s why your menu is placed on top,however when the screens is larger tablet and above i would like two slides to appear side by side,you could make your css mobile-first meaning that all properties are optimized for mobile then as the screens gets wider you apply tablet and desktop optimized css for elements,if the screens is bigger than 1024px it should be green my tablet screens is but the background stays yellow,in the css i replaced the floats with flexbox in the form the height is 100vh so your form will take 100 height of any screens .left and .right have a flex-basis of 30 and 70 keep using relative units for better responsive design than using pixels also use media queries for mobile and tablet i used one breakpoint here to stack the two parts in a column view if the screens is less than 480px i will link a refrence to flexbox it s a very useful resource also you can see this code in the linked codpen,if you want to force all the views to have the same ration on screens the only way is to give it a size dynamically after you calculated the ratio between the desired pixels and the screens density which is not really a solution what you need to understand is when working with various screens sizes each one suppose to behave differently a tablet screens is expected to be bigger and thus having more room for elements to show up on screens a phone screens is smaller and less elements should be showed on screens you need to adjust your view to support all of a phone screens and the proper way is to write a different layout for tablet and phones or even different layout by screens dpi,usual practice for android platform is to provide images only for different screens densities not for screens sizes;if i use tablet i don t want to see big images i want see more information,"
"screens","tablet","smaller in size phone mobile, is wider in tall wider smaller,higher consider as  in size phone mobile,much smaller in size phone mobile,bigger desktop in size phone mobile,diagonal greater in size phone mobile, size not in size phone mobile, that is bigger in big bigger images,more room in space available fonts, density is higher in size phone mobile, width is more in media mobile solution,","17474847,34345594,57050917,28425758,26487177,30998774,17585778,38964329,30422496,46746178,36638122,","what i would like to do is use css media queries to have the title be above the tabs when the screens gets smaller or on a tablet or iphone,if the screens is wider than it is tall then we infer that the device is in a landscape orientation and vice versa for portrait,if you want to decide device is tablet or phone based on screens inch you can use following device 6.5 inches or higher consider as tablet but some recent handheld phone has higher diagonal value,however the number of people using these giant tablet is much smaller than the number of people with small screens laptops,my aim is to use a bootstrap 3 dropdown to display links at mobile screens size and use a list to display the same links when the screens is bigger desktop and tablet size,to determine device is tablet or mobile i use criteria if device s screens diagonal is greater than 7 iches the device is tablet,in android we use resolution in dp to measure the screens size not resolution in px;both of your two tablet have the same resolution in px but their resolution in dp are quite different,suppose you were to choose a new tablet and you wanted a new tablet to have 1 a big screens and 2 a screens that is bigger than 20cm,it simply comes down to the space available a tablet gives more room to developers thus allowing developers to put more on one screens,your device s screens density is higher than xhdpi and your element does not list higher density options than xhdpi;when i say tablet i mean devices with aspect ratios that are leaning towards the more square size in other words not 16 9,here i am using css media to hide the modal when the screens width is upto 598px mobiles tablet;if screens width is more than 598 px desktop laptop then the modal will display,"
"screens","tablet"," has a lower overall,width smaller in size phone mobile,simply more in space available fonts,more in space available fonts,size reasonable bigger in size note devices,size much smaller in size phone mobile,such as a  in size phone mobile,smaller then in size phone mobile, width is larger in size phone mobile,size smaller in size phone mobile,less in size phone mobile,","25234859,22564398,23608495,43912613,29104954,23130737,52478359,40391766,41573969,43424851,41948896,","ote android also supports low-density ldpi screens but you normally don t need to create custom assets at this size because android effectively down-scales your hdpi assets by 1 2 to match the expected size;in your case whats happening is that your galaxy tablet has a lower pixel density and android down-scales the image from a xxhdpi to whatever density the tablet has hdpi or xhdpi ....so the tablet your image is a 512px image android would down-scale it to 341px for xhdpi or 256px for an hdpi device,this will only be added if the screens width is smaller than 480px else the tablet version would show the mobile version,you will also notice that the fonts are not scaling as the 7 tablet simply has more screens space available vs the 4 phone,a tablet offers more screens space than a phone,having different ones based upon screens size is reasonable bigger margins on tablet though the mix then would be something like res values dimens.xml and res values-sw720dp values.xml,splitactionbar works only on phones because the screens size is much smaller than tablet and it takes space at the bottom for extra space for your action items,an xlarge screens is defined as a screens that is significantly larger than a large screens such as a tablet or something larger and may require special care on the application s part to make good use of it though it may rely on resizing by the system to fill the screen. the default value for this actually varies between some versions so it s better if you explicitly declare this attribute at all times,inside the container i have a panel with the text set to left but when the screens gets smaller then a tablet to something like a phone i want the text in the panel to center,another layout for larger devices like tablet where the screens width is larger than your image,for example when the screens size becomes smaller to a tablet or mobile you could do this as an example,the issue is that the phone screens height in landscape mode is likely much less than a tablet or pc,"
"screens","tablet"," is smaller overall, as well overall,such as a  in larger smallest but, or not overall,larger standard overall, is smaller in size phone mobile,always smaller in size phone mobile,smaller than a  in size note devices,width larger in size phone mobile, is less in media mobile solution, has higher overall,","54162918,32532101,37931171,29269371,40848484,53253650,13551919,52485008,46595075,46633191,14034137,","here s what for example a form looks like on my site with a computer i would like that when the screens is smaller tablet like ipad or mobile the name of the field for example email go alone in a line and take like class col-sm-12 and the field who is related to the name field go on the line of underneath and also take col-sm-12. as the screens of a mobile is quite small i would also like to have a single field per line and not 2 as is currently the case in the case where the user uses a mobile or tablet,in you manifest xxhdpi screens aren t listed in the supported screens as well as xxxhdpi ones;tablet usually are hdpi or even mdpi in density number of pixels per square inch,an xlarge screens is defined as a screens that is significantly larger than a large screens such as a tablet or something larger and may require special care on the application s part to make good use of the application s though the application s may rely on resizing by the system to fill the screens,below method is calculating the device screens s diagonal length to decide the device is a phone or tablet;the only concern for this method is what is the threshold value to decide weather the device is tablet or not,in fact when a tablet has a larger than standard screens size the tablet with detachable keyboards available these days mobile versions of the site can look over-optimised for space which is the reason why browsers allow tablet users to opt to display sites in desktop mode,what i am looking for is a way to show the desktop view possibly a larger desktop when using the builder on a smaller laptop or tablet so the width of the users screens is smaller than what the desktop view would be,that hopefully explains why a typical 320dp phone screens is always smaller than a 720dp tablet screens although the smaller screens can have more pixel than the larger,note while it would be nice to allow all screens sizes for this application it is not currently possible for devices smaller than a tablet to have a usable experience,in my actual code i ve set the display to none if the screens width is larger than tablet size because that s the only time i feel like i need the button,you may want to have a different layout for your image when you are in a mobile or tablet so you can add this media query at the end of the css document;so the new css class img will override the old one when the width of the screens is less than 768px,by contrast a nexus 7 tablet has higher actual screens resolution than an ipad mini but a nexus 7 tablet reports to your web page a smaller screens size,"
"screens","tablet"," will display part overall, please someone overall,less than the  in size phone mobile, is made smaller in size phone mobile, have high dpi overall,smaller in size phone mobile,higher in size phone mobile,higher resolution in size note devices, and is cross-browser in media mobile solution, is smaller in size phone mobile, layout not overall,","21575197,7607934,56709520,54056265,15238626,32098658,23728938,28366729,55657235,54894025,23611936,","the data looks good when the screens size is bigger than 600px so i add a breakpoint at 600px and hides 1 less important column when the screens size is smaller;devices with big screens such as desktops and tablet will display all the data while mobile phones with small screens will display part of the data,also other android tablet phones can be used with pens even if they re technically not made to support pens but those pens are just special styluses that emulate the capacitive touch of your finger so with those devices the screens can not tell the difference between your fingers and that pen so you can not assign them different functions which is definitely not as good;as far as i know only htc supports a pen on some their android tablet please someone correct me if i m wrong i may not be up-to-date on these things,despite the screens size being less than the tablet size,i m trying to create my first website and ive added content to it however when the screens is made smaller to that of a tablet or mobile screens size it doesn t scale down properly and leaves everything disorganised,using px as a measurement would actually be worse - as new tablet have high dpi screens meaning 100px is going to be visually smaller on a tablet than 100px would on a mid spec phone,this only happens when the window is on a computer in full screens mode but when the screens is made smaller to a phone tablet size the divs function how they should,screens resolution i.e higher than tablet i.e col-md and col-lg in bootstrap language should only cover 11 grid with offset of 1,i can t use screens size because of devices like galaxy note with huge screens and some phones have higher resolution than some tablet same with dpi i think,this solution fits all screens and is cross-browser compatible the above method will equal cells height and for the smaller screens like mobile or tablet we can use the media method mentioned above,when the screens is smaller tablet phone they are side by side,use relative values for the screens layout not pixels or dp;the root cause is that one of your tablet is reporting an incorrect pixel density,"
"screens","tablet","higher in size phone mobile,size smaller in size note devices,","24642555,18287856,","i know that obviously a tablet has a higher screens resolution than a phone because it has more pixels dots on the screens from what ive been reading tablet also have a higher dpi dots per inch than phones as well,basically my layout has 1200px grid width but i figured that there will be a problem with 1024px screens resolution 20 of the population bla bla so i created media queries when the screens size is smaller than 1199px the grid to change its width to 960px and so on for tablet phones etc..,"
"html5lib","lxml","better job overall, s the most overall,slower rate in faster parser broken,faster parser in faster parser broken,parser generally faster overall,","37009855,16269242,29017860,34370585,37933417,","html5lib parser does a better job than lxml or html.parser handling the debate element in this case,try installing lxml which is more lenient and much faster;if that doesn t work html5lib is your best bet as that doesn t work html5lib s the most lenient but also the slowest,the standard html.parser option handles broken html less well than other options while the html5lib option is closest to how a modern browser would handle broken html albeit at a slower rate than lxml would handle html parsing,lxml is the faster parser and can handle broken html quite well html5lib comes closest to how your browser would parse broken html but is a lot slower,lxml parser is generally faster html5lib is the most lenient one - this kind of difference would be relevant if you have a broken or non-well-formed html to parse,"
"string.format","tostring"," has a bunch overall, no formatting etc; overall, is faster overall, to do something overall,","12319077,9289339,20542163,9505126,","for formatting a single numeric value tostring is marginally more efficient than string.format because string.format has a bunch of overhead to parse the format string out of the curly braces and then pass the format string out of the curly braces to tostring,it provides no conversions beyond calling tostring no formatting etc;string.format is a much richer allowing format patterns etc,changing his code for string.format to use iformatprovider;change everything again without tostring is faster for 200 nanoseconds,you might as well replace showexp with tostring which is declared in object unless you want tostring to do something different;using string.format is a little cleaner than concatenating several strings in my opinion,"
"django","web.py","easier overall,more low-level overall, is considerably more overall,more suitable overall,","4759565,11982403,12399834,18149831,","web.py has a templating language of it s own it looks easier than django s,web.py is more low-level comparing with django,but you should really accept blender s answer as flask and others like bottle and web.py are the right way to accomplish your task;as you ll see the simple way in django is considerably more involved,i m new to sever-side programming.some people told me that web.py is more suitable for beginners like me than django,"
"addition","exponent","smaller operand overall,clearer overall,","34136689,3284371,","since fp addition shifts the smaller operand s mantissa until both operands have the same exponent you can add a certain magic number to force it,in addition for doing powers of two bitshifting is usually a little clearer than doing exponent though choose whatever works for you,"
"liblinear","libsvm","faster overall,more scalable overall, is not easily in goals mind different, is not easily in goals mind different,far faster overall,faster overall,","22134252,13275267,23507404,23507404,44401270,11508788,","liblinear is considered faster than linear libsvm and often used for large scale data set,you can also try sklearn.linear_model.logisticregression and sklearn.svm.linearsvc both implemented using liblinear that is more scalable than libsvm albeit less memory efficients than other linear models in scikit-learn,not to mention that the algorithms are coded with different goals in mind libsvm is written in a way to allow switching between different kernel functions while liblinear is optimized to always be linear and have no concept of kernels at all;which is why libsvm is not easily applicable to large scale problems even with a linear kernel and often it is suggested to use liblinear when you have large number of instances,which is why libsvm is not easily applicable to large scale problems even with a linear kernel and often it is suggested to use liblinear when you have large number of instances;furthermore regarding multi-class problems with k classes libsvm by default implements the one-against-one approach by constructing binary classifiers while liblinear implements the one-vs-the-rest strategy by constructing k binary classifiers it also has an alternative method by crammer and singer for handling multi-class problems,and i ve read that using liblinear is far faster more memory efficient for such tasks as such i ve ported my libsvm classifier to accord net like so,and how do the differences make liblinear faster than libsvm,"
"linux","ubuntu"," is actually really overall, not overall, is not overall,better in better alpine docker,hyper v with a  overall, cygwin is smaller overall,distribution far more overall, convention is simply overall, isn overall, is a bit more overall,however less forgiving overall,","5523339,37054078,2535829,42328333,55693335,2535830,3281759,27241197,52148514,37011229,38578513,","you can download vmplayer and ubuntu and get started really quickly;linux isn t as scary as it used to be and something like ubuntu is actually really easy to use,i suspect this is because the binary was compiled on ubuntu not on amazon linux;i also setup amazon linux that aws lambda based on red hat actually runs on,i used virtualbox + ubuntu + eclipse for several projects i worked on;if you decide that linux is not for you and your project was in eclipse then you will have no problem switching back to windows since eclipse is available for both operating systems,you can use alpine linux docker image it is very light and is better than both ubuntu and centos image to be used as the docker,now i m using hyper v with a ubuntu linux instance to run all my containers because docker in windows was giving me headaches,if you just want the fun of linux command line programs without access to all of ubuntu cygwin is smaller and might be faster,sometimes a linux distribution contains far more than an operating system but is informally referred to by a single name such as ubuntu and so the line between what the operating system is the linux kernel and standard libraries perhaps and the applications that merely ship with that operating system the gnome and kde environments on linux is pretty gray,though ubuntu has more focus on getting apps running regardless of platform;know that linux has no single convention but linux convention is simply derived from pragmatism not by theory,prebuilt distribution packages are only available one some of those architectures though all of centos debian fedora and ubuntu are available for 64bit x86 and ubuntu packages are available for all supported architectures;lists the supported architectures and 32-bit x86 linux isn t one of them,it depends on what distro you re using but for example you can shut down lightdm on ubuntu using sudo etc init.d lightdm stop;unfortunately installing the nvidia drivers on linux is a bit more complicated than it should be so i would recommend that you have another device handy that you can use to search for solutions to problems you may encounter along the way,linux however is less forgiving and has a filesystem that is case sensitive at least my dev ubuntu version does but i guess it is the default for linux,"
"linux","ubuntu"," is even better overall, is the way overall,general overall, does not overall, included in newer overall,familiar with  overall,similar with   overall, is not as overall, works better overall, runs much overall, works faster overall,","4924571,2985463,53593499,12952211,28853038,48730308,57095255,36323038,51197793,6058730,53905441,","however linux pretend i said ubuntu is even better environment for ruby on rails development and ruby on rails development installs great in vm,ubuntu is the more user-friendly of the two i think ubuntu is actually one of the most newbie-friendly linux distros so if you are new to linux ubuntu is the way to go,windows xampp had a lot less to configure compare to mac ios but now with linux ubuntu i had a few more since there are more going in linux a good thing,linux mint is an operating system based on the linux distribution ubuntu;linux mint adds many features that baseline ubuntu does not have,perf 1 part of the linux-tools-common package in ubuntu makes use of a kernel-based subsystem called performance counters for linux included in newer kernels,i currently have a python script at home ubuntu test test.py when this script runs it writes to a file home ubuntu test test.txt i am completely new to cron and not very familiar with linux in general,i am trying out this simple commands in wsl ubuntu and it has a different output if the same commands executed in linux i was expecting that wsl works very similar with ubuntu linux but i m having some difficulty trying to get this to work,afaik currently apple provides this software only for 64-bit ubuntu 14.04 or ubuntu 15.10 make sure you download the correct version;another thing to note is that swift on linux is not as usable as it is on mac os x,but i want to use all of the stuff on ubuntu since linux works better for the stuff i want to develop. everything works fine from the point of downloading anaconda for linux running it in the terminal and installing it,it works fine - in fact i rarely boot into windows anymore;ubuntu runs much quicker - even my wife prefers it;it s a bit of a leaning curve using a linux desktop - but that s what i wanted anyway,some details can be found at consider using linux distribution for development;event virtualbox vm with ubuntu works faster than windows for all sap hybris related tasks,"
"linux","ubuntu","out-of-date than the  overall,familiar with   overall,faster overall,numerous things with  overall, it works perfectly in arch slower machine, that supports the newer overall,fuser -n overall, ami has a lot in ami familiar aws-cli,simpler in distro shortcut instructions,  directly overall,much easier overall,","50990949,54024620,46994413,68674,47859948,2197183,4925111,40053023,39281563,52357712,39520120,","you now have three clean options a standard ubuntu package docker.io a snap package from docker inc which has some restrictions on where your dockerfile is and is 5 months more out-of-date than the ubuntu package but works on more linux distros the official docker-ce package from docker inc which requires their private repo and more install steps for more helpful instructions details and references see askubuntu docker-ce or docker.io package,disclaimer i m not very intimately familiar with linux ubuntu docker etc,i m testing some simple benchmarking calculations on win7 and linux ubuntu 16 to compare the timings and being wonder win appears to be much faster than linux,ubuntu is getting closer but there are still numerous things with linux that will keep the grandmothers of the world from using ubuntu,and what is even more worriyng under linux platform such as manjaro or ubuntu it works perfectly fine,for windows and mac osx not for linux;you will need to install an svn client on ubuntu that supports the newer repo format,one one ubuntu distro fuser -n tcp 3000 will report a process if i own the process yet on another linux distro i think centos it won t report the process even if i own it,the ubuntu 14.04 ami doesn t have the aws-cli tool installed nor the unzip command out of the box nor docker for that matter -- so you d need to read over and run in order to get up and running;the aws linux ami has a lot more bells whistles out of the box,see but if you are using a linux distro there may be shortcut instructions that make it simpler under ubuntu for example there are shortcuts in,i am on windows 10 and have installed linux ubuntu directly from the store using the guidelines given here i also have xming and have set display 0 in my bashrc file. programs such as firefox and pycharm run just fine but vscode does not run. it s not the same as this issue and the produced error messages seem different,even if you re on linux it s much easier to get a precompiled version sudo apt install r-cran-rgl on ubuntu if you have the appropriate cran repositories set up,"
"linux","ubuntu"," is alot more overall, kernel using this config overall,easier other in distro shortcut instructions,familiar with  in ami familiar aws-cli,much easier overall,more familiar in ami familiar aws-cli,slower in arch slower machine, is better if you only in better alpine docker,things worse overall, is way more overall, package is just more overall,","3436045,50064565,1383228,51562602,34846588,34131232,38866429,13053667,46774580,24904805,54010640,","i started using ubuntu for rails development a few weeks ago and have noticed a big improvement over windows although i still dual boot due to the following;terminal in linux is alot more robust than command prompt,the best solution to boot fs mode is the following but imho ubuntu is not a good option because systemd slows down the boot phase a lot;compile the linux kernel using this config file,although if you aren t interested in using adobe cs you can use ubuntu distro which is easier than other linux distro and quiet popular so you won t have any problem finding solution,i got a etherdelta-like decentralized exchange script and decided to install it on cpanel as i am not very familiar with linux commandline as the installation instruction is done with ubuntu commandline,also i would recommend doing a dual boot to ubuntu it s much easier to work with opencl in a linux cli fashion,i am new at a firm and i was given a project that was on amazon linux ami now we need to transfer project on another server and because i am more familiar with ubuntu than with amazon linux ami i was wondering if there much difference and will project work perfectly,i don t know why arch linux is slower than ubuntu on your machine,so hard i recommend you uses the 32 bits if you want programme on linux;and in my vision the ubuntu 32 bits is the better plataform to developer to android how to i need make androoid and ios apps i m working on mac but the ubuntu is better if you only programmer on android,also very particular that this should be done on centos or amazon linux which makes things worse for me as i have been on ubuntu for long,ubuntu is simply not intended to be used like that for a long time;puppy linux is way more suitable for your needs since it is originally created to live on usb drive forever,one macos the other an ubuntu 16.04-based linux os elementaryos loki both using the official packages from docker.com;my theory is that the linux package is just more restrictive out of the box security-wise than the macos one,"
"md5","salt"," is not in secure security secondly, any easier in user string visitor-supplied, -hash isn overall, is a better in user string visitor-supplied,general in secure security secondly,stronger in random ways salt,more fancy overall, is more in user string visitor-supplied, it taken only once overall, you cannot in random ways salt, should not overall,","27667591,349805,10267807,317990,15676683,9542772,1459923,16146593,55716229,25353651,34275278,","firstly hashing password without salt will defer the security;secondly md5 is not considered secure any more so usage in new application is not recommended,the time-based salt will not make md5 any easier to break;you re still relying on 1 the user having a good password to defeat brute force calculations and 2 md5 being a decent hash,regarding the question sha1 with salt works fine for most use-cases unless you write software for the cia or such;the point is that even salted md5 -hash isn t worth to get cracked on most sites,by computing a hash from a visitor-supplied string plus some salt of course i can tell whether the user provided the same password twice without the security risk of allowing my application to be able to decrypt the provided password possibly maliciously;my sense is that encode and decode are probably good solutions when you want the data to be recoverable but that unrecoverable hash using crypt md5 is a better approach for stored passwords,an unsalted md5 hash is not much safer than storing,encrypt the passwords with one-way encryption algorithm with a random salt like the common opinionï¼š sha-256 is stronger than md5,decode the passwords yourself and re-encode them with a salt i recommend something a little more fancy than md5,that is when the user creates a new account and enters a password you build a string from the password plus user name plus a salt which is just a small string that s the same for everyone on your site--this is to thwart rainbow table attacks;then you run that string through a good hash algorithm--something like sha1 is fine even md5 is more than adequate despite reports to the contrary,so i will write it partly as pseudo code note that i m merging the password bytes and the salt bytes not the password string and the salt string;then the md5 it taken only once from these merged bytes,md5 is ways too fast and one should include a random salt;because of the salt you cannot just recalculate the hash and compare it with the stored hash,usually a salt is randomly generated for each user and stored together with the password in the same object database row;furthermore md5 should not be used anymore,"
"md5","salt"," that is longer overall, is not overall,slower in secure security secondly,secure than the  in secure security secondly,more secure in secure security secondly,","1109329,14538083,7063356,56267762,29353379,","anyways i want to reiterate that plain md5 hash are easy to crack for most passwords since people like short and easy to remember passwords. use a salt and or a more complex algorithm;i d recommend both and use a salt that is longer than two characters and not limited to numbers,using the mailaddress as salt is a good idea;but using md5 is not,bcrypt is considered the most secure way to implement password hashing with salt because it is slow - much slower than an md5,use -c to create it tried the command with -c flag but still same error docker run --entrypoint htpasswd registry 2 -b test password auth htpasswd -c the sha algorithm does not use a salt and is less secure than the md5 algorithm,using sha256 with a salt will be much more secure than md5,"
"filtering","grouping"," that contain more overall,na  by  overall, yourself is probably less in efficient query categories,fewer items than other  overall,slowier solution with  overall, is larger in greater rows count,more overall, and not overall, is greater in greater rows count, are just overall, is stricter overall,","49245198,55540411,4527030,26959748,48275799,42233596,43834645,48915536,15535029,56480220,54326798,","what we can see here is concat the arrays into 1 list then grouping by the chars into grouping and then filtering the grouping that contain more than 1 element and in the end revert the chars into list of chars instead of list of grouping,we can fill the grouping column to replace the na elements with the preceding non-na elements filtering out the na rows based on the grouping values that are still na grouping by grouping slice the rows until the stop value in the type column assuming one start stop combination per grouping,again this is perhaps more clear than using raw sql and only requires one query but this one query could be very large this one query will return all items and this one query categories and doing the grouping filtering yourself is probably less efficient than letting the database do this one query for you,but since you are filtering by item 59 and you don t have an index on the item column your query will be slow;i suspect that your first query is fast because grouping gr1 contains fewer items than other grouping,use transform with boolean indexing detail another slowier solution with filtering timings setup caveat the results do not address performance given the number of grouping which will affect timings a lot for some of these solutions,you can grouping the rdd by productid and then filtering the rdd based on if the length of the grouping is larger than the threshold 1 here,solr - the collapsingqparser is really a post filtering that provides more performant field collapsing than solr s standard approach when the number of distinct grouping in the result set is high,a bit field or bloom filtering could also be included. row index entries provide offsets that enable seeking to the right compression block and byte within a decompressed block;note that orc indexes are used only for the selection of stripes and row grouping and not for answering queries,a filtering will return only those grouping where the sum of othervalue of the rows in this grouping is greater than zero,if a row matches the regex but has more than expected grouping the additional grouping are just ignored the solution is to use intermediate table + filtering rows when selecting from it,filtering supports the use use of the .data pronoun filtering supports some rlang features filtering supports grouping filtering supports n and row_number filtering is stricter it trigger errors if the input is suspicious,"
"filtering","grouping"," has more overall, is greater in greater rows count, is greater in greater rows count, which has less in greater rows count,general overall,faster in faster collapse solutions, anywhere else overall,original shape after  overall,greater in greater rows count,efficient than  in efficient query categories, which is far more overall,","23596420,23719347,32686930,52587588,12684644,34719319,33784216,48135237,43574945,29653715,4135139,","this would be my first implementation simply filtering out duplicates and seeing if the list shrinks;and here s another using the groupby method and seeing if any grouping has more than one element,use groupby filtering;this will just return the rows of your dataframe where the size of the grouping is greater than your cutoff,ere i am grouping by id and config_id then remove the rows using filtering;if the number of elements within the grouping is greater than 1 and the number of distinct elements in day is equal to 1 we remove id,we filtering the grouping which has less than 9 rows and for those grouping we repeat the last row for 9 - n times and add these rows to original dataframe using bind_rows,but without any useful filtering oracle has to hash the entire table;tbone s query that only use a grouping by looks nicer and may run faster,from my experience collapse filtering is much faster than grouping,for example this will display only secondlist items which have a grouping property equal to selectedgroup.id;you can use a custom filtering if your filtering is more complicated or a filtering function in your controller triggered with ng-change if you don t plan to re-use this filtering anywhere else,you can use elemmatch as a projection operator but this will only bring you first matching subdocument second way is by using aggregation framework here the point is that you need to grouping back your unwinded subscriptions to get back original shape after filtering,i am using groupby on tickets then filtering my dataframe to those records where the count in that ticket grouping is greater than 1 using filtering,please note this approach is much less efficient than grouping and filtering in a dataset query if this approach is based on a database,so if you had a recordset of 1000 rows tags in your db and only 4 potential tags words in your title if you loop through the rows in php using the proposals above- the loop has to run 1000 times to simply identify 4 possible matches...if you move the criteria identification to the sql the loop only has to run 4 times in order to build the initial filtering which is far more efficient;what the loop will also do is automatically prevent against duplicats - though if they exist in your db simply append grouping by tag to,"
"filtering","grouping","faster than  in faster collapse solutions, is greater in greater rows count,more overall, which has more overall, that handles latter scenario overall,faster by  overall,more overall, by is usually more overall, by is giving more in greater rows count, to return only overall,important than  in better index indexes,","49832875,52922040,14151083,54114413,50323936,56418600,29776048,52939219,57300010,54980144,57547054,","of 7 runs 10 loops each jacquot solutions sees to be 10x faster than grouping then filtering,using dplyr we can group_by country and year and filtering negate the rows where number of rows for each grouping is greater than 1 and,2 i used link1 and link2 to filtering out duplicate users existing in more than one grouping,1 find duplicates groupby key fields name and type filtering grouping which has more than 1 elements 2 select ids flatten the grouping select ids putting it all together for example output for above example 1 3 2 5,thus in your code you would use the functionality or role names you whish but behind the scenes everything would be translated in ad grouping membership. i have written an authorization filtering that handles latter scenario and i am happily sharing it with you if you need it - you have tagged your question with asp.net-mvc,i have actually tried something similar but pandas is much much slower taking around 1s while dplyr takes 200ms just one example i was able to make it faster by filtering the dataset before grouping and aggregating but this takes away the freedom of doing multiple filtering and comparing them in a single line of code,unfortunately this not possible using openldap because your filtering is returning more than 1 object multiple grouping each with a unique dn,note that filtering before the grouping by is usually more efficient so i recommend where instead of having,the grouping by is giving more than one record for the count and you are not filtering for the same user row by row so please use the below - i created a memory table to show some results you dont need this part declare vstaffjobpositions table staffid int staffnameexternal varchar max jobpositiondescription varchar max and here added some test data as you did not provide any insert into vstaffjobpositions values 1 s1 relief teacher 1 s1 somthing else1 1 s1 somthing else2 1 s1 somthing else3 1 s1 relief teacher 2 s2 relief teacher 3 s3 somthing else1 the sql part that you can use is as below change only the table name to yours,example getting the sum of a number field that are greater than 0 grouping by country so my jason facet would look like in the above request i need to add a filtering to return only those cty buckets where,note how it lists the where columns first on the assumption that filtering is more important than grouping index active deletedon -- in either order updatedon tacking on frame_id id in either order but at the end would turn it into a covering index thereby gaining a little more speed,"
"filtering","grouping"," is better in better index indexes, aggregation is less overall, has more overall, data is more overall, is much easier and more overall, that sends log overall,specific by  overall, is no greater overall,more than one  overall,greater in greater rows count, aggregation is more in efficient query categories,","28082548,43225587,54769435,28173741,30500633,56999535,52164938,44028065,55802502,23948347,48337103,","indexes on the fields that appear in where or grouping by or order by clauses are most of the time useful;loading all data before filtering is better that loading the filtering data,we can filtering ignore all document where state is gte 410 the document in first pipeline and then grouping;so the number of documents for grouping aggregation is less,a simple query which i can t get my head around sample dataset ach_date code 1 31oct2018 a81001 2 31jan2019 a81001 3 31oct2018 a81002 4 31jan2019 a81002 5 31oct2018 a81003 6 31jan2019 a81004 i want to group_by on the code variable and filtering on ach_date whereby if a grouping has more than one row remove the row where,filtering on the grouping data is more processing-intensive because the grouping must be completed first,the ldap search filtering you could use is;but as noted in the comments a grouping is much easier and more maintainable,from there my cloudwatch api log grouping will have a subscription filtering that sends log data to this lambda as described here,so ive been designing a site where you see a table with persons and can filtering that table by searching for an expression and make your search more specific by filtering by grouping member guest etc. first name last name and company the person works for,use first fast boolean indexing for filtering and then groupby + first;solution is a bit complicated if in some grouping is no greater value as 10 - need expand mask with duplicated,ex brand_premium however we would like to only show a filtering grouping when there is more than one filtering available within a grouping ex all products on a given collection collection page are the same brand,grouping that by acct to do the count by acct and when the result is greater than 1 filtering it using a having clause,your query will need to process quite a lot of rows and will likely never be subsecond however you re using the output of an analytic function as a filtering where grouping aggregation is more efficient so by rewriting your query as below you ll see a slight improvement,"
"filtering","grouping","better performance than a  in better index indexes,","51622863,","you want to filtering the records so think where not grouping by in most databases this will have better performance than a grouping by if you have an index on document_history doc_id created_date,"
"aes","des"," rijndael is definitely in secure use aes,faster result in faster slower current,generally slower overall,larger triple in size known theoretical, it is no more in secure use aes,more in secure use aes,worse than   overall,faster in faster slower current, is generally better in size known theoretical,encryption less vulnerable overall,more overall,","18559744,9052760,10282754,13404970,51234588,21082690,38488337,25612825,257043,3929325,17019046,","to start with aes rijndael is definitely recommended in favor of des as the encryption algorithm;des is not considered secure enough any longer,aes will indeed yield a considerably faster result than des,though it s unrelated to your actual question des is generally slower than aes at least in software so unless you really need to keep the key small aes is almost certainly a better choice,note that the effective key size of aes is larger than triple des,but don t use des it is not secure use aes it is no more difficult to use and is secure,also see why aes is more secure than des,edit 3des is better than des in the sense that it s significantly more secure but still less secure than aes but it performance is of necessity significantly worse than aes des or twofish because you re essentially applying des three times,this shows that the timings are sensitive to buffering and that aes is faster than des,triple des is generally better but there are some known theoretical attacks;if you have a choice of cipher you might want to look at aes instead,which steps of aes encryption makes it less vulnerable than des,turns out this was a hardware failure the aes commands need more power than the des crypto1 ones 50 more which the antenna failed to deliver at the reading range i was testing with,"
"aes","des","rijndael-128 more in size known theoretical,general in faster slower current,easier than  in secure use aes,encryption more secure in secure use aes,faster in faster slower current, and was officially in secure use aes,better speedup overall, is not in secure use aes, is not longer in faster slower current,slower in faster slower current,slower in faster slower current,","29881041,7161481,54527358,11269211,10282566,34111505,21886363,2568198,28290622,35037543,7007053,","according to this analysis aes rijndael-128 is more than twice as fast as des 3des with a bigger key size more secure,personally i would recommend using aes as its very easy to implement and with its being sensitive personal data would provide enough encryption to keep people out unlike something like des,use aes - using des isn t any easier than aes,i ve read that aes encryption is more secure than the triple des encryption ms is using in their example above,using des assuming it s a little faster than aes and requires a smaller key and,and no des is not secure and in fact has been in brute-force range for at least a decade depending on attacker capabilities;it has been deprecated since about 1999 in favor first of triple-des formally tdea and then aes and was officially withdrawn about 2005 as i recall,if you used aes then you might see a better speedup over the des 3des observations,des is not considered secure anymore you should always avoid using it;aes is still considered safe you could use 256 bits keys,aes is the current preferred symmetric encryption algorithm;des is not longer recommended for new work and has some known weak keys,des turned out to be even slower than aes but for my current requirements a much simpler algorythm rc4 is sufficient,des is usually substantially slower than aes on modern hardware and has keys that are far too short for modern use,"
"aes","des","faster in faster slower current,ek plaintext with  overall, is no longer in secure use aes,faster in faster slower current, is not in secure use aes,","9928310,51387491,632342,29881041,525745,","if aes is negotiated it s faster than des and 3des used by default by older applications,ek3 dk2 ek1 plaintext that is the definition of triple des 3des not des that is encrypt plaintext with des as with key 1 decrypt that result with key 2 encrypt that result with key 3 3des or triple des encrypts three time with des;so aes would be ek plaintext with aes,sorry - triple des is no longer considered best practices;aes is simply a better algorithm so if you can use it then you should,aes can be even much faster than des or 3des when the cpu supports aes-ni,des is extremely easy to crack nowadays;please use aes or at least 3des if aes is not available,"
"qgraphicsscene","qgraphicsview","larger in default necessary scroll, isn overall,better connection overall,larger than the  in default necessary scroll,","17923539,11240968,32891725,8881046,","if i set a pixmap to a qgraphicsscene that is larger that the window it will add scrollbars so is it possible to get what is displayed in the qgraphicsview,i have done a similar thing to use a qgraphicsscene qgraphicsview to display items in a model;despite it s name qgraphicsview isn t part of the model view framework so i implemented a custom view class which drew the model data on the qgraphicsscene,generaly qgraphicsscene better using with connection qgraphicsview,by default when you scale a qgraphicsscene larger than the qgraphicsview in which it is displayed it will show the necessary scroll bars,"
"exponent","modulo","smaller in number lower bit,substantially shorter in rsa shorter public,smaller in rsa shorter public,smaller in rsa shorter public,larget than  in java.security.invalidkeyexception files der,always less in rsa shorter public,normally shorter overall,longer in rsa shorter public,better in rsa shorter public,value faster overall,shorter in rsa shorter public,","6662179,5377967,2927816,2927816,48771440,26422542,23875419,7835481,15114205,31221104,35015694,","in rsa signing a message m means exponentiation with the private exponent d the result r is the smallest integer 0 and smaller than the modulo n so that,i must also add that designing the rsa key so that the private exponent is substantially shorter than the modulo to speed up operations is a security risk if the exponent is smaller than 29 of the modulo length then the key can be cracked,the private exponent is always smaller than the modulo so you should be able to encrypt it using the raw rsa operation if you make sure to remove the prepended zero,edit as gregs points out in the comments you cannot be sure that the private exponent of the key you want to encrypt is smaller than the modulo of the key you want to use to encrypt with,i can t create a a new .jks file using instructions here create new jks it gives me error java.security.invalidkeyexception exponent is larget than modulo make sure the ca files are in der format,when i generate rsa key pairs by openssl it seems like private key private exponent is always less than public key modulo,little wonder you get errors the exponent is normally shorter than the modulo which is always the same size as the key size,the private exponent must not be longer than the modulo,so no choice of the public exponent for this modulo is better than 19 using the public exponent to decrypt will work for at least half of the messages when eâ² 9 16 and in many cases for almost all the messages when eâ² 1 16,using the pow function and passing a modulo value is faster than computing the full exponent and then taking the modulo because the modulo can be applied to the partial products at each stage of the calculation which stops the value from getting too large 10 6 to the power of 10 6 has 6 million decimal digits with a modulo applied at each step the values never have to grow larger than the size of the modulo - about 13 digits in this example,the rsa private exponent may actually be shorter than the modulo,"
"exponent","modulo","lower in number lower bit, is lower in number lower bit,larger than  in java.security.invalidkeyexception files der,","25461270,49171175,53052280,","1024 bit private exponent large number lower than the modulo,i assumed the biased exponent of the modulo is lower than the one of the number to be operated on as in the other case the mask is all ones and the number is not affected by the mask,generating a certificate with common name test01.local.lan and key strength 2048 issued by ca with certificate from c weblogicser wlserver server lib certgenca.der file and key from c weblogicser wlserver server lib certgencakey.der file failed to generate the certificate java.security.invalidkeyexception exponent is larger than modulo make sure the ca files are in der format,"
"uibutton","uicontrol"," doesn overall, doesn overall,more customization overall, to send action overall,finer control overall,cleaner overall,","48514696,11603395,15008437,6834762,4895037,12469991,","you can create a custom uicontrol it has touchup event and put any kind of components on it;if the uibutton doesn t meet your need then you can make a custom uicontrol,if uicontrol doesn t do what you want you ll need to roll your own event management;uibutton is a subclass of uicontrol,a solution i ve used is a simple subclass of uicontrol which allows more customization than subclassing uibutton and less hassle than nsattributedstring et al,uibutton uses the target-action design pattern through the methods inherited from uicontrol to send action messages to a target object;uibutton does not make use of the delegation design pattern,sounds like you want to use some of the inherited uicontrol methods such as sendaction to forevent this offers even finer control than with uibutton,instead of working around the 1 label and image you get in a uibutton it s cleaner to start from scratch with a uicontrol and create your tiles,"
"cassini","iis","better performance in integrated pipeline purpose, but not overall, doesn overall,better in integrated pipeline purpose,better in integrated pipeline purpose, by creating an application overall, express see this link overall, cannot overall,much more overall, but more overall,use  to do overall,","1849455,544949,3171322,26091712,11386063,6729409,6626326,9036366,10254781,8396060,60119,","while iis does give us better performance than cassini we would still like to be able to hit f5 to run our application from within visual studio,if you have doubts if something runs on cassini but not in iis;a dev or local iis is the only solution,cassini doesn t do url rewriting;you might want to look at the recently announced iis express version which will,cassini was much better than iis express but doesn t properly support the integrated pipeline,iis express is a web server albeit better than cassini iis express is not your problem,you are currently hosting under the development web service cassini which doesn t accept off machine connections;you will need to host in iis by creating an application and pointing it to the physical directory where the .svc file lives,however sadly cassini does not allow remote connections;you might be able to use install iis express see this link without administrative privileges,a little reading asp.net dev server cassini iis express and multiple threads;the asp.net dev server cassini cannot handle multiple threads,what i would suggest is to make sure you are using visual studio 2010 sp1 then install iis express which is an upgrade to cassini and is much more like real iis and then switch your project to use iis express and see if you get the same exception,iis express would only be used by people who don don t have access to iis or want something that is lighter than iis but more like iis than cassini,cassini does not support https;however you can use iis to do this if your using iis 5.1 download the ms iis toolpack for iis 6.0 it comes with a tool to create self signed ssl certificates,"
"cassini","iis","slower overall,slower overall,server better in integrated pipeline purpose, does not overall, does not overall, this was no longer overall,","12954150,1082128,31310823,4100546,6677840,17944587,","what could be the reason that makes iis slower than cassini,for my webapp the integrated visual studio server cassini ist much slower than iis,from this question i found out that iis server is better than cassini for my purpose but i don t know how to check the default server of my website,cassini does not support s;cassini runs as your account whoever is logged on iis runs as a service which means some things change quite a bit,try iis express as an alternative for cassini that s why ms released it;as far as i know this is not possible - cassini does not support comprssion,this only happened when using cassini;when switching to iis this was no longer that case,"
"homebrew","macports","more software overall, are up to date otherwise overall,many more packages overall, interferes less overall, puts  stuff overall,newer in emacs newer, has less overall, is more overall,less overall,recommend  over  overall, does not overall,","6541578,17760868,24198365,39174144,18254617,12184308,12892837,27865394,21198888,18699411,3899467,","con you can encounter issues related to library versions and dependencies macports has more software than homebrew,the homebrew solution is cleaner in usr local and is based on existing libraries if fink and macports are up to date otherwise the homebrew solution installs the homebrew solution own versions of the libraries,currently macports has many more packages 18.6 k than there are homebrew formulae 3.1k owing to its maturity,if you have never installed fink or homebrew or macports i recommend choosing macports because macports interferes less with other tools,overriding choice for me is macports as macports has many more packages than homebrew and macports puts macports stuff in opt local to stay out of the way of other programs,i m using homebrew as it s newer than macports and i don t know enough to choose between them,removing macports is a good idea;homebrew has less problems integrating into os x,ll in all most warnings are about potential incompatibilities homebrew like most package software managers tries to hold some grip not too tight though on what potential incompatibilities homebrew installs and what is around because backwards incompatibilities and such can mean that newly installed software from source brewed software doesn t properly install or run;as an example macports is more strict at least an example macports was years ago when i used an example macports and will download matching known compatible versions of required extra software thus you could end up with four different versions of a c compiler,homebrew has less of a stranglehold on your machine doesn t require as much reading as macports,i m guessing sphinx didn t correctly link up with mysql when you installed t via macports i generally would recommend homebrew over macports these days,still macports does not seem to have a way to install a previous version of imagemagick so i installed it from source;homebrew might be a better option,"
"homebrew","macports"," to install the package overall,luckier overall,prefer  over  overall,nicer overall, is more overall,available than  overall,newer emacs by  in emacs newer, doesn overall,","13629348,13467237,20692116,11704224,34953783,20586592,25863949,3446210,","your configuration of macports was not built with libvpx;try uninstalling ffmpeg and using homebrew to install the package instead of macports,it may be that macports doesn t help i have been luckier with homebrew than macports when installing cgal,i am not experienced using macports and would rather prefer homebrew over macports at the moment,homebrew is generally a bit nicer than macports as it doesn t require lots of sudo action,i m partial to macports so i ll give you instructions for that;i get the impression homebrew is more popular but haven t used the impression homebrew myself,in my experience macports has many more packages available than homebrew and the dependencies are installed with a lot less fuss and bother than trying to do macports yourself,recent packages does not work on install newer emacs by homebrew or macports,unfortunately the is very normal with macports and is the reason that people many have switched to homebrew;however i found this post because homebrew doesn t include meld,"
"apache","lighttpd","less ram overall,more overall, which is lighter weight overall,more suitable overall,faster overall,even faster overall,less overall, which doesn overall, is rarely overall, you want something overall,","23856324,172177,53536743,6608307,2769643,371821,12411541,5978844,3946868,5055680,","nginx or lighttpd in fastcgi mode use less ram than apache and they can handle more concurrent connections,the benefit of both apache is more powerful and extensible useless if you don t need that power but anyway... and lighttpd is faster at static content,you would then ensure that apache or maybe lighttpd which is lighter weight is running on your rpi,this is my first experience setting up lighttpd as i thought it would be more suitable than apache in this case,- if you move towards more static content or go the fastcgi way lighttpd is faster than apache,for instance in some benchmarks lighttpd is even faster at serving static resources than apache,it may even use a different server software say nginx or lighttpd that has less overhead than the traditional apache setup,speaking only of unix-based environments i d say that fortunately you only have to think of this if you are going to use php with apache web server in which case you are advised to go with the prefork mpm of apache which doesn t use threads and therefore php thread-safety doesn t matter and all gnu linux distributions that i know of will take that decision for you when you are installing apache + php through their package system without even prompting you for a choice;if you are going to use other webservers such as nginx or lighttpd you won t have the option to embed php into them anyway,since you ve switched to lighttpd the webserver itself is going to use fewer resources than apache would but apache is rarely the bottleneck unless you ve run out of ram or seriously misconfigured the webserver itself,this is the correct line for apache but not the correct line for lighttpd;for lighttpd you want something like this,"
"subviews","uiview"," were not overall, is much easier overall, is going to crop in bigger layer smaller, that spans larger in screen new taller,less overall,more in screen new taller,smaller frame in screen new taller, subclass not overall,more than a  overall, but not overall, but let the controller overall,","13154422,45677006,41592606,48216808,16751084,6275623,1361527,5311776,47972319,26819699,11730665,","i had subclassed uiview added it inside uiscrollview and the gesture recognizers of my subclassed uiview s subviews were not firing - except for the ones that were initially visible as before any scrolling;the simple fix was to add the uiview s subviews directly to the uiscrollview instead of my subclassed uiview,i realize that this may not be possible in all cases but the regular uiview is much easier to customize to the app s appearance than the toolbar and navigation bar where apple has control of the button positioning;instead of setting we custom button as the custom view of the ui bar button object it was we set it as a subviews of the blank ui buttons in the custom view,because the uiview is smaller the uiview is going to crop out the larger subviews,another way i came up with this to try to work is to create a uiview that spans larger than the scrollview to allow the child view to be a subviews of this larger view that has the scroll view as its parent,the issue is that once the alpha of the uiview is less than 1.0 i can see all the subviews sides and the one that gets me the most is the outline of an uiimageview can be seen that would otherwise not as it is the same colour as the uiview,of course uiscrollview can have more than one content view just as any uiview can have more than one subviews and this case it will be more difficult to render its layer,i want to add a uiview of smaller frame as subviews to parental view but i am not getting the needed,this can be done for any uiview subclass not just uibuttons;looping through a uiview s subviews is an unreliable way to find a button,so if you know what you re doing or are very willing to learn i would strongly suggest not having any fear of creating your own stuff like a custom tab bar which for your purposes is probably nothing more than a uiview with either uibutton subviews or other uiview subviews with tap gesture recognizers attached to them,the calayer and its sublayers if any is a representation of a uiview but not its subviews;each subviews has its own calayer,the problem here is that the uiview doesn t own its uiviewcontroller;in the first block of code you held the uiview around by adding it to a subviews but let the uiviewcontroller go away,"
"subviews","uiview","more memory overall,version definitely faster overall,bigger layer as a  in bigger layer smaller,taller in screen new taller,more overall, that fills header view in background size color,smaller overall, that take up part in screen new taller, gets this margin in background size color,4px larger in screen new taller, - not overall,","11065866,22545659,4747607,38314415,6756801,27281214,19924321,12190604,21344489,3357911,7699482,","you are seeing a noticeable jump in interface response because subviews do in fact consume quite a bit of memory uiview are very expensive compared to their underlying calayers and as such calling -removesubview not only unloads stress from the gpu but also frees up more memory as the subviews is usually released afterwards,the subviews version is definitely faster since having the controls loose on the uiview took more like 2 seconds to update,the problem was that i was trying to add the bigger layer as a sublayer of the uiscrollview directly instead wrapping the bigger layer in a uiview and adding the bigger layer as a subviews worked,the uiview is taller than the screen so in ib i pulled it out of the scroll view so i could see it all but i never moved it back to being a subviews of the scroll view,change the contentsize of your scrollview.your uiview size and uiscrollview size is same if the uiscrollview contentsize is more than its subviews size then scroll will happen change the line overviewscroll setcontentsize cgsizemake screenframe.size.width screenframe.size.height-25,if you want to add several views and change the background color of you header consider adding uiview that fills header view thanks to autolayout constraints and then all the subviews you would like to have to header view i am calling it customcontentview,though earlier uiview and uiscrollview have the same area after keyboard shown the blue uiview becomes smaller but its subviews button text fields is outside its area,you should be doing it as a uiview not a uiviewcontroller;the general rule in ios development is that view controllers take up the entire screen and views are used for subviews that take up part of the screen though this is less the case for ipad,on that add a uiview as a subviews with a white background and arrange the size so that a uiview becomes a smaller rectangle inside your cell so a uiview gets this margin effect,do i just create a new uiview that is 4px larger than the selected object and and make the selected view a subviews of it,either a xib interface builder view or a class that inherits from uiview are equivalent to a user control;they are ways to aggregate sub views think views and subviews - not controls,"
"subviews","uiview"," will not in background size color, add in parentview overall,bigger in screen new taller,","16853235,50055562,11452372,","i think your problem is the size of the very root uiview the superview of the uiview you talk about of your views;when a certain subviews is larger than a certain subviews superview a certain subviews will not receive touch events in this portions of a certain subviews s frame that go outside of the superview frame,you can check the sample working project to add subviews link required code class object to be added as subviews add in parentview as subviews where - firstcontainer is the parent view in which subviews is to be added viewcontroller class object whose view is to be added as subviews note- this can be used in containerviews as well as normal uiview too for adding a controller as subviews,when using embed in - uiview the new view will be a bit bigger than the subviews,"
"calayer","uiview","lighter overall, doesn overall, at once overall,better in better worse performance,worse performance in better worse performance, method has better animation overall,even more overall,more overall,lighter overall,smaller overall,more overall,","33563019,8996405,9386661,4882069,34019830,11337217,24280489,36186311,5538394,15638505,3911370,","this is better than using a second view a bit larger as a calayer is lighter than a uiview and you don t have do modify the frame of myview which is good for instance if myview is a uiimageview,ote that there s a layer property in uiview of the type calayer;however coding directly with uiview is easier but uiview doesn t expose the full powers of core animation,this does not work because the uiview you are subclassing i assume this is a uiview already is the delegate of a uiview own calayer a uiview cannot be the delegate of more than one calayer at once,is calayer better than uiview in terms of performance,i noticed calayer had worse performance than uiview,uiview method has better animation responsiveness;in my case the uive fps is about 60 but the calayer is about 34,in fact each uiview has even more than 1 corresponding calayer,a uiview is no more than a wrapper for an underlying calayer,i am using calayer s because as suggested in documentation calayer s are lighter than uiview and i have hundreds of them,i want to add a calayer inside an uiview and this calayer will be smaller than the uiview,does that mean the the view controller has multiple uiview or that the uiview has more than one calayer,"
"calayer","uiview"," you cannot overall, does not autosize itself so overall, is not overall, has a greater overall,more overall,","39641857,6393068,3783636,8028787,36827987,","a uiview is not a calayer you cannot add an avplayerlayer to the storyboard;instead after your view has loaded programmatically add your avplayer s playerlayer to the uiview s layer,set view.layer.masklayer on an appropriate view a uiview containing the scroll view should work to something suitable a calayer with contents contentsscale contentscenter set appropriately;more of a pain since calayer does not autosize itself so your wrapper view will have to do the necessary things in -setframe -setbounds -layoutsubviews,if you want a button that actually works on top of a calayer put that calayer into a uiview which is a subclass of uiresponder and add a uibutton to that view so it can get added to the event response chain;a calayer is not an event responder so trying to hook it up to a touch event handler will do nothing,edit unless you have a good reason want to use some services of uiview which are not offered by calayer you should use calayer as mattdipasquale suggests;uiview has a greater overhead which might not be a problem in most cases but still the other solution is more elegant,a uiview is no more than a fancy wrapper for a calayer â bringing uiresponder events animation conveniences among many other things,"
"execute","prepare","more overall,statement overall, them is larger overall, is slightly faster overall,execution faster overall,","39779542,1363067,52921974,47954857,3553157,","my question is how can i execute more than 1 prepare statement,any execute statement becomes prepare sooner or later it need to be parsed optimized compiled and then execute,if my arrays are very big and the time taken to prepare them is larger than time to execute task the program serves my purpose as the threads execute task in a sequence as they are called,in some tests it even appears that prepare execute is slightly faster,prepare execution is faster than direct execution for statements execute more than three or four times because the statement is compiled only once while statements execute directly are compiled each time they are execute,"
"keras","tensorflow"," it cannot overall, which is much more overall, is more overall, and allows arbitrary graph overall,  so overall,sanity-preserving than  overall, file was still overall,different between  overall, is a better in better level page,slower in training faster gpu, to resolved this problem overall,","46524360,55472342,45022621,49871967,52021654,54895171,52516793,51546176,40793654,47484573,55313108,","it is similar to keras in the sense that it provides higher-level abstractions than tensorflow and allows rapid prototyping of neural networks;the principle difference is that unlike keras it cannot use theano on the backend,another thing is in the event where you re not actually messing with the internals of layers is to just use keras which is much more straightforward to use for the layman than tensorflow and it sits on top of tensorflow so you re using the same tech,lternatively using just tensorflow you could use this strategy;but i personally think keras is more elegant plus this strategy adds a whole lot of more useful features such as model.output model.input and much more,tensorflow is a lot more generic than keras and allows arbitrary graph architectures so showing such a structured summary does not make sense for arbitrary tensorflow graphs,i ve read numerous posts regarding how to have variable length sequences in batches and i understand the replies to these posts however the only post i ve found regarding why is here on data science with the answer being within a single batch you must have the same number of timesteps since it must be a tensor this is typically where you see 0-padding . however this seems to be an unnecessary restriction i am not very familiar with keras tensorflow so my question from a perspective not specific to any api,all in all there is a reason why pytorch keras is more readable and sanity-preserving than tensorflow,i was having similar issue with keras cannot import abs;tried updating and found tensorflow file was still in use,when training a neural network with some toy data the resulting training curves are very different between tensorflow and keras and i do not understand why,keras is yet to officially provide support but you can proceed at your own risk;for multiprocessing tensorflow is a better way to go about this my opinion,is keras faster slower than tensorflow during training,unfortunately keras does not have a convenient way to get each components of the gradients;therefore i used tensorflow to resolved this problem,"
"keras","tensorflow"," api is more overall, without also in windows errors mac, just not overall,more gpu in training faster gpu,high level than  in better level page,familiar with the  in wrong right api,simpler quicker overall,similar issue with  overall, to get the right in wrong right api, backend alone overall,rnn models with   overall,","54050325,56695223,43770736,45092008,50793865,55285139,45411755,54483232,50400765,50562195,52053062,","since the keras api is more recommended there is probably interest in converting a regular tensorflow graph session to use the keras api with as few alterations to their code as possible,i had the same problem with this tutorial and as others have mentioned the version of keras doesn t support separableconv1d;however just updating the version of keras without also updating tensorflow caused other errors in the jupyter notebook,theoretically this make senses and should be possible and it is possible with tensorflow just not keras;the problem keras requires an explicit batch size for stateful rnn,is keras consumes more gpu memory than equivalent tensorflow model training,for such models would keras be a better option since it is more high level than tensorflow,i am much more familiar with the tensorflow api than with the keras one,keras provides a simpler quicker way to build and train models in tensorflow at no performance cost since the models are still being run by the same tensorflow engine,loose the tf.reset_default_graph and you should be good. as for the memory leaks be sure you are running keras 2.2.4 and preferably tensorflow 1.10 has better keras integration i had a similar issue with keras 2.2.2 crashing when loading multiple models in sequence and it disapeared after i updated to keras 2.2.4,something like but i know for sure that is wrong-- i m just not familiar enough with keras and tensorflow to get the right code,having said that using functions from tensorflow directly is ok as well say from import tensorflow and use those inside a custom layer might give you more functions than keras backend alone,we can use deep learning cnn rnn fast rnn models with tensorflow keras for object detection or yolo model refer to the this article for car detection with yolo model,"
"keras","tensorflow"," it has much less in windows errors mac, is a higher in better level page, code runs the model overall,common than  overall,familiar with  overall,faster in training faster gpu, uses xavier initializer overall, to use the gpu overall, is better in better level page, doesn overall,higher-level than  overall,","49366670,50206803,57379074,54023988,52715744,46210187,51546809,52933947,53403137,56081613,55466431,","regarding your issues you are installing cpu only and old mac version of tensorflow which shouldn t work on windows. you could try using the following command python3 -m pip install but i suggest you use anaconda for the windows to install tensorflow and keras it has much less errors on windows you can find a guide for it here,keras is a higher level library that is much easier to learn than tensorflow and you have more sample code online,i wrote simple sin function predictors using keras and tensorflow with lstm but found the performance of keras code is much slower which runs about 5 min while tensorflow code runs the model just in 20 seconds,here for example using numpy library it is arguably more common than keras or tensorflow,i define a custom function my_sigmoid as following and then define a custom loss function called my_cross_entropy my keras backend is using tensorflow. and the error shows typeerror must be real number not tensor i m not familiar with tensorflow and don t know how to use custom loss,more than this methodology i would suggest to you to do the training directly in keras as it claimed that keras optimizers are 5-10 times faster than tensorflow s optimizers,tensorflow uses no initializer on the other hand keras uses xavier initializer,it runs much quicker than the keras example above. import tensorflow # creates a graph. a tensorflow.constant 1.0 2.0 3.0 4.0 5.0 6.0 shape 2 3 name a b tensorflow.constant 1.0 2.0 3.0 4.0 5.0 6.0 shape 3 2 name b c tensorflow.matmul a b # creates a session with log_device_placement set to true. sess tensorflow.session config tensorflow.configproto log_device_placement true # runs the op. print sess.run c i would greatly appreciate your kind help in finding out why keras can t see my gpu i use python 3.6.5 tensorflow-gpu 1.11.0 tensorflow not installed keras 2.2.4. i need to mention that i had to fiddle a while to get tensorflow to use the gpu and i still don t know why it suddenly did but it does so consistently now,i recommend you check this page also i think keras is better to begin with than tensorflow,adding to the above two answers ensure your tensorflow keras environment is using python 3.6;keras tensorflow doesn t work very well with python 3.7 as of may 10 2019,keras sits on top of tensorflow and thus the framework is relatively higher-level than tensorflow itself,"
"keras","tensorflow"," i then overall,high level than  overall,familiar with  in training faster gpu,compatible with  overall,compatible with  overall, 2.0 instead overall, to address its research overall,easier than  overall, utilities;here is a blog overall,more than one  overall, is more time overall,","54034952,55610772,56961681,55424897,55048808,56663120,53239472,52534202,45466355,56284053,51492457,","i then read the install documentation for keras and it says that tensor flow needs to be installed first so i did that by doing this in the anaconda prompt after checking pip install tensorflow i then got an error message and it said that the version was not supported so i found out that the latest anaconda version that i just installed which was built with python 3.7 and was not compatible with tensorflow which is limited to python 3.6. so i then created a python 3.6 virtualenv within conda by doing i then activated the virtualenv conda activate virtualenv then within this virtualenv i installed the tensorflow pip package that was supported with windows and python 3.6 by looking at the version list here so i copied the link for the version developed for windows and python 3.6 cpu only which is and then i added that url into the command to install it the install completed and installed some other stuff too successfully installed absl-py-0.6.1 astor-0.7.1 gast-0.2.0 grpcio-1.17.1 h5py-2.9.0 keras-applications-1.0.6 keras-preprocessing-1.0.5 markdown-3.0.1 numpy-1.15.4 protobuf-3.6.1 setuptools-40.6.3 six-1.12.0 tensorboard-1.12.2 tensorflow-1.12.0 termcolor-1.1.0 werkzeug-0.14.1 wheel-0.32.3 i then read the keras documentation from their website and then i installed keras i then closed the anaconda prompt window and relaunched it and didn t have any problems so to recap do not use conda use pip to install tensorflow and keras,short answer keras is more high level than tensorflow in the sense that you can write code quicker with keras but it s less flexible,i m more familiar with tensorflow graph training than keras but i m trying out keras here,here s a simple numpy implementation of this but in this case our array will have a fixed size 100 instead of tensor this works perfectly for defined constants but in our case n is a tensor so is index_array therefore where k is keras.backend and hence assignment of values in pythonic manner will yield an error typeerror tensor object does not support item assignment in tensorflow question i want to find a way to replicate a code in keras-based backend alone without the use of tf.variable and tf.sparsetensor so that it can be fully compatible with keras,in ubuntu 18.04 and later you could install scipy and keras for python 3 with sudo apt install python3-scipy python3-keras and you d be good to go however you are using ubuntu 16.04 and you installed scipy for python 2 which is not compatible with tensorflow for python 3.4 3.5 and 3.6 so install the default scipy package for python 3 instead with this command sudo apt install python3-scipy for further instructions on installing tensorflow in ubuntu read this answer,current keras is not compatible with tensorflow 2.0 which has not been released as stable just as a beta so do not try to use the official keras with tf 2.0 yet use stable tensorflow like 1.13 or use tf.keras from tensorflow 2.0 instead,it appears that keras doesn t seem to suit the needs of deepmind;so deepmind came up with sonnet a high-level object oriented programming library built on top of tensorflow to address its research needs,i know that this is a silly example but when you want to do more complicated stuff i thought that using tensorflow would be easier than keras functions as there are more functions available,keras does not include by itself any means to export a tensorflow graph as a protocol buffers file but you can do it using regular tensorflow utilities;here is a blog post explaining how to do it using the utility script freeze_graph.py included in tensorflow which is the typical way it is done,this error only occurs with me if i try using more than one tensorflow keras model in the same server,but building a network and debugging it in tensorflow is more time consuming than keras,"
"keras","tensorflow","complicated than  overall, doesn overall, backend not in training faster gpu,better than  in better level page,","57495483,50260936,47640306,54435925,","keras doesn t have an eager mode like tensorflow and it depends on models or functions with placeholders to receive and output data;so it s a little more complicated than tensorflow to do basic calculations like this,masking in keras doesn t produce zeros as you would expect it instead skips the timesteps that are masked in upstream layers such as lstm and loss calculation;in case of rnns keras at least tensorflow is implemented such that the states from the previous step are carried over tensorflow_backend.py,if both are installed than keras will use only cpu version;i solved it with the answer given by keras with tensorflow backend not using gpu,the point is that keras performance is much better than tensorflow code,"
"activemq","rabbitmq","way more overall,better than  in worse sense easy, amqp is bit overall,worse in worse sense easy,","333440,23815467,23242121,7044157,","apache activemq which is way more popular than qpid or rabbitmq - or indeed any jms provider would work just fine,it seems activemq is better than rabbitmq in the sense that it is easy to set up and maintenance for cluster,activemq is heavier memory consumption;activemq is perfectly integrated with apache camel with rabbitmq you d better use spring integration camel rabbitmq amqp is bit rough,what does rabbitmq do better or worse than activemq,"
"128bit","64bit","faster in skylake scalar avx,larger than  overall, stores costs more overall, computer doesn overall,faster in transactions bit adder,faster than  in skylake scalar avx, operands is a much overall,smaller than  overall,slower in transactions bit adder,larger in number prime factor, has more overall,","34476509,55693061,56022985,13362093,27018111,55083822,24969828,56171017,17645369,1434405,13662043,","in a benchmark test the 128bit intrinsic function performs faster than the 64bit intrinsic,see emulating fp64 with 2 fp32 on a gpu most other architectures don t have hardware for floating-point types larger than 64bit therefore they chose the ieee-754 quadruple-precision format for ease of implementation and better forward compatibility since if one day support for 128bit floating-point came to real hardware it ll most likely be ieee-754 quadruple-precision,but moving data to xmm registers for 128bit stores costs more uops and also hits a 64-bit-per-clock bottleneck. if you only needed 64x64 64bit multiply you could drop the multiply,no decimal is 128bit to start with;also note that running on a 64bit computer doesn t necessarily mean you re running the 64bit clr,128bit transactions tend to be faster than 64bit which tend to be faster than 32 bit,but with for avx1 then running the code on skylake this running scalar 64bit is actually still slightly faster than 128bit avx auto-vectorized asm from njuffa s code,divide and modulo return 64bit results given 64bit arguments - so that s not an issue as you have defined the problem;dividing 128bit by 64 or 128bit operands is a much more complicated operation requiring normalization etc,decodes to a pure load uop on intel cpus no alu uop needed so it has 2 per clock throughput instead of 1 old cpus like merom and k8 have slow shuffle units that are only 64bit wide so shufps is pretty slow because it s a full 128bit shuffle with granularity smaller than 64bit,so a 128bit adder will be slower than a 64bit add,and i ll just add to previous comment if 128bit number has prime factor larger than 64bit then it certainly has a factor less than 64bit,128bit has more than 10 38 different keys;64bit has more than 10 19,"
"128bit","64bit","math faster in skylake scalar avx,larger in number prime factor,more overall,faster in skylake scalar avx, debugging is harder overall,","9318444,1433909,1464230,32549371,53350652,","in a few more years there might be an architecture where 128bit math is faster than 64bit but i don t think any exists today,i ll just add that if the 128bit number is prime or has a prime factor larger than 64bit then there will be no solution to your problem,perhaps 128bit distributed-system internet-wide pointers but no more than 64bit in a system call or perhaps even a legacy 32-bit limit,actually on intel sandybridge-family at least mul imul 64bit 64bit 128bit is faster than imul mul 32bit 32bit 64bit,imo you have 3 options for implementing equals hashcode use an application generated identity a uuid implement it based on a business key implement it based on the primary key using an application generated identity is the easiest approach but comes with a few downsides joins are slower when using it as pk because 128bit is simply bigger than 32 or 64bit debugging is harder because checking with your own eyes wether some data is correct is pretty hard if you can work with these downsides just use this approach,"
"extends","super"," behaves more overall,more barely overall,handler methods than to  in class time handler,more in class time handler,much more overall, is a lower in upper lower,lower in upper lower,more than one  in class time handler, is a lower overall,general overall,","35990799,44554369,55402738,26528384,44317988,21222207,4343237,57231045,52171071,24938761,","also there are no calls to super methods in c;extends behaves more like what you d expect from the keyword,here the content for the length item is this is a super duper long snippet because we need to see how labels behave when they more than barely extends onto multiple lines. the content for the power cycle item is have you tried turning it off and back on again,in airflow source repo one word of caution due to the way logging multiprocessing and airflow default handlers interact it is safer to override handler methods than to extends handler methods by calling super in a derived handler class,no java prevents a class from directly extends more than one super class,it s useful to know that extends bound is much more common than super,where extends is an upper bound super is a lower bound,super is a lower bound and extends is an upper bound,so when one class extends from more than one super class we get compile time error,extends drink is the upper bound - so every object must then extends from drink to be legally assigned into that list;super is a lower bound - that means that any instance specified must be no lower in the inheritance hierarchy than the specified type;super drink is the lower bound - so every object must then either be a type of drink or a type of its ancestor - in this case object,he actual explanation for the behavior is that to extends class a you need to meet every requirement that class a presents to you;in this particular case class a is saying that sub classes need to implements these methods the fact that a super class has implements sub classes is irrelevant a super class is adding a more specific requirement itself,"
"jsf","seam","more powerful overall, is a more overall,better overall,certainly far better overall,","7062618,1252126,6377582,1253366,","seam is more powerful with jsf but not necessarily richfaces or icefaces for they mostly just extend the component set which is anyway fall down to standard html components when rendered by jsf,i like jsf and i evaluated seam not long ago;jsf is a web ui framework whereas seam is a more general web application framework that integrates not just jsf but conversational contexts workflow jbpm and object persistance preferably ejb3,well for that you ll need jsf that is better integrated with seam and jsf does not handle file uploads that actually everyone use so you have to put also icefaces that actually are better managed by maven that you currenty are not using... neverending story,seam certainly is far better than using plain jsf refer the link posted by damo a couple of answers above,"
"tensorflow","theano","better computational overall,faster in respects wider faster,slower than using  overall,slower than with  overall,faster in respects wider faster,better overall,","44823584,45300440,38229175,53953240,44823584,44823584,","tensorflow has better computational graph visualizations than theano and torch,it theano runs much faster than tensorflow,in my testing the slow-down from using tensorflow ranges from about 1.5-3.0 times slower than using theano but theano performance will depend on your application,however when i run the exactly same lstm model with exactly the same data with theano backend is almost three times slower than with tensorflow backend,theano is still faster than tensorflow in many respects and supports a wider range of operations,tensorflow is better at marketing itself than long-time players of the open-source market like torch and theano,"
"subclassing","superclass","lower visibility in can not lower,fewer arguments than the  in object specific smaller,more flexible in object specific smaller,same functionality as the  in value method specific, doesn overall, but not in value method specific,more accessible in constructor default constructors,less classes overall,bigger in bigger second statement, is more specific then in value method specific,more overall,","11190974,31996234,6694223,36418780,6795919,4959636,41011604,10137483,12938232,488946,13079329,","still you cannot inherited from a as superclass cannot have a lower visibility than subclassing,with explicit superclass calling your subclassing can accept more or fewer arguments than the subclassing didn t superclass and can decide the subclassing didn t what to pass when calling its superclass,a subclassing is more flexible and is treated as an entire object which responds to all superclass methods plus it s own,the other way round returning a subclassing that is a more specific class does not break the contract because a subclassing that is a more specific class has at least the same functionality as the superclass,this way the superclass doesn t have to know what the subclassing need;each subclassing defines its own behavior,add a protected getter method in the superclass to return the balance value such as this;this method will be visible to subclassing but not visible externally,in other words if a subclassing is more accessible than its superclass then the access modifier of the superclass loses effect,hence your subclassing is accepting less classes than the superclass contract promises,in this case superclass is bigger than subclassing that s why the second statement is correct,if the subclassing is more specific then the subclassing might fill in all by 2 of the arguments to the subclassing superclass __init__ method,here having a superclass makes more sense or at least having a realnumber subclassing of number,"
"subclassing","superclass"," constructors have still in public methods instance,less in optional placeholders argument,actually more overall, is not necessarily in value method specific,weaker reference in constructor default constructors,more specific overall, i see only overall, methods not in can not lower,more in object specific smaller, is returning more in value method specific, cannot be the solution in public methods instance,","42562050,36196748,15635368,10804408,40404884,22031943,46970809,24477743,29938056,55764506,710597,","maybe the superclass did not even have a constructor;however the subclassing constructors have still called superclass constructors in those case,if you subclassing has less arguments than a superclass and you could make them optional in the superclass just add placeholders in the subclassing,even in your original post the subclassing is actually more restrictive than the superclass so doing something like,i suspect that you have some oop background so i won t go too much into subclassing other than to say a subclassing is very often a specialized or more specific version of the superclass;in other words every subclassing is a kind of its superclass but every superclass is not necessarily a type of subclassing,so i inspected the compiled firebasemessagingservice which was the subclassing of com.google.firebase.iid.zzb and had zzae intent method as private but its subclassing firebasemessagingservice had protected access i know that subclassing cannot have weaker reference than its superclass so i guess there is something wrong with firebasemessagingservice library,for example you want to use abstract-class in such a case that each of the subclassing is a more specific type of its abstract superclass fruit apple orange banana grape strawberries etc. and you want to use non-abstract superclass in a relationship such as dad and son,as other answers describe the superclass cannot know the type of the subclassing unless it s passed in runtime;if the issue you re trying to avoid is to have to create the map-constructor in each subclassing i see only two solutions each with its flaws,superclass cannot handle subclassing exceptions;subclassing can replace wrap superclass methods not the other way round,the general problem is that the subclassing is more specific than the superclass,i m looking for a clean way to express that a subclassing is returning more specific types from methods on the superclass in typescript,my superclass does not handle any jaxb annotations it doesn t have to and i would like my subclassing not to include superclass properties while marshalling;adding the xmlaccesortype on superclass cannot be the solution as i have no way to modify the superclass,"
"subclassing","superclass"," is not in value method specific, variable using the super in value method specific, it inherited from;ultimately overall,higher altitudes overall, doesn in object specific smaller,always smaller in object specific smaller, will also overall, car assume the vehicle in constructor default constructors, to vector won t overall, has more in constructor default constructors,more in value method specific,","20857878,34281876,56038238,5527347,39404261,839664,9441026,14510127,8640353,56811503,23594257,","you are passing the value into the super constructor but the field value in the superclass is not a class variable it s a local variable;the field value in the subclassing is global but not getting set by the super constructor due to this,same variable declared in subclassing does not override the value in superclass;to reflect the value associated with superclass you need to pass it to the constructor and set the superclass variable using the super keyword,this is not a python convention it is what oo best practice suggests you should do given the language you happen to be using leaves this decision up to you the superclass doesn t know about the subclassing but the subclassing is expected to know about the semantics of the superclass it inherited from;ultimately it is up to the subclassing to maintain a consistent behavior of a true sub-typing which unfortunately the python language does little to help the programmer with,yes but if you think of your diagram as a topographic map the subclassing have higher altitudes than the superclass,casting an object from a subclassing to a superclass doesn t require an explicit cast;casting an object from a superclass to a subclassing requires an explicit cast,for my understanding the superclass is always smaller less complex then the subclassing,which means that another function that can take more arguments counts as a subclassing since it can process anything that the superclass can process and if the superclass produces a smaller set of results that s okay since the superclass will also obey the superclass construct that way,since the constructor is a method that is invoked on the construction of the object in the memory heap then once you create a subclassing that inherited from a superclass the constructor of the superclass is not invoked by default;for instance if you have a class vehicle and a subclassing car assume the vehicle constructor is as follows,it s because superclass is a lower bound annotation meaning classes that are super to vector;if vectorb is a subclassing to vector then vectorb is a subclassing to vector won t work,the constructor in the subclassing doesn t have to match the one in superclass but it should call it using super ... subclassing should provide a constructor that calls super .. like this or even this superclass has more than one explicit constructor subclassing are forced to provide at least one constructor that calls any of the superclass s constructors subclassing could be any of the following or or or any combination of those constructors,shouldn t a subclassing interfaced be able to take in more than the superclass interfacec and use the same method,"
"subclassing","superclass","more in object specific smaller,narrower subtype in value method specific,better than a  overall,more overall, doesn overall, is much harder overall, methods are only in public methods instance,less methods in public methods instance, doesn in value method specific, will not overall, does not in constructor default constructors,","13144639,15037922,3418152,34829663,17510184,891854,8433855,16097420,22754927,18944072,22470339,","if we are creating an object of a subclassing and invoking the method of superclass and if subclassing extends more than one class which superclass method should be called,however if the subclassing returns a narrower subtype of the superclass method return this is called a covariant return type and is allowed in java since jdk 1.5,most likely this class doesn ll still be tightly linked with implementations generally a bad thing or if not bad this class doesn s not especially good but i think a structure like this is better than a superclass figuring out what all of a superclass subclassing are,in enhanced er modelling subclassing inheriting from more than 1 superclass is called multiple inherited,constricting the access modifier of a superclass method is an invalid override because it s breaking the superclass contract and invalidates the substitution principle a subclassing object is-a superclass object as well;if this was allowed the above client code would break because your subclassing doesn t have that method public,subclassing must be aware of superclass es implementation details;creating the superclass is much harder when you have to think about how it can be extended,but an instance of the superclass is not necessarily an instance of the subclassing;thus the superclass methods are always available in both but the subclassing methods are only available in the subclassing,you cannot have a subclassing with less methods than a superclass,when you call a method using the super keyword you re specifying that you want to use your superclass s implementation of that method;if your subclassing doesn t override a method defined in super then performing the selector on self will trigger a miss on your class s method lookup table and find the method definition on your superclass,inherited refers to how members of the superclass are accessible through the subclassing;the subclassing will not actually get copies of its own,if the superclass has explicit constructors all of which take arguments then the subclassing needs an explicit constructor as well since without such a constructor there would be no way to know what arguments the superclass constructor should be called with;if the default superclass constructor accessed through super is available then the subclassing does not need to have an explicit constructor,"
"subclassing","superclass","more in optional placeholders argument, variable is no longer in public methods instance, are not in public methods instance,more specific in object specific smaller, does not in constructor default constructors, is not necessarily in public methods instance,force  to overwrite overall, explicitly call super initialize overall,more than one  overall, will not in generalized inheritor becomes,more overall,","45940822,26334635,15596211,4523120,19074611,8433855,22463880,21682496,57244204,10678494,17820307,","subclassing must allow more than the superclass not less and going from an argument being optional to not optional is allowing less,when is done in the subclassing the subclassing makes the superclass variable hidden so that the superclass variable is no longer available,only public methods that are defined in superclass are accessible;methods defined in subclassing are not,however they are not the same because the subclassing has more specific functions and data members that accomplish a more specific task that the superclass,a subclassing does not have to have any constructor with the same number of parameters in the constructor as the superclass but it does have to call some of its superclass constructors from its own constructor;if the superclass has a no-arg constructor it is called by default if an explicit call to a superclass constructor is omitted or if the subclassing has no explicit constructor at all as is your case but since your superclass does not have a no-arg constructor compilation fails,think of it this way an instance of a subclassing is also an instance of the superclass;but an instance of the superclass is not necessarily an instance of the subclassing,but a subclassing can be a delegate of its superclass if the superclass does not implement certain methods and you can enforce that the delegate implements these methods if the superclass specifies the protocol required methods and the subclassing adopts it;in obj-c it is not possible to force subclassing to overwrite methods of its superclass,superclass receive this message before their subclassing;the superclass implementation may be called multiple times if subclassing do not implement initialize the runtime will call the inherited implementation or if subclassing explicitly call super initialize,as described in the above diagram we have here a superclass called animal which has three subclassing mammal bird and fish some animals share common behavior a cat and a dove can both walk but the cat cannot fly. these kinds of behavior are orthogonal to this classification so we cannot implement these behavior in the superclasses. if a class could have more than one superclass it would be easy we could create three other classes walker swimmer flyer,beyond that the rules and guidelines for abstract-class interfaces and general superclass are the same;any class interface which is being subclassing will not be resolved through magento factory methods,watch out you can t query a superclass if you have more than 1 subclassing then something like in jql,"
"subclassing","superclass"," is thus overall, does not in value method specific,smaller in object specific smaller,more selective overall, is explicitly in constructor default constructors,more overall, and shouldn in public methods instance,no longer in public methods instance,presumably more memory in object specific smaller,more general in object specific smaller, has no constructor in constructor default constructors,","11279273,5614047,8469968,4872117,14251298,17098610,13789620,22727300,20343870,34325101,22470338,","in this case your superclass is not meant to throw an exception;this is a case where your subclassing is thus throwing an exception which is not expected by the overlying software architecture,casting it to the superclass does not invoke the superclass method;the subclassing s method is called automatically,2 since superclass is smaller than subclassing one should use memory object carefully,the fact that the subclassing is more selective only actually writes about what it writes is new functionality especially in light of the fact that the superclass promises nothing,if the superclass does not have a no-argument constructor you will get a compile-time error;super t in the subclassing is explicitly invoking a no-args constructor in its superclass class b,take note of this there is no way to subclassing more than one superclass at a time,then in your superclass s static usesubclass method you can iterate through that list of subclassing instances find the particular one you care about maybe specified by some argument and then do something with it;in general you cannot do that from a superclass and shouldn t,this breaks the inherited and the subclassing is no longer an instance of the superclass,if new member fields are declared in the subclassing then yes a subclassing presumably uses more memory since it has all the fields declared in the superclass plus all the fields declared in the subclassing,superclass defines more general features of the objects of its subclassing,a subclassing needs a constructor if the superclass does not have a default constructor or has one that is not accessible to the subclassing;if the subclassing has no constructor at all the compiler will automatically create a public constructor that simply calls through to the default constructor of the superclass,"
"subclassing","superclass"," would not overall, does not in constructor default constructors, is not in public methods instance, are not in constructor default constructors, adds 2 more then overall, that doesn overall,less acts in object specific smaller, shouldn in value method specific, would be outputted correctly in can not lower,more overall, initializers are automatically overall,","53918670,13559935,27675068,56811503,1192000,2806177,6113785,16494353,20668058,20306151,31459131,","if at some point you need to talk to a subclassing in a way that the superclass would not understand or cause an error needing to use instanceof then what you need is a different class not a subclassing;try to find ways to use inherited and polymorphism instead say for example in your base class you can have a method called performmainjob and using polymorphism you would define that to be a call to a method solveproblem for an engineer subclassing and a call to a method verifybooks for an accountant subclassing,if the superclass does not contain any default constructor the compiler will provide its own default constructor;no its not necessary to have a argument constructor in superclass if subclassing contains argument constructor,there is a third scenario in which a method of the subclassing has the same signature as a method of the superclass but the method of the superclass is not accessible to the subclassing it s either private or package-private and the superclass is in a different package;in this case the method of the subclassing is not overriding the method of the superclass,no because subclassing are not forced to have the same constructors as their superclass they can have more or fewer constructors;superclass has no explicit constructors subclassing are not forced to have an explicit constructor,say all instance variables are 32-bit ints for simplicity if the superclass has 3 and the subclassing adds 2 more then each instance of the subclassing will allocate 5 x 4 20 bytes -- 8 for the subclassing own instance variables plus 12 for the instance variables of the superclass,so the superclass defines the flow from an abstract viewpoint and the subclassing implement them appropriately;in your case the simplest options are to either just leave performtransform empty in the superclass or implement it as an empty method in the subclassing that doesn t require it when you mix this approach with a short comment you get a more maintainable system imo,these relations naturally arise when you impose restrictions on what you can handle--then if a subclassing means that the method can handle less acts as a superclass of since can handle everything that the subclassing can handle and more,the subclassing is then free to change the value of the properties or even provide different properties accessor and code in the superclass that uses the properties will just work;in general the superclass shouldn t have to know anything about its subclassing,one thing that i could do is just catch and log an error only when the superclass cannot be investigated;so the subclassing would be outputted correctly,the superclass has a more stringent constraint on a property content blank false than the subclassing content nullable true and i am using tableperhierarchy false,subclassing do not inherited their superclass initializers by default;however superclass initializers are automatically inherited if,"
"subclassing","superclass"," this is now more in object specific smaller, becomes more in generalized inheritor becomes, does not in deinitializers deinitializer end, will not in public methods instance, and has even more overall,more parameters than the  in object specific smaller, does not in deinitializers deinitializer end, will not in constructor default constructors, but not in public methods instance, prototypes header overall,weaker access in constructor default constructors,","6627903,17084059,24019895,8674687,34636755,56098374,40304609,14643407,38555726,3807968,31177870,","imagine your superclass has an object member but in your subclassing this is now more defined to be an integer,our superclass is a generalized class but your subclassing will be a specialized inheritor of your superclass;your subclassing becomes more specialized and less generalized as you move down the inherited hierarchy,superclass deinitializers are always;called even if a subclassing does not provide its own deinitializer,if a method in a superclass refers to a particular field static or otherwise of that class only that class s declaration of the field will be in scope at that point;any fields static or otherwise of subclassing will not be in scope,more mathematically you can think of a class a set of all objects with some similar characteristic and then a subclassing of pets surrounds its superclass and has even more characteristics,i have an arraylist executed like this im trying to pass in a subclassing object with more parameters than the superclass,superclass deinitializers are inherited by their subclassing and the superclass deinitializer is called automatically at the end of a subclassing deinitializer implementation;superclass deinitializers are always called even if a subclassing does not provide its own deinitializer,in your example above the superclass constructor calls an overridden method test;this works but is potentially dangerous since the subclassing constructor has not been called and your subclassing will not have been fully initialised,and correct way to call the superclass method is no need car.super;since the private instance variable is inherited in the subclassing but not accessible in subclassing you can use the reflection to access it,this is because when a third class imports the subclassing s header file when a third class imports the subclassing s header file needs the declaration for the superclass prototype and you don t want to have to import all the superclass prototypes header files,the subclassing overridden method cannot have weaker access than superclass method,"
"subclassing","superclass"," they shouldn in public methods instance,more methods than its  in public methods instance, would not in public methods instance, is not overall, does not in value method specific, no longer overall, is more in object specific smaller, isn in public methods instance, initializers are automatically overall, cannot in public methods instance,more values than the  in values reasonable,","4675973,53830848,16199052,41157948,22750382,25496079,50710444,26965164,56828319,36415237,50410744,","if the subclassing doesn t contain all the methods of the superclass they shouldn t be related in such a way;defining b as a subclassing of a is akin to saying b will provide all the functionality that a does plus whatever else you decide to define,however i only pass subclassing of errormessagepojosuperclass as arguments the class errormessagepojobundle has more methods than its superclass,by making the derived method private you d be changing the contract made by the superclass preventing any further subclassing from accessing the originally public method;a function passed a parameter typed to be of the superclass would not know in advance whether it s allowed to call the method or not,if superclass is not serializable then all values of the instance variables inherited from superclass will be initialized by calling constructor of non-serializable superclass during deserialization process;in case superclass is not serializable than to serialize the subclassing s object we must implements serializable interface in subclassing explicitly,the subclassing is a form of the superclass the superclass is not a form of the sub class;that is because the superclass does not have that method,letting subclass1 call through to a constructor of superclass over the head of its own superclass would break encapsulation of superclass;you cannot skip levels of the constructor if the class in the middle subclassing does not expose a constructor that calls a particular constructor of superclass subclassing of subclassing no longer have an ability to access these constructors,in the given question the superclass is the one that has a more general object as the parameter while the subclassing is more specific,you have two private_method methods one in the superclass that is private and an overriden one in the subclassing that is public;your code works because the self.private_method in the superclass isn t calling its own private private_method but the one that is public in the subclassing,i get no error when i run the following output in init and without init first read this automatic initializer inherited as mentioned above subclassing do not inherited their superclass initializers by default;however superclass initializers are automatically inherited if certain conditions are met,a subclassing is an extension of the superclass and can access any public protected package members and methods in the superclass;the superclass cannot access members methods of the subclassing unless you cast this to the subclassing,so what this says is that provided your subclassing doesn t have any more values than the superclass used to determine ordering implementing is reasonable,"
"subclassing","superclass"," should not overall, is always in object specific smaller, instance isn overall, doesn in bigger second statement, is not overall, and has more overall,more values in values reasonable,","2051084,50710444,4331653,933047,43007697,56970828,41760105,","the is maybe superfluous but it shows the fact that the parent superclass should not be instanciated;the subclassing has no additional property so the subclassing tag is empty,in my question the subclassing is less specific than the superclass meaning the subclassing is always able to handle whatever the superclass requires,as described here - one common reason to override readobject and writeobject is to serialize the data for a superclass that is not serializable itself;the state that you think you have in your subclassing instance isn t really visible to the serialization as its not going via your api or reflection,you can t cast a superclass in a subclassing because a subclassing may have a bigger interface than the superclass means a subclassing can have functions like getunixtimestamp that the superclass doesn t have,you are referencing the oak variable as class tree so any property you reference from it should be of class tree not any subclassing because exactly the superclass does not have them thus compiler does not know how to address memory from branches property;worse this addressing might technically differ from subclassing to subclassing an that s why directly addressing properties of a subclassing via reference of type superclass is not possible,admin is a subclassing from superclass and has more privileges than normal user,so what this says is that provided your subclassing doesn t have any more values than the superclass used to determine ordering implementing is reasonable,"
"antialiasing","transparent","less overall,much better overall,","23305649,1445339,","minus doesn t truly work for antialiasing less than 100 transparent pixels,and it seems working fine antialiasing is much better however this beats the idea of a transparent background in the process,"
"instrumentation","profiling"," isn in time es code,better in time es code, overhead becomes significantly larger overall, key is set correctly overall,general overall, profiled code overall,template better overall, blog;another recommended option overall, seems more overall, emit right overall, is much much more overall,","25556782,12452402,3227242,49366527,10807427,2081470,29622159,3867552,4333016,14959402,15403397,","the time profiling instrumentation isn t designed to profiling opengl es code;apple provides separate instrumentation for profiling opengl es code,if you re writing a mac app the opengl profiling tool profiling opengl code better than instrumentation,this is due to the fact that profiling need to instrumentation the code to keep track of invocations - this can interfere with the jit s ability to inline those methods and the instrumentation overhead becomes significantly larger than the time spent actually executing the methods body,enabling profiling does more profiling of your code but if your only goal is to get sql queries installing extension is all you need assuming instrumentation key is set correctly,i would definitely recommend using instrumentation to profiling what message is taking the most time so you can really break what message down,a profiling will not be a solution to a coding style that is x slower than optimal however you still need to spend time fine-tuning those parts of your code that are used more often than others;because of instrumentation profiled code on average will run slower than non-profiled code,paulw11 s comment helped--the time profiling template is better suited for this task than the system trace instrumentation,as you noted tier interaction profiling tip gives you more detailed information about calls to databases right down to the sql statements for more information refer to the profiling blog;another recommended option when profiling asp.net is to profile using instrumentation mode,knowing where inlining was really being done would give me a warm fuzzy feeling;but profiling seems more promising and useful generally;i fear the benefits are more intuitively real than actually and compiler writers are better off pursuing c++0x features run-time instrumentation introspection or writing d on the side,this is useful for code profiling but not so much for memory profiling;build mono.cecil for the cf and use it to instrumentation emit right on the target device,with the code you have here your instrumentation is much much more expensive than the work being done which makes the profiling data useless,"
"instrumentation","profiling","more info overall,","6131971,","so my question is beyond instrumentation and the profiling using memory leaks which tells you the location of creation of the leaky object but not the root cause are there any other tools i could run that could give me any more info than instrumentation re tracking down the root cause point,"
"ceil","floor","greater overall,higher in fractions direction std,print 1 as  overall,integer division as a  overall,greater overall,idx lower in fractions direction std,always higher in fractions direction std,greater than 0  overall,slower than   overall,","1719752,31774804,56203162,30584410,30812908,18961058,39355314,34590955,56249728,","the floor times the ceil are greater than the floor xy...that s very much possible,now if floor is higher than 0 or ceil is lower than 0 on any axis it means that there just as many tiles outside of the camera scoop,5 so you have to print 1 as floor and 10 as ceil then array becomes 2 so print 1 as floor and 5 as ceil and array becomes like this m queries are given,but full floating point division and ceil is more expensive than necessary;instead since c defines integer division as a floor operation you can add the divisor - 1 before dividing to the get the effect of a ceiling operation,works because ceil a b is always one greater than floor a b except when a b is a whole number,so floor idx is the lower element and ceil idx is the higher,on the other hand if what you want to do is not rounding but elimination of fractions into one direction then you have std ceil that goes always higher and std floor that always goes lower,a simple function to follow would be if greater than 0 floor else ceil using a multiplier to raise greater than 0 floor above the decimal point temporarily whilst doing it,to stop the confusion on this page actually this is the best answer which is fast and works for both positive and negative values of x i ran speed tests of 10 million computations on php 7.2.15 and even though both solutions give the same results fmod is slower than floor ceil,"
"memcpy","memmove"," is better overall,overhead than  overall, .; is faster in faster platform little,more in efficient memmove std,slower in faster platform little,more efficient in efficient memmove std,more in efficient memmove std, function properly overall,faster in faster platform little, should have that behavior overall, which has better overall,","4480128,9161130,3155805,41071676,22793669,6283024,36731330,4241239,19585930,388090,18950039,","however because of additional checks that memmove performs when the buffers are small and surely does not overlap memcpy is better,note that memmove has more overhead than memcpy because memmove has to determine which direction of copying is safe,memmove instead of memcpy .;memcpy is faster but it s not safe for moving blocks of memory where the source and destination overlap,memcpy is more efficient than memmove. in general use memmove only if you have to,bad news is that the asmlib version of memmove is slower than the glibc version it is now running at the 300ms mark on par with the glibc version of memcpy,as an aside my c c++ is rusty but is not memcpy more efficient than memmove if you know you don t have overlapping memory,as already pointed out in other answers memmove is more sophisticated than memcpy such that it accounts for memory overlaps,the memmove function properly handles overlapping source and destination;memcpy is faster on some platforms and can be safely used to copy between strings,so in what platform and how memcpy can be significantly faster than memmove if there is none why providing two similiar functions instead of just memmove and lead to a lots of bug,memcpy should have that behavior;memmove doesn t by design if the blocks of memory overlap it copies the contents starting at the ends of the buffers to avoid that sort of behavior,libc s memcpy is likely to be much better optimized using larger-than-byte units platform-specific performance tricks example inline assembly sse on x86 etc;there s also memmove which has better specified behavior when the buffers overlap,"
"memcpy","memmove"," is more in efficient memmove std,slower in standard overlapped ub,slower std in efficient memmove std,cache friendlier overall,2x faster in faster platform little,faster overall,slower in faster platform little,slower in faster platform little,slower in faster platform little,faster than  overall,slower in faster platform little,","41071643,17552734,44970454,28623895,22793669,4480146,28623895,41963723,1201337,49837278,18176857,","memcpy is more efficient than memmove. in your case you most probably are not doing the exact same thing while you run the two functions,while memmove will be only slightly slower than memcpy due to the assumptions it needs to make about the source and destination in memcpy they cannot overlap it should still be far superior to any standard loop,std memmove may be very slightly slower than std memcpy emphasis added because it has to first check whether the source and target ranges overlap,in addition as mats petersson said memmove is cache friendlier than memcpy,edit memmove is 2x faster than memcpy on the server,if you know buffers cannot overlap memcpy is fine and may in any given library use optimizations that allow it to be faster than memmove,memcpy is still a little bit slower than memmove,why is memcpy so much slower than memmove or hand rolled copy on the server,this means that memmove might be very slightly slower than memcpy as it cannot make the same assumptions,if you know for sure that src and dst don t overlap call memcpy as it won t matter which one you call for the result both will work correctly in that case but memmove will never be faster than memcpy and if you are unlucky it may even be slower so you can only win calling memcpy,why does memcpy perform slower than memmove on my system,"
"memcpy","memmove","greater overall, is potentially slower in standard overlapped ub,faster in faster platform little,faster in faster platform little,faster in faster platform little,efficient than  in efficient memmove std, is not in efficient memmove std,slower in faster platform little,","8205192,51699331,18176857,35210091,19585930,11717714,33147994,44966288,","it is entirely possible that in most implementations the cost of a memmove function call will not be significantly greater than memcpy in any scenario in which the behavior of both is defined,according to the c c++ standard using memcpy with overlapped memory is ub on the other hand memmove is potentially slower,from reading other so questions such as this or this gives the impression that memcpy should work faster than memmove and intuitively this should be so,on some arm platform im working on memmove was 3 times faster than memcpy for short unalligned load,the question is about is there really any platform where memcpy is faster than memmove,std copy to be more efficient than memcpy or memmove because std,or you can use memmove which permits overlapping memory space memcpy is not safe for overlapping copies;memmove is more efficient than a loop though optimizing compilers may solve that,and it said memmove might be very slightly slower than memcpy,"
"png","tiff","approach smaller in bit depth sink,smaller overall,much more overall, format uses better compression overall,larger in bit depth sink,better in bit depth sink,","32949970,9806763,34695038,2336582,44229626,4362006,","the resulting file from this png approach is smaller in size than a tiff file and i guess may rescale better,jpg or png should get you smaller than tiff,today png is much more popular than tiff so if you re writing files outside of your own data store png would be a more common choice and you d need to work through nsimagerep to get there,digital photographers use image file formats capable of reproducing a greater range of colors such as tiff raw or the lossy jpeg which is more suitable for compressing photographs;the png format is a popular alternative to gif images since the png format uses better compression techniques and does not have a limit of 256 colors but pngs do not support animations,the resulting png compression may produce a file size larger than your tiff compression,tiff â any bit depth any compression lossy or lossless everything including the kitchen sink â and no better than png,"
"struct","typedef"," isn overall,general overall, declare the account overall, to catch the mistake overall,foo easier overall, also provides stronger encapsulation overall, is not already overall, is better in struct to use,more overall,better choice in struct to use, is actually overall,","2355059,57741633,17938448,48843829,8422898,42335607,47860315,18424464,31443691,32624293,4977633,","for as long as i remember c had typedef but i don t know if it was true when struct have been introduced handling of typedef is a nuisance in the c grammar;in the old code i ve seen using typedef for struct isn t done and there are things like unix,you re allocating the size of a pointer here instead you should do this although i would prefer removing typedef struct list list,within struct declare the account after typedef;typedef doesn t declare,before this lines there are more than 6k typedef struct like this and even that i make comments over each typedef struct to catch the mistake i couldn t figure out how,struct foo is easier to parse then typedef d foo as the name-lookup is simpler,now there s only one way to refer to the struct which is easier to read;the typedef also provides stronger encapsulation should there be a radical change to the type like switching to an integer which indexes other storage not likely but you never know,but it s seems a bit weird to typedef a struct that s not yet declare at my opinion;when you are declaring your struct your typedef is not already effective the typedef is usable at the end of the structure declaration ie after,why a typedef struct;to use a struct is better than use individual variables,this is a good example of a time to know the basics - i think understanding the ins and outs of struct is more helpful than typedef since you can do so much more with it,also i think typedef is a better choice when you want to define a struct,when the error message indicates that ttf_font is actually a typedef not a struct;the struct is actually called _ttf_font,"
"struct","typedef"," code provides a smidgen overall, is clearer overall, element not overall, not only overall, isn overall, studentrecord makes for cleaner overall, is larger overall, does not overall,s incredibly longer overall, is no longer overall, is better overall,","29451986,16279245,7474998,11595647,22153884,12342238,36896454,10037463,41018933,53970630,23240206,","if you re using typedef you no longer need to write struct all over the place by useing typedef code is more cleaner since useing typedef code provides a smidgen more abstraction,one camp most notably the linux kernel people thinks that struct a is clearer than the typedef;the other camp pretty much everyone except the linux people thinks that typedef is clearer,note that you need to refer to the type as struct element not element within the definition itself since the typedef name element isn t visible yet;the fact that the struct tag and the typedef have the same name may seem confusing but it s perfectly legititimate,without the tag you are typedef -ing a tagless struct;you need to provide the tag for your struct not only a typedef,in c a struct is not a type by itself the type corresponding to a struct called s is struct s which is usually named via a typedef;in c++ the typedef isn t needed and the syntax is largely irrelevant,o your typedef above is creating a synonym for a student record struct so you can pass around student records without having to call them struct studentrecord every time;struct studentrecord makes for cleaner and more readable code,since the struct is larger than this you re writing to members that are past the memory offset of the allocated size,typedef does not create types in c but creates alias names for existing types;struct my_struct is a name of a type my_struct is a tag name and not a type,you can verify with a simple objdump symbols objfile.obj that the length of decorated symbols by using typedef s is incredibly longer than their similar counterparts split into struct s microsoft compilers have historically used a proprietary name mangling scheme,o assign to items you must assign the address of each struct to each element in items remember you have storage for pointers not storage for struct item -- and you must ensure fruits remains in scope for the duration of your use of basket putting those pieces together you could do something similar to the following note i have simply used typedef and removed the struct labels themselves -- typedef s up to you;further you should validate the return from find_first_in_range is not null before printing example use output also note i ve bracket the high low range for memory use error check in any code you write that dynamically allocates memory you have 2 responsibilities regarding any block of memory allocated 1 always preserve a pointer to the starting address for the block of memory so 2 typedef can be freed when typedef is no longer needed,ou also need to typedef the struct to pic_t not make the struct as an structure object;well the struct is better explained with code in user3386109 s answer,"
"aggregate-functions","sum"," is greater overall,more general in answer version general,more in answer version general,better with an  overall, population is greater then overall,more overall, is probably more overall,","2678137,10256374,5653498,57523688,57303330,3323081,53873819,","hen using group by you should only return columns that are named in group by clause or that include an aggregate-functions;therefore the inner select here gets the col2 values where the sum is greater than or equal to zero then the outer select grabs the entire row for those values,aggregate-functions is more general version of sum,as an added answer there is also the aggregate-functions which is more general than sum,if you want to get a single value from the query it s better to use the value function or even better with an aggregate-functions like sum the error you have is because you are trying to sum two standard objects the get function returns an array of standard objects you can also solve using this line with your own queries,having is used to calculate aggregate-functions;like in your above case if you want to find out name of the countries having sum population is greater then 2 000 000 then you can apply having clause like this for more info about the same you may find this link,i changed out total for sum which is more consistent with other databases sqlite aggregate-functions,with conditional aggregation you usually need the condition inside the aggregate-functions like so also while composing this i noticed sum is probably more appropriate for what you want,"
"openrasta","wcf","much more mature in mature web api,more in mature web api,better overall,simpler overall,","6227583,6227193,6487982,2288413,","openrasta is much more mature than wcf web api,though openrasta looks more mature than wcf web api i m still a bit confused,is openrasta better than wcf,building restful services with openrasta is much simpler than with wcf in my experience,"
"fat","filesystems","easier overall, permission based way overall, does not so overall, is not in consideration platforms facilities, is more or less overall,more complex overall, is generally but not always overall, is very overall, doesn overall, is not in consideration platforms facilities,","8219726,7419841,24342583,56376599,55701488,2398216,17607465,8925017,3689681,51431426,","granted fat is an easier problem but they claim to support many filesystems and it would be my first choice,since the filesystems is more than likely fat or fat32 protecting the filesystems permission based way would be more convoluted,note if you are using the default file-based session handler your filesystems must keep track of access times atime;windows fat does not so you will have to come up with another way to handle garbage collecting your session if you are stuck with a fat filesystems or any other filesystems where atime tracking is not available,it says the filesystems library facilities may be unavailable if a hierarchical filesystems is not accessible to the implementation or if it does not provide the necessary capabilities;some features may not be available if they are not supported by the underlying filesystems the fat filesystems lacks symbolic links and forbids multiple hardlinks,a filesystems is more or less nothing but some description how files are stored on a disk;if a disk uses the fat16 filesystems there are three areas on a disk the fat the root directory and the clusters,most modern filesystems are also considerably more complex than fat which would add further difficulty to the implementation,but a proper filesystems must be used and fat is not the only filesystems jffs2 yafs ...some other proprietary filesystems;the filesystems is generally but not always implemented on flash memories nand flash nor flash,different filesystems not different operating systems have different capabilities for storing metadata;ntfs has plenty of possibilities while fat is very limited and ext are somewhere in between,naturally the underlying filesystems must also support a file that large;on windows ntfs does but fat doesn t for instance,also take into consideration that std filesystems might not be available on all platforms the filesystems library facilities may be unavailable if a hierarchical filesystems is not accessible to the implementation or if it does not provide the necessary capabilities;some features may not be available if they are not supported by the underlying filesystems the fat filesystems lacks symbolic links and forbids multiple hardlinks,"
"fixed-point","floating-point","more expensive time-wise overall,more resolution than  in bits resolution exact,arithmetic less overall,faster software overall,faster overall,accurate than  overall,more exact in bits resolution exact, not overall,","30960317,56370504,9581376,14769407,13997412,56370504,3737403,6248325,","floating-point calculations are more expensive time-wise than fixed-point which is why fixed-point remains popular in microcontrollers and embedded systems,therefore using the same number of bits fixed-point has more resolution than floating-point,it s not necessarily true that the matlab fixed-point arithmetic provides less precision it can be used to provide more precision than ieee floating-point types,anthony williams fixed-point maths library provides a complete analogue of the standard maths library for a fixed data type that is typically around 5 times faster than software floating-point on the same target,armv7 is usually better but for arm fixed-point arithmetic is usually a lot faster than floating-point implementations,note that the fixed scale means fixed-point will sometimes be less accurate than floating-point,fixed-point can be much more exact than floating-point as long as the number s exponents remain in range,this is because ieee floating-point is stored essentially as scientific notation so range is favored over uniform precision;if it was uniform precision it would be fixed-point not floating-point.,"
"richfaces","trinidad","more mature overall,more basic overall,","2402860,2402860,","i think richfaces has a more mature ajax integration and their ajax components and events are more easy to use for me than trinidad ones,regarding skinning i feel trinidad default skinning to be more basic than richfaces one but i think both have ways to make them as beautiful as your css skills allow you,"
"mouseleave","mouseover","better overall,more reliable in reliable, event not overall,better overall,more reliable in reliable,","46256578,11345144,53548641,8060841,13688947,","in this case mouseenter mouseleave has better behaviour and prevents bubbling compared to mouseover mouseout,also you might want to use mouseenter and mouseleave which are sanitized by jquery and are a little more reliable than mouseover and mouseout,try this instead or to make use of jquery to reduce code you could simply do this mouseover will cause this to fire every time the mouse moves one pixel over the element so this could result in rapid flashing you may want to use the mouseenter and mouseleave to have better control over this behavior;the problem is that you are assigning the result of the function to the mouseover event not the function itself,it should also be noted that mouseenter and mouseleave work somewhat differently and usually much better than mouseover and mouseout,imho mouseenter and mouseleave are much more reliable than mouseover and mouseout which tend to flicker,"
"unicorn","webrick","faster overall,faster overall,faster overall, is not overall,","22414367,17107729,22414422,21633735,","i ve also found unicorn to be faster than webrick especially in production applications and apps running on heroku,my personal experience is that webrick is faster in my development environment than unicorn and thin os x in a pretty big rails app lots of gems routes etc,unicorn is supposed to be faster than webrick,so unicorn is ok but thin and webrick are not mmmm;my guess is that the working directory used by the daemon process for thin and webrick is not rails.root,"
"redmine","trac","better in multiple projects box,more overall,more overall,more clean overall,nicer in multiple projects box,better in wound authors easier,nicer overall,better in wound authors easier,more complete overall,better in wound authors easier,better in wound authors easier,","386835,1936043,1275337,3989495,294097,1277646,5309957,1050501,380757,323542,5747934,","redmine is an open source ruby on rails application that supports multiple projects much better than trac and seems to be much easier to administer,i understand redmine is more of a trac clone but retrospectiva seem to support agile methodologies via a nice plugin,we ve recently switched from trac to redmine where i work and i think i like it a little more than trac and use the wiki functionality almost everyday,for next projects i m going to try redmine seems more clean and hipe than trac,redmine handles multiple projects and sub-projects right out of the box and overall seems nicer than trac,redmine was written to be a better trac than trac,redmine handles multiple projects and sub projects far nicer than trac,we found redmine to be a better than trac simply because it is easier to use,i d recommend redmine like the other posters as it is more complete than trac,the authors of redmine are trying to create a better trac than trac,i wound up going with redmine and it s way better than trac,"
"redmine","trac"," doesn overall,","927797,","i know redmine has an additional feature that checks for these bug numbers in the commit logs and attaches the revision to the bug so you can see from the bug all the revisions that were involved in fixing it;i d be suprised if trac doesn t have something similar,"
"nerdtree","netrw","better in better last,better in better last, called  help in default plugin help,more overall, plugin is more in default plugin help,more overall,","23161254,23161453,17917339,8950449,2271760,19765721,","last question is nerdtree really much better than netrw,is nerdtree really much better than netrw,however vim comes by default with another plugin which does a lot more than nerdtree called netrw help netrw,vim includes netrw which is already a filebrowser and i like it a lot more than nerdtree,the nerdtree plugin gives you a collapsible project tree;personally i find that the netrw plugin is more than sufficient and this usually comes installed with vim,in the netrw plugin it would be easy but this one is not used any more as nerdtree just replaced it,"
"factors","mean"," is obviously better overall, is less overall, level here overall, is smaller in growth larger smaller, or is slower overall,more than a  overall,bigger than  overall,slower by a  overall,larger in growth larger smaller, throughput is much higher overall,","2963234,20741194,54155368,21847850,25721448,50587214,55727749,55923178,10197144,53655161,","aking a guess at what you mean a subquery in the with clause is typically executed only once before the main query is executed;for large datasets subquery factors is obviously better since you re executing a subquery in the with clause only once in most if not all cases,note that this does not mean that the largest prime factors is less than sqrt n but that if there is a prime factors greater than sqrt n there is only one such prime factors,it s important to note that when calling t-test with the formula interface y x and where your independent variable is a factors then using the setting will test whether the mean in the lower factors level in the case of your data inter is greater than the mean in the higher factors level here that s mono,if the factors is smaller than the golden mean 1.6 and the previous allocations are contiguous with each other each other ll eventually add up to a chunk that can satisfy a later requirement,it means that asymptotically x is either only faster than y by a constant factors or is slower than y,the benchmark shows dplyr is slower by more than a factors of ten microbenchmark microbenchmark myfun1 myfun2 myfun3 myfun4 myfun5 myfun6 unit microseconds expr min lq mean median uq max neval myfun1 5356.6 5739.90 6320.338 5967.45 6327.75 11177.7 100 myfun2 6208.1 6676.55 7220.770 6941.10 7172.55 10936.3 100 myfun3 8645.3 9299.30 10287.908 9676.30 10312.85 15837.1 100 myfun4 4426.1 4712.40 5405.235 4866.65 5245.20 12573.2 100 myfun5 168.6 250.05 292.472 270.70 303.15 2119.3 100 myfun6 141.7 203.15 341.079 237.00 256.45 6278.0 100 the code,in order to run the factors analysis removing the outlier those which is smaller than mean minus 3 standard deviation and bigger than mean add 3 standard deviation is quite important from my understanding,the code for the character-counting solution is simpler but slower by a factors of 3-4 for integers 1 unit nanoseconds expr min lq mean median uq max neval ndigits 100 1711 2139 2569.2819 2566 2994 2234046 1e+05 ndigits2 100 0 428 861.5435 856 856 5670216 1e+05 for really tiny decimals unit nanoseconds,if your growth factors is larger than the golden mean that can t happen,since i wasn t sure if perhaps i had a hidden blocking io request somewhere i figured it would be worth testing the example project from the akka http site alternatively you can bootstrap a new sbt project with akka http already configured using the giter8 template i ve gone ahead and boot strapped it as per the instructions and run the server on local host i ran some very trivial tests with ab tool simple test performing sequential requests we see that the in this case now i bumped the concurrency up to 5 now time per request has increased quite sharply 2.040 ms mean throughput is much higher though and again bumping up to 50 concurrent requests here the latency is extremely high at 13.861ms vs the first case which was at 0.880ms latency increased about factors 16 this simple server has no blocking io,"
"mips","x86","more complex instructions in complex instructions care,more orthogonal overall, is more overall, instruction set bitwise overall,more complicated in complex instructions care,more straightforward overall,easier in easier b data,easier in easier b data,less complex in complex instructions care, config has better overall, is less overall,","454121,9219871,39440536,45323133,40397887,2656592,1382636,16701685,12800438,6964301,7754188,","x86 have more complex instructions than mips,mips is much more orthogonal than x86 could ever dream of being,other than that everything looks really obvious and straightforward if you ve seen x86;iirc mips is more like the one step version since instead of a flags register its compare instructions use gp registers as outputs,as i don t do mips programming i m not sure which instruction can be used on mips on x86 the not would do;so check through the mips instruction set bitwise operations probably,i have studied the x86 assembly and architecture and it appears to be a lot more complicated than mips,a better bet might be to install spim and to learn mips assembly which is more straightforward than x86 anyways,do any of the other common architectures like arm mips sparc etc have an easier than x86 instruction set,if you have not programmed assembly before i suggest you choose mips since it is easier than x86 and then b looking at how to transfer data with the serial port to begin with since this illustrates memory-mapped i o that is used,the wording in your question seems to suggest you don t care as much that the output is mips but rather you want the output to be less complex than x86,update x86 config has better comments;for arm mips sparc and others archs which limits unaligned access to memory the alignment requires of any machine instruction may be recorded in arch.md file in sparc.md,if you pick up something like mips and get the hang of something like mips x86 is less of a monster to learn,"
"rjdbc","rodbc","slower overall, which is in cran overall,longer overall,better overall,","21188796,29319354,46314548,5965234,","now the speed of sql server interaction with r has affected because rjdbc is slower than rodbc but its not too bad,rodbc isn t really supported on osx;what i ended up doing is to use rjdbc which is in cran,rodbc takes longer than rjdbc,rjdbc may also be a more stable option in the linux environment especially now that the fetch code has been re-written in java as of development release 0.2-0 on r-forge its performance is on par with if not better than rodbc,"
"uilabel","uitextview"," which is more overall, does not in right bigger inherent, to get access overall, do not overall,bigger in right bigger inherent,much more overall, may not overall,more overall, is smaller overall, and restore the string overall, is derived from uiscrollview overall,","21611750,17891208,21838393,4183683,39268477,18561194,48392840,32473074,51154321,861775,30921563,","i tested above code with about 4000 string rendered into 100 different widths and all results were equal to the uilabel i used for comparison;i did the same to calculate dimensions of uitextview which is more complicated and required to set up an nslayoutmanager,uilabel does not have any inherent padding applied;uitextview has padding that nsstring s drawing methods do not have,uilabel isn t built with textkit on ios7 at least not in a way that is accessible to developers;you d need to use uitextview to get access to textkit,but my suggestion is that instead of using multiple uilabel just concentrate on nsattributedstring;find uicontrollers that draw nsattributedstring because uilabel uitextview do not support nsattributedstring,i think the problem happened because the padding left right of uitextview is bigger than uilabel,i m aware that uitextview is much more suitable for this as it implements uitextinput protocol which has all the methods i need but due to a bug in ios i m limited to uilabel,apple changed uilabel to pull down orphan words so even if you get the lines right a uilabel may not give you the look of what you are doing since it could pull a word down;use a uitextview and disable scroll editing disabled and possibly selection disabled,my problem is uitextview is more complicated than uilabel,in my situation using the uibutton was the best solution because i had a simple single line text i didn t want to use uiview as a container for uilabel i wanted to simplify math calculations for autolayout in my cell i didn t want to use nsparagraphstyle because tailindent works incorrect with autolayout width of uilabel is smaller than expected i didn t want to use uitextview because of possible side effects i didn t want to subclass uilabel less code fewer bugs that s why using the contentedgeinsets from uibutton in my situation become the easiest way to add text margins,uilabel are not editable however you can detect a touch with a uilabel or it s superview;when you detect the touch on the uilabel you simply restore the hidden uitextview and restore the string you saved,uilabel doesn t support selecting and copy;uilabel is derived from uiview but uitextview is derived from uiscrollview,"
"uilabel","uitextview"," but not overall, did not overall,slower overall, not overall,same as a  overall,","20348612,26369394,4925945,29460445,33563883,","if you want to use sizewithfont it works fine for uilabel but not necessary with uitextview;uitextview is a subclass of uiscrollview and you should use content size after you set up a text to get the size,there my uilabel were localized properly on the device not in the simulator;however my uitextview did not get localized which took me another 30 minutes to figure out,also consider using uilabel instead of uitextview if you don t need to edit information inside since uitextview take longer to allocate and init and are generally slower than uilabel,uilabel is a view uitextview is a view too the method boundingrectwithsize is not the same thing as a view frame is just the bounding rect to contain that text;as suggested by aaron uitextview contains padding while uilabel not,for this you might want to consider using uitextview instead of uilabel which doesn t give you access to how it manages layout the text;by disabling editing selection and scrolling uitextview behaves roughly the same as a uilabel except some padding you have to remove,"
"pre","textarea","better overall,more line overall,","30758628,15693980,","also you can do this with textarea or better with pre html element,textarea has one more line than pre,"
"mockito","scalamock","longer overall,more overall,","41273504,41272971,","mockito has been around a lot longer than scalamock and is much more actively maintained because of it s much larger contributor base,scalamock looks more natural when developing applications in scala however from what i read it seems to support fewer test scenarios compared to mockito and it s also less mature,"
"swing","swt","better overall,faster overall, is simply easier overall,many tools as  overall, doesn overall, has far far overall,harder overall, are not overall, offers however in alternative eclipses portable,more overall, that doesn overall,","20930869,1298464,5329644,5198877,531083,9085526,4508736,9990747,2927116,269117,11805018,","in java if you want a graphic user interface you can use swt or swing and swing is better than swt,is swt faster than swing,so in my view using swing is simply easier unless you have more experience in swt,swt community is smaller and it doesn t provide that many tools as swing does,however there are a few things that swt does that swing doesn t do as well or at all -- such as launch the system s native browser and execute javascript scripts in this browser;if you consider swt consider it carefully,even though i liked many design aspects of swt and it is simpler than swing imo swing is the one to learn because imo imo swing has far far better documentation examples etc,fixing bugs in swt is much harder than in swing most classes in swt are final or contain hostile checks in the constructor to make sure no one extends them the package is signed so you can t simply replace classes and so far i tried several times to compile swt from sources - and failed,if you re using swt have a look at this;unless explicitly otherwise stated in the javadocs swing components assuming you re talking about swing are not thread-safe,he only alternative swing has is swt eclipses toolkit however the only alternative swing s not portable as swing and not as flexible as well;the only alternative swing offers however faster performance and the use of native components which might be what some people actually want,swt is more simple to use than swing especially if this is your first ui,this library also lets swing applications run unmodified using an swt backend since your program is already written using swt that doesn t help you but it will others,"
"swing","swt"," is more overall, aren in swing in contrast, is actually newer and more overall, requires slightly more in swing in contrast,more platform-specific overall,slower overall, is faster overall, has become much faster in alternative eclipses portable, is more overall, you get some support overall, is directly overall,","27206810,5863681,16695135,2914733,2281068,35038895,3072211,2625704,4579928,9086223,1809053,","these two are completely different logically speaking and you should choose only one of t to go further with note that swt is more customizable and tries to combine the best from awt and swing while swing is more limited and uses the platform s native widgets,by contrast swt actually uses the operating system s windows so naturally it has less memory usage and less work to do on its own;the windows in swing aren t really windows they re just made to look like windows,also swt is actually newer and more actively maintained than swing,addendum gilbert le blanc raises an excellent point about the ease portability of swing;in contrast swt requires slightly more effort to deploy but some users prefer the greater fidelity of org.eclipse.swt.widgets.filedialog as shown here,swt has a more platform-specific look about it but java ships with swing built-in there s no messing about with external libraries as with swt although the use of eclipse may make that much easier i still develop quite a bit of my stuff from the command line unfortunately,swt became slower than swing because it uses proxy to os widgets,but in practice it all depends on swt and swing implementations on each platform;in my experience swt is faster in windows but it is not faster on mac os x,we rely on swt for we java guis;yes you have to include an external native library but the look and feel is native more responsive although swing has become much faster in the past few years and seamless with other apps on the target platform,there are two serious contenders for a gui swing and swt;swing is more mature arguably is part of the standard jdk no deployment issues very flexible and well-documented;swt makes it easier to behave like a native application across different oses but this also means significantly more portability issues,so if you are programming raw swing you get some support but still have to do a lot of work by yourself;swt asks for more basic effort and discipline from your side,this is usually problem of swing not swt swt is directly linked to os framework provided widgets - a quote from a gentle introduction to swt and jface 2;swt is a library that creates a java,"
"swing","swt","uglier api overall, is more overall,simpler overall,weaker overall,more familiar overall,usually easier overall,faster overall,easier overall,","6819467,4476826,3186430,15597946,8136079,1037130,8700142,9333610,","i find swt to be an uglier api than swing in general though but not really a deal breaker,it is commonly argued on the internet that swt and jface are more suitable for quick out-of-the-box type ui development on the eclipse platform because swt and jface work so well together eg;jface wizards databinding and validation whereas swing is more suitable for enterprise level development in netbeans that requires more ui customising,give swt a go the api is a lot simpler than swing,swing is the weaker of the gui technologies relating to accessibility in java compared to swt at any rate,i m not too eager for using swt though since i m more familiar with swing,it can be hard to get the layuots exact the way you want in swt it s usually easier in swing,swt is claimed to be faster than swing although in modern versions swing is also fast enough,in my opinion swing is easier to learn and to use but the results of swt are often nicer - you can often feel that a swing application behaves not completely right,"
"uipickerview","uitableview"," is more overall,much more customizable overall, why not overall,nothing more overall,","17018327,1440450,26348924,14856860,","a uitableview is more suited to displaying a large number of choices;similar to above use a uipickerview inside a pop over,uitableview is much more customizable than uipickerview,if so rather than use a uitableview why not use a uipickerview also placed below above beside the labels;a uipickerview would accomplish the same thing only in a vertical representation,the uipickerview is nothing more than a uiview with one or more uitableview and background and selector views,"
"django","flask","recommend going with  overall, is a more in framework smaller micro,smaller in framework smaller micro,more complicated in simpler comfortable friend,familiar with  in familiar site difficulties,better choice than  in cherrypy web2py django, is probably overall,general overall,recommend  over  overall, has more community has more overall, but provides more functionality in framework smaller micro,","55605471,57546829,13186525,12781655,55680000,53845615,11760761,27008507,56144191,25871904,52784532,","i would recommend going with flask rather than django as you ll have far less to set up and far less to learn to get your project running,flask is a simple micro framework whereas django is a more advanced mvc like framework,flask has a really great albeit smaller than django community and there are a lot of extensions available for common web-app extensions in the extensions directory,but a friend told me to begin with flask as it s simpler and will help me when i start learning django as django is more complicated than flask,i m trying to set up a site in flask formerly familiar with django and i m having difficulties getting templates to render properly,if in fact you just need a web api wrapper flask is probably a much better choice than django simply because django is huge and you d be using only a fraction of its capability,but django is probably not the fastest solution;flask is lighter,if you project may grow big or you will use python to write big web project django is worth to learn as it s the most powerful web framework in python;if you only need a simple web application that even doesn t need db flask is simpler to learn as paulo said,flask is very similar to django but i d recommend django over flask if you plan on building this out as a larger project,yes flask is simpler;but django has more community has more batteries,using the django web framework which is harder to learn than flask but provides more functionality,"
"django","flask","better integration in integration mongodb first,familiar with  in familiar hsname request.post.get,simplier overall, development is faster overall, is better for beginners precisely in integration mongodb first,familiar with  in familiar site difficulties,comfortable with  in simpler comfortable friend,more light overall, not overall,minimal than  overall,more lightweight overall,","18607458,57419286,44696924,29556469,25872121,55793703,49206357,32937016,41900544,51379585,17385593,","but if the use of mongodb is a must then i suggest that you reconsider using django in the first place because mongodb has a better integration with flask through mongoengine,as i am not familiar with django or flask,current task looks like flask is ok for it since it is smaller and simplier than django,django is a safe tool it handles many security issues but it might be longer to update the code;flask development is faster but you might spend more time on database orm settings configuration,now you can get all those same features with flask if you re prepared to do some integration work but the argument can just as easily be made that django is better for beginners precisely because the argument does come with all those things built-in,you can of course use django instead of flask for the server though i m more familiar with flask and django may introduce new dependencies,if you re not comfortable with django yet consider starting with flask instead as it s simpler to get going,our team use django as our frame before but now we use flask as our frame because it is more light than django but we still use django template.,you can also have a look at alternative implementation using flask not django on raspberry pi for controlling gpio;django is written in python,flask abstracts all the boring wsgi http stuff nicely but flask s more minimal than django,either will do the trick but bottle or in my preference flask will be faster as it is much more lightweight than django,"
"django","flask","easier overall,better off with python  in things better framework, is considerably more overall,more comfortable overall, and has url routing overall,more overall, is better in cherrypy web2py django,more out-of-the-box overall,smaller in framework smaller micro, is more in things better framework,general overall,","43979250,15142566,12399834,10778346,6487945,27329343,20761439,46994273,3777454,38278549,54440759,","both frameworks are fairly easy to implement flask is much easier than django imo although django has a built in authentication layer that you can use albeit more difficult to implement in a client server scenario like you need,i can think of two relatively straightforward things you can do without ditching php though i have to mention that php doesn t have much to recommend ditching php and you would likely be better off with python django python flask or ruby rails,but you should really accept blender s answer as flask and others like bottle and web.py are the right way to accomplish your task;as you ll see the simple way in django is considerably more involved,i m building an application in both bottle and flask to see which i am more comfortable with as django is too much batteries included,that is an option i would recommend flask s a lot lighter weight than django and has url routing and templating,flask is more focused on simplicity instead of functionality while django has more functionality, ve played with flask cherrypy and web2py;django is better than all of django even if i just want to build something small and lightweight i d choose django,django is very batteries included meaning that it comes with much more out-of-the-box than say flask and to me a large part of that the user auth system,if you are looking for the latter then flask is a micro framework that is considerably smaller than django,django is more of an opinionated framework which may be better if you re just starting out and want to get up and running quickly;flask is more open-ended but will also require more from you both in time and knowledge,can be whit flask django or any technology,"
"django","flask"," it is much easier overall,better overall,familiar with  in familiar hsname request.post.get,","31780800,28570968,50162640,","the code below uses flask but pure sqlalchemy code is almost the same;as already noted by kevin-christopher-henry there is little point using django orm with non-django framework as well as the other way around if you use django it is much easier to stick to django orm,i read that flask for beginner is better than django because educational reasons at the same time django have django oscar,you could put your button in a form with a hidden input and access the value with request.form.get hsname or request.post.get hsname depending on how flask implements this i m more familiar with django,"
"mdpi","screens"," is probably more overall,resolution bigger in size larger common,shorter in size larger common,destiny more overall,density less in size larger common, phone has a smaller in size larger common, has smaller in g1 bitmaps device,slightly taller in size larger common,smaller in size larger common,size larger in size larger common,smaller than a  in g1 bitmaps device,","15455987,15987663,28262431,28115430,13482946,28541798,18303913,11429834,46668634,19835542,1951706,","regarding the ldpi-icon issue i guess the nexus 10 and only relevant for the launcher icon not the screens density in general that s still xhdpi are simply becoming less relevant as fewer and fewer devices are sold with a ldpi-density and downscaling from mdpi is probably more cost-effective than having an extra image-file in your resources folder,i read in some tutorials that android automatically scales up the images when the screens resolution is bigger than mdpi,i have used this link to generate nine-patch images however even if i add the files to the drawable ldpi mdpi ldpi xhdpi and xxhdpi folders the image is still shorter than the screens width,if device screens destiny is more than mdpi - use default values folder,if the screens density is less than xhdpi i load smaller sampled sizes of the 96x96 image - as 36x36 for ldpi 48x48 for mdpi 72x72 for hdpi,addressing issue #2 if your mdpi phone has a smaller screens than 600dp your mdpi phone has a smaller screens than 600dp will use the layout in res layout folder,in this case android will pick the images from the appropriate folder drawable-mdpi depending on the device s screens density but the actual dpi value encoded inside the image file is still irrelevant;images in these different folders should have different physical pixel sizes mdpi has smaller images than hdpi but the images pixel densities aren t used,proportionally the hdpi screens is slightly taller than the mdpi screens,ldpi assets will look bad on high density screens but are exponentially smaller than mdpi which is exponentially smaller than hdpi etc,since the screens size is larger than the common mdpi screens sizes 320x480 the text size looks smaller in this emulator,so a g1 will show mdpi bitmaps slightly smaller than a mdpi device with a true 160dpi screens but this is a very small difference so this s not a big deal,"
"cloudant","couchdb"," doesn in offers different scalable,more features overall, has supported this kind overall, and probably overall,more overall, doesn in offers different scalable,","38987604,20685886,44021417,30769972,16593266,38987604,","cloudant offers full text search couchdb doesn t;cloudant has a different authentication system,due to many improvements to couchdb and that cloudant has more features than vanilla couchdb my suggestion would be to use a c++ http library to communicate with cloudant,note however that cloudant does not support rewrites as functions;according to the couchdb docs couchdb has supported this kind of rewriting as stringified functions since couchdb 1.7 but cloudant s documentation doesn t speak about this particular functionality only rewrites from arrays,proper handling of the last case is more complicated under cloudant and probably couchdb 2.0 due to the last case totally broken quorum handling but that s a different topic.,regarding cloudant it s more or less just couchdb so to connect you can use these libraries from the couchdb wiki,cloudant is horizontally scalable with respect to data size couchdb isn t;cloudant offers full text search couchdb doesn t,"
"filesystems","ntfs"," is much harder overall, doesn overall, has much higher resolution again overall,less than  overall, is not overall,much more overall, partition took 3x longer overall,more tricky overall, or not overall, however is a little overall, not overall,","21170079,47803352,8055818,35627,55360322,3551199,11597004,21863136,55271015,16487504,8925017,","both zfs and ntfs are implemented in fuse;if you go the kernel route you will find your filesystems is much harder to debug and that the development process is much more complicated,according to my best understanding the simplest way to detect whether an ntfs filesystems contains a bootable windows is checking that any of the files bootmgr or ntldr exists in the root directory because one of these files will be loaded by the boot code;the ntfs boot sector first 512 bytes of the filesystems doesn t contain definitive information about bootability because it can be exactly the same for bootable and nonbootable filesystems,i wrote about filesystems esp;fat since the timestamp resolution of fat is someabout 2 seconds while ntfs has much higher resolution again this could have an impact when comparing timestamps.,when people say that unix filesystems are better people might mean to be saying oh ext3 stores bits in such as way that corruption happens way less than ntfs but people might also be talking about design choices made at the common layer above,your filesystems is not set up correctly;by default ntfs is case-preserving but case-insensitive,ntfs is much more complex and time consuming due to the more complex nature of this filesystems,perhaps my real issue was that since they were on different filesystems my bottleneck was i o;converting just one file from the ntfs partition took 3x longer than if i moved it to the ext4 partition,if you need to load from a well-defined filesystems fat or ntfs this is more tricky you have only 450 bytes of space because 60 of the 512 bytes are used by the filesystems internally for code that interprets the data of the filesystems finds the file containing the code and loads it into memory,even if the ntfs wikipedia page mentions the guid ebd0a0a2-b9e5-4433-87c0-68b6b72699c7 for ntfs in the gpt case this guid is in fact unrelated to filesystems see comment below about this;it s probably easier to use getvolumeinformation instead and just compare if the result is the ntfs string as in the other answer in my particular case i initially wanted to test if a volume is ntfs or not before attempting an indexing with deviceiocontrol hvol fsctl_enum_usn_data ... because i thought such mft querying would be limited to ntfs volumes,a fat filesystems saves files into clusters by 16 bytes or 32 bytes explaining the label of fat16 fat32 and basically wasting disk space;ntfs however is a little more dynamic than that files can be saved as that files own size but again the ram causes the files to be saved randomly on to disk therefor to clean up these empty spaces it is best to run disk defrag a default program on all versions of windows,different filesystems not different operating systems have different capabilities for storing metadata;ntfs has plenty of possibilities while fat is very limited and ext are somewhere in between,"
"filesystems","ntfs"," has a much overall, is more overall, are not overall, not overall, file encryption overall,trickier overall, will use its journal overall, type but i already overall, is much more overall,slower overall,simpler ones overall,","2091344,50901544,34266530,1802269,25562592,10080420,51736933,51674421,11824814,28177009,2983099,","the limit probably originates with the filesystems;fat32 has a limit of 4gb whereas ntfs has a much higher limit in the terabytes,quick and dirty answer the simple answer is blobs smaller than 256kb are more efficiently handled by a database while a filesystems is more efficient for those greater than 1mb;of course this will vary between different databases and filesystems there is a microsoft technical report here compare blob and ntfs filesystems,for example if your filesystems does not support utf-8 but the file-name is encoded in utf-8;as you are using windows it is worth noting that windows filesystems fat fat32 ntfs are not utf-8 aware,but the case conversion table is stored on the filesystems itself for ntfs and it does change between versions for instance the vista case conversion table was brought to the unicode 5 level so vista ntfs and xp ntfs have different case conversion rules;and the thing that matters is the os that formatted the filesystems not the current os,if the filesystems is not ntfs or the user you are running under does not have rights to modify the ntfs settings edit or ntfs file encryption is disabled you won t be able to use the file.encrypt functions;file.encrypt does ntfs file encryption,actual mounting of filesystems is trickier business and really depends on what you are mounting - ntfs fat ext3 xfs nfs cifs webdav etc etc,this makes use of the ntfs journaling which applies to filesystems metadata not the contents of your files assuring that only one of two possibilities can happen either you have the old file entirely intact or else the new one also entirely intact;if power dies in the middle of making a change ntfs will use its journal to roll back the transaction,i could also have chosen ntfs filesystems type but i already have lots of those usb sticks around and wanted to try something different,it s not too hard for filesystems as it s quite simple and well documented but it s more difficult for ntfs as filesystems is much more complicated and not-so-well documented,the problem is that windows ntfs is slower than typical linux filesystems for these lookups,although i myself have prior experience in implementing filesystems much simpler ones than ntfs xfs or ext2 i would not tackle this job,"
"filesystems","ntfs"," this created migration overall, filenames cannot overall,better overall,","37348529,56119344,32622710,","when ms switched to the fat32 filesystems that used longer names and later to the ntfs this created migration issues,for example the virtualbox shared-folder filesystems allows control characters colon and pipe in filenames;in windows ntfs filenames cannot contain the ascii control characters 0x01 - 0x1f,generally the performance of hard filesystems like ntfs is better than that of traditional unix filesystems,"
"guava","lambdaj","more popular in guava another specialized, which seems more in guava another specialized,","19599464,5930700,","the guava library is much more popular than lambdaj and does allow you to avoid for while loops by using preficates and filter methods,recently popular is guava;another one is lambdaj which seems more specialized,"
"jasmine","qunit","clearly more bdd overall,better overall,","18155477,12772542,","jasmine is clearly more bdd focused than qunit although i could see doing bdd with qunit by describing the tests in a behavior-oriented way,performing this asynchronous testing is actually possible in qunit but is handled better in another javascript testing framework jasmine js,"
"addition","modulo","higher precedence in higher precedence priority,slower in slower integer costly,higher precedence in higher precedence priority,weaker overall, trove provides an more overall,much cheaper in slower integer costly,slower integer in slower integer costly,more tightly in higher precedence priority,higher precedence in higher precedence priority,higher in higher precedence priority,slower in slower integer costly,","40602532,31494852,41520049,16363111,10787175,23462464,898763,40679020,25297793,23395245,8132236,","the modulo has a higher precedence than addition,then you can process any length number using very few division remainder modulo operations which is important because they are much slower than addition,note parentheses are redundant as division and multiplication have the same priority and modulo has higher precedence over addition,is the modulo really weaker than the addition,in addition trove provides an more memory-efficient implementation for storing primitive values than standard java collections;if the array is not sparse and you really really do need the whole blob in memory you will probably have to use a two-dimensional structure with a map matching offsets modulo 1024 to the proper 1024-byte array,the addition is much cheaper than other operations like modulo and division and array access,integer multiplication division and modulo are much slower than integer addition and subtraction,because the string formatting operator shares precedence with the remainder or modulo which binds more tightly than the + addition operator,and keep in mind that the modulo has a higher precedence than addition and subtraction,the modulo has a higher operator precedence than the addition operator therefore it will happen before the addition,division and modulo are indeed costly hardware operations whatever you do this is more related to hardware architecture than to languages or compilers perhaps ten times slower than addition,"
"addition","modulo","better overall,higher precedence in higher precedence priority,higher precedence in higher precedence priority,","31351662,15075753,32254191,","the result of the addition was better than the modulo the by 0.0070000 milliseconds over the course of 2 million or 200 000 iterations,modulo can also cause a divide-by-zero and it has a higher precedence than addition,multiplication division and modulo have the same precedence and they all have higher precedence than addition and subtraction,"
"factors","matrix","slower overall, it is probably better overall, has more overall, cannot overall,more overall, cannot overall,more complicated overall,lower triangular overall,worse overall, l is unit lower overall, multiplication.. one more overall,","26356042,10097098,36735141,23809506,23217722,48482514,11123968,12901850,9356573,51628603,56373070,","i measured the time it takes to calculate the distance between a vector and the rows of a matrix when they are in the object and it work slower by a factors of 3 then the normal distance function,and although the matrix is possible to use the last element as a uniform inverse scaling factors it is probably better to do your scaling with the upper left 3x3 submatrix and leave the bottom row exclusively for the camera,so although the contrasts for missing factors levels are dropped in terms of estimating the coefficients those factors levels still appear in the model matrix;this means that the model matrix has more columns than the length of est1.qbar the vector of estimated coefficients so matrix multiplication is not going to work,the unknown scale factors cannot be responsible for the reconstruction errors you see;regardless of global scale the result of projecting onto the image pair a 3d point estimated from good matches and with a valid essential matrix should be consistent,in the code below i use block multiplication to speed up your code for a 1024x1204 matrix by more than a factors of ten 7.1 s with old code and 0.6s with new using only a single thread without using sse avx,factors is a complex data type made up of chars and int types;matrix cannot hold two types at a time,eta to answer your question in more general terms let s say we had the number of subjects and levels set up in advance increasing the number of factors is more complicated unless i m mistaken because then it would no longer be a two-dimensional matrix,finally note that by default ichol references the lower triangle of the input matrix and returns a lower triangular factors,however it still performs worse by initially a factors of 3 but as the matrix size increases asymptotically worse by a factors of exactly 2,gaussian elimination with row interchanges is used to factors a as a p l u where p is a permutation matrix l is unit lower triangular and u is upper triangular,benchmarking parallel work correctly is a tough task it is affected by the great many factors and features of a cpu cache;try to replace your workload with one that is very suited for multiprocessing working in a parallel on different parts of an array matrix multiplication.. one more important thing spawning the new processes also takes time and for one more important thing spawning the new processes to pay off the work done in each process needs to be significant if you increase your loop s range a little bit the difference should be in favor of the multi-process version on my machine this outputs 0.44509196281433105 1.3775699138641357,"
"factors","matrix","smaller model overall, is the more overall, is properly overall, and not overall,","44734705,11800714,12835140,48089583,","consider a model with lots of factors or nonlinear terms like bs ns or poly the model frame is much smaller compared with model matrix,those three factors can be applied in the duotone matrix;the larger a factors is the more tinted the image will be with that colour,if your code is slow this probably means that your factors is not that sparse sparse anymore;you need to make sure that your matrix is properly reordered to minimize the fill added non-zero entries during sparse factorization,we can do this with mutate_all as grepl works on vector matrix and not on data.frame or using str_replace with base r we can use lapply it is better to create character columns instead of factors,"
"smarty","twig","better overall,better in experience functionality syntax,cleaner in experience functionality syntax,","5769381,2894515,25748940,","if you use twig which i like better than smarty - although i still prefer self-restraint+plain php most ide s that support django templating should work with it since the syntax is nearly the same,if your in the market for a templating engine twig a new templating engine used by symfony is much better than smarty imho,either way i intend to use smarty partly because i have lots of experience with it and partly because its much more well documented and has much more functionality than blade and to me the syntax is slightly cleaner than twig although the template engine is not the point of this question really,"
"minimum","range","greater than  in maximum input property, signed character value in value platforms character,lower than the  in number lower numeric,less in value platforms character,greater than the  in value platforms character,lower than the  in number lower numeric,less than the  overall,lower than the  in number lower numeric,less overall,lower than the  overall, and less overall,","57010562,52595332,49255503,46186489,50108939,54979957,52045658,52658265,28241607,55644267,53140021,","it also makes little sens to check both for an id to be greater than minimum less than maximum and within that range,the limits.h header defines constants for various type ranges - for characters it defines the following constants uchar_max - maximum unsigned character value 255 on most platforms schar_min - minimum signed character value -128 on most platforms schar_max - maximum signed character value 127 on most platforms char_min - minimum character value either 0 or schar_min depending on platform char_max - maximum character value either uchar_max or schar_max depending on value to keep this code simple i m only worrying about character codes in the range,this is wrapping in contrast to saturating. in computer programming an integer overflow occurs when an arithmetic operation attempts to create a numeric value that is outside of the range that can be represented with a given number of bits either larger than the maximum or lower than the minimum representable value,the unfortunate side effect is that half your circle will be cut off as it extends into the range that is less than your minimum value,in other words taking the partition of those invoices with a due date greater than the minimum of the range which we can get quickly from the index and filtering out the few where the due date is greater than the maximum isn t a too bad approach,in computer programming an integer overflow occurs when an arithmetic operation attempts to create a numeric value that is outside of the range that can be represented with a given number of digits either larger than the maximum or lower than the minimum representable value,here is the conditional formatting equation i came up with to highlight cells that do not pass muster to highlight cells that do pass test first equation for each cell in my range a3 a100 i am checking if 1 my cell is less than the minimum between the start of the range and a mycurrentrow and that the value of cell is not 0 we could set up dynamic range to... second equation here we are doing the same thing but simply flipping the to a and adding an to account for the fact that at times cell was the earliest wake up time in the range,this is due to integer overflow in computer programming an integer overflow occurs when an arithmetic operation attempts to create a numeric value that is outside of the range that can be represented with a given number of digits either larger than the maximum or lower than the minimum representable value,this obviously means that range is less than its minimum of 1,an example which shows this using some random data i generated it before you added the data is this will produce the easiest way to fix this though it does not fix the underlying data and won t work in all cases is simply to set the limits manually to account for the offset - which will give you if you want to include the months lower than the minimum 0 1 2 you can set the xticks and xticklabels manually - ax.set_xticks range -3 9 ax.set_xticklabels range 0 12 which will give you,for the case where answer is neither 1 nor max of the range +1 for this case we will need a segment tree that will give smallest missing positive value that is greater than minimum value of the range and less than maximum value of the range . we will use value 0 to represent there is no missing value for this range . for leaf node we will set missing value to 0,"
"minimum","range"," is higher overall, is greater in number lower numeric, which is contradictory in number lower numeric,more control over the  overall,lower in number lower numeric,not less in maximum input property,i than a  overall, and doing the math overall,higher than a  overall, using any_of overall,less than its  in number lower numeric,","46279122,3651941,38290395,47952946,45656389,47093441,56631014,35589409,51523535,53614137,50108939,","for example in kinect an object has to be minimum 0.5m meters away to detect depth of any object;thus kinect v2 could be useful if the range is between 0.5m to 5m though the range is higher for this device,e need the minimum within a range defined by the segment;if the minimum is less than our segment s lower bound then the minimum of the segment must be at the lower bound because quadratic curves only have 1 turning point and if the minimum is greater than our segment s upper bound then the segment s minimum is at the upper bound,to get the same difference with two different numbers one of two different numbers one of them has to be either less than the minimum or more than the maximum of the initial range which is contradictory has to be either less than the minimum or more than the maximum of the initial range which is contradictory,try using min so you would have more control over the minimum year range you wanted without worrying if the selectyear still reaches the current year,the cause for an integer overflow is when an arithmetic operation attempts to create a numeric value that is outside of the range that can be represented with a given number of bits either larger than the maximum or lower than the minimum representable value,minimum 1.6.0 is not less then range maximum 1.0.0 inclusion is,to have sth like i have thousands of tables i am thinking of a loop that will somehow put the first set which will be df time i 12 00 or df time i 1 00 and i than a minimum number as am but not sure if there is a more effective solution something that will define that the first range of values is id 1 10 and the 2nd range is 11 22,you asked for the minimum ips not in the range;since you can t do math on varbinary i would suggest simply getting the max in range and doing the math outside of sql,in a specified date range subset of larger spreadsheet covering more dates before and after i want to find the date the lowest electricity kwh usage occurred but higher than a minimum of 9 but also only that measured for a 24 hour period,which i see you re already doing but you can do it a little cleaner using an istream_iterator as far as evaluating out of range you can easily evaluate if there are any elements less than minimum using any_of and either a lambda or a bind statement for example live example,that is the number of due dates that are greater than the range s maximum is far smaller than the number of them being less than its minimum,"
"minimum","range","much closer overall, s not in standard types short,general overall, is greater than 1 then in value platforms character, value occurs more overall,longer overall,less in number lower numeric,bigger than suggested  in number lower numeric,greater in number lower numeric,smaller in number lower numeric,lower than the  in number lower numeric,","47293435,10771280,54489610,53140021,36964135,10923040,45109856,53721826,40184847,1990334,47616325,","for any particular set of hyperparameters this range is much closer to the minimum response than to the maximum,a larger range is quite useful;the standard specifies minimum ranges not the precise range itself,low power levels are also useful for limiting detection range but this is more reliably handled with minimum rssi thresholds on the receiving side because leaving the transmitter at max power provides a higher signal to noise ratio and more reliable detections,for 1 2 answer is 3 for 1 4 2 answer is 3 for the case where answer is 1 if minimum value in the range is greater than 1 then answer is 1,or if you have a fixed range say till row 25 use following formula and change number of rows as required;if you ve repeating minimum value in column b minimum value occurs more than one then try this formula,to summarize the problem given time range t1 and t2 on day d how can i determine the remaining time left in d that is longer than the minimum time block m,i have hinted at my analysis of the problem finding the common range of depth values across all datasets and you should be able to track through how i have implemented this in excel to cater for cases where some datasets might contain depth values which are less than the minimum of the common range or which are greater than its maximum or possibly both,it means when i have values bigger than suggested minimum chart will be dynamically expose range until lowest value hit suggested minimum and values with lower date will not showed anymore,the smallest most negative value that does not cause a range error is the one greater than the minimum exponent minus the number of bits of mantissa,a range is contiguous when there is no value one smaller than the minimum and no value one bigger than the maximum and there is no gap within the range,this is happening due to integer overflow in computer programming an integer overflow occurs when an arithmetic operation attempts to create a numeric value that is outside of the range that can be represented with a given number of bits either larger than the maximum or lower than the minimum representable value,"
"minimum","range","more overall,lower than the  in number lower numeric,general in maximum input property,greater than your  in value platforms character,lower than the  in number lower numeric, prediction is greater in number lower numeric,higher overall, is yet more overall,less than the  overall, required by the  then in number lower numeric,lower value than the  in number lower numeric,","10861750,54846989,55876233,12935794,47795191,38831288,31088127,51404964,50055050,49639021,50108939,","the objective function is guaranteed to be finite and contionuous in the interpolation range along with its first and second derivatives and has no more than one minimum in this range if it has no minimum it is monotonic,taken from wikipedia in computer programming an integer overflow occurs when an arithmetic operation attempts to create a numeric value that is outside of the range that can be represented with a given number of digits either larger than the maximum or lower than the minimum representable value,the maximum range input property max is always more than the max property of the minimum range input,n s e w nw sw ne se you keep going until the distance to the buckets and therefore all the points in the buckets is greater than your minimum range,an integer overflow occurs when an arithmetic operation attempts to create a numeric value that is outside of the range that can be represented with a given number of bits either larger than the maximum or lower than the minimum representable value,threshold values are in range of 0 1;in your case regardless of threshold you will always get all observations into true class as even minimum prediction is greater than 1,145 the output is like â ºâ â â ºâ â it doesn t reverse when the range is higher than 145 it works fine in gcc for borland turboc the minimum range must be 65 otherwise the program prints strange values instead of reversing it,the u+0080..u+009f range is yet more useless control characters which is why those are not showing anything;1 technically 0 255 is the minimum required range for unsigned char,you will get incorrect values for most values of x that are less than the range minimum range 0,if the number is smaller than the minimum required by the range then the minimum of the range will be returned,but an ascending index can help to quickly partition the set of records into those records that have a lower value than the minimum of the range and those that have a higher value than it,"
"minimum","range"," maximum is much more in maximum input property,larger in number lower numeric, that is greater in value platforms character,wider in standard types short,greater in number lower numeric,general in maximum input property, number is greater in number lower numeric,greater in number lower numeric, percentage is higher in number lower numeric,","42949444,3821910,57189899,3539872,5234591,55876233,55053046,31084948,55560070,","you don t have to sort the lists to find the range;finding only the minimum maximum is much more efficient o n compared to the best case of comparison sorting or worst case,if the 32-bit float range is larger than -1..1 then you need to find the minimum and maximum values and calculate a scale factor that gets the samples within the int16 range,is there a way to update the column next_date in a table rates in such a way that the rows reflect the minimum future_date in the range that is greater than the future_date in that row,for example the language standard only mandates minimum range for types like short int and long but they may be wider than the minimum requirements,how about for integer values you specify a range and a predicate that specifies the value must be greater than minimum less than maximum instead,the minimum range input property min is always less than the min property of the maximum range input,the following assumptions rules apply when the range is invalid minimum number is greater than maximum then the regex will be _ which will make all test values fail,the reductive point here is the expression can actually be simplified to just the range that has the greater minimum value and the lesser maximum value,ascending means that the range percentage is higher as the variable increases above the minimum and descending means that the range percentage value increases as the variable decreases below the maximum value,"
"cairo","gtk","larger overall,more common overall, performs a lot better overall,","12490400,1734609,7606313,","the problem is that the canvas drawn by cairo is larger than the area of gtk s scrolled_window,with gtk it s more common to use cairo already mentioned by jeff foster,cairo;you are right in saying that gtk performs a lot better on linux and other operating systems as compared to windows,"
"setinterval","settimeout"," it s better in better time previous,more useful in need method useful,more useful in need method useful,better in better time previous,much more in timers promises exact, is better in better time previous,better than  in better loop event, is better overall, but not overall,use  to make in timers promises exact,bigger overall,","55164506,14174099,41731274,7197129,9578359,55297555,50624979,6788970,355790,31642419,32307605,","also with setinterval and settimeout it s better to provide a function reference than to provide a string of executable code as the first argument,setinterval is more useful than settimeout here as it recurs automatically and you don t need to keep setting it,now a routine to initiate once per second - settimeout is usually more useful than setinterval,it uses settimeout however settimeout is a better solution than setinterval because it will only queue a new one if the previous one is complete,setinterval is much more suited for countdown timers and things you need to run continually since settimeout only runs once and you need to keep on calling it,and your logoutfunction will look something like this i mistakenly said in comments to use setinterval;i assume this will be a one time execution settimeout is better to use,obviously your checkrate will not measure thefunciwanttomeasure correctly so just to show that a settimeout fn 0 loop will fire at higher rate now even if a nested settimeout loop is better than setinterval what you are doing is a visual animation,as a rule setinterval is good for doing small processes regularly;in this case you just want to put a wait into the processing which suggests to me that settimeout is better,well the first thing to say is that if you re calling settimeout but not changing the interval you should be using setinterval;edit update from comment you can keep a reference from the closure if used as a class and setinterval clearinterval don t require re-referencing,correct the interval for setinterval cannot be changed once it is started;you ll need to use settimeout to make the interval different each time,it seems that settimeout has bigger priority than setinterval which could be delayed,"
"setinterval","settimeout","better way in better time previous, is actually better in better time previous, is faster overall, will run only once in delay callback specified,more accurate in need method useful, is more in delay callback specified,better approach in better time previous,often better in better time previous,general overall, are not overall,better idea in better time previous,","37051171,57354895,31391138,13054742,43600221,54721057,39790991,16539647,48446187,45139905,47316194,","setinterval would be the better way than settimeout,if the code inside the setinterval takes longer than the time you have set the setinterval will create another process before the function finishes messing everything up;so choosing settimeout is actually better,ou have to use a settimeout or setinterval or requestanimationframe which is introduced from html5 so you browser can have the control to change the properties on the page;you can see a example from the snippet although the second that use a settimeout is faster than the first one which use for-loop the first one will not change a settimeout color as browser not able to change color during for-loop,you should use settimeout instead of setinterval as setinterval will not stop until you stop it using clearinterval or by unloading the page;at the other hand settimeout will run only once for each loop with delay of 6000ms,well setinterval and settimeout essentially try to do the same thing but for your case setinterval method will be more accurate than settimeout,so if you called setinterval mycustomfunction 1000 it will repeatedly execute mycustomfunction after every 1s;this is not the behavior you want you only want a delay of 1s and for that settimeout is more appropriate,also settimeout is a better approach than setinterval as you explicitly reset it on each round trip,also setinterval is often better replaced by settimeout,i will recommend using settimeout instead setinterval try this,note that settimeout setinterval has a minimum value generally considered 10ms so if it s less than that value to the next second add 1000;also note that settimeout setinterval are not 100 accurate but for the nearest second will likely suffice,using settimeout for such purposes is better idea than setinterval as you don t have to clear them and they won t get crowded - if your fight takes longer than period then this won t run into next fight,"
"setinterval","settimeout"," is only in better time previous, executes once in delay callback specified,safer in better time previous, is a safer overall,simpler in need method useful,flexible method than  in need method useful,better in better time previous, makes it easier overall, it is better in better time previous, which performs better then in better time previous, is more overall,","35214904,34845320,20550018,49475297,46886457,48983956,3030830,729994,31090522,47999671,55995281,","the reason i like to use settimeout and not setinterval in these situations is that settimeout is only called after the previous function has completed;setinterval can get back-logged and freeze up your page,first you want setinterval not settimeout;settimeout only executes once after the specified delay while setinterval executes once every interval,i think using settimeout is safer than setinterval,you have two setinterval calls which will stack up and eventually make your page hang;settimeout is a safer method as you then are responsible for calling a next settimeout,as for me settimeout is simpler than setinterval in this case as you won t need to clearinterval in the end of the array,recursive settimeout is a more flexible method than setinterval,if so then you can just pop lines off the array using settimeout which is better than setinterval for most animations,the setinterval makes it easier to cancel future execution of your code;if you use settimeout you must keep track of the timer id in case you wish to cancel it later on,btw when you use settimeout or setinterval it is better to pass the timer an actual function instead of a string with the source code for a function,it uses recursive settimeout which performs better then setinterval,i suggest to only change the src of the image instead of the whole tag also settimeout is more what you need than setinterval,"
"setinterval","settimeout"," is a better in better time previous,more relevant in need method useful,better way in better time previous,better in better time previous,better in better time previous, why is requestanimationframe better in better time previous,timer more preferably in timers promises exact,less than  overall,less cpu overall, is much better in better loop event,general in better time previous,","52211587,20139502,20182174,31064972,22424960,50035366,34249135,52762664,37951980,50898879,57530075,","in general if you want something to occur at a set interval setinterval is a better alternative to settimeout,settimeout is more relevant than setinterval since the first method just waits for a delay and executes a logic whereas the second function is meant for repeating a logic on periodic intervals,i understand from searching on so that setinterval is a better way to approach this that settimeout which i was using initially but i haven t figured out how to specify that the function needs to repeat,but settimeout would be better than setinterval because with setinterval if your previous ajax request is not complete and you start another request there will have multiple requests which would sooner be problematic,using settimeout is considered better than setinterval because of the screwy ways that js s event loop works,here s my simple updated demo with class based moving objects also see here why requestanimationframe is better than settimeout or setinterval why is requestanimationframe better than setinterval or settimeout for keypresses i suggest you make an object that holds boolean values for all keys and then on the update-functions of different objects like player for example you just check if the keys it needs are being pressed,the use of one settimeout timer is more preferably than several setinterval timers,meaning the time set for settimeout is always less than setinterval s,i read that settimeout is less cpu resources intensive than setinterval,to thoroughly understand this behaviour you have to understand all stages of event loop and especially how settimeout and setinterval is handled mdn article on event loop mdn event loop rising stack article will help you in getting a clear understanding of event loop along with micro and macro task rising stack event loop explained in nutshell all settimeout gets processed in same tick of the loop;also for your case setinterval is much better,i prefer using settimeout and call it each time when needed that way you don t need a global variable to maintain the value returned by setinterval,"
"setinterval","settimeout","continuous polling with   overall, will use more processor overall,better in better time previous,more in little cleaner calls,general overall,more than  in timers promises exact, that fits the case in timers promises exact,change  to be in timers promises exact, not ; in delay callback specified,such as  in timers promises exact, is better in better time previous,","53505174,46857547,34253539,23111155,50782309,53990887,57339610,53982467,31684469,52345041,7445138,","if you read the advanced integrations section they suggest doing this adds an event listener for the load event on the script element that loads stripe and sets the state when it is available so no continuous polling with setinterval settimeout is needed,keep in mind that using setinterval is less efficient for this case then using settimeout which doesn t require comparing times at all as it schedules the alarm to occur at a particular time as setinterval will use more processor cycles to both schedule a check and then to compare the times,setinterval is better than settimeout for this task,and im not shure about this statement javascript likes settimeout more than setinterval which gives a little performance boost,i would recommend to prefer using settimeout instead of setinterval indeed the browser may be overhelmed by heavy processing and in that case you would probably prefer updating the clock less often instead of queuing several updates of the state,generically for those just reading this answer and want a way to do a simple timer here is a version that doesn t take into account the op s original code nor their need for a way to start and stop the timer independently however you may be wondering how to use a more exact interval given the fact that settimeout can drift more than setinterval,as mentioned above setinterval does not play well with promises if you do not stop it;in case you clear the interval you can use it like it seems that it s settimeout that fits the case,you can see this in action here you can change setinterval to be settimeout and you will get the exact same behaviour;settimeout is not a persistent clock but it doesn t matter since they both get cleaned up anyways,if you want to repeatedly call a function you need setinterval not settimeout;settimeout invokes its callback once after a delay,to give some further clarification i ve included the diagram presented in the node.js documentation these phases can be read about in much more detail at in short summary timers scheduled callbacks and anything set on timers will execute here such as setinterval or settimeout,note that i use settimeout instead of setinterval since you can t be sure every request will return in 3 seconds;using settimeout is better in general,"
"setinterval","settimeout","more aesthetic experience in better time previous, to call a function in better time previous,worse than recursive  in need method useful, is better in better time previous,better in better time previous, will run it only in better time previous, calls the included code only overall, wouldn in need method useful,more appropriate overall, for recurring callback function in delay callback specified, and not in need method useful,","13234026,48459697,49875471,4075835,16126379,28226221,38863713,26081179,17169595,31809377,43880078,","also in my experience at least settimeout offers a much more aesthetic experience than setinterval or requestanimationframe,using setinterval is not recommended since the body function theoretically may take longer than the interval causing a stackoverflow;a better way is to use settimeout to call a function which at the end of executions schedules a new timeout for itself,i ve already removed setinterval as it was worse than recursive settimeout,setinterval is better when you want the behavior to repeat forever;settimeout is better when you want to stop the behavior later,settimeout is better than setinterval here because setinterval will start at every 5 secs but settimeout will start once each time your job is done and wait for 5 seconds which will make your script adaptive to server response timings,setinterval is not the same as settimeout;setinterval will run a piece or block of code periodically in a given interval while settimeout will run it only once or rather each time appviewmodel is invoked,here we need to defined different functions for each step and need to call them one after another via settimeout and not via setinterval;the setinterval calls the code infinite time after each interval where as settimeout calls the included code only once after specified time interval,in my case using ajax the function was being call asynchronously so the setinterval wouldn t care if a result was returned;b if i changed the code to use settimeout i could control the outcome more,also i advise you to take a look at javascript s setinterval since it is more appropriate than settimeout for what you want to do,note keep in mind to clear the setinterval when not required as it may cause abnormal behaviour may slowdown the javascript engine;call setinterval instead of settimeout for recurring callback function execution after definite time interval delay,setinterval takes a function as its first parameter and you re passing undefined since hidehero function doesn t return anything;btw you should use settimeout and not setinterval in your code there is no need to call hidehero every 3 seconds,"
"setinterval","settimeout"," that fits the case in timers promises exact, does not in better time previous, which is the better in better time previous, is actually better in better time previous,cleaner in little cleaner calls,","52184548,50757867,5064925,17126971,41311136,","setinterval doesn t play well with promises because it triggers a callback multiple times while promise resolves once;it seems that it s settimeout that fits the case,if the settimeout lasts more than 1 second that the setinterval lasts i break the loop since the setinterval does not wait for that time to pass it is executed again,instead initiate all the settimeout s you need for the entire animation at the same time with increasing delays in a loop and let them run them course or better yet use setinterval which is the better way of doing animations and how for instance jquery s animations work,setinterval is actually evil if the code inside setinterval takes longer than the time you have set setinterval will create another process before the function finishes messing everything up;so choosing settimeout is actually better,i find setinterval is a little cleaner than chaining settimeout calls,"
"max","minimum","smaller than your  overall,lower in loop start increment, complexity is o n overall,less overall,bigger than  in loop start increment,less in loop start increment,longer overall,less than the  overall,less than the  overall,more than one  overall,less overall,","50320618,22326765,48881412,44407942,55256993,40118146,19796176,7127313,54426704,55369154,11870987,","as such all you d need is a simple call to std max to determine the bigger value of two inputs when using the new size it will basically resize the window to match your object unless the object is smaller than your minimum size,my question is can we set max stack size lower than the minimum stack size which may be 4 kb for the goroutines,max solution runs certainly faster that row_number solution because max complexity is o n while row_number complexity is at minimum o n.log n where n represent the number of records in table,now you are iterating exactly like your example with one change to know what is the next set you are moving to you have to replace every number in the current set with the next number in the array and replace the max option that less than the minimum you are saving,in the loop start with minimum value between n and m and increment the i value by one until value is bigger than max of n and m,all numbers will always be equal to or greater than the minimum value or equal to or less than the max value,why is the max time which happens on the first iteration of the loop 2-4x longer than the minimum time,look-up in the case of failure should be constant time if the current element is less than the minimum element of the heap containing the max m elements we can reject it outright,autosize does not allow the max to be equal or less than the minimum then it isn t auto sizing anyway,0 0 6 so minimum k 6 my current approach find the max node and start from it if there is more than one max node the one which comes sooner is chosen i define an array let s call it possible nodes and i add the node found in step 1 to it,i m simply checking the values of two input textboxes and alerting the user if the max price is less than the minimum price but they re evaluating backwards,"
"max","minimum"," is larger stick in loop start increment, is generally more overall,less than  in loop start increment,higher in loop start increment, is anything less overall, are not overall,smaller overall,lower in loop start increment,greater overall,more in account date difference,less in loop start increment,","37155338,17771927,52316621,39976775,22957088,41482471,35101099,32300572,3967656,9952736,13993794,","check to see if the absolute maximum is bigger than the absolute value of the minimum;if the max is larger stick with the larger value and perform a max calculation,to determine if all the names are the same i like to compare the minimum to the maximum;calculating the min and max is generally more efficient than a count distinct,use linq it will return record that it price more than minimum value and less than max value,in this example i would want to flag the following records in my data 2 lower than the minimum for x and 4 higher than the max for y,still you must increase max also if your minimum is anything less than 15 the adk is now building an extra folder new stuff in truth the new version of the adt is annoying i am just starting and now you will see they have enforced the use of fragments,which either means that values have to be doubled up two distinct bit patterns represent the same value or the usual implementation choice in practice that the maximum and minimum are not equal in magnitude;this is also incidentally the reason that std numeric_limits max typically gives an odd value for unsigned integral types,the minimum requirement and smaller than the max requirement,for group 1 the minimum value is actually 2 however i need minimum indexed lower than max value index position,if it were possible to find such a cut then max-flow min-cut would not be true the max flow of the network would be greater than the minimum capacity of an s-t cut,first i run a query to find the min and max for each security id then find the difference between the min and max and finally find a value that is 10 more than the minimum like this,from here you also need to take into account if the user inputs a value higher than the max year of the data and also less than the minimum,"
"max","minimum","larger in loop start increment,value greater in loop start increment, value is trickier overall,less overall,select record with  in account date difference,","26761734,30945059,55893743,34556171,50926717,","how can the max value be larger than the minimum,4.if minimum value is greater than the current value in array than add difference of than in ans add the difference with current value update max from left,basically you create all your variables on top of the file then loop until 5 numbers are entered in this example and do the necessary operations to calculate total average max and min values;the minimum value is trickier there is a workaround in the code below you could also add the check in your if elif block for minimum with a little help from the or operator,if i is rather little just iterate and keep i minimum datas insert every new data in a binary tree of the i most little datas if it is less than the max of these datas,below is my table let s call account i want to select record with max date and also the difference of score with the records having tracking_date as minimum for that accountid,"
"haskell","sml"," offers much overall,stricter overall, does not overall, has better overall, is less overall,better overall,","813646,38068762,827859,813646,497332,811801,","time and space costs of haskell programs can be very hard to predict even for experts;sml offers much more limited ways to blow the machine,also sml has stricter precedence rules than haskell,sml ocaml f# and haskell allow closed sealed algebraic types to be defined to strengthen static typing by conveying more specific constraints implicitly;ocaml and f# also allow open sum types whereas sml does not and haskell requires an elaborate workaround described by oleg kiselyov,there just isn t a nice way to deal with type signatures in sml;haskell has better concrete syntax,haskell is extreme lazy pure has active users lots of documentation and makes runnable applications;sml is less extreme strict impure has active users formal specification many implementations sml nj mlton moscow ml etc.,because sml is eagerly evaluated the execution model is far easier to comprehend and debugging via printf works a lot better than in haskell,"
"backtracking","greedy","fewer overall,less overall, and creates a larger overall,","33869801,2833879,49101987,","since the next is not far the number of backtracking steps is much fewer than with greedy matching,the capturing group 1 in the first pattern is greedy it first matches everything and takes as less as it backtracking,combine the amp rules as a general rule of thumb avoid using the matching as the is greedy and creates a larger backtracking,"
"okhttp","retrofit","newer with  overall,better in wich thread safe, without using  then overall,more overall,better pure overall, it s s much overall,wich better in wich thread safe,more overall,","52831568,25939659,49464021,38392143,42459992,50430294,25939659,38431343,","using okhttp 3.10.0 or newer with retrofit 2.3.0 does not work and also retrofit 2.4.0 with okhttp 3.9.1 does not work,the point of using okhttp instead of the defaulthttpclient by apache is that okhttp is threadsafe for android and better supported by retrofit,i just used retrofit without okhttp. here is my code i aslo added okhttp3 by researching the error.here is the part that deals with okhttp3 but i got the same error.i think there is no need to add okhttp3 for the error because previusly i worked with retrofit without using okhttp then it worked as expected. here is the log output,first you have many different library you can use instead of retrofit the basic one and i think the simplest is okhttp but you have to understand that okhttp is no more supported but still work when you downgrade your sdkversion and you can use volley to perform the same service as retrofit,note that if you have json and rest then retrofit is going to be better than pure okhttp,i always said to my friends why do you use retrofit or valley if it s seems complicated to you;instead you can use jsoup or okhttp it s s much easier and i realy love jsoup,since you re using android and retrofit i suggest using okhttp wich is better supported by retrofit and android thread safe the way to this is the following,the basic one and i think the simplest is okhttp but you have to understand that okhttp is no more supported but still work when you downgrade your sdkversion the best solution is to use volley or retrofit,"
"sorteddictionary","sortedlist","less in memory,easier overall,faster insertion in removal operations note,faster in sorteddictionary on quicker,worse overall,same advantages than  overall, is even more in sorteddictionary on quicker,faster in sorteddictionary on quicker,faster than a  in removal operations note,faster in removal operations note,less memory in memory,","472158,33731851,21734216,389813,12412950,9517785,31378470,9932109,54790541,472158,24681338,","sortedlist uses less memory than sorteddictionary,i had one further issue that related to the oncollectionchanged - the notifycollectionchangedeventargs required an index as opposed to the item of the sorteddictionary which is doesn t come out of the box it s easier with a sortedlist but anyhow,sorteddictionary has faster insertion and removal operations for unsorted data o logn as opposed to o n for sortedlist,if the list is populated all at once from sorted data sortedlist is faster than sorteddictionary,you should also keep in mind that sortedlist performs worse than sorteddictionary during construction if the items are not inserted in already-sorted order although in this particular case it is highly likely that dates are inserted in chronological sorted order which would be perfect,sorteddictionary offers same advantages than sortedlist but performs better if values to insert are not already sorted,in a sortedlist;i think that a sorteddictionary is even more efficient in this case,sortedlist is faster than sorteddictionary,well the answer is that if you are going to load the sortedlist up-front the insertions will be slower but because array indexing is faster than following object links lookups are marginally faster than a sorteddictionary,sorteddictionary has faster insertion and removal operations for unsorted data o log n as opposed to o n for sortedlist,â sortedlist uses less memory than sorteddictionary,"
"sorteddictionary","sortedlist"," has faster insertion in removal operations note,faster in sorteddictionary on quicker,less memory in memory,faster in removal operations note, is slightly quicker in sorteddictionary on quicker, fits better overall, is faster overall, has faster insertion in sorteddictionary on quicker,faster in sorteddictionary on quicker,smaller overall,","53192831,33649821,1427158,10950851,1754080,23769058,3689215,3689215,890438,2037401,","sorteddictionary has faster insertion and removal operations for unsorted data o log n as opposed to o n for sortedlist,sortedlist is faster when you want to enumerate the elements and you can access the elements by index and sorteddictionary is faster if there are a lot of elements and you want to insert a new element in the middle of the collection,sortedlist tkey tvalue uses less memory than sorteddictionary tkey,note after doing some benchmarks i found that sorteddictionary is faster for removal but sortedlist is faster for adding and indexing by key,as you can see on sorted data the sorted list is faster than the sorteddictionary;on unsorted data the sortedlist is slightly quicker on retrieval but about 9 times slower on adding,to roughly paraphrase if you require raw performance sorteddictionary could be a better choice;if you require lesser memory overhead and indexed retrieval sortedlist fits better,from sorted data sortedlist is faster;than sorteddictionary,sorteddictionary has faster insertion,if the sortedlist is populated all at once from sorted data it s faster than sorteddictionary,ignoring the cost of providing sorted input the oh of sortedlist is smaller than the oh of sorteddictionary,"
"hex","octal"," was more overall,more popular overall, which is more overall, is much more overall,less error-prone overall, is easier overall,more popular overall, was used primarily for older overall,","4676830,37348054,26783758,18244754,18558347,13960625,9234172,1840924,","itshifts just go easier with hexadecimal than decimal and is often more convenient to read than octal;bitshifts boils down to whoever wrote the code you re reading probably thought hex was more understandable than decimal in that instance but this is obviously subjective,apparently octal format was more popular than hex format,27 is the decimal ascii value for escape but you use an octal character code;you should say 27 or 033 or x1b for hex which is more common,hex is much more compact when scaling to larger masks,the octal encoding mechanism is less error-prone than hex so i ll demonstrate using octal,n all-true-bits byte 1111 1111 is 377 in octal but ff in hex;hex is easier for most people to convert to and from binary in most people heads since binary numbers are usually expressed in blocks of eight because that s the size of a byte and eight is exactly two hex digits but hex notation would have been clunky and misleading in dennis time implying the ability to address 16 bits,from what i understand octal was more popular than hex among users of 18-bit architectures since a word would be exactly 6 octal digits,and octal was used primarily for older systems that used 12-bit bytes;hex made for a more compact representation of data when compared to displaying raw registers as binary,"
"cobertura","emma","more overall,more success with  overall,better overall,","14691985,2013862,8070899,","however cobertura looks more modern active last update from 2010 instead of 2005 and generates nicer reports so i would prefer it over emma,at one point i inquired about emma on a scala mailing list and i was told that by some that some had more success with cobertura,if you re going to stick with maven and want a plugin for maven that will do the code-coverage job i think cobertura is a better choice as emma stable last build is from 2005,"
"contains","intersect","slower overall,better overall,greater overall,","22813163,13715546,422343,","edit just checked the performance of intersect it is slower than using .all with contains,i d say the intersect method see answer by dasblinkenlight + any must work better than contains + any,given two ranges a a b and c c d do they intersect is one greater than the other or does one contains the other,"
"aes","rsa","faster in slower data sizes, which is much faster overall,faster in slower data sizes, is not overall,much slower in slower data sizes, not overall,less secure 4096-bit overall,more in slower data sizes,key less overall,much slower in slower data sizes,more in secure,","35236158,29512737,2743437,13501360,22911417,2556403,28474359,38483747,24333679,9237803,38065154,","the aes key is encrypting much more data but is much faster than rsa encryption,you could for example generate a random aes key encrypt it using rsa and store it in the output file and then encrypt the file itself with aes which is much faster and doesn t have any problem with large inputs,so the 115 seconds will be reduced to 3-4 secs plus the encryption decryption time used for aes which is much faster than rsa,rsa is not intended for bulk data encryption;instead use a symmetric cipher like aes to encrypt your large string,one of the reasons to do so is that rsa is much slower than for example aes,a rsa key as its name says is for rsa not for aes;aes uses a symmetric key namely an array of arbitrary bytes of length 16 24 or 32 bytes,while 256-bit aes might sound less secure than 4096-bit rsa they might actually be quite similar from the offered protection,execution of aes is more faster than rsa for same key sizes,but if you use public key encryption to encrypt messages you are a limited to small messages -- a 1024 bit rsa key encrypts less than 128 bytes and b going to pay in performance because public key encryption is much more costly than symmetric key encryption such as aes encryption,like you heard asymmetric cryptography like rsa is much slower than symmetric cryptography aes but it does have it s advantages simpler key management a single private key to protect,asymmetric encryption ex rsa is no more secure than symmetric encryption ex aes,"
"aes","rsa","less strenght overall,less user-friendly overall,less in possible performance first,symmetric key with  in slower data sizes,less overall,slower than  in slower data sizes,longer than what  overall,more secure in secure,less secure in secure,faster in possible performance first, is not overall,","23744165,124248,15027631,18920739,124248,26673893,57516414,37534974,33871712,20925115,29127962,","in your particular case an rsa key of 2048 bits has a lot less strenght than an aes key of 256 bits,algorithms like rsa are much less user-friendly than aes,there are two reasons for that performance aes is faster then rsa and resources aes is less resource hungry than rsa,you should be encrypting your data with a symmetric key like aes then encrypting the symmetric key with rsa;rsa should not be used to encrypt this kind of data,block crypto algorithms like aes do suffer from this problem too but without a pki aes is no less safe than rsa,rsa is much slower than aes,i want to encrypt and store a token which is longer than what rsa permits but i can t use aes because if i want to secure it with biometrics then i have to ask for user authentication even when i am encrypting the value and not only when decrypting it as with rsa,asymmetric key encryption ex rsa is no more secure than symmetric key encryption ex aes,how is aes less secure than rsa in this scenario,and regarding your first question it is definitely possible to encrypt decrypt messages directly using rsa there are only technical and performance reasons aes is much faster than rsa why rsa is used only to encrypt a session key and aes is used to encrypt decrypt the messages themselves,rsa is not a substitute for aes;aes is a symmetric cipher that can quickly encrypt and decrypt large messages,"
"aes","rsa","faster overall,larger overall, to encrypt the message overall,such as  in slower data sizes,more so overall,expensive than  overall,faster than  in possible performance first, key preferably overall,slower overall, is much faster in slower data sizes, is not overall,","34582962,32471157,4915145,33885257,124248,31408821,57634763,16260499,31413617,46243875,52584065,","the whole purpose of using aes to secure the communication or any symmetric key encryption is that it s a lot faster than rsa or any public key encryption,considering most rsa moduli are at least 1024 bit this will be much larger than an aes key,if you re looking for privacy rsa isn t the way to go use rsa to generate a private public pair and then use them to excahnge keys -- or exchange keys out of band;use a streaming algorithm like aes to encrypt the message,i will also note that encrypting with rsa is much much slower than encrypting with a symmetric algorithm such as aes,that is actually not the case with rsa which is --- more so than aes --- just a math equation,note however that doing so means that each encrypted chunk has each encrypted chunk own padding and that rsa is much more computationally expensive than aes,note that it s also possible to create aes keys that require authentication when decrypting but not when encrypting which is rather cool aes is much much faster than rsa,rsa is an asymmetric encryption method that encrypts a number less than the modulus of the rsa key 255 bytes would indicate that you re using a 256 8 2048 bit rsa key modulus;what you need to do to encrypt values greater than that is to generate a key encrypt the data using a symmetric cipher aes is not a bad choice and encrypt the aes key using your private rsa key preferably along with some other random data,as far as efficiency rsa is going to be orders of magnitudes slower than aes so the trade-off you make is that you give up simplicity you give up the simplicity of using aes in favor of some rsa chunking in return for poor performance you get the slower performance of rsa.,in general asymmetric encryption such as rsa is not used to encrypt data data is in general encrypted with symmetric encryption such as aes.the choice usually boils down to the need for separate encryption and decryption keys and or pki;both are as secure at comparable key sizes and aes is much faster,rsa is not a replacement of aes nor the other way;aes is a symmetrical algorithm and rsa is any asymmetric one,"
"aes","rsa","encrypt faster in slower data sizes, encrypted message overall, is much shorter in slower data sizes,","14642408,31408821,51157466,","it shows that rsa encrypt is faster then aes encrypt,if the message you re encrypting is large enough not only will the message you re encrypting is large enough take more time to process but the rsa encrypted message might be larger than an rsa encrypted aes key plus an aes encrypted message,since aes is much faster than rsa and aes is much shorter than the actual message,"
"compiled-language","interpreted-language","slower in slower faster ball,slower than  in slower faster ball,slower in slower faster ball,faster in slower faster ball,slower in slower faster ball, is faster overall, doesn overall,less performant in performant level assembly,better in easier bytecode possible, is faster in slower faster ball,probably easier in easier bytecode possible,","1737240,55350504,4079920,5670283,41413195,48015634,27613709,29574827,38491212,51682265,47286728,","fact is that interpreted-language like php are always slower than a compiled-language,below are some differences between both technologies django is based on python which is scripting language interpreted-language so code is interpreted which is quite slower than compiled-language,php is an interpreted-language so will run a little slower than a compiled-language,that being said a compiled-language like c will almost always be faster than an interpreted-language like javascript,naturally interpreted-language will run slower than compiled-language as compiled code can be ran blindly by the cpu where as compiled code needs to be checked ran line by line,there are many discussions about compiled-language such as golang vs interpreted-language such as ruby but generally a compiled-language is faster because the compilation phase optimizes the code and because the target machine can natively run the program without the need to use an interpreter extra layer,an interpreted-language must do this same translation at the same time it s trying to run the program;that typically slows it down though modern interpreters and virtual machines like to brag about how they can do a few things faster because they have the extra information that a compiled-language doesn t,interpreted-language are inherently less performant than compiled-language - c will generally outperform python - some operations more than others,are compiled-language better than interpreted-language or vice-versa,if you wonder why this kind of vectorization is useful it is because a loop written by a compiled-language is faster than a loop written in an interpreted-language,while java could be described as a compiled and interpreted-language it s probably easier to think of java itself as a compiled-language and java bytecode as an interpreted-language,"
"compiled-language","interpreted-language","slower natively in slower faster ball,less common overall,smaller overall,slower in slower faster ball,slower in slower faster ball,faster than an  in slower faster ball, it s fairly overall,significantly better in performance examples better,more performant in performant level assembly,slower in slower faster ball,slower in slower faster ball,","44258776,1347760,15727516,6619136,3899501,223029,4329229,42017374,37105652,7993308,1694508,","it should be noted that interpreted-language are inherently many time slower than natively compiled-language,this is usually seen in dynamic interpreted-language but is less common in compiled-language,my guess is that in interpreted-language the efficiency benefit in using switch statements is indeed smaller than in compiled-language,an interpreted-language will typically run one to two orders of magnitude slower than a compiled-language,interpreted-language tend to be but not always are significantly slower than compiled-language,a compiled-language will generally run faster than an interpreted-language so i think ruby and php start behind the eight ball but it really comes down to how you use a compiled-language and how you structure the code,in c# this won t be so easy because c# is a compiled-language not an interpreted-language;in an interpreted-language it s fairly easy to parse raw-text as the language and see the results,performance of programs in compiled-language is significantly better than that of an interpreted-language,then c which is one those languages closer to the processor level is very performant and generally speaking compiled-language because they turn your code into assembly language are more performant than interpreted-language,this makes interpreted-language generally slower than compiled-language due to the overhead of running the vm or interpreter,this is a good question but should be formulated a little different in my opinion for example why are interpreted-language slower than compiled-language,"
"compiled-language","interpreted-language","slower in slower faster ball,much slower in slower faster ball,faster in slower faster ball,slower in slower faster ball,better performance in performance examples better,slower in slower faster ball,more overhead overall, can execute code overall,faster in slower faster ball,slower other overall,surely easier in easier bytecode possible,","41849399,3019070,26712393,17216232,6361528,5149958,10049694,43282878,30971543,18388172,21665739,","interpreted-language execution speed are slower than compiled-language true but once there is need for more speed you can call in compiled stuff through gems or micro services,and perl like any interpreted-language is much slower than a compiled-language,from what i know a compiled-language such as c++ is much faster than an interpreted-language such as javascript,mostly interpreted-language are a bit slower compared with compiled-language but i guess the difference is almost negligible in coffeescript javascript because of node.js,writing in a compiled-language java or c++ in your examples would almost certainly give better performance than an interpreted-language like php,while ruby and python are both interpreted-language and operation-for-operation slower than compiled-language the reality is in executing an application only a small portion of cpu time is spent in your code and the majority is spent into the built-in libraries you call into which are often native implementations and thus are as fast as compiled code,especially in an interpreted-language like php where classes add more overhead than a compiled-language,only interpreted-language can execute code at runtime;compiled-language cannot,in my general programming experience compiled c c++ programs generally run faster than most other compiled-language like java or even compiled python and almost always run faster than interpreted-language like uncompiled python or javascript,python is an interpreted-language so by definition is slower than other compiled-language but the drawback in the execution speed is not even noticeable in most of applications,an interpreted-language surely makes it easier but this is still entirely possible with compiled-language like c,"
"ftp","telnet","more overall, server doesn overall,protocol more complex overall, server webdav server overall,easier than  overall, client not overall,better overall,","21419290,16898180,1741448,21419290,1059847,5191896,10818924,","telnet is more general than ftp and is generally used for command and control,it s probably injecting telnet byte sequences into your ftp stream it often starts with trying to enable telnet s linemode which the ftp server doesn t understand;i m not really sure why people use a telnet client to interact with arbitrary text-based protocols like smtp and ftp,ftp protocol is more complex than http or telnet form example,ideally the dal insulates the business entities from knowing if the data store is an rdbm ftp server webdav server nosql db or anything else;telnet is a little more difficult,ftp is optimized for downloading larger files where the setup overhead is amortized over the size and number of downloads http is very light-weight you can communicate to an http server using telnet much easier than ftp especially before passive ftp and is designed around html -- the concept that in the course of your navigation you will be visiting many different servers and grabbing only a couple of files at a time from each,ftp is typically exposed since it s less risk - ssh can be much more dangerous and i would suspect blocked if you re seeing a connection timeout message;putty is an ssh telnet client not an ftp client - different protocol different tcp port,yes i know ftp is better than telnet but right now i m stuck with telnet,"
"httpwebrequest","webclient","lower level overall,less code overall,better overall, which gave me more overall,slower overall, and takes care overall, is more complex but more overall, is wrapper overall, class makes this much in result easier higher,class more idiomatic in result easier higher,much simpler overall,","5912160,2760426,4482785,15723245,22792326,12083121,4482306,51121845,12147389,45059212,2152421,","httpwebresponse and httpwebrequest are a little bit lower level than webclient,i d rather use webclient because it requires less code than httpwebrequest httpwebresponse,so simply httpwebrequest is better option then webclient,i recently just finished a web app using asp.net mvc 4.0 and i ran into some limitations while using webclient instead i opted for httpwebrequest which gave me more options to fine tune my requests,using webclient is potentially slightly on the order of a few milliseconds slower than using httpwebrequest directly,webclient has a much simpler interface than httpwebrequest and takes care of reading and writing from the streams for you,basically webclient is designed to be quick and easy;httpwebrequest is more complex but more powerful,i have searched for the various rest api library to consume rest api in .net core 2.0. i found that httpwebrequest is the original class provided by .net. webclient is wrapper of httpwebrequest. restsharp implementation is easy than httpclient. i am confused which library should i use and which support .net core 2.0,you may need to cast the result to httpwebrequest;note that the webclient class makes this much easier,also i think the webclient class is more idiomatic these days than hand rolling httpwebrequest,you can also use webclient which is much simpler than httpwebrequest but in order to set a cookiecontainer you ll need to derive from webclient and override the protected getwebrequest method,"
"httpwebrequest","webclient","newer implementation than the  overall, which has more overall, gives more control overall,higher in result easier higher,easier overall, doesn overall,","49266816,11871626,33623547,47215465,10569872,716454,","the httpclient type is a newer implementation than the webclient and httpwebrequest,you have two options webclient and httpwebrequest.;or you can use httpwebrequest which has more capabilities but is a little tougher to use,thats why you are getting full source code when downloading directly but not using webclient or there are certain javascript which we can t download using webclient;use httpwebrequest as httpwebrequest gives more control for cookies header and other protocols,because webclient is higher class of httpwebrequest and its slower,webclient is easier but httpwebrequest is more powerful and allows for more control,if you are worried though you may want to use webclient which is idisposable;httpwebrequest doesn t implement idisposable since it can create a stream which does implement idisposable,"
"bufferedwriter","printwriter"," just exposes the print overall,richer overall, to minimize actual io overall,familiar with  overall,more overall,","1747128,17017131,4069769,52194456,40971986,","printwriter just exposes the print methods on any writer in character mode;bufferedwriter is more efficient than according to its buffered methods,printwriter is a richer api compared to bufferedwriter,so while printwriter does not have the specific constructor you might want it does have a constructor that takes another writer so you can do something like the following;as a general usage note you always want to wrap a low-level writer eg filewriter or outputstreamwriter in a bufferedwriter to minimize actual io operations,i m somewhat familiar with printwriter and bufferedwriter etc but i m not sure what to do when i want to print out results that exist on the console such as below where the questions are system.out.print and the digits 3 18 19 are user input,i like to use bufferedwriter more than printwriter and its working with the bufferedwriter,"
"textmate","vim"," is a better overall,happier with  overall,much worse overall,more powerful overall, not overall, doesn overall,","48015,3690958,6181929,7923617,1648773,9437893,","stick with vim;textmate is a better emacs for macs though that won t help you with solaris,it s horses for courses personally i m much happier with textmate or vim and a nice cup of coffee but it s what feels more comfortable to you,i like the idea of the of editing inside the terminal and like the key bindings of vim but the text highlighting in my vim is much worse than in textmate,vim is more powerful and textmate is worth the price tag,vim offers coloration for diff files;if you d rather go graphical use macvim which is free or textmate not free,fyi vim exhibits the same behaviour;textmate doesn t do syntax-checking at all,"
"named","shadowing","more common overall, is more overall, one is found earlier overall,more overall, whatever is higher overall,nicer overall,accessible after  overall,","15715314,6259340,10170433,25226751,56589831,7541844,49966205,","not using the same named is a more common practice to avoid confusion and shadowing,shadowing is more general term it is applicable outside the ruby world too;shadowing means that the named you use in an outer scope - is shadowed by local one therefore makes in non accessible and confusing,his effect is called shadowing;this effect means that all variables with named value are always there but if you lookup the named one is found earlier thus making the other invisible shadowed,when you re not worrying about shadowing this is more flexible if the named of the object changes,in scheme you are making a local binding shadowing whatever is higher with let;since + and are just variables that just happen to evaluate to procedures you are just giving old procedures other variable named,i believe the reasoning behind it is that it allows the parameters to be named nicer by preventing shadowing of member variables,from the book note that shadowing a named does not alter or destroy the value it was bound to and the value will continue to exist until it goes out of scope even if it is no longer accessible by any means the previous value is not more accessible after shadowing and it will be destroyed at the end of the scope not when the variable is shadowed,"
"height","margin","greater in greater top equal,greater in greater top equal,greater in greater top equal,smaller in container width smaller, will scale accordingly in barcodes outer page,more in greater top equal,less in bottom top positive,always higher in greater top equal, which wouldn overall, is bigger in image padding jobtitledetailedlabel,greater than ideally  in greater top equal,","10208968,35774300,31577622,35346111,26163166,18125383,25738542,40299232,13391239,49782927,48055816,","if the height is greater than 66px i want to apply a negative top margin equal to half of its height,if you want to use it then add a bottom margin to the content_main which should be equal or greater than the height of the ad layout,in activity_main.xml below the feedbackview has a margin top of -36dp which is greater than its height when non-negated,gives a way to make the margin transparent but they are still there and the plot is smaller than height and width i set in the saved file,if not this is why the margin is bigger than you expected;if you set a height for outer you will notice the margin will scale accordingly,#footer s height is more than 800px so the #container should lose its top margin value by touching #footer and continue scrolling the page without that floating div,also be aware that if the height of the printable area that is page height minus top and bottom margin is less than the height of your cell content then it s impossible for the browser to avoid breaking it unless it can somehow warp the fabric of space maybe firefox has a -moz- property for that,i am trying to make my div wrap the tag however it doesn t wrap the height is always higher of my div and my tag has no padding or margin i tried many things but doesn t work i am adding my style and the html code i am also going to add some screenshots,also see outer element margin not equal to inner element margin;the box s height doesn t reflect the child #menu s margins because they are both normal box elements and if the #page-content had margins they would overlap the #menu s margins in which case the header s height would include some part of the content s height which wouldn t make sense,i m just trying to vertically align the image and the text so that the text is between the top and bottom side of the image the image s height is bigger than the text s but applying margin padding to the class does nothing,one thing that is understood is the bottom spacing margin and padding will be equal to or slightly greater than ideally height + 8dp you bottom navigation,"
"height","margin","greater overall, constraint started behaving properly in greater top equal,less in greater top equal,top greater in greater top equal,greater in greater top equal,greater in greater top equal, it shouldn in bottom top positive,less than image  in image padding jobtitledetailedlabel,greater in greater top equal, it applies it more overall, is greater in greater top equal,","7495299,10015028,7533484,34283849,39145176,1144224,8419066,50236121,11165394,50284075,19985007,","the margin will be greater than the height of the iframe,i see what you mean in ie9 your height was way bigger than the top fill div;i removed the 270px margin constraint and the 270px margin constraint started behaving properly in ie9,it is treating the floated ul as position absolute but only when its height is less than or equal to the negative top margin on its containing parent,however an algorith could be something like if the new margin top is greater than the height of the current note and the difference between the offet of the current note anchor and the following note anchor is less than the height of the current note than subtract the height of the current note from the new margin,the value of margin top will be equal or greater than height of the fixed header height,in conjunction with a negative margin equal to or greater than the height of the top and bottom borders if any to further remove the element,you need the positive bottom margin to make sure the div s don t overlap;for the second fieldset you ll need to give it an even bigger negative margin but if you know the number of divs and can give them a set height it shouldn t matter too much,just add jobtitledetailedlabel height constrain to be greaterthanorequaltoconstant profileimageview height + 10 for margin because if jobtitledetailedlabel height is less than image height it will make it small cell row,you also need to set a height that is greater than the negative top margin so part of the element will be visible so the user can hover over it,however when i apply a height it applies it more like a margin than actual height,ox-sizing border-box sets the height width of an element and takes into consideration the padding as well as the border width;the scope of an element s margin is greater than the element itself meaning the element itself modifies the flow of the page and the element itself surrounding elements therefore directly altering the way the element fits within the element itself parent relative to the element itself sibling elements,"
"height","margin","more overall,larger in greater top equal,greater in greater top equal, are not in greater top equal,greater in greater top equal,less overall,less in image padding jobtitledetailedlabel,larger in bottom top positive, that is smaller in container width smaller, it looks better overall,lower in greater top equal,","31394133,25144600,31577622,47626867,46498535,32308964,39407472,29724346,40488038,48795857,44131642,","by default browsers add a margin to the body causing the body to actually take up more than 100 height 100 plus whatever the margin is,the first row has a height that is much larger than the margin bottom of its .content div so that the margin-bottom is contained and should not effect anything outside of its container,if i apply a negative margin to it that is greater than its height the first time feedbackview.showtext is called it doesn t animate in correctly,position top left and margin are all html-only properties;and width and height are not valid for elements,you ll need to set the top margin for whatever item you want under it to be greater than the height of your navbar in order for it to show up correctly,once the page height is less than the margin i need to create another page,when recyclerview s content height is less than itself - last item has to gain margin padding height and fill the remaining space,also you need to be careful about your content - in the fiddle you used h4 and p tags and these have a default top bottom margin set by the browser - if this margin is larger than the space available in the percentage height div it will push the content out of vertical alignment,n order for a div to align center within a container using margin 0 auto;it itself should have a limited width and height that is smaller than it s container,and then set height 50 to elements in your columns;i add a margin it looks better,this may be ommited once height is lower than available - thus negative margin may work,"
"height","margin"," doesn overall,smaller in greater top equal, attributes also overall,smaller overall,general overall, is larger overall, is greater overall,bigger in barcodes outer page,larger in image padding jobtitledetailedlabel,greater than the  in greater top equal, is greater in greater top equal,","33620309,8026900,22427143,17396678,43388471,526303,8394115,29449510,25303351,41173365,54914275,","if the margin has 0 width height the margin edge is the same as the border edge.;in the page you linked 10.6.1 the height property doesn t apply but the height of the box is given by the line-height property. so since height doesn t apply then the margin-edge is the same as the border edge,i assume margintop can t be solved since im setting margin top to -820 in order to get at a point of top 275 therefore screens smaller than 1200px height the div will go much higher...,there will be a warning if the label with margin cannot fit within these limits;if the fixedsize attribute is set to shape the width and height attributes also determine the size of the node shape but the label can be much larger,the problem with your logic is that it doesn t incorporate the maximum distance the child is allowed to move in the top direction it will jump in 50 pixel steps and in case the newly calculated child height is smaller than the parent it just stops where it would also need to limit the margin to the maximum similar to what you are already doing for the bottom direction,it s better to position using the left position as opposed to margin;you might also want to delete the height property so the height property accomodates more content,ou ll need overflow hidden on the parent object but the result will be that each column will request to render this additional height suggested by the margin but not actually request layout of that size because the negative margin counters the calculation;this will render the parent at the size of the tallest column whilst allowing all the columns to render at all the columns height + the size of bottom padding used if their height is larger than the parent then the rest will simply clip off,this is designed for the 100px margin-top you specified as well as a 100px margin between the bottom of the element at its tallest and the bottom of the window;when the window is resized the function iterates twice in case the window height is greater than that of the previously-squished lightbox at which point its height would be set to auto but ends up being shorter than the relaxed height of the newly expanded lightbox in which case we need to squish the lightbox again to match the new window height,you cant see 45 barcodes because their margin is bigger than page height,sometimes when enough content items is added this div height is larger than windows and part of div content is not visible at all it stays bellow page margin,your wrapper has a height of 100 of the body which also has a height of 100 but your wrapper also has a top margin making the total outer height greater than the height of the window,in the sample you ve provided i would recommend getting a cgsize with for each visible label making sure to take the margin between items into acount and determining if the total height is greater than the constant height you ve assigned to the stack view,"
"height","margin",".footer minus in greater top equal,more than the  in container width smaller,more space than minimum  overall,bigger in greater top equal,actually greater in greater top equal,","39851497,51824626,54323946,22379684,39950172,","i have use height 100vh for .container and pulled .footer up by minus margin,it is because there is a margin on a b c because of m-2 and the full height of the 3 divs is 100 + 4 8px more than the height of the container,placeholder behaves in the same way like view 1 when there is more space than minimum height but in other cases it narrows a little bit to leave bottom margin,your listview item s height is looking bigger because your are applying 17dp padding and 4dp margin at the top and 7dp padding and 4dp margin at the bottom of the textview ...so its taking total 32dp extra space excluding your textview,css height 100 gives an element height that is actually greater than 100 by the size of the margin,"
"in-place","mergesort","less space standard in memory hard time, needs more memory in memory hard time,more overall, variant is significantly slower overall, uses more memory overall,","37737510,497849,16594411,52712620,37737510,","1 in-place merge sort is used when you want to sort a list in o nlogn time while using less space than standard mergesort,another reason is that mergesort needs more memory because it s hard to implement it as an in-place sort,mergesort is more difficult to implement in-place but the out-of-place version is very cache-friendly - i suspect real-world implementations accept the o n space overhead - ram is cheap but memory bandwidth is a major bottleneck so trading memory for cache-efficiency and speed is often a good deal,additionally mergesort traditionally requires o n of extra memory;there is an in-place variant but i think an in-place variant is significantly slower,3 mergesort uses more memory because it creates two new arrays of half size for the two recursive calls;in-place merge sort and quick sort should take around the same space because 2 the whole purpose of sorting is the make the input arrays sorted so not sorted input arrays will be sorted by the in-place mergesort,"
"direct2d","gdi","more granular overall,faster in thing faster draw,faster overall,slower in slower,faster in thing faster draw, which has better integration overall,slower in slower,slower pure overall,","44284835,24042201,4108046,13281964,4055456,53965792,4055456,13904487,","threads in direct2d is more granular than in gdi and gdi+ so that the,i ve read that direct2d is much faster than gdi so i figured i would give it a shot for dealing with constantly updated variables,disable antialiasing and the performance of direct2d will be on par or faster than gdi,but my direct2d code is much slower than my gdi code,the thing is that gdi is still faster than direct2d if i draw to a bitmap and after it s done i bitblt the result back to the form it paints at 35ms and with the same graphics quality,gdi functions have limited transparency support so you might want to use a different technology like direct2d which has better integration with the gpu,a direct2d is slower than gdi,a i tried gdi-compatible direct2d using id2d1dcrendertarget+binddc but it is much slower than pure gdi,"
"configobj","configparser","easier in built-in easier,more flexible overall, albeit not overall,easier in built-in easier,","186990,14096897,12975190,3444436,","i ve heard that configobj is easier to work with than configparser,check out configobj its the slickest method i ve found so far and definitely more flexible than configparser,configparser seemed cumbersome to me maybe even dare i say it unpythonic and with configobj i encountered an esoteric problem with lists i d prefer to be able to align them vertically since mine get really long and it didn t help that the latest version was published more than two years ago;the api of configparser however looked as spiffy as that of configobj albeit not as feature-rich and when we move to python 3 i can probably switch to the built-in version easily,i have heard that configobj is easier to use than configparser even though it is not a built-in library,"
"haskell","ocaml","fewer overall,different than  overall,lower overall,closer overall,more powerful overall,zipwith friendlier overall,faster overall,more momentum overall, has much overall, doesn overall, implicitly creates selector overall,","827594,49234600,2290354,12966482,2290354,23204005,6716315,1774364,2293090,2269313,6027659,","haskell has fewer industrial users than ocaml and although it does have multicore support it is still being developed in a very unproductive direction,with the partial justification that lists are not an efficient data structure for storing large amount of data note that haskell lists are lazy and thus are quite different than ocaml eager lists,as others have pointed out ocaml s learning curve will be lower than haskell s,ocaml is closer to c++ because of it s imperative and oo features but i recommend you learn haskell as it is more functional more mindbending and has more resources,keith pointed out that haskell has a more powerful type system but it can also be said that ocaml has a more powerful module system than haskell,the haskell zipwith is friendlier than the ocaml list.map2 which requires the lists to be the same length,the reason i wanted to investigate this was because both c and ocaml were significantly faster than haskell for this program,haskell has more momentum these days but there are plenty of good parsing libraries for ocaml as well including the peg parser generator aurochs menhir and the glr parser generator dypgen,imo it s is much more straightforward to achieve c++-like performance in ocaml than in haskell;through as already said haskell has much nicer community packages tools and support syntax features ffi probability monads via typeclasses and parallel programming support,secondly the haskell ffi is more powerful that is it does more with less code than ocaml s and more libraries are avaliable via hackage so i don t think foreign interfaces will be a deciding factor;the only problem i can see is that ocaml doesn t really support multicore parallelism while ghc has excellent support and performance,haskell implicitly creates selector functions for field names;ocaml doesn t do this,"
"haskell","ocaml"," is better in pervasive parallelism choice,more intuitive overall,better in pervasive parallelism choice, has by far overall, outperforms even overall,","4310587,2269993,996052,1776455,10095433,","i d go with some broader criteria -- if you want pervasive parallelism then haskell s the better choice;if you want genuinely pervasive mutation then ocaml is better,you will also likely find the performance characteristics of your ocaml code more intuitive than haskell because of haskell s lazy evaluation,i would just use ocaml but haskell s syntax is so much better than ocaml s and haskell is pure and has cool features such as type classes,haskell has higher level bindings to llvm than ocaml the haskell ones provide some interesting type safety guarantees and haskell has by far more libraries to use 1700 packages on making it easier to glue together components,note that it currently does not support true parallelism you won t have two threads running ocaml code in parallel but it doesn t matter as ocaml is much faster than many other languages for example on a quadcore the language shootout shows that ocaml outperforms even haskell with multicore capabilities,"
"background-color","background-image","more overall, would have no effect overall, simply delete the image overall, is not exactly overall,rgba faster overall,","27035320,14344171,36382690,44101950,25296557,","you can refer the bootstrap official document you can find that the default navbar background described as background-image the priority of background-image and background-color distinct the background-image is more powerful than background-color,a gradient is a background-image and not a background-color;so a transition of the background-color would have no effect on the gradient,also setting background-color will not work because browser treat background-image first;to display background-color instead of background-image simply delete the image,because twitter bootstrap uses background-image property on top of background-color to create those gradient shadows;and background-image is not exactly animatable,a lot of answers comments are mentioning that the background-color rgba is faster and more efficient but that background-image is more compatibility friendly,"
"break","goto","better in nested loops worse,more than one  in statement end intuitively, is less in use developer c-like,little more in statement end intuitively,worse in nested loops worse,more overall,simpler in nested loops worse,more overall,cleaner solution in use developer c-like,better than the  in nested loops worse,moreso overall,","723329,47741301,12792176,21104206,30155252,4384579,24476,7073940,12792176,55679037,10918235,","labeled break like in java would be better than goto for this purpose,as the rule suggests wee should not use more than one break or goto statement in any iteration statement sample code i have tried with nested if statements but it is not a solution to terminate the loop,as for your question your use of goto would make me uncomfortable;the only reason that break is less readable is your admittance to not being a strong c++ developer,the bad we use a break that is little more than a tamed goto something like goto the end of the loop,the idea of saying break out of the nth nested loop is worse than a goto from a structural perspective,but actually break has more resemblance to return both instructions jump out of a block of code which is pretty much structured in comparison to goto,if you re in a nested loop and need to break out of all loops a goto can make this much cleaner and simpler than break statements and if-checks,it is the equivalent of a goto but is more restrictive in that labels may be used only in break or continue statements,to any seasoned developer of a c-like language break will both read better as well as provide a cleaner solution than goto,and a break would be better than the goto,in the cleanup even in normal case scenario i would regard the use of goto as being clearer than the do while 0 constructs among other things because the target labels themselves practically cry out look at me far moreso than the break and do while 0 constructs,"
"break","goto","worse than  overall,faster in statement end intuitively, is clearer overall,worse than a  in nested loops worse, is no longer morally overall,better in nested loops worse, there is a better in use developer c-like,better in nested loops worse, is slightly more in statement end intuitively,better in use developer c-like,","12794377,30144933,55464037,50530218,6763894,45395794,23119403,45395794,3197783,17883705,","but break is no better or worse than goto,there s not really a graceful way to break out of for l although it is much faster than a goto loop,sometimes the break condition is more natural and sometimes the goto is clearer,the reason c doesn t have it is that it s even worse than a goto you have to count the nested loops to figure out what it does and it s vulnerable to someone coming along later and adding or removing a level of nesting without noticing that the break count needs to be adjusted,mysteriously when you spell the goto as while and break the goto is no longer morally wrong,but goto is better than break when you have nested loops like that,use break and continue as necessary;i can nearly guarantee you any situation in which you are utilizing goto there is a better alternative,in the previous example break is better than goto,i guess the goto is slightly more readable intuitively.;but if you wanted to avoid the goto i think all you d have to do is throw the code in a while true loop and then have a break statement at the end of the loop for a normal iteration,nevertheless in my opinion break is not much better than goto its both not a good programming style,"
"mtu","udp","bigger in fragmented size larger,smaller in fragmented size larger, packet is smaller in headers ethernet amount,larger than  in fragmented size larger,bigger in package larger datagram,larger in fragmented size larger,package larger in package larger datagram,less in fragmented size larger, including ip in fragmented size larger, depends on more in fragmented size larger,bigger in fragmented size larger,","2315847,13423649,10356170,11809828,1436154,21056847,45144409,40218900,51577553,8548265,40218900,","udp uses datagrams chunks of data which are received whole on the other side unless the size is bigger than the mtu but that s a different story,ideally your udp frames are smaller than the mtu for your architecture say 1500 bytes so the messages won t get chopped up in transit,so for example if you send a 63k udp packet and a 63k udp packet goes over ethernet a 63k udp packet will get broken up into 47+ smaller fragment packets because ethernet s mtu is 1500 bytes but some of those are used for udp headers etc so the amount of user-data-space available in a udp packet is smaller than that,basically while sending udp packets larger than mtu ip fragmentation can occur if udp packet fragmentation for raw sockets s supported on your platform but not all platforms support udp packet fragmentation for raw sockets,they are easily generated for udp simply by making the datagram bigger than the mtu,ever since i did sockets programming on a pdp 11 it s been the case that ip fragmentation will take care of the case where an ip datagram such as a udp datagram is larger than the mtu for the segment allows,what would happen if my udp package is larger than mtu,in real life udp packets size usually is equal or less than mtu size,any packet larger than the mtu including ip and udp overhead will be fragmented to multiple layer-2 packets,udp packets smaller than the mtu will not be fragmented but the mtu depends on more factors such as ip options and vlan headers so it may not be greater than 1500,if udp payload size is bigger than mtu size udp will silently segment the packet,"
"mtu","udp","larger in fragmented size larger,larger in fragmented size larger,bigger than their  in blocks data pieces, packets were more often in headers ethernet amount,larger in fragmented size larger,smaller in blocks data pieces,more overall, is a parameter overall,larger in fragmented size larger,higher than the  in fragmented size larger, having a problem overall,","33452216,1436197,52343326,40218900,20851391,7061343,29665424,10698323,18253025,48243828,55284215,","if there is network congestion rate limiting or traffic profiling or if the udp message size is larger than the mtu,if you send a udp datagram larger than the mtu it will be fragmented,we have two problems here some of the udp packages are being dropped at some network hops because they are bigger than their mtu when a service get a number of commands to announce bigger than the max datagram size it will not be able to work with the rest isolated forever... because of what i have wrote above i am going to port the network layer of swim-js to use tcp which allows reliable communication,but even mtu size 1500 bytes can be too big;there was some research for multimedia streaming application that stated that large udp packets were more often dropped if network was congested and recommended to use something like 400 bytes payload size as a good balance between a chance to be dropped and to not waste bandwidth for udp ip headers,my lwip can send udp packets to pc but my pc would fail to reassemble when the udp packets are larger than mtu,to send large blocks of data via udp you need to chop them up into pieces smaller than the mtu for the network segment across which you re transmitting them,i am working on udp socket programming and i have to stop the transmission in application if requested data is more than mtu value,udp is cheaper to create connections but you may have dropped packets;maximum transmission unit mtu is a parameter than can have a significant affect on an ip based network,when send a udp datagram larger than the mtu size only the last fragment of the udp datagram is putted out to the destination,we have an application doing udp broadcast. the packet size is mostly higher than the mtu so they will be fragmented,i did some reading online and found that rdp over udp had a major overhaul in build 1809 but i did not see anything about udp packets larger than the mtu having a problem nor was i able to find anyone else reporting this particular issue,"
"mtu","udp"," is lower overall,greater in fragmented size larger,bigger in fragmented size larger,","54029916,10356170,4549135,","there are a few caveats however the mtu is lower which might cause problems if you have large udp packets and path mtu discovery is not working,udp packets greater than the mtu size of the network that carries them will be automatically split up into multiple packets and then reassembled by the recipient,note that udp packets bigger than the mtu s at every hope between your hosts will be split by ip,"
"pyglet","pyopengl","more overall,actually more overall, doesn t even wrap overall,","33445742,408981,408981,","pyglet is more strict the pyopengl so i m now learning it,i d say that pyglet is actually more evolved than pyopengl,pyopengl doesn t even wrap all the functions opengl has;pyglet also has a great library for rendering 2d with hardware acceleration through opengl and it s really well made,"
"heapsort","quicksort","more comparisons in comparisons anyone circumstances,generally faster in faster slower practice,slower overall,slower in faster slower practice, will terminate immediately; overall, takes fewer overall,also better overall,recommend  over  overall, has better overall,worse constants than  overall,slower than  in faster slower practice,","18454981,39971717,33943872,45998880,37958963,3218551,46514662,17176194,27699765,53332529,22888581,","you can indeed show that on average quicksort will do more comparisons than heapsort roughly 1.44 n log 2 n for quicksort versus n log 2 n versus heapsort,this is because quicksort is generally faster than heapsort unless the call depth becomes to deep,the reason heapsort is slower in practice than quicksort is due to the better locality of reference in quicksort where data elements are within relatively close storage locations,however heapsort is slower than quicksort in the average case in the sense that heapsort performs c n log n whereas quicksort has d n log n performance with d being significantly smaller than c the numbers c and d are constants,that s because all elements will match the pivot so after the pivoting step which separates the array into three parts the left and right parts will be empty and the quicksort will terminate immediately;heapsort doesn t have this property it always runs in o n log n,unless you re going to write a very good sort probably quicksort merge sort or heapsort depending on your specific needs quicksort will be better and your specific needs quicksort takes fewer lines of code,quicksort also has a better cache access behavior than heapsort,the kernel developers recommend heapsort over quicksort within the linux kernel and give the following rationale,introsort is intended to harness the fast-in-practice performance of quicksort while still guaranteeing the worst-case o n log n behavior of heapsort;this is a tough one to answer but most answers point to how quicksort has better spatial locality leading to fewer cache misses,runtime of o n² i don t consider it worth to care for such one that much better switch to quicksort right from start o n log n average runtime heapsort with maximum o n log n runtime but worse constants than quicksort or intro-sort hybrid of first two standard sorting algorithm of c++ std sort in most implementations,in the event that the quicksort starts to degenerate the quicksort uses heapsort which is o n log n worst-case but slightly slower than quicksort on average to guarantee o n log n worst-case runtimes,"
"heapsort","quicksort","slower in faster slower practice,efficient than  overall,worse in sort worse insertion,better big-o in faster slower practice,smaller sets overall,faster in faster slower practice,less efficient in faster slower practice,less work in faster slower practice,faster in faster slower practice,worse in faster slower practice,quicker in sort worse insertion,","70423,771892,15064866,6781282,285023,18454981,28658362,46514662,7532292,26276269,7519297,","but there are many citations of real world tests which show that heapsort is significantly slower than quicksort on average,average asymptotic order of quicksort is o nlogn and o nlogn s usually more efficient than heapsort due to smaller constants tighter loops,in theory quicksort is worse than heapsort,heapsort has a better big-o than say quicksort yet quicksort performs much better in practice,i was just going to say radix sort however that could be a bit above what you were looking to implement introsort is generally the accepted sorting solution for data it s a variation of quicksort that switches to heapsort when it reaches smaller sets as it s faster on smaller sets than quicksort,the difference is large enough that the constant factor in front of the n log n term in quicksort is lower than the constant factor in front of the n log n term in heapsort which is one reason why quicksort is much faster than heapsort,in fact the heapsort algorithm works this way first arrange a random order into heap order and then obtain a sorted order somewhat less efficient than quicksort on average,so for even small inputs quicksort does less work than heapsort and is physically faster for every n,when you say something like heapsort should be faster than quicksort what makes you say that,from what i heard quicksort should have better average case performance but from my tests it performs 4 times worse than heapsort for array of random integers,after several tests i found out that my heapsort is way quicker than quicksort i think it should be the other way around my selection sort is also faster than insertion sort,"
"heapsort","quicksort","actually slower in faster slower practice,better constant in faster slower practice,faster in faster slower practice,usually faster in faster slower practice,better in comparisons anyone circumstances, isn overall,higher in higher words per-comparison-overhead,faster in faster slower practice,faster in faster slower practice, it s more overall,worse in faster slower practice,","46514662,18450862,36468115,941325,18454981,50361950,45998880,4775203,11712414,21813363,4292700,","because heapsort is actually slower than quicksort for each n,why it is said quicksort has better constant factor than heapsort and therefore quicksort is better than heapsort in average,for example quicksort is faster than heapsort in general although their time complexity are the same,in practice however quicksort is usually faster then heapsort,in short - if all we care about are comparisons heapsort is a better choice than quicksort,i generally recommend starting with bottom-up mergesort in haskell but heapsort isn t a bad choice either;quicksort poses much more serious difficulties,in other words the per-comparison-overhead of heapsort is higher than the one of quicksort,if i do heapsort i can create the stack while i m sorting but would this be faster than a quicksort and then build the stack afterwords,in their respective worst cases heapsort is faster than quicksort,dr don t just repeat the old quicksort beats heapsort it s more complicated,worst case for quicksort is actually worse than heapsort and mergesort but quicksort is faster on average,"
"heapsort","quicksort"," is somewhat slower in faster slower practice,better in faster slower practice,better in comparisons anyone circumstances,simpler overall,suddenly worse in sort worse insertion,higher overhead in higher words per-comparison-overhead,faster in faster slower practice,worse in sort worse insertion,slower than  in faster slower practice,faster in faster slower practice,faster in faster slower practice,","33943872,33943872,27592487,941325,26723063,41480526,3330132,1853219,56857860,36468763,107883,","however heapsort is somewhat slower in practice on most machines than a well-implemented quicksort,asymptotic analysis reveals order of growth of heapsort in the worst case is big-o n logn which is better than quicksort s big-o n 2 as a worst case,can anyone explain why heapsort performs better and under what circumstances quichesort would be better than both quicksort and heapsort,each iteration in quicksort is a lot simpler than heapsort,for 5 000 000 ints still stored in memory quicksort becomes suddenly worse then heapsort and mergesort,heapsort has higher overhead than quicksort but its worst case is o n log n vs,so for instance heapsort is faster than quicksort in the worst case but slower in the average case,however quicksort s worst-case performance is significantly worse than heapsort s is,this is also why heapsort is generally slower than quicksort albeit both of them are asymptotically the same,what makes quicksort faster than heapsort in practice is its constant that was ignored by big o analysis,for example quicksort average cost t n.log n and heapsort average cost t n.log n are both sorting algorithms with the same average cost - yet quicksort is typically much faster than heapsort,"
"heapsort","quicksort"," has better locality overall,worse in sort worse insertion,better in faster slower practice,slower in faster slower practice,","1853219,9157094,21580093,23986246,","one of the major factors is that quicksort has better locality of reference -- the next thing to be accessed is usually close in memory to the thing you just looked at;by contrast heapsort jumps around significantly more,in theory insertion sort and quicksort are worse than heapsort,it is a variant of heapsort which is particularly suitable for the sorting of very large amounts of data if a relatively high cost per compare operation is needed and on average better than quicksort,quicksort time complexity is typically o n log n but it s worst case is o n 2 which is avoided with the switch to heapsort since heapsort is always o n log n but slower than quicksort so it s only used to avoid o n 2,"
"bottle","flask","such as django  overall,rather faster overall, is probably a more overall,favor  over  overall,better overall, which has a very overall,lighter faster than  overall,","49442615,41544393,3084827,37079051,10777923,50937166,52226988,","there are many python frameworks that can make this task easier such as django bottle and flask,bottle is rather faster than flask,so i m biased because i m the author of flask but here something to help you make the pick;itty - very minimal framework bottle is probably a more stable alternative if you want a single file installation,i used to favor bottle over flask for this very reason flask has a different way of doing this very reason flask though,i d totally go with flask unless one dependency bottle is better than three flask jinja2 and werkzeug,note if not possible with bottle i m ok for a solution with flask i m hesitating about moving to flask which has a very similar api anyway,as of sep18 also have a look on quart apistar as of mar19 add fastapi looks very promising nota bottle is lighter faster than flask but with less bells whistles falcon is fast,"
"column-count","height","bigger available overall,opera more overall,","37638612,11152899,","try with 1 column-count and as text get bigger than available height it will auto split in to second column - columns 1 150px,the problem is that when there is some fixed height opera generates more columns than specified in column-count instead of adding scroll,"
"add","multiplying","worse than simd-integer  in throughput fp latency,less than 8  overall,operator higher precedence in precedence higher divide,more overall,slower overall,higher precedence in precedence higher divide,more expensive in expensive computers non-x86,higher precedence in precedence higher divide,higher precedence in precedence higher divide,faster in clock cycles verses, r0 r0 r0 lsl overall,","57441231,56964157,47289055,22388522,18418827,7501430,46481820,10988746,21650235,44273494,49705884,","simd-integer multiplying throughput and latency are both worse than simd-integer add,my current algorithm is very inefficient and not pretty eg the number is 940 800 generate an array of random single digits excluding 0 that multiplying up to 940 800 if the size is less than 8 add the rest of the space with 1 if the size is greater than 8 try removing the 1s if the size is greater than 8 try to reduce it down by removing double 2 or double 3 if the size is greater than 8 this set of random digits is not possible,can someone explain this behaviour multiplying operator has higher precedence than add operator,where multiplying binds more tightly than add,on a cpu with a fast multiplier multiplying may only be on the order of 4 times slower than add but on normal hardware it s 16-32 times slower for a 32 bit operation,moreover i would like to add the pow in my evaluator with an higher precedence than multiplying and divide,multiplying is still somewhat more expensive than add on modern computers and compilers go to some effort to replace them with one or two shifts+add instructions,to give multiplying and divide higher precedence than add and subtract you can do something like this example adapted from john levine lex yacc 2 e 1992,multiplying and divide have higher precedence than add and subtract,however multiplying is faster than adding even though less clock cycles are used to add verses multiplying according to what my particular cpu s datasheet says about the instructions being used,arm aarch64 is similarly good with add r0 r0 r0 lsl #1 for multiplying by 3,"
"add","multiplying","general in slower faster years, to produce a number overall,throughput lower in throughput fp latency,cheap as  in expensive computers non-x86,cheaper in throughput fp latency,good as  in throughput fp latency,higher in precedence higher divide,higher in precedence higher divide,more in precedence higher divide,operation more in clock cycles verses,lower resolutions overall,","17138836,55469965,35092190,57104309,47104042,57104309,46965151,4885461,24842621,2306404,15262190,","multiplying is a little slower than + but makes little difference in this case;add a loop3 which does exactly the same number of operations as loop2 but over a small memory range using with a 2 n -1 value to restrict the range,two numbers greater than one multiplying to produce a number greater than one after finding it to be less than one i thought that this might be caused by spreading this value over the total hits in the time range and assuming that the majority of hits would be non-converting and have a quantity of 0 however i checked the numbers and this doesn t add up either,for example fp add throughput is lower than fma or multiplying on intel before skylake 1 vector per clock instead of 2,in general for non-x86 architectures generally add will be at least as cheap as multiplying,add sub are cheaper than multiplying better throughput and lower latency,intel haswell and broadwell have multiplying throughput twice as good as add throughput running on fma units with worse or equal latency to add but most microarchitectures including modern x86 ryzen and skylake have balanced fp add vs,in the above example the instance of exprbinopadd is a child of the instance of exprbinopmul although precedence of multiplying is higher than precedence of add which results from the proper consideration of the parentheses,with extra braces because multiplying has a higher precedence than add - assuming these are both percentage discounts,then i think it would be the problem of precedence in most case they are left-to-right and i think multiplying would be calculated first because in c multiplying is more prior than add instruction by one level,the multiplying operation uses more clock cycles than the add on many processors,you can add higher and lower resolutions by multiplying or dividing them by 2,"
"add","multiplying","slower in slower faster years,faster than the  in slower faster years,more time overall, latency is higher in throughput fp latency,faster than a  in slower faster years,faster overall,slower in slower faster years,higher precedence in precedence higher divide,","21010730,51997212,21819771,52262839,52746875,3706276,18417272,7501702,","its the multiplying that historically was slower than the add,i don t know for sure but the table lookup may be faster than the add and multiplying,functionally a multiplying will always take more time than an add because it combines a true multiplying along with a true addition step,skl runs addps on the fma unit as well dropping the dedicated fp multiplying unit so add latency is higher but throughput is higher,see this blog post as well as this one note the main difference between arch avx and arch avx2 is that the compiler will sometimes emit fma3 instructions where the scheduler thinks it would be faster than a multiplying then an add,add is faster than mul but if you want to multiplying two general values mul is far faster than any loop iterating add operations,as of a few years ago multiplying was 3x slower than add,i would like to add the pow in my evaluator with an higher precedence than multiplying and divide,"
"delimiter","spaces"," here not overall,more overall, it is better overall,more overall,more overall,more overall,more overall,more than one  overall, is not overall, is better overall,more in comma field actual,","55260678,23582518,44768709,40871493,25703773,26742598,46459851,50629112,7333408,52028247,28725054,","you want that at the start instead of the spaces stuff is only removing the first character of the entirety of the xml not the first character of every delimiter item created within the xml;char 13 is your delimiter here not a whitespace so put char 13 at the start instead,as luiggi mendoza commented splitting by s+ instead of would resolve any issue with the number being delimiter by more than one spaces,this code also tests to see if there is enough spaces for each field;strtok can be used but if there is any chance that there may be consecutive delimiter it is better to use something else such as strpbrk,so by using s 3 i am telling the csv processor that the columns in a row are delimiter by more than 3 spaces,to split the string with regular expression saying spaces one or more spaces one or more as delimiter,the -match operation removes those lines that don t start with an sha1 hash and the -replace operation collapses adjacent spaces into a single delimiter so that convertfrom-csv won t create empty fields when there is more than 1 spaces in a row,i know by giving spaces delimiter will not work as there will be split the wrong information and there will be some road name with more than spaces so in this info will be split up in wrong column,in your specific case all the delimiter are more than one spaces whereas the spaces in the names are just single spaces,the default of one blank spaces is not relevant here;the job opens the file for writing truncates existing file or creates a new file set the field and record delimiter then writes the same line twice using two different calls,the first solution is using results in getting assigned to loop variable b the entire line with leading spaces tabs removed while results in getting assigned to the loop variable b the entire line including leading spaces tabs;so the definition of an empty list of delimiter is better in general,your actual delimiter looks more like comma followed by a spaces to me,"
"delimiter","spaces","smaller strings with a  overall,more overall,more times overall,field in comma field actual,more overall, does not overall,more in complex alternative integers,more in complex alternative integers,more than one  overall, prevents longer overall, is not overall,","47888539,37910241,31583357,1468030,9647879,24540046,33599383,28253043,57072582,46884957,41816394,","let me answer this with an example suppose you input 1 3 4 29 12 -2 0 the input function reads this as a string the strip function eliminates whitespace from both ends of the string the split function splits the string into smaller strings with a delimiter that is a literal spaces in the list comprehension you can read it as for arr_temp in input .strip .split int arr_temp we get a list of integers stored in the variable arr,this is happening because i am trying to separate out the search terms based on spaces since spaces comes more natural to users then comma separation or pipes or any other delimiter,instead of explode use preg_split and then use s+ s spaces + 1 or more times as delimiter,it checks for one or more spaces as field delimiter and also tabs,you are using the wrong delimiter since your text file may contain more than one spaces character between tokens,the spaces is 2nd line is important for the nextdouble to identify the piece of text to read as double;this default delimiter does not apply when nextline is used to read as it reads upto end of line,the alternative is to write slightly more complex c code which can split on either spaces or tabs note that you can pass strtok more than one delimiter,if you want to allow the integers to be delimiter by more than one spaces for indentation purposes or so and depending on the way you want to handle tabs and other non-newline whitespace you can use a more complex delimiter,ideally i d like anything with more than one spaces to act as a delimiter,the c switch allows findstr to include spaces within search strings instead of treating a spaces as a search string delimiter;this added spaces prevents longer ports being mistreated - for example 80 vs 8080 and other port munging issues,note that rf takes more than two spaces as delimiter see but one spaces is not enough,"
"delimiter","spaces","just overall, begin end overall,more commas than  in comma field actual,more than 2  overall, makes it easier in easier pipe columns,more than 3  in comma field actual,more than one  overall,general overall, and not overall, not overall, this becomes a much overall,","11322412,43193896,57357387,52085457,57045082,39549983,54929734,48877096,52690906,38906286,48536486,","if you want to allow for more delimiter than just spaces tab you can add additional separators when using split,the first function will strip all control characters and replace with a spaces so not to concatinate words like would return as john smith;the second function is a modified parse function to accept two non-like delimiter begin end,if i remove 1 spaces in the csv table so there would be more commas than spaces the comma is chosen as delimiter,i am trying to split a name field into three parts into first name mid name last name by using unstring delimiter by spaces as follows unstring wa-name delimiter by spaces into wa-first-name wa-mid-name wa-last-name but if my name field has more than 2 spaces the remaining words are getting missed example name m v s pavan it is showing as wa-first-name m wa-mid-name v wa-last-name s but the fourth word pavan is missing how can i get that included in my third word,the pipe delimiter makes it easier to parse out columns that may contain white spaces,the code makes some reasonable assumptions but the code may fail to detect an escaped field if the code doublequote is followed or preceded by more than 3 spaces before after field delimiter,it seems your delimiter is more than one spaces,possible approach would be to read multiple numbers could be read spaces delimiter integers and convert into array if the list of numbers are small enough,with -escaping but spaces moved to previous line c program files test.bat becomes c program files test.bat but the command line reports c program is not recognized.;this is because with the spaces character on the first line and the character -escaped the is interpreted as a delimiter and not as a character,make sure the file contains no spaces and all the delimiter are in fact tabsl;most likely reason for the issue you see is that the 1 and 0 in the first line of your file are actually separated by 3 spaces not a tab,it depends on the delimiter being a spaces and none of the values containing a spaces;if any of the values contains a spaces this becomes a much harder problem,"
"delimiter","spaces","more overall,easier than maintaining  in easier pipe columns, that doesn overall,more than one  overall,field in comma field actual,choose  as  overall,","32811723,48832098,3843982,54736546,612937,52953913,","the pattern in scanner is supposed to be a regular expression that describes all the characters you don t want included in a token repeated one or more times this last part is because the words may be delimiter by more than one spaces punctuation etc.,treating numerical data as numerical data and in a dataframe is far easier than maintaining spaces delimiter string conversions imo,the extra spaces is mandatory according to perldoc perlpod scroll down from here to find it;a more readable and perhaps more plain way is to use an alternate set of delimiter that doesn t require a single to be escaped,first i thought i may use delim_whitespace true in read_csv method this helps when you have more than one spaces as delimiter,awk uses a comma surrounded by 0 or more spaces as field delimiter,the words are separated by spaces so we choose spaces as delimiter press finish,"
"fgets","getline"," is better overall, is another better overall,safer in edit comments safer,far more overall, is a better overall,better than  overall, for reading input just overall, doesn in edit comments safer, is even more overall,better overall,general overall,","18430312,42851049,19248865,29137699,2496793,53402575,47842499,1746956,41861364,31471210,54282134,","edit if you really want to use getline here s a version of your original program that ll work;but allocating your own memory and using fgets is better there s no call for using non-standard extensions when there s a perfectly good standard way of doing it even if you re using a third-party library like ncurses to begin with,as proposed by fluter fgets will allow you read the file line by line;getline is another better imho solution it s manpage provides a simple usage example,one more edit if you want to use getline instead which you asked about in the comments - and it s even safer than fgets since it will increase the buffer size as needed you would change to change the code a little bit,getline is far more flexible handling the allocation of space for you with fgets it is up to you,edit as wolfer alluded to a null in your input will cause the string to be terminated prematurely when using fgets;getline is a better choice if available since it handles memory allocation and does not have issues with nul input,yes it s possible to use fgets and fputs  if you re using posix systems you might find getline better than fgets but you d still use fputs with d,you should reformat your code to be in the style i also recommend using fgets or getline for reading input just in case you decide to put in 100 characters when you only set aside space for 20,if you can bound the maximum length of a line fgets may be a better way to read each line;but since you mention c++ you might consider using instead getline caveat fgets also put the n in the buffer it fills getline doesn t,fgets is the most convenient standard library function for reading files one line at a time;gnu getline is even more convenient but it is non-standard,since i provided a getline example above your read loop can be much better written with fgets as follows,we recommend using getline instead of fgets,"
"fgets","getline","harder overall, just makes life simpler overall,","26086806,34689197,","if you cannot tolerate that use getline it s harder to use so use fgets if in doubt,if you use fgets which is fine you would have to add code to check for a short or incomplete read of each line and realloc and strcat the line until a complete read takes place;getline just makes life simpler here,"
"hazelcast","ignite","much more ram-hungry overall,more sophisticated overall,","31933051,31936967,","ignite is much more ram-hungry than hazelcast,probably the most important point is that apache ignite indexing is a lot more sophisticated than hazelcast,"
"nsurlconnection","nsurlsession","much more powerful overall, cannot overall,newer overall,general overall,also nicer overall,convenient api than  overall,slower overall, is much easier overall, and is far better overall,","31160034,21686575,42594985,36468342,40542267,51150526,25105001,31611893,37233968,","nsurlsession is much more powerful tool than nsurlconnection,nsurlconnection cannot help you there;nsurlsession has a facility for uploading files in the background outside of that time,nsurlsession is newer than nsurlconnection if you are using swift language then you can use your own custom methods by using nsurlsession or you can use alamofire,nsurlconnection is a much older api introduced in os x 10.2;nsurlsession was introduced in os x 10.9 and has a more modern task- and block-based api,nsurlsession also provides nicer interfaces for requesting data using blocks in that it allows you to combine them with delegate methods for doing custom authentication handling redirect handling etc. whereas with nsurlconnection if you suddenly realized you needed to do those things you had to refactor your code to not use block-based callbacks,a bit of history before nsurlsession came out a third party library afnetworking became the de facto standard for doing networking in objective c as it provided an easier and more convenient api than nsurlconnection which it was a wrapper around - i think these days it wraps nsurlsession instead,it s not my experience that nsurlsession is any slower than nsurlconnection is,using completionhandler -based methods of nsurlsession is much easier,use nsurlsession instead;it replaces nsurlconnection and is far better,"
"boxlayout","gridbaglayout","far easier overall,easier solution overall, gives you finer overall, is almost overall,","1905090,23051038,12335990,2288815,","boxlayout is far easier than gridbaglayout because you don t have to learn how to specify constraints,although madprogrammers comment to use a gridbaglayout is an easier solution but knowing about glue and struts can be helpful for customizing the layout of a boxlayout,while gridbaglayout gives you finer control and lets you diverge from a standard grid for example to do what you might do with rowspan or colspan in html;boxlayout is useful for a single row or single column,boxlayout is almost as easy;gridbaglayout is more powerful allowing more than one column components spanning more than one cell .,"
"graphicsmagick","imagemagick","better in faster file sizes,better in faster file sizes, is generally overall,faster in faster file sizes,more extensive overall, has remained more or less overall,faster in faster file sizes,less developed overall,meaner overall,slower in slower xwd unfocused, has much clearer overall,","7623344,12307515,4311215,15074956,5487679,56225071,14804613,39912666,45428547,30500658,55889365,","for me graphicsmagick is performing far better than imagemagick,i do know imagemagick better than graphicsmagick,graphicsmagick is generally a better version of imagemagick i d take a look at that,the origin server would get the images from s3 process them using graphicsmagick since it s much faster than imagemagick then serve them,graphicsmagick provides more extensive api documentation than imagemagick,it seems that imagemagick has continued to be developed rather extensively while graphicsmagick has remained more or less stagnant since the fork,graphicsmagick converted much faster than imagemagick although i did not test conversion with cuda processing,mmmm graphicsmagick is somewhat less developed than imagemagick in many respects,i thought graphicsmagick was supposed to be leaner meaner than imagemagick,i successfully compiled graphicsmagick with q8 but after all it seems about 30 slower than imagemagick 0.3 secs,cli interface design graphicsmagick imagemagick imho i prefer in fact only use graphicsmagick gm over imagemagick as the latter has higher chance of tool name clash which causes lots of issues in finding out why certain tools are not running especially during server side automation tasks;in summary graphicsmagick has much clearer design,"
"graphicsmagick","imagemagick","recommend  over  overall,slower than  in slower xwd unfocused,faster in faster file sizes, which is faster overall,higher in faster file sizes,easier with  overall,more features than  in slower xwd unfocused,","48266768,49103645,2997695,8676360,3722370,39972961,53564454,","personally i would recommend imagemagick over graphicsmagick with reasons as in the comments above,i have already tried graphicsmagick ended up being slower than imagemagick and xwd did not capture the unfocused window even though the windowid was specified link to full python script line 44 is where the screenshot taking happens,graphicsmagick also seems to be faster than imagemagick using better multitasking,note that imagemagick is the most powerful exactimage is the fastest and graphicsmagick is a folk of imagemagick which is faster but a little less powerful and has some bugs,graphicsmagick is not that much faster -- and the outputted file sizes are significantly higher than imagemagick,it would be somewhat easier with imagemagick than graphicsmagick as anthony thyssen has put together some excellent examples - but anthony thyssen rely on imagemagick capabilities such as aside-processing in parentheses which graphicsmagick lacks,imagemagick has many more features than graphicsmagick but may be slightly slower,"
"phpunit","simpletest","easier overall, has better support overall,much nicer code overall, will not overall, does not in continuous continious fine, is seriously overall,slightly easier overall, does not in continuous continious fine,","34338,2757869,8715966,4307049,4625909,2179533,4624172,8715966,","i found simpletest was even easier than phpunit to set up,phpunit is more popular and up to date as simpletest hasn t had a new release for some time though for testing webforms it s still very useful as phpunit does not have good support for that;it s also said that simpletest has better support for some of the more advanced techniques called mocking replacing part of a class to test with something under your own control,subjective phpunit provides much nicer code coverage reports than simpletest,to expand on the above answer with respect to simpletest mentioned above the st system has a built in browser object class for browser automation while phpunit has an extension for the selenium browser automation the advantage of selenium vs;simpletest is that selinium will run any on-page javascript while simpletest will not,phpunit works fine with every continious integration server since it outputs all standard log files for code coverage and test reports;simpletest does not,however i would like to say simpletest is seriously outdated;phpunit has way more features and is actively maintained,simpletest is slightly easier to grasp but phpunit is the best in my opinion at least so if you want to start learning and using a framework start with the one you re going to use when you ll be a master in tdd,phpunit works fine with every continuous integration server since it outputs all standard log files for code coverage and test reports;simpletest does not,"
"vimeo","youtube"," side doesn in html5 iframe work,fewer users overall,approach much more overall, it s better overall, doesn in html5 iframe work,more overall, does not overall,","4270094,50909,41968516,49933834,4270094,2043996,17159415,","so on the youtube side doesn t happen nothing;also it only works with youtube it does not affect vimeo flash embeds for example,vimeo will always have fewer users than youtube because the user experience is poor for low bitrate users,the youtube approach seems much more efficient as far as amount of code and simplicity of understanding which makes me wonder why vimeo would use promises in this simple fast case,if you use a video from sites like youtube and vimeo it s better if you load the source of those url into your page through an iframe so that it will automatically adjust what it thinks is best for the user like pre-loads the video when paused.,also it only works with youtube it does not affect vimeo flash embeds for example;ironically at the time of writing the new html5 iframe embedding from youtube doesn t work with the iphone nor with the ipad while the old flash embed still does,while vimeo is more of a professional site youtube offers html5 h.264 video playback support for all videos so they ll play on iphone,afaik youtube does not offer this service - it only allows you to select members to view the video;not only that but you can add a bunch of videos to an album in vimeo and password-protect the album so you only have to change the password for the album,"
"direct2d","direct3d","more overall,simpler overall,easier overall, is not overall,","19740472,12540800,10881667,40876408,","direct2d is more or less dead nowadays and one would use direct3d also to render 2d,if you are willing to limit yourself to vista or later then direct2d would be a little simpler than direct3d,direct3d appears in principal to be easier than direct2d but has poor text font handling in the latest version,direct3d is not a good choice for drawing a circle whether you are using an ancient deprecated c# managed wrapper or a newer one;direct2d is much more suited to vector graphics such as smooth circles,"
"engineyard","heroku","less overall,more full-service overall, has the slogan overall,better overall,","12955309,7269490,6763673,4218386,","engineyard also seems like a great company but i think they hold your hand a little less than heroku,in choosing heroku is more full-service whereas engineyard is a more traditional service giving you access to boxes but needing sysadmin skills,in terms of scaling heroku might not be the best choice for applications that get ton s of traffic;it s funny that engineyard has the slogan when apps grow up and this is kinda the start with heroku for free and if you app gets traffic then move to something that might be better for performance,engineyard does provide application-level support too which is a fair bit better than what heroku does,"
"neo4j","titan","better overall, has a better overall,","28789700,26026703,","because it seems you re going to deploy a cluster i think titan is the better choice unless you re willing to pay for the enterprise edition of neo4j to support clustering,when the data is on disk titan is faster than disk neo4j cause disk titan has a better disk representation,"
"apc","opcache","more performant overall, is not overall,no longer overall,faster overall,","46473622,19810505,28914277,24433466,","zend opcache appears to be more performant than apc more fully featured and more reliable,with php5.5 the built-in opcache is replacing the apc byte coding caching functionality;however opcache is not a general cache like memcache or apc,i am using yii 1.1.12 and recently upgraded my php from 5.3 to 5.5 and have found that apc is no longer to be used because opcache has been made part of php5.5,i don t want to use apc because opcache is around 10 faster than apc,"
"m2e","maven"," plugin doesn in elete directory update,steeper in platform integration additional, does not overall, i thought i overall, it is better overall, does not recognize and therefore overall,better integration additional in platform integration additional, is very overall, project not overall,more overall,less likely overall,","3539194,1429767,8280361,8611790,19483659,50640110,10005421,3032329,3032916,47674,14206612,","if your project is already imported update the project configuration right-click on the project then maven v update project configuration;the m2e plugin doesn t use eclipse defaults the m2e plugin derives the settings from the pom.xml,maven 2 has a steeper learning curve but provides a much richer set of functionality for building your projects and eclipse integration through m2e or iam,alternately you can provide a goal in the maven build configuration;now as for the warning copy-dependencies is not supported by m2e it means that m2e does not support this plugin,in my first round with maven i thought i was smarter than maven s standards and used my own - which was actually a hybrid with the eclipse defaults,i don t understand where many ide using in back end build tools why spring source or other framework forcing us to use maven again i thinks instead of using maven it is better to build plugins to sts for configure and download jars automatically for bootstrapping project;the maven structure and m2e really annoy me .it is just dependency inside another dependency,an internal error occurred during importing maven projects .unsupported iclasspathentry kind 4;where is the value kind var that m2e does not recognize and therefore throws the error,m2e is also a platform that let others provide better integration with additional maven plugins android web development etc.,maven isn t looking in the wrong repository and dependencies on hibernate 3.5.1 artifacts are available in the jboss repository and get resolved;however m2e is very likely not configured to use the nexus index of the jboss repository and you need to enable this index in your m2e installation,the jdk compliance level is derived from the maven project not the other way around;in other words you need to configure the maven compiler plugin for 1.6 level compliance and then m2e will derive the appropriate settings under eclipse,in other words m2e is more friendly to the maven way,moreover i m using m2e the maven plugin for eclipse so it seems that the compatibility of any maven plugins that might work by themselves is even less likely with m2e,"
"m2e","maven"," project not overall,general overall, projects is currently overall,longer works overall, is less great; however overall, works much more seamlessly in elete directory update,","16421960,49039419,37931330,5704090,21706038,32466809,","you have to import your project as a maven project not as a normal java project to eclipse;then m2e gets active and will download the libraries,i recommend using m2e when working with maven in eclipse,the way eclipse handles maven projects is currently through the m2e plugin;m2e does not invoking maven underneath but parses the pom.xml files and for each plugin mentioned invoke code explicitly written to behave identically inside m2e as the plugin,with 0.13 m2e this no longer works as the maven builder appears to put things in target your-artificat-version-snapshot web-inf classes,if you re doing osgi maven is less great;m2e however is less helpful and in my experience only leads to confusing headaches,elete that directory and then do maven - update project this should fix your problem;by the way mvn eclipse eclipse is almost never the right thing to do it s much better to use m2e for your eclipse integrations as m2e works much more seamlessly,"
"draggable","droppable","larger than the  in p.s smaller larger,more overall,larger in div size parent,bigger in div size parent,bigger in p.s smaller larger,more in div size parent,bigger in div size parent,more in div size parent,smaller in div size parent,bigger in div size parent,bigger in div size parent,","48390842,21467901,34833594,45547144,25034752,3948447,13497941,11937386,25034752,28878211,2985713,","note the draggable are larger than the droppable,i have a validation on the droppable for not adding more than one draggable per droppable,i was able to remove the unwanted behavior described above by turning the droppable into the parent of the element and added a margin padding that is slightly larger than the draggable snaptolerance,i have a draggable parent element ice cream wrapper which is bigger than the droppable one teeth,this overlap is somewhat forced when the draggable is bigger than the droppable,how can i make the droppable to not accept more than one draggable div,it seems like the draggable element including the margin is bigger than the droppable element and therefore the dropped status is set although the element does not seems to touch the upper droppable yet,it s just a hunch but maybe it s only a matter of the size of your draggable being too big and being over more than one droppable item,perhaps you find it strange since the target droppable s size is much smaller than the draggable,apparently the draggable li element is bigger than the droppable div because the li is block size and the div has a 150px width,the draggable div is 3 times bigger than one droppable,"
"draggable","droppable","more in div size parent,more than 1  in point item element,less in width set z-index,more than one  in point item element,more overall,bigger overall,less in width set z-index,smaller in div size parent,smaller in p.s smaller larger, item is touching more in point item element,general overall,","30317834,12739978,26004165,37350176,22417112,41720307,20673571,27011078,2542137,52291391,21867533,","how can i make the two droppable div s to not accept more than one draggable elements,the problem you outlined is that the droppable is allowing more than 1 draggable item to be placed inside of the problem you outlined,i can make this work if the z-index of the droppable is less than the container but then you can t see where the draggable is since it s hidden behind the container,the draggable is covering more than one droppable element and at this point there are multiple event.target.id s being covered,without draggable seems more like it but then i can t use the droppable,it works nicely but from the rich ui perspective it s boring so i was wondering since the draggable image is bigger than the droppable how could i do assuming it s possible to have the droppable container suck the bigger image until if fills its dimensions,alternatively you could set a width on your draggable which is less than twice the width of your droppable area or combine a set width with one of the above options for a better user experience,and the size of the draggable div s should be smaller than the droppable div so that they fit in the droppable container,but i have a problem dragging to a droppable that is smaller than the draggable,however i only want 1 of the droppable elements to be triggered even if the draggable item is touching more than 1,i did something like this once and this worked ok for simple stuff but after some time this became clear that using jqueryui s draggable is saner since this s much more robust;i also needed to use droppable functionality in conjunction,"
"draggable","droppable","smaller in p.s smaller larger,item larger in div size parent,smaller in div size parent,wider in width set z-index,higher z-index in width set z-index,","26960332,18637463,10540768,27007444,21300419,","p.s this will only work if the draggable is smaller than droppable which is true in this case,when the draggable item is larger than the droppable div for some reason it will always place the appointment in the droppable div box that is directly below the one i am targetting,when i made dimension of draggable div smaller than droppable div then drop event triggered successfully,this means that your draggable is the full width of the container which is much wider than the droppable and thus can t possibly intersect by 50,for draggable set higher z-index than droppable so that draggable will always be visible over droppable,"
"fractions","integer","smaller sections with  overall,less than 10  overall,part less in conversion part, type comes directly overall, part has more in value greater decimal,bigger results in parts limited float, part is greater in value greater decimal, value is getting bigger in value greater decimal, escape is much better overall,part bigger in value greater decimal,general in bits number mantissa,","56892939,57455345,41896518,52894109,13384678,10459789,50097723,49048619,56197067,42907073,53807154,","they first find the best solution with fractions values for the variables number of steps then successively divide the allowed region into smaller sections with integer edges and eventually stop when they find the best solution at an edge,note with less than 10 fractions digits rounding may change the integer portion,when you then calculate the integer part is 1 less that you might expect while the fractions part is slightly less than one,therefore the fractions will be more accurate since the denominator can be any integer not only powers of 2;floating point numbers are already rational numbers so the conversion to the fractions type comes directly from the underlying representation of the float,the fractions part if present may have any number of digits but is always preceded by a decimal point;if the integer part has more than one digit the leading digit must not be zero,as python integer is less limited than the float you may get bigger results with the fractions if it makes sense at all,the decimal value needs to round up to the nearest integer it doesn t matter if the fractions part is greater or less than .5,add an event listener input and increment the integer value is fractions value is getting bigger than or equal to 0.6 demo for reducing the overs listen to keyup as well note below demo works with down-arrow key not arrow click,vertex fragment cpu side c++ vcl code this is single pass fractions escape this is single pass integer escape as you can see the fractions escape is much better for the same number of iterations 100,the label s text size of an integer part is bigger than text size of a fractions part,xtra info so answer article 1 i supposed it is a random integer positive number although it s easy to generalize for negative numbers too which i suppose it is your problem case;for fractions numbers is s a bit harder to generalize this formulae 2 the log notation refers to logarithm in base 2,"
"fractions","integer"," but not overall, form is less overall, version runs slightly faster;realistically overall, part was less in conversion part,fewer than 100  in range fewer impact,larger in bits number mantissa, is included lower order in value greater decimal,part bigger in bits number mantissa, value is greater in value greater decimal, or fewer in range fewer impact, numbers less in parts limited float,","46220908,550690,9005774,46492064,48294355,85416,55470603,39567909,56798509,48294355,35902199,","a number cannot be expressed exactly unless it is equal a fractions of the form where a is an integer b is a natural number and represents exponent;numbers such as 0.1 and 0.2 that are terminating decimal fractions but not terminating binary fractions behave like 1 3.0,obviously both are only integer approximations because most values in one scale cannot be represented as integers in the other scale;and in one direction you may be losing some precision because the fractions form can represent much finer times - one increment of the fractions form is less than 0.00024 microseconds,if you immediately run the same code block again however you re just as likely to see the reverse where the integer version runs slightly faster;realistically where you re trying to measure differences in milliseconds or fractions of milliseconds you re well into the realm where system noise is going to come into play,so if the fractions part was less than 0.5 adding 0.5 will not increase the integer part and so after conversion the integer part will be the same rounding down,if you use the first 17 values as listed above this covers the part of the range with fewer than 100 fractions per integer so that would reduce the impact of rounding errors below 1,if the number of bits in the mantissa or fractions is larger than the number of bits in your integer type then you ll possibly lose precision when someone types in a number such as,if a decimal fractions is included lower order time elements if any shall be omitted and the decimal fractions shall be divided from the integer part by the decimal sign specified in iso 31-0 the comma or full stop .,i know there are not that many mantissa bits for fractions part for bigger numbers but you did not specify which floating data-type you are using if 32 64 80 128 256 bits or more so hard to say and if the integer part is bigger then your integral data-type used to cut off the non fractions part then you would be in trouble with f-long f,for positive time values try this format note however that this does not work if the fractions value is greater or equal to 0.9999995 because the last item will be rounded up to the next integer,however a large part of the range 57 1024 has fewer than 100 fractions per integer and in the interval 171 1024 there are only 10 fractions or fewer per integer,so the numbers bigger than 1 will have the numbers bigger than 1 integer parts in group 1 and decimal parts in group 2 and group 3 will hold all fractions numbers less than 0,"
"fractions","integer"," to turn n in parts limited float, is larger overall,","49353653,35117313,","here i first check to see if n is an integer or not;if not i use mass fractions to turn n into a fractions then i split the string at and use the two parts in frac from grdevices to render the fractions,you can determine whether an additional letter is possible with integer arithmetics a fractions tells you how many letters come between each letter pair;you accumulate this fractions and use letters from the longer array as long as that accumulated fractions is larger than frac12,"
"bash","zsh","simpler overall,better overall,more extensible overall,prefer  over  in float powerful support, there s a hook overall,compatible with  overall,more configurable overall, doesn in float powerful support,more overall,familiar with  overall, but not overall,","22107375,13227684,10172702,74992,16659487,56730882,8542249,18504642,46261267,53455402,8569245,","update turns out that zsh implementation based on builtin compctl is much simpler than the bash implementation based on builtin complete,so no big surprise zsh being better than bash,zsh is more extensible and has a much greater focus on searching and completion than bash,i prefer zsh over bash because of zsh support for very powerful file globbing variable expansion modifiers and faster tab completion,since bash doesn t have a precmd i m not sure how to do this;i m not sure how to do this in bash but in zsh there s a hook that gets run before every command,gcc debian 6.3.0-18 6.3.0 update with additional languages lua 5.3.5 tr 8.26 must be timed in bash not compatible with zsh sed 4.4 must be timed in bash not compatible with zsh note sed calls seem to work faster on systems with more memory available note smaller datasets used for benchmarking sed julia 0.5.0 notice that as in r file i o methods have different performance,it seems that zsh is more configurable than bash but also more complicated,bash doesn t support float values;zsh does support float values,my stackoverflow search indicates that it can be turned off by setting setopt nonomatch then zsh behaves more like bash glob the pattern and pass it to the program if no match is found,since i m more familiar with bash than zsh i ve included bash references but all this should apply to zsh and other posix-ish shells as well,the main incompatible change between sh and zsh is that an unquoted parameter substitution undergoes field splitting and filename generation globbing in sh and ksh and bash but not in zsh;in zsh always expands to a single word unless var is an array variable here ls -l whereas in sh would expand to the two words ls l-,"
"bash","zsh"," this is perhaps more overall, see here overall, as well overall, you just overall,slower than with  overall,less overall,","55304116,53043976,573102,53011842,51253995,1254726,","consider this totally unsurprising bash exchange now look at the result in other shells like dash ksh or zsh this is perhaps more correct but i doubt many people would expect it,in bash this is much faster than running nvm directly;for timings and zsh see here,another solution to the command line too long problem is to use a c-style for-loop within bash;this works in zsh as well though i bet zsh has some niftier way of using zsh i m just still new to zsh,it s very easy with zsh globs if you are using bash you just need to upgrade,the accepted answer works for me but with zsh shell terminal.integrated.shell.windows c msys64 usr bin zsh.exe things are much slower than with bash,how can you get similar highlightings to zsh s less than bash s less in ubuntu,"
"httpclient","okhttp","more slower overall,","40933697,","but the post method okhttp is more slower than httpclient s post always more slower than okhttp s get method,"
"express","koa","more low level overall,req.header authorization overall,middleware much simpler overall, is more widely supported more overall,better overall,faster overall,","45632452,41459229,45490890,54995658,45490890,42270893,","koa doesn t provide this kind of thing out of the box - it s designed to be a little more low level than express,i should also say that i have the token at ctx.request.get authorization koa based i think it s something like req.header authorization with express in all routes,koa middleware is much simpler and less hacky than express middleware due to the way middleware flows in a stack-like manner,express the older koa is more widely supported more cloud vendors have various associated services hosted databases that differ significantly,see this blog when you have 10000 request koa performs better than express,it shows that koa is faster then other framework but as this question is about express and restify express is faster than restify,"
"noexcept","throw"," exceptions instead overall,more overall, is a specifier overall,keyword more overall,","45688132,34877452,56459215,30456801,","noexcept allows for more efficient code generation in that it does not have to perform rtti on throw exceptions instead if an exception is throw from a call-frame underneath a noexcept-declared function std terminate is called short-circuiting the crazy std unexpected machinery specified by the 98 standard,unwound before program execution is terminated. he said code using noexcept is more optimized than code using throw,a bit off-topic but worth mentioning that if you d like to say that function template fun is noexcept only if t 1 doesn t throw then should be the first noexcept is a specifier the second one is an operator,i ve heard that noexcept keyword is more like it should never throw an exception rather than it doesn t,"
"bundler","rvm","more in familiar process gem, is a better overall, not overall, is really more overall,better way overall, which i m more in familiar process gem, have a look overall, is more overall, is older overall,","15014912,7771286,12571621,13482483,9771172,9181409,4977259,5826510,57590247,","rvm supports more then just .rvmrc one of this files is gemfile with either the bundler directive to specify ruby,in contrast with rvm rbenv does not manage gemsets;bundler is a better way to manage application dependencies,you could uninstall the newer version s of bundler though this is a crappy way to deal with the issue or since you re using rvm just create a gemset for rails 3.0.1;note this requires that bundler not be in your global gemset for the ruby version you re trying to use,rvm is really more for personal use;for a system service i would just install the version of ruby you want someplace that doesn t conflict with the system version like usr local and use bundler in deployment mode to get the gemset,bundler is a better way to manage application dependencies. there is a plugin rbenv-gemset for getting the same results as rvm s gemsets but sam clearly favors using bundler instead,in the case of rvm which i m more familiar with the process would be to capture a list of currently installed gems such as the output of gem list and transform that into a gemfile that bundler can use to reinstall bundler,i hope you ve got rvm if not i strongly sugget you to install it this will allow you not only to use different versions of ruby but also to set different gemsets therefor one each project you ll find it useful;once in rails 3 you can use bundler have a look at this episode of railscasts by the way this is a very good site but you may know it already,rvm is more like a containment unit;while bundler is like a manifest dependency manager of what the application will require or use in it s lifecycle among other things,interestingly when i log into the server i see that rvm has installed ruby 2.4.1 after taking a closer look at mina s output i see warning the running version of bundler is older than the version that created the lockfile,"
"max","rank","larger overall,faster overall,less overall, and then overall,usually slower overall,lower than no  overall,greater overall,","30495806,886136,9870286,55072621,18858163,24588036,21983695,","every time a rank finds a larger number than that stored have it send its new max to the root rank,while i think gbn s answer is probably sufficient i m wondering whether use of an over clause to establish a max date per id attribute with which to reduce the select in a where clause wouldn t be faster than a rank,if your rank is less than 4 it takes 0 instead it s the max part,intermediary dataframe the intermediary dataframe is col1 col2 col3 group feature g1 f1 12 9.500000 1000 f2 0 8.000000 200 g2 f1 2 7.000000 330 f2 3 7.000000 331 f3 1 7.000000 100 g3 f1 7 7.666667 101 now for the final dataframe which finally gives group feature col1 col2 col3 0 g1 f1 1.0 1.0 1.0 1 g1 f2 2.0 2.0 2.0 2 g2 f1 2.0 1.0 2.0 3 g2 f2 1.0 1.0 1.0 4 g2 f3 3.0 1.0 3.0 5 g3 f1 1.0 1.0 1.0 for the second part of the question i would just change the indexing of the intermediary dataframe and compute rank after grouping on feature which gives feature group col1 col2 col3 0 f1 g1 1.0 1.0 1.0 1 f1 g2 3.0 3.0 2.0 2 f1 g3 2.0 2.0 3.0 3 f2 g1 2.0 1.0 2.0 4 f2 g2 1.0 2.0 1.0 5 f3 g2 1.0 1.0 1.0;i would use groupby on to produce an intermediary dataframe containing the sum avg and max columns not the rank and then again groupby on group only to produce the rank,you might replace the max subquery with a rank max is usually slower only when cus_id is the pi rank might be worse,if no rank is lower than no rank number there are no gaps in the table the query returns the max number + 1,so i want to generate rank for second set of values but starting with value greater than max from first set,"
"declarative","imperative","better overall, machine is much cheaper overall, code is better then overall,sql usually simpler in sql faster pl,bigger difference than  overall,more generally between  overall, programming has followed more overall,effects better overall,way cleaner in sql faster pl, seems more overall,simpler in sql faster pl,","14346528,8909591,52187601,30022972,12196486,12447646,2614454,34623840,21401916,53054537,19199806,","angular is built around the belief that declarative code is better than imperative when it comes to building uis and wiring software components together.,while it is possible to explore non-imperative computer concepts it doesn t seem to be worth the effort at the current time - building a declarative machine as an interpreter on top of an imperative machine is much cheaper and faster than a true declarative machine,typically declarative code is better then imperative code,declarative sql is usually simpler and faster than imperative pl sql so it s usually best to do most of the work in sql and just glue it together with pl sql,traditionally there was a huge difference in speed imperative has fewer overheads because it s more directly like the computer works but some more modern compilers of declarative code seem to be in the top few of the speed tables a lot of the time - compiled verses interpreted makes a much bigger difference than imperative vs declarative which is why python etc are often slower even though the time - compiled verses interpreted re imperative,there is a big discussing between object-oriented and procedural approaches and more generally between declarative and imperative ones and each approach has each approach upsides and downsides,and historically work on functional programming the main branch of declarative programming has followed more from backus s turing lecture on liberating programming from the von neumann bottleneck,using declarative effects is better than thunks for testability but the saga pattern can be implemented on top of imperative or declarative code,i know that declarative way is cleaner but sometimes imperative programming makes your business safer,imperative seems more clear and obvious but a declarative approach scales very nicely for larger applications,declarative programming is not always simpler than imperative programming,"
"declarative","imperative","code easier overall, is usually better overall, code is more overall,","382518,13942,36252935,","declarative code is easier to make bug-free than imperative code,imperative;declarative is usually better for anything where you really don t need the fine-grained control over how something is done,however building a value an array in this case using a for in does force us to use mutable values and imperative code;we should avoid both as much as possible because immutable values are safer and declarative code is more clear and potentially faster,"
"dill","pickle","better in serializer socket modules, which is a better overall,better in serializer socket modules,better job in serializer socket modules,slightly more overall,better serialization in serializer socket modules, does not overall, it s less overall, code easily as well in serializer socket modules, is more overall,","28639107,26164781,41269538,43642444,44487109,31962272,28622163,50825883,32838740,32838740,","i searched a bit and found that dill can perform better than pickle with classes but i am having problems to implement it,then you d pickle a mock object which would then pickle the class source code and then you d be able to pass that to the django cache;i m the author of dill which is a better serializer and also the author of klepto which is a caching package and this is exactly what i do to store any object in a sql table on disk or in an in-memory cache,dill has a better serializer that can pickle socket objects on any os and thus sending the socket object with multiprocess works in either case,modules such as dill are doing a better job than pickle,you may also want to have a look at dill which covers slightly more cases than pickle,i think you can do exactly what you want with dill which provides better serialization than pickle,this means pickle does not work on objects defined in __main__ and it also doesn t work on many dynamically modified objects;dill registers __main__ as a module so it has a valid namespace,if you re using multiprocess which is a forked version of multiprocessing that pre-substitutes dill for pickle it s less obvious how to patch that,cloudpickle might be a good idea for your problem as well cloudpickle has better support in pickling objects than pickle not per see better than dill and you can pickle code easily as well,you use pickle might fail dill is more stable,"
"jscript","vbscript","more overall,much more readable overall,simpler overall,nicer overall, does not overall, is a much more overall, doesn overall,infinitely more overall, statement. says msdn; overall,experienced with  overall,","22345923,35567873,23531981,1910089,787394,8772814,11344149,495528,9430846,2874749,","on contrary jscript is more c-like do not require explicit enabling of script running accepts relative paths case sensitive and loosely typed both are imho advantages for scripting language compared to vbscript,...i suggest doing all of the scripting from within the vbscript file and avoiding the use of .cmd .bat files completely if you can as vbscript is much more readable and powerful though i prefer using the jscript language instead but that s just me,the inclusion of jscript code into a batch file is simpler than vbscript and the translation of a small code segment from vbs to jscript is not problematic,asp with jscript is 100 times better cleaner simpler nicer than vbscript and makes my job a joy rather than a vbscript head ache,vbscript has that ability as a global function but other windows script host languages might not;for instance jscript does not have a global createobject function i believe it does however have a syntax so you do not need to use wscript.createobject in jscript either,vbscript is a much more natural partner for adodb that jscript,asp vbscript doesn t understand the concept of timezones just what the current server time is;jscript asp does and luckily enough you can mix and match your scripting languages in asp,when writing code in jscript as i am wont to do as i have never been a fan of asp.net and jscript is infinitely more elegant than vbscript you can call upon the arguments collection,for scripting a dynamic property can be any legal jscript or microsoft visual basic scripting edition vbscript statement. says msdn;jscript is not javascript,i am much more experienced with vbscript than jscript but i will give vbscript than jscript a shot since not many takers on this question,"
"verilog","vhdl"," needs more code overall,shorter fact equivalent in experience designers easier,more popular overall, is easier overall,better support than  in experience designers easier,more feature-rich overall,more widely-used overall,less common overall,prefer it over  overall,","53597155,15961408,177565,179813,49678341,178555,4987325,764074,51742901,","vhdl needs more code to describe a program but it often catches errors missed by verilog emphasizes unambiguous semantics and is portable just to name a few,my experience is that designers can use whichever they prefer usually and most agree that verilog is easier to use and the code is shorter fact than equivalent vhdl,vhdl is more popular in europe and verilog is dominating in the us,for hdl choice verilog is to c as vhdl is to ada;so verilog is easier to get started with but you can make mistakes more easily,as far as tooling my understanding is that verilog seems to have equivalent or better support than vhdl,yes vhdl was once much more feature-rich than verilog but later revisions of the language verilog 2001 verilog 2005 systemverilog etc.. have cherry-picked most of the interesting features and there is far more robust toolchain support for verilog and its variant these days in addition to it being the dominant language in use in the us in my experience vhdl is only used here when dealing with extreme legacy blocks and in academic contexts partially due to the tools support mentioned previously,by these numbers and only these numbers vhdl seems to be more widely-used than verilog,pacoblaze was written in verilog which like adam said less common than vhdl,with verilog being closer to c syntax it might be easier to start there and i also prefer it over vhdl but let s not get into religious discussions i know that emacs and sigasi editors provide auto-format options for rtl languages but their format options are strict and i don t like the outcome hence i intend to make the formatting style configurable i ran into this work it might be useful to build on it if possible,"
"binary","hex","better than say  overall,much also with  in size byte heads,more readable overall,more than 5  in characters conversion input,even simpler in characters conversion input,readable than  in readable useful dump,easier overall, values print format value1 in value format string, not in form decimal too, is short more overall, representation not in value format string,","41315665,50162673,36923657,26458977,12081779,1712946,15667899,49447366,29066121,24716771,14038388,","base64 is usually used in instances to represent arbitrary binary data in a text format base64 has a 33.3 overhead but that s better than say hex notation which has a 50 overhead,just have in mind that the best result is with negative hex and binary integer interpretations simple numbers not so much also with hex you can set up the byte size,hex encoding is far more readable than binary that s why sublime uses it,if the hex input is more than 5 hex characters you will overwrite the memory allocated for binary_line,the conversion from hex to binary is even simpler since you can simply expand each hex digit into the corresponding binary for example 0xa4 - 1010 0100,hex is somewhat more readable than binary if i happen to be loading a data dump in a text editor etc,- i usually find debugging memory in hex x command is easier than binary so i will not use my solution,this values can be transformed in binary values print format value1 016b # 1000000100100011 print format value2 016b # 0010010010001111 each one of these bit can be indexed from 0 to 15 from right to left and then we can split them as described in the documentation value1 d3-d0 0011 value1 d7-d4 0010 value1 d8 1 value1 d15 1 for each subset of bit the documentation provides us a number in hex value and each number in hex value can be converted in binary d3-d0 00h bin 0000 normal 01h bin 0001 overvolt 02h bin 0010 undervolt 03h bin 0011 low volt disconnect 04h bin 0100 overtemp and same for other sets.,call integer.parseint str 16 in a try-catch if you get a numberformatexception it wasn t a valid hex encoded int;computers perform math with binary not decimal and not hex,hex is short more readable and you have less of a chance of leaving a mistake in your code;if you want to be really explicit i d still recommend using the binary literal over the parseint method as it is safer,if you consider look at the coa8010b c0 in hex is equal to 192 in decimal a8 168 01 1 and 0b 11;in short this is a binary representation not a string representation of 192.168.1.11,"
"binary","hex","much more readable in readable useful dump,easier in size byte heads,representation better way in representation head first,easier than reading  overall, which is a larger in form decimal too, digit controls the value overall, while not overall, may not overall,way more space overall, data is usually in single octal bits, code makes more sense overall,","21578250,13960625,484066,23532067,8124202,49435268,37084871,45245832,38749931,48324651,48366941,","and because hex is much more readable and useful than binary - it s often used and shown,hex is easier for most people to convert to and from binary in their heads since binary numbers are usually expressed in blocks of eight because that s the size of a byte and eight is exactly two hex digits but hex notation would have been clunky and misleading in dennis time implying the ability to address 16 bits,it seems the from a readability and usability standpoint the hex representation is a better way of defining binary numbers,but when anyone human looks at machine code anyone human look at machine code in hex using a hex editor which is much easier than reading binary,first of all 0x00001111 is in hex which is a larger number than 255 - and byte overflows;look here how to represent binary numbers or just use,if the user can specify several registers in one command invocation then binary is more reasonable though not necessarily wholly reasonable but do consider the merits of hex over binary one hex digit controls the value of four bits,the reason hex is seen quite a bit in code is simply that it allows the common 8-bit width of bytes to be represented with exactly and just 2 hex digits which is reasonably concise and not too hard for humans to get used to;it s easy enough to mentally convert back to binary while not losing track of which digits you re looking at the way you can with a 32-bit or 64-bit value in binary,the hex file has a checksum on each line;the binary may not have any checks,unfortunately using hex consumes way more space and takes significantly longer i m dealing with 500gb of data and around 1 2 million records so i would really like to get the straight binary method to work,it is usually better than octal base 8 because each hex digit is 4 bits and binary data is usually arranged in clumps of powers of 2 due to how our computer bit-addressing works,since blob stores binary data and not all bytes are displayable the hex code makes more sense to display the data as hex encoded to human readers,"
"binary","hex","much more convenient in single octal bits, is just overall,just less verbose overall, is 1 too; in form decimal too,more overall,conversion usually faster in characters conversion input,numbers larger overall,familiar with  overall,string significantly longer overall,more in representation head first, number not in value format string,","45780733,35043172,484849,35256897,32646938,34334729,12679024,55900424,2558762,12081888,52979274,","as others have pointed out hex is much more convenient than binary anyway - you just need to remember how each of the hex digit 0-f looks in binary and replace groups of 4 bits with a single hex digit,the bit shift operators and move each binary digit one place over depending on the direction of the arrows and so on;the hex is just a nicer format to visualize the bits without having to write out every single bit,hex is just less verbose and can express anything a binary number can,00000001 is really a number and you can writer it on any other form but coincidently for decimal is 1 for hex is 1 too;binary is not olny used as a number system to represent a number but also can represent some objects and used as char,format_int in binary case it loops 4 times then 4 times more than hex and dec cases,this is still accepted by calls like inet_addr and has several advantages all fields are fixed width there are only 8 characters to update and the binary to hex conversion is usually faster than binary to decimal,those hex values seem a bit odd they re powers of two in decimal but in any case 0x128 the 0x is a standard prefix for hex numbers is the larger of the numbers in magnitude and its binary representation is 100101000,as in kingsley s answer if you re not familiar with hex notation you could use the binary format,a hex string is significantly longer than the corresponding binary string,converting between decimal and hex is more involving and at least to me it s been easier if i have to do it in my head to first convert the decimal into binary representation and then the binary number into hex,hex is a text serialization format for binary integers;the value in is a 32-bit binary number not hex or decimal,"
"binary","hex","alot shorter overall, is what you really overall, string not in value format string,friendlier most in editors files editing, digits; is used much in single octal bits, file is a text in editors files editing,side greater overall, slice isn overall,","16513841,21948667,45604363,21591637,4882583,46571166,37554151,42301343,","one important reason is because hex is alot shorter and easier to read than binary is for humans,it does produce hexadecimal rather than binary but it s very likely that hex is what you really want,decode 63697a61f161 hex produces the bytea value x63697a61f161 if bytea_encoding is hex or ciza 361a if bytea_encoding is escape;either way it s a representation of a binary string not text,i ve been using images to store data since editing binary data through paint.net is much friendlier than most hex editors,each single hex character describes four binary digits exactly while an octal character describes 3 binary digits;hex is used much more often than octal,do not open hex files in a hex editor because hex editors are designed for editing binary files not text files;a hex file is a text file that you can open in notepad emacs or any standard plain text editor,if the right side the binary side is greater than or equal to the hex side then you print true if not then you print false,decoding a string of hex characters into a binary slice isn t currently part of the crystal standard library so i wrote a decoding function myself;this function takes a string containing hexadecimal characters then decodes it into a slice uint8 or returns nil if the hex is invalid,"
"modulo","subtraction","typically less overall,slower overall,more expensive overall,higher overall,slower overall,faster division-based overall,","27467857,414763,22368203,31666977,43115704,24115313,","the conditional test and subtraction is typically less expensive than a modulo especially if the sum does not frequently exceed mod,as you can see modulo is about an order of magnitude slower than subtraction,as i said this may increase the speed especially in an environment where modulo is more expensive than simple subtraction but you would want to actually benchmark it to be certain,with regard to implementation it also takes advantage of a bit of a non-obvious property of r precedence rules actually this is true of other languages as well such as c c++ and java namely that unary negative is higher than modulo which is higher than binary subtraction thus the calculation for is equivalent to,the reason is that the modulo is slower than subtraction,that is essentially the one case in which repeated subtraction 0 or 1 times a special case of repeated subtraction can be and commonly is but not necessarily faster than division-based modulo,"
"dpkt","scapy","better overall,slower overall,","37963958,41680711,","scapy is a better performer than dpkt,or is it just that scapy is slower than dpkt,"
"flac","wav","better in comment better ogg,general overall,smaller than the  overall,better in comment better ogg,smaller overall,","45383521,51564006,49595923,45417810,8301955,","i also know that it can deal with flac better than wav and ogg,it seems wrong to recommend using flac or wav and having a 180 minutes of length in audio limit if the server can t even handle my hour long audio file that has been encoded to amr_wb,the problem is that the flac output bit rate is too smaller than the wav file,regearding your comment i also know that it can deal with flac better than wav and ogg that is not really the case,flac is smaller than wav,"
"except","next","greater overall, i still overall,solution faster overall, it s more overall,more overall,example more overall, stopiteration logging.info no more overall,greater max1 overall, using goto overall,accordion imho better overall,","10924146,12076317,19105903,52617765,4559066,22646025,56907567,23263562,13895699,5690589,","now i would like to take the ceiling of this number except in cases where the amount it is greater than the next lower integer is smaller than some epsilon,the next docpara isn t working like i would expect;now the code works except i still don t go to the next paragraph,the next solution is faster than using in and except clauses,this is a similar situation to the keras argument which saves the hidden states for the next batch except it s more complicated so i can t just use that as-is,in both cases you may also have to shift one or two elements off the start end of some blocks to the end start of the previous next block again at most o n 1 2 times to maintain an invariant that no two blocks differ in size by more than 1 except for the last which is used to take up slack,the next example does more or less the same except from an oop perspective,this is my current approach while true try next it except stopiteration logging.info no more results to process. return except socket.timeout logging.warning encountered timeout. if i retry the same item enough times by rerunning the script manually the timeout will resolve itself so i d like to be able to retry the problematic item programmatically instead of just repeating the search multiple times,for such numbers as 54321 the else statement will be executed never because any next digit is greater than max1 except the first two digits,all in all consider the flow of your program in order to determine what happens next not what to skip;you cannot do that except using goto which is taboo and should not do that,for me they serve pretty same purpose to hide complexity except accordion is imho better for continuous-reading it s easier to quickly go to next section while tabs seems to be more random-access,"
"installation","upgrade"," that uninstalls the older overall, has an older overall,much better overall, is older overall, does not overall, patching etc... overall, checked the release overall, is older overall, do not overall,slower overall,smaller overall,","25074930,54783793,765929,43528005,39662248,52710390,57612062,56692723,48651809,12791808,12159596,","yes restoring the old application versioning via rollback upon an installation failure is actually a built-in feature of windows installer but you need to configure things correctly to get it to work as you require;windows installer rollback will work as you request if you use 1 a minor upgrade or 2 a properly sequenced major upgrade that uninstalls the older versions after successfully updating all files,your visual studio 2017 installation has an older versioning of the git credential manager for windows;upgrade to the latest versioning and configure this specific installation in your global git config to ensure visual studio is aware of the latest gcm that s available to it,as for production side of things upgrading your database in transaction msi-style installation is much better than attempting to upgrade at each app startup since you can potentially end up with desynchronized database-application versioning,the python-virtualenv versioning from your distribution or installation is older than what s required by certbot-auto;install or upgrade to a compatible virtualenv versioning for python 3 via,no without the user s permission there is no way an app can be installed to an android device except if it is connected for development with a computer.the user s privacy permission policy as implemented by google for app installation does not allow any app or device to install a fresh app w o user s permission;however frequent app upgrade can be done w o user s permission,i find msi s per-user installation constructs not ideal;it relates to poor serviceability upgrade patching etc... and a number of other details,you may also need to run the database upgrade several times. also make sure you move your old 1.14 wiki installation to an extra folder and upload a fresh mediawiki installation do not just overwrite your existing files in the 1.14 one with the new mediawiki versioning;in the end i went the long route and upgrade versioning by versioning first on a local xampp dump then on the server after i identified all the issues for each versioning upgrade checked the release notes for each versioning and made the required changes to my custom skins replaced deprecated extensions etc,the col function was added in solr 7 so if your solr installation is older than that the function won t be available;upgrade to a more recent version,late uninstall major upgrade installation do not uninstall and reinstall files that exist in the same location in the old and the new setups - and this works even when the files in question are not set permanent and never overwrite allowing proper uninstall of the files as well - without any added custom logic;this kind of upgrade basically install as a patch - leaving files that are unchanged between releases untouched and then upgrading other files according to the file versioning rules described above,does an eclipse installation perform slower after the upgrade,this can enable the user to download an upgrade patch that is much smaller than the installation package for the entire product,"
"malloc","realloc","way smarter overall,often more overall, is much more overall,less overall,more overall, which is way faster overall, is defined to return null overall, call asks for less in pointer call knows, size then overall,worse overall,faster in faster memcpy free,","27200845,21358484,14349977,26126313,39043954,15901802,34145209,56830622,53379772,7188551,3108761,","i mean i think realloc is way smarter so why do we even need malloc,malloc often gives you more memory that you ask and stores the actual value in a special location that realloc can access at a later time,for that you have realloc;with malloc you would end up with a fixed size for your table just like if you had only declared it has static but with later initialization well i m a bit agains this sentence because malloc is much more than that but for this purpose this purpose is safe to say this purpose,in the case of overflow a free malloc pair costs less than realloc because of its internal hidden memcpy,but if it works with large datasets the users will notice that using the malloc -only program slows down other programs much more than the realloc -using program with the same data,if you work with data that doesn t need construction destruction and requires reallocations a large array of ints then i believe malloc free is a good choice as malloc gives you realloc which is way faster than new-memcpy-delete it is on my linux box but i guess this may be platform dependent,as a result if one wants code to be compatible with aggressive compilers one must refrain from having any pointers point within an object which is going to be realloc ed unless one can guarantee that such pointers will never even be examined after a successful realloc takes place;as far as the c standard is concerned the behavior of realloc in all cases where it succeeds is equivalent to copying the memory block to some arbitrary location calling free upon it calling malloc to create a new block of the requested size and returning a pointer to the new block in cases where the malloc wouldn t succeed realloc is defined to return null without disturbing the original block,this is evidently what happened in your example since your realloc call asks for less memory than was allocated by calloc and we can see that it didn t do that here since the address of ptr when free is called is the same as the block allocated by calloc;if the existing allocation is much bigger than the request realloc may choose to copy the existing memory block into a smaller allocation possibly acquired with malloc,if no storage is allocated or deallocated between the malloc and realloc the size of the realloc is known when the malloc is performed and the realloc size is larger than the malloc size then it may make sense to consolidate the malloc and realloc operations into a single larger allocation,realloc is worse than malloc in that you will need to have the old and new pointers valid during the realloc,a realloc can occur significantly faster than a malloc memcpy and free,"
"malloc","realloc","faster 2nd overall, increases the size overall, keeps its block overall, are just overall,more memory in faster memcpy free, not overall, is a lot more overall, could use that space overall, there is a memory in pointer call knows,","41428974,10864646,3186640,52710960,4190023,14164507,24593643,26094437,13049628,","the justification of realloc is that it s faster than 2nd malloc manual copy free,also i should note that in same cases you don t have to do a copy even when realloc increases the size for example if the next block in the heap is free;malloc does not initialize memory to zero,at the moment it s possible that there is some space after stringarray into which you are storing the wordlength-arraylength extra pointers when you calloc them and realloc doesn t move stringarray;it s quite probable that 0xb49010 is one of the pointers you calloc d and you re overwritten the memory where malloc keeps its block size,however functions like realloc do not have this property as they can return a pointer to storage containing pointers;as you pointed out yourself nonnull and malloc are just macros,that s why realloc can temporarily require more memory than a malloc free pair,allocate the block using malloc not new and then use realloc;realloc knows how much free space is available after the block for expansion,of course the real realloc is a lot more complex because the real realloc checks the current block to see if the current block can be expanded before the current block allocates new data and probably doesn t call regular malloc but the functionality is roughly this,so your call to realloc may not have resulted in a physical page being returned to the system so it s still mapped to you program and can be used;however a following call to malloc could use that space or it could be reclaimed by the system at any time,the runtime is complaining about an invalid pointer which indicates that the pointer you are passing to realloc is not a pointer that was created with a call to malloc or to calloc;it knows because whenever you do a malloc there is a memory management header that is part of the data area and the pointer you are given is a pointer to allocated memory after the header,"
"freemarker","velocity","much better native overall, is better in apache complex richer,more overall, has more overall, works well and is very in simpler powerful velocity,more knoledge overall,more powerful in flexible experience velocity,simpler in simpler powerful velocity, is also a lot more in flexible experience velocity,worse overall, has a more in apache complex richer,","4146126,48168384,3703341,4411575,1176723,30106906,463420,463420,1459435,10524352,1004380,","freemarker provides much better native whitespace handling recent velocity releases provide more interesting content controls #define #evaluate # literal block #,you can use template engines for these kind of requirements there are multiple template engines are available like apache velocity apache freemarker thymeleaf pebble but i feel apache freemarker is better,freemarker is more advanced but a little complex as compared to apache velocity,freemarker has more i18n support built into the core engine;however velocity with the addition of the velocitytools project offers more i18n functionality i believe,freemarker works well and is very powerful;velocity has a simpler syntax is somewhat less powerful and is a lot more forgiving wrt,i m trying to create a template in velocity since i have more knoledge than freemarker,freemarker is more powerful than velocity,velocity is simpler than freemarker,freemarker is also a lot more flexible in my experience;velocity isn t really under active development any more,velocity is worse than freemarker and stringtemplate has got its own issues,freemarker has a more complex but correspondingly richer markup language than apache velocity,"
"nltk","scikit-learn","nicer output overall,much better overall,directly more overall,less memory overall,","35614800,13041143,19868470,14131456,","the nltk library includes a confusion matrix that is simple to use and produces a nicer output than scikit-learn,what you re looking for is linear regression and scikit-learn is much better than nltk for this see,or scikit-learn directly .for more details nltk 3.0 documentation,if you are worried about memory then do look into scikit-learn since equivalent models can use significantly less memory than nltk,"
"inherited","subclassing","more accurately in class ireadwrite implements, do complete only overall,trickier in class ireadwrite implements,less overall,general overall,favor composition over  in better custom resource,more overall, but instead overall, which has more overall,mapping more overall, usually means more complexity overall,","1331150,124591,12945925,44097475,55875517,34405918,39741858,12470268,16669714,15552495,10678637,","since ireadwrite implements ireadonly ireadwrite is said to be a subclassing of ireadonly although subclassing is more accurately used to describe a class which inherited a base class rather then implements an interface--for the sake of simplicity they are very nearly the same concept,when you proceed this way there is not a problem with the fragile class or at least the designer is conscious or the threat and the subclassing do complete only those parts the designer allow them;inherited is more to specialize the classes in the same fashion a truck is an specialized version of car and mostertruck an specialized version of truck,with subclassing is trickier because any private members of a class are not inherited by the subclassing but protected and public are,with inherited this is less likely to happen due the contractual nature of subclassing abstract classes,composability having frozendict have a _dict properties is much easier to reason about than inherited subclassing dict in many cases,if you want to work your way from the top down especially if the subclassing selection depends on the choices made when constructing the superclass then the superclass s probably much better to favor composition over inherited,if a subclassing needs more information than the standard parameters which came up for us you have the option of a second parameters class type that you use as a second parameter but then you have two types of constructors in the tree or using inherited in the parameters class hierarchy,while the methods is a debated recommendation to favor composition over inherited my view is to ensure that an abstract-class devoid of abstract methods is not simply a convenient container for methods convenient to subclassing but instead that each subclassing could stand in for each subclassing superclass,see if you want to use inherited;have a base class with fields that you want to share with everyone and a subclassing which has more restricted data,but native hibernate support regarding inherited mapping is more powerful than standard jpa and single table per class hierarchy or table per subclassing mapping strategies are more suitable for polymorphic queries and associations than table per concrete class strategy,my general rule of thumb before using inherited consider if composition makes more sense;reason subclassing usually means more complexity and connectedness harder to change maintain and scale without making mistakes,"
"inherited","subclassing","more than one  overall, is more in superclass class lines,prefer composition over  in new functionality hashmap, is a superclass in superclass class lines, is also overall,more useful in new functionality hashmap, becomes more in hierarchy generalization specialization, without affecting the precedence overall,more appropriately overall,more in superclass class lines,general overall,","52843769,3360821,40780913,50691450,12082945,154057,17084059,33222237,6020740,5527347,54816340,","in your case using inherited is tight coupling and also breaks in encapsulation if you have more than one subclassing also same for abstract class,perhaps a 1000 lines class should be decomposed in other classes or subclassing if in your case inherited is more valuable than composition so every class has a well defined responsibility in the system,also i recommend you use hashmap instead of subclassing hashmap prefer composition over inherited,this is also a situation where aggregation one class has other classes makes more sense than inherited a subclassing is a superclass,c++ inherited is also slightly more complicated than java inherited because multiple inherited is supported and as such virtual inherited is also an option,inherited is more useful when a new subclassing wants to change the way a method works if you just need to change the data the class uses to work probably an approach like this would do the trick,your subclassing becomes more specialized and less generalized as you move down the inherited hierarchy,for example all new-style classes inherited from object so any case of multiple inherited provides more than one path to reach object;to keep the base classes from being accessed more than once the dynamic algorithm linearizes the search order in a way that preserves the left-to-right ordering specified in each class that calls each parent only once and that is monotonic meaning that a class can be subclassing without affecting the precedence order of a class parents,i generally use interfaces are too enforce a common behavior that a group of classes share whereas subclassing is more appropriately used in cases where you can achieve serious code re-use through inherited functions properties,similarly as the subclassing gains more methods it inherited the list of superclass in the order in which they were named that precede it,the abstract_class properties is more or less a way to subclassing models without invoking sti single table inherited,"
"inherited","subclassing"," is better overall, specializes the more in hierarchy generalization specialization, which allows for more overall, is harder overall, is actually more overall,prefer composition over  in new functionality hashmap,better in better custom resource,","1337871,2145814,229695,217675,15635368,3150597,20999393,","from a good database design point of view joined table inherited is better;joined table inherited enables you to have foreign keys to subclassing enforced by the database it s a lot simpler to have non-null constraints for subclassing fields,inherited thus implies a generalization;specialization hierarchy wherein a subclassing specializes the more general structure,ut a list of childthing1 s cannot handle a childthing2 or indeed any implementor of iparentthing other than childthing1 - in other words if the was allowed to cast as a the would have to be able to deal with all subclassing of iparentthing not just iparentthing and childthing1;note that java generics have a way to say that i want a list of anything that inherited from this in addition to i want a list of anything that this inherited which allows for more interesting and in my opinion elegant solutions to some problems,note that inherited is harder to maintain than composition;also i would rename tests to testcollection if you subclassing list or something like testunit if it would contain list of test classes,it is elegant and honors basic inherited contracts between superclass and subclassing;even in your original post the subclassing is actually more restrictive than the superclass so doing something like,we prefer composition over inherited because when we add particularly or change functionality by subclassing we couple that new functionality to the class - so anywhere we need the new functionality we need that class,depending on how your application is structured this means you may want to have your own base class implementing iresource which creates your custom not found resource for all of its subclassing or better make a wrapper since composition is better than inherited,"
"android-query","picasso","much more overall,better than  overall,","19800655,39345309,","if picasso doesn t satisfy you for any reason you can also try android-query which does much more than picasso it s not only for images but the same rules apply,android-query is better than picasso because while in offline mode no internet connectivity picasso tries to fetch record form server whereas android-query maintains its cache form where its fetched the images,"
"max","percentile","less overall,temperature higher overall,greater than 75  in column outlier 95th, which have a peak overall,greater 95th in column outlier 95th,","47277190,45871305,48087725,49839450,38463823,","if you notice the 99th percentile is less than 40mb but the max sized partition is still reported to be 3.44gb,the code included in the sapply call will test each day starting from day n. 7+1 8 against the 15-day sliding window as defined before and check if the max temperature is higher than the 90th percentile of that window test1,look into the max of the column to point out the outlier if its greater than 75 percentile of values,you 99 percentile is showing average time from the start of the test reducing lowest value 2.5 + 0.5 - remove lowest 0.5 and returns 2.5 2.5 + 0.5 + 0.5 - remove lowest 0.5 and returns 3 2 - 1.5 2.5 + 0.5 + 0.5 + 0.5 - remove lowest 0.5 and return 3.5 3 - 1 as you can see 99 percentile is correct and have higher value than max which have a peak at start,i can get the max values like this but i need values greater than 95th percentile for every 5 minutes,"
"jackson","org.json","better overall, code not overall,more powerful overall,better overall,","42106801,23489038,45917091,38062608,","in this case jackson performs better than nashorn which performs much better than org.json,your gist shows org.json code not jackson;jackson has a perfectly able .equals implementation for all jsonnode s,p.s. my recommendation would be to remove jsonobject conversion and instead return an object of actual class as internally spring uses jackson which is more powerful json framework then org.json,quick takeaway of the benchmark jackson performs 5 to 6 times better than org.json and more than twice better than gson,"
"cvs","svn","more popular overall,better in better areas work,less disk overall,bigger in rollback commit feature,better in better areas work,better in better areas work, not in rollback commit feature,more efficient overall,better in better areas work,easier than in  overall, it is easier overall,","782375,159971,10996036,11879403,1397047,160037,21496996,191616,161216,1580034,948285,","in addition another coworker said cvs was a lot more popular than svn,svn is better than cvs,wondering mainly if svn uses less disk space or more than the same in cvs,in this case i converted the cvs to svn and found the file size on the hard drive it was on my own laptop at the time but it was much smaller in svn than in cvs but there could have been some compression applied i dont know but everything on the web said svn would actually be bigger than cvs,if you use svn then it still okay because svn handles binary files much better than cvs,svn is better than cvs because it was designed to be - it s roughly the same thing with some simplications and new features,cvs supports rollback commit feature while svn not you need to commit a second time and overwrite your previous commit manual rollback;cvs only tracks modification file by file while svn tracks a whole commit as a new revision which means that it is easier to follow the history of your project,in fact svn does stores binaries a lot more efficient than cvs for more info see the svn-faq,svn was supposed to be better than cvs but in some areas that didn t work well,part of the point of svn was to make the use of branches and merging much easier than in cvs,there is only one good argument i ve heard in favor of cvs and that is in situations where you have a project that will have many different releases and branches in cvs it is easier to just push an arbitrary file into the branch or tag you need when you need it;svn is more formal about these things and makes it harder to just pick on one file and merge it as you need it,"
"cvs","svn"," made the downsides small enough overall, s not harder overall, is legacy system overall,twisted than   overall,better system in better areas work,more comparable format overall, doesn overall,much better in better areas work,better in better areas work, book includes a note overall,better in better areas work,","5056164,1124137,34442357,2104081,7871646,161202,102176,10859451,1589215,5677762,245323,","he cvs options for move were awful and bad and svn made the downsides small enough to warrant renaming files as needed;we managed to throw away most of we custom makefile recipes because our custom makefile recipes weren t needed any longer svn was that much better,cvs svn s not harder to use than csv but has some advantages renaming is possible,at first svn is not cvs;while cvs is legacy system with many problems svn is a modern centralized version-control system git fans may disagree but this is another topic,it s model it s much more twisted than svn cvs git,i can only guess at the reasons and i don t worry much about the occasional downvote but perhaps some readers think i m advocating cvs as a better system than svn or git,the first one can be solved by using svn +ssh which is the more comparable format as cvs uses its own protocol as well,in cvs this was called a tag;svn doesn t use a separate mechanism for tags it just creates a branch,svn is much better then cvs and git or mercurial are even better again,svn in much better than cvs and have lot of mature tools for every platform,subversion differently than cvs cannot change the content of the code coming from a commit;the svn book includes a note about this question,that being said i think svn is better in every other respect and you probably shouldn t start a new project with cvs,"
"cvs","svn","better enviroment overall, does not overall,more useful features overall, is much newer overall,","1101716,2615034,161216,51375418,","for all the reasons linked in the comment and elsewhere svn tends to be a much better enviroment than cvs or vss unless you have some weird requirements - so if you can successfully import your history and save the company money i wouldn t see why the bosses wouldn t go for it,cvs svn allows different files in the working directory to come from different revisions while dvcs es usually have one revision for the entire wc always even if a file is be reverted to an earlier version it will show as modified in status as it is different from the file in the checked out revision;svn cvs does not show this always.,the other distributed tools are a lot faster svn is slow as hell even cvs can be faster sometimes have much more useful features than svn are developing rapidly while seeing any new feature in svn takes years,cvs is the much older of the two and it has been the standard collaboration tool for a lot of people;svn is much newer and it introduces a lot of improvements to address the demands of most people,"
"conda","virtualenv"," worked well overall, is a bit overall, is not in environment compatible different,better than pip  in root directory better, but not in environment compatible different, - only overall, s corresponding site-packages folder overall,better in root directory better, is better practice in root directory better, is more overall,general overall,","55626722,54475042,37648206,52672463,51980100,56141684,51921008,26663944,57067900,53485990,48850848,","if you are using much more complex libs such as numpy pandas virtualenv is a better way you can add -archives to send the env to cluster;refer to the writing updated i tried above virtualenv in our online env and find some problems.in the cluster there is some errors like could not find platform independent libraries then i tried the conda to create python env the conda worked well,i ve tested various ways to manage my project dependencies in python so far installing everything global with pip saves spaces but sooner or later gets you in trouble pip virtualenv or virtualenv a bit of a pain to manage but ok for many cases pipenv pipfile a little bit easier than virtualenv virtualenv but slow and some vendor-lock virtual envs hide somewhere else than the actual project folder conda as package and environment manager great as long as the packages are all available in conda mixing pip conda is a bit hacky poetry - i haven t tried this one .,i finally resorted to using conda to set up an environment rather than virtualenv;apparently virtualenv is not compatible with anaconda,even though i heard that the conda packagement system is better than pip virtualenv,and they sometimes need different python versions this can be done in conda but not virtualenv;virtualenv environment is usually for python developer,also conda can has larger control over the environment and can for example have a different version of python installed inside of it virtualenv - only the python available in the system,supposed i were to call pip freeze or pip list whether i m in a virtualenv or not wouldn t matter;it returns exactly the same list of packages from conda s corresponding site-packages folder,if you are using anaconda conda is a better alternative to virtualenv as it manages conda packages which are not limited to just python packages,i ve been using pip to install packages directly to my root directory but i read recently that using a virtualenv like conda or virtualenv is better practice,you can use some commands like this the caveat is that you will need to use this conda installation whenever you need to use the aws cli but its easier than messing with system-wide installations or pip configs imo and conda is more robust than a python virtualenv,if you do not want use conda instead virtualenv select virtualenv but virtualenv from miniconda as far as miniconda is more tested and work out of the box without such errors,"
"conda","virtualenv","better in root directory better,","20567658,","an additional bonus i find that conda works better as a package manager than virtualenv,"
"ibm","websphere","easier overall,technote more overall, bundled java; jvm overall,newer libraries overall,older overall,","3825114,40174142,20978944,2183918,39102887,","ibm makes it easier for those who can t afford websphere application developer or rational application developer which are both eclipse flavour to use eclipse,this ibm technote has more details on the signals part what is a signal and why does this matter for websphere application server,ibm websphere bundled java;ibm jvm doesn t like indexed png files,other solution is to upgrade websphere with newer libraries as per ibm websphere upgrade service instructions,websphere development studio v7 wdsc is older than ibm i 6.1 your build date is 2007 the article you re referencing came out in 2008,"
"mp3","wav","lower overall,larger than your  in smaller files counterparts,smaller files than its  in smaller files counterparts,general overall,much less memory overall,file much easier in base64-encoded data little,general in base64-encoded data little,smaller in smaller files counterparts, files takes a lot more overall,better in base64-encoded data little,much smaller in smaller files counterparts,","41771353,57054389,54209971,57173102,40874574,1427191,35519940,43825846,21193738,1628241,7596704,","question iâ m seeing some compromise in mp3 quality my converted mp3 is lower file size when compared to wav but my audio quality is little poor than wav wonder if i can increase the quality of the mp3 file,as mp3 compress the audio your wav file will always be larger than your mp3 file,midi files offer much smaller files than its mp3 or wav counterparts at the expense of speech and sound variation,i want to have an image when i click it audio will be played mp3 wav,for using a fileformat i thought mp3 uses much less memory than wav because all the formats are based on wav but just compressed,either way applying gain and or attenuation to time-domain sample data as in a wav file is much easier than trying to apply these effects to frequency-domain data as in an mp3 file,it ll only play base64-encoded wav data though;if you want to play an mp3 that gets a little trickier,ok i am new to audio with unity but despite reading all the unity posts regarding audio adding 2 short .wav clips i heard wav was smaller than mp3 to my app has added over 200mb,i don t know why the player exception was caught with the mp3 temporary file but nothing goes wrong with wav;this is not really the answer i sought since the wav files takes a lot more space but at least it works,if you ask if a uncompressed wav which contains pcm data is better than mp3 then yes but the question sometimes is how much this better really matters to the human ear and how much postprocessing you have to perform on that data,i understand that mp3 is much smaller but it has worst audio quality when is compared to wav files,"
"mp3","wav","smaller in smaller files counterparts, you more overall,larger in smaller files counterparts, using something overall,smaller in smaller files counterparts,smaller in smaller files counterparts,","44688938,45580922,44531357,15021662,8063108,23678277,","you should definitely pick mp3 because they are about 10x smaller than wav files of the same duration,hen you take a cd and convert a cd to wav you more or less have a copy of the original;when you convert to mp3 a cd uses fewer reference points and detail is lost,wav files are about 3 times larger than mp3 files so it would need 1.8gb additional space to do the conversion,yes you have saved a wav file not an mp3 file;either convert to mp3 using something like lame.exe or just use the wavefilereader instead of the mp3filereader,now i assume you are worried your techno might not read a compressed mp3 which should be smaller than a wav from my memories,if you are still having problems jdk 8 has the ability to play mp3 files which are significantly smaller than wav you may want to try this,"
"qhash","qmap"," which is more overall,faster in values lookups faster,faster in values lookups faster,","22164607,7123723,8907753,","also with the method of using qpair from the other answer this has the particular advantage that you can access elements by rows easily if it matters for your use case you could use different container for different dimensions like if you need one dimension to be sorted use qmap which is sorted by key for that and qhash which is more efficient for the dimension which doesn t need sorting,qhash is faster but qmap values are sorted by key if you iterate through them,qhash provides faster lookups than qmap,"
"beautifulsoup","pyquery","more intuitive overall,significantly faster overall,","2752712,21335822,","pyquery uses the css selector syntax familiar from jquery which i find more intuitive than beautifulsoup s,also pyquery is significantly faster than beautifulsoup in many cases for processing results,"
"division","subtraction","higher in precedence higher operator,operation tighter in slower reduction n, that cannot overall,higher precedence in precedence higher operator,higher precedence in precedence higher operator,stronger slower more in slower reduction n, is more overall,higher preference in precedence higher operator,higher precedence in precedence higher operator,slower overall,operator higher in precedence higher operator,","14438062,20687808,18165141,7851837,41697652,47150236,54066935,36193506,6851760,3704402,3423368,","without parentheses math.exp c b is executed first as division has higher precedence than subtraction -,the division operation binds tighter than i.e is evaluated ahead of the subtraction so you are taking a square root of a negative number,division requires iterative subtraction that cannot be performed simultaneously so it takes longer;in fact some fp units speed up division by performing a reciprocal approximation and multiplying by that,that s because the division operator has a higher precedence than the subtraction operator -,remember multiplication division and remainder operators are all higher precedence than subtraction,this is called a strength reduction optimization because division is stronger slower more expensive than subtraction,if you re on an embedded cpu where division is more expensive you can accomplish exactly the same thing like this the total amount added to accum here is just like the first example but the division is replaced with an incremental repeated subtraction,this is because division operator has higher preference than the subtraction operator - in the first example you use are not using brackets therefore division takes place first you can change your code to,in this case division has higher precedence than subtraction parenthesis around the division or not,this platform is probably not representative of your microcontroller but the test shows that on this platform the subtraction is considerably slower than the division,because the division operator has higher precedence than subtraction,"
"division","subtraction","higher precedence in precedence higher operator,trickier overall,easier overall,higher precedence in precedence higher operator,greater precedence in precedence higher operator,cheaper operation in slower reduction n, is not faster in slower reduction n, is a bit in precedence higher operator,operator higher precedence in precedence higher operator,greater precedence in precedence higher operator,faster than regular  in unsigned 64-bit 32-bit,","13190071,1218185,32897752,29579783,19693646,39379901,2027842,10659772,37897197,34893808,51712907,","division has higher precedence than subtraction so in the first two examples only the second number is being divided,subtraction is similar using subtraction of the base type and borrow instead of carry multiplication can be done with repeated additions very slow or cross-products faster and division is trickier but can be done by shifting and subtraction of the numbers involved the long division you would have learned as a kid,it may not be the most elegant method but when you just need to convert something ad-hoc thinking of it as comparison and subtraction may be easier than division,division has higher precedence than subtraction,i don t think they have a natural precedence unlike say multiplication and division being of greater precedence than subtraction and addition because they can be built from subtraction and addition,this is called a strength reduction operation because subtraction is a weaker and cheaper operation than division,again this is not ambiguous once you know what the terms of reference are but beware of incorrectly comparing the first analysis i gave of euclid s algorithm with division o n 2 against this analysis of the algorithm with subtraction o n;n is not the same in each and subtraction is not faster,usually simple operations like addition subtraction and multiplication are very fast;division is a bit slower,doesn t get evaluated the way you are expecting the division operator has higher precedence than the subtraction operator,in this since division has greater precedence than subtraction therefore x 10 will execute first and here we are dividing two int irrespective of the fact that the variable where final answer is stored is a double so answer will be an int i.e 5 10 0 and then subtraction of an int and double will be done here int will be promoted to a double,unsigned division of 64-bit division by 32-bit divisor by repeated subtraction of divisor from dividend with remainder may be faster than regular division if the dividend is not expected to be much bigger than the divisor,"
"division","subtraction","slower in slower reduction n,simpler thing than  overall, is faster in unsigned 64-bit 32-bit,","21475774,48318668,55833042,","i am a bit suspicious of the performance because modulo tends to use division which is slower than your subtraction operations,similar to sam westrick s definition number of times you can subtract n2 from n1 without going negative you could also do integer division with addition and greater-than using the definition number of times you can add n2 to integer division before integer division is greater than n1 .;addition might seem like a conceptually simpler thing than subtraction,floating point number division is faster than integer division because of the exponent part in floating point number representation;to divide one exponent by another one plain subtraction is used,"
"archetypes","dexterity","stronger overall,arguably more overall,more lightweight overall,","13387479,8760668,14759857,","variety of supporting widgets is probably the single area in which archetypes is still stronger than dexterity,dexterity arguably has more better documentation than archetypes see plone.org products dexterity as well as my book professional plone 4 development,the good news is that dexterity content types are more lightweight than archetypes content types and doing raw listing by iterating over folder.contentitems in your template should not be that expensive,"
"oncreate","onstart"," or leave it there overall,less commonly overall,earlier overall, is not run again overall, not overall,","17274203,46291545,33901690,27018834,53876265,","when putting the phone to sleep onpause and onstop get called but ondestroy doesn t so that oncreate doesn t get called when turning the phone back on;onrestart onstart and onresume do get called checked on nexus 7 by modifying the activity lifecycle training app to add log messages in all lifecycle methods so try to move as much code in initialisetabhost as you can from oncreate to onstart leaving only the minimum necessary in oncreate or leave it there and add the relevant part of it to onrestart,onstart is less commonly used than oncreate,in my fragments data handling is taking place starting from oncreate which happens earlier than onstart,oncreate is run whenever an activity is first created followed by onstart followed by onresume;as you can see when a user returns to an activity oncreate is not run again,you should be setting your content view in oncreate not onstart;onstart can be invoked multiple times for the same activity instance,"
"tree","trie","admittedly more difficult overall, affects its performance overall, is substantially more in efficient ternary node,better overall,more in efficient ternary node,faster overall,more overall, lookup works better in string better ou, is more overall,more similar in search binary similar,better way than a  overall,","2688976,2688976,10970637,5953369,15458289,6723322,4203216,23328099,55360255,5347445,53083490,","finally b+ tree is admittedly more difficult to implement than a trie it s more on a red-black tree level of complexity,on the other hand trie are not meant for requests like all words between xx and zz;note that the branching factor of the b+ tree affects its performance the number of intermediary nodes,for cases where each node in the trie has most of each node in the trie children used the trie is substantially more space efficient and time efficient than th ternary search tree,a trie is better suited to this kind of thing because it lets you store your symbols as a tree and quickly parse it to match values or reject them,most likely a trie is more efficient and you didn t sort your dictionary and it doesn t use a binary tree or ternary tree,the suffix tree is lighter and faster than the trie and is used to index dna or optimize some large web search engines,use a radix tree wiki or trie wiki if you are concerned about performance.the radix tree is more memory efficient compared to a trie,ou can use a binary tree or a trie dependeing on what your id is whether it is a string or an int;for a trie lookup will be o k o 1 but a trie lookup works better with strings,for matching keywords phases trie tree is more faster,recursive is usually used for traversal and binary search tree but this tree is more similar to trie of only 2 character in alphabet,for example is there a better way than a trie tree or a way to implement it more efficiently,"
"tree","trie"," has worse locality in faster significant ps,better in string better ou, which is a more overall, is much more in search binary similar,less in suffix dummy nodes, is more in radix compact,usually faster in faster significant ps,better in search binary similar,more efficient overall,more in suffix dummy nodes, looks more in radix compact,","14445868,38671242,28391618,24051289,34732260,13765985,13988231,3596741,27587903,402806,5435784,","this is faster than the red-black tree by a factor of o log n which could potentially be significant for large collections;however the trie has worse locality since there is a lot more pointer-chasing involved and no contiguous arrays of characters to scan,the article says that a trie is better than binary tree as for a string of length m for trie it takes o m time and for binary tree it takes o m logn time,as for a beter datastructure you should consider using a trie;a beter datastructure s also known as a prefix tree which is a more fitting name in this context,note that while a trie works for the specific case of keys which are strings a binary search tree is much more general and only requires that the keys can be ordered,a suffix tree has less dummy nodes than the suffix trie,what you re describeing is called trie;a compact radix tree is more compact,ps radix tree is usually faster and more compact then trie but suffers from the same side effects of trie comparing to hash tables though less significant of course,a trie is better than a binary search tree for searching elements,should i change my project to trie or is there any other good reasons where avl tree woud be more efficient than trie in case of phonebook,a suffix tree is more or less an advanced trie here you can also search for any substrings in o c as for the trie,this is what the a radix tree would look like;whereas the trie looks more like,"
"16-bit","64bit"," amd did a much overall, ah represents the h overall,large as  overall,less overall,narrow as  in sizeof longer convenient,larger range in sizeof longer convenient,less efficient overall,no longer convenient in sizeof longer convenient, alignment is more overall,greater in platforms greater bits,more than  in platforms greater bits,","50869671,57174465,48225906,24587308,56780189,35725586,942017,6388799,6165260,28664552,29523624,","if you write the bottom bits al ah or ax nothing happens to the upper 16-bit of eax which means that if you want to promote a chars into a int you need to clear that memory first but you have no way of actually using only these top 16-bit making this feature more a pain than anything;now with 64bit amd did a much better job,rax is the total 64bit eax is the lower 32 bits ax is the lower 16-bit ah represents the h igh 8 bits of ax al represents the l ow 8 bits of ax therefore if your 64bit register contains a0b1c2d3e4f5a6b7 eax e4f5a6b7 ax a6b7 ah a6 al b7 you can identify this visually very easily by opening the windows calculator and setting this to programmer mode hexadecimal mode and a qword value,some conversions however are not obvious or not portable cannot be converted to no matter what the respective values of x and y are isize and usize have a platform specific size anywhere as small as 16-bit but as large as 64bit,that s obviously quite a bit more than the 16-bit that s mandated for an int but equally obviously less than the 64bit mandated for a long long,iso c++ allows int to be as narrow as 16-bit or arbitrarily wide but in practice it s a 32-bit type even on 64bit cpus,on today s desktop systems an int is usually 32 or 64bit wide for a correspondingly much larger range than the 16-bit 32767 32768 you are talking of,in fact for x86 64 processors performing 32-bit or 16-bit operations are less efficient than 64bit or 8-bit operations due to the operand prefix byte that has to be decoded,most machines now end up with sizeof int sizeof long because 16-bit is no longer convenient but we have long long to get 64bit if needed,without knowing more about what exactly you need to do it seems to me that it would be more efficient to use a struct containing a 32-bit unsigned int for the number of seconds and a 16-bit int for the number of milliseconds the remainder;or use a 32-bit int for the milliseconds if 64bit alignment is more important than saving a couple of bytes.,how many chars you can pack into it depends on the size of int which varies across platforms typically one of 16 32 or 64bit but it could be anything else greater than 16-bit,if the ints are 16 bit only one stack push of 32 bits is needed which is less than a pointer on 64bit architectures but on the ints are often more than 16-bit,"
"16-bit","64bit"," number takes more space overall, xor maybe overall,","6825085,54873757,","i d do the same thing with a 16-bit number as well;a 64bit number takes more space in memory as well,although depending how your inputs were written using 32-bit xor could be better than 16-bit xor maybe avoiding partial-register stalls if something later reads the full 32 or 64bit registers,"
"calloc","memset","more overall, to set the array overall,slower in certain conditions better,slower in certain conditions better,better served by using  in certain conditions better,slower in certain conditions better, isn overall, etc generally overall,","37555852,50167540,2688466,17898722,29905673,16536738,17898722,4309323,","in other words calloc is no more type-wise than memset,going back to the initalization though this is how you would set every element in a stack-allocated array on declaration if you dynamically allocate the array i recommend using calloc or memset to set the array elements to a default value for the same reason,my question is why is malloc + memset so much slower than calloc,see also why malloc + memset is slower than calloc,you would be better served by using calloc rather than malloc followed by memset .. 0 ...,malloc + memset is slower than calloc under certain conditions,or you can also use memset explicitly to initialize memory allocated by malloc call;note calloc isn t magic either - it will also use a loop somewhere to replace the garbage with all zeroes,it s like plus a memset but feels cleaner to me;you also shouldn t have a cast on malloc calloc etc generally speaking it can obscure useful warnings and you need to allocate six slots like i said so you can have the null-terminator which is the zero-valued character so we don t need to set it explicitly,"
"division","multiplication","faster in faster slower integer,more costly overall,much more costly in expensive costly cost,slower overall,faster in faster slower integer,slower overall,slower in faster slower integer,longer in longer pen paper,much simpler overall,slower overall,slower in faster slower integer,","41185802,39722216,26490416,40589149,13771806,7460216,7459618,21155463,34925635,27709697,14094072,","as to why multiplication is faster than division and when the divisor is fixed this is a faster route,the intuition is that division is a more costly affair than multiplication,knowing that a division is much more costly than a multiplication,division is slower than multiplication is generally - and definitely using regular expression matching is going to be slower than multiplication is..,i used multiplication for both operations because multiplication is typically faster than division,on most processors division is slower than multiplication for the same data types,yes division is usually much slower than multiplication,division takes a lot longer than multiplication - just do it with pen and paper to see,division though is an iterative process in logic the implementations you see on educational sites verilog vhdl are simply doing the same thing we did with log division in grade school but like multiplication it is much simpler than grade school you pull down bits from the numerator in the long division until the number being checked against the denominator is equal to or larger basically the number can either go in only zero times or one times into the next number under test unlike decimal where it can be between 0 to 9 times,hardware integer division is always slower than multiplication and the gap in the relative latencies of these instructions continues to widen,is it possible that the division is six times slower than multiplication and,"
"division","multiplication","slower integer overall,many more clock in cycles fewer common,faster in faster slower integer,typically faster in faster slower integer,slower than  overall, is still faster overall, is faster so overall,faster in faster slower integer,faster integer in faster slower integer,less time overall,slower in faster slower integer,","42840754,35689883,31618399,21188088,51352837,28690939,47321155,8658217,5459132,18706614,1643009,","it is well known that integer division is slow operation typically several times slower than integer multiplication,it is common knowledge that division takes many more clock cycles to compute than multiplication,multiplication is usually faster than division,the compiler or the jit is likely to convert the first case to the second anyway since multiplication is typically faster than division,know that division is far slower than multiplication and square roots are even more expensive think distance magnitude,integer division is slower than floating point division but using floating point multiplication on integer causes two conversions;the conversions aren t too bad so in total the floating point multiplication is still faster,change the half to 0.5 and you should be golden for the math part also multiplication is faster so use the math part instead of division when possible,for the division-to-multiplication case you are assuming that multiplication is faster than division,in many processors integer multiplication is vastly faster than integer division,multiplication takes less time then division so you can try this,the tostring should be slower than parse since division is generally slower than multiplication,"
"division","multiplication"," has a higher in precedence higher presedence,much more complicated overall,higher precedence in precedence higher presedence,faster overall,more overall, will not overall,slower than  in faster slower integer,faster in faster slower integer,higher precedence in precedence higher presedence,faster in faster slower integer,faster in faster slower integer,","22178187,1117688,40372189,1168616,7554853,12574349,49046049,26656329,42255570,22488897,32910219,","he line works because of integer division the line works because of modulo;the line works because of operator precedence multiplication has a higher precedence than addition + so it will be done first,the cpu operation for float division is much more complicated than multiplication,in some of the academic literature implied multiplication is interpreted as having higher precedence than division,this can be a major clock-cycle saver since multiplication is often much faster than a division operation,or is there something about multiplication that is more convenient than division in programming,i know that most of us grew up with the idea that division is sloooooow and must be avoided;however division unlike multiplication will not overflow,on mobile arm cpus i would expect floating point division to be somewhere 10-100 slower than multiplication,in usual programming practice one wouldn t bother and simply multiplying by the floating-point representation of 180 ï because multiplication is so much faster than division,does multiplication has higher precedence over division or it is other way round,multiplication is much faster than division,i would also suggest to replace terms like a l1 0.3e1 with as multiplication is faster then division,"
"division","multiplication","more overall,faster in faster slower integer,faster overall,complex with   overall,worse in complexity addition worse,higher than  in precedence higher presedence, is heavier than  so overall, are a bit in faster slower integer,more in expensive costly cost,usually slower overall,worse in complexity addition worse,","35693173,16212577,38977905,12718592,37236595,51422769,57621379,9815550,4109348,19938457,45899202,","in fact if the intent is to divide by 22 10 or some other real value that isn t necessarily exactly representable in binary floating-point then half the times the multiplication is more accurate than the division because it happens by coincidence that the relative error for 1 x is less than the relative error for x,this because 1 x is simpler than y x and multiplication is faster than division,as a rule of thumb multiplication is faster than division on all cpus,it can get a bit more complex with multiplication division but the main downside is performance,formally it means division cannot have a complexity worse than multiplication,it s an exponentation operator with a precedence higher than multiplication or division,let s assume that i have code like this as far as i know division is heavier than multiplication so as an the optimization i changed the code as follows however since value is an integer i have to cast it but as far as i know a type cast is also considered a heavy action,multiplication and division are a bit trickier but again a few tips;multiplication is the easier of the tasks just remember to multiplying each block of one number with the other and carry the zeros,division is more expensive than multiplication,if the numbers are huge dividing x by b might be betterâ division is usually slower than multiplication but getting out of the huge-number domain early might help more than avoiding division,division has worse latency than multiplication or addition by a factor of 2 to 4 on modern x86 cpus and worse throughput by a factor of 6 to 40,"
"division","multiplication","faster in faster slower integer,worse in faster slower integer,faster in faster slower integer,slower in faster slower integer,slower in faster slower integer,much faster in faster slower integer,usually fewer cycles in cycles fewer common,longer in longer pen paper,costlier overall, is more overall,slower long overall,","226515,14013678,45688463,15745819,35506226,1135698,17883291,18165141,12977241,15710193,6498884,","multiplication is faster division is more accurate,but since division is pretty expensive i think that this is even worse than 2 multiplication,multiplication is faster than division see fog s tables,but i wonder why is division actually slower than multiplication,division is slower than multiplication due to some reasons,integer multiplication is much faster than division,floating point multiplication usually takes fewer cycles than floating point division,why does division take so much longer than multiplication,well if it is a single calculation you wil hardly notice any difference but if you talk about millions of transaction then definitely division is costlier than multiplication,for multiplication the technique described at is a reasonably easy thing to implement and is better than serial addition;division is more complex in general but a good place to start is,if multiplication are o n 2 this is slower than long division for large numbers o n 2 vs o n 2 log n,"
"division","multiplication","more expensive in expensive costly cost,higher precedence than the  in precedence higher presedence,more time in time line for-loop,faster overall,faster overall,faster in faster slower integer, is not necessarily overall,longer than  overall,cheaper in time line for-loop,higher in precedence higher presedence, has larger complexity in complexity addition worse,","8886384,29579783,21610045,41582594,17069961,3851309,57030380,54410251,15817184,46323336,5649131,","division is always much more expensive than multiplication,division has higher precedence than subtraction;this subtraction operator occurs within the second brackets and so has a higher precedence than the multiplication,i have heard division takes more time then multiplication but beyond that i could not determine whether writing this in one line or multiple assignment lines was more efficient,i wonder why everybody missed that multiplication is much faster than division,here s one idea which uses one multiplication and one shift so it ll be faster than a division on most systems,as hroptatyr mentioned the multiplication is quite fast and it s much faster than division,when tried with only multiplication division try without sum assignment operations;edit as you can see multiplication is not necessarily the faster operation here,in follow-up to phuclv comment i checked the code generated by jit 1 the results are as follows for variable 5000 division by constant for variable variable because division always takes longer than multiplication the last code snippet is less performant,following advise i received multiplication is cheaper than division i revised one code line and interestingly enough 71.2 dropped to 1.7 but the if statement just below shot up to 64.8 â i just don t get it,since multiplication is of higher precedence than division,if you do it cleverly you only do addition not even multiplication;and division has larger complexity than addition,"
"division","multiplication","faster overall,more complex overall, is faster overall,slower in faster slower integer,higher precedence than  in precedence higher presedence, is always slower especially overall,generally longer in faster slower integer,more expensive in expensive costly cost,more expensive in constant expensive compiler,slower in faster slower integer,faster in faster slower integer,","42074376,23199400,53755233,42840878,54514749,49046049,2652547,38319640,36160101,655581,7720907,","from the performance side float multiplication is faster than division but i don t think that in the gui code it can create significant difference,division and square roots for huge number of bits are not much more complex than multiplication,every now and then in code it comes up that i need to divide several numbers by the same value since multiplication is faster than division i will often write this as i m wondering if i m really saving any time by doing this,both works but division is generally slower than multiplication,so this is evaluated as note that this behavior is different from most programming languages where unary negation has higher precedence than multiplication and division vb javascript,math.h defines m_log2e to the value of log e if you define _use_math_defines before inclusion of math.h even though usual approach is to do log n log 2 i would advise to use multiplication instead as division is always slower especially for floats and more so on mobile cpus,it s just as fast as going the opposite direction if not faster given that division generally takes longer than multiplication,division is much more expensive than multiplication,it will be much slower i don t have benchmarks but i would guess at least an order of magnitude maybe more decimal will not benefit from any hardware acceleration and arithmetic on it will require relatively expensive multiplication division by powers of 10 which is far more expensive than multiplication and dividion by powers of 2 to match the exponent before addition subtraction and to bring the exponent back into range after multiplication division,division algorithms are slower than multiplication algorithms in most cases,multiplication is usually significantly faster than division,"
"division","multiplication","much less overall,slower than hardware  in hardware slower correct,more complicated overall,integer  with  overall, happen first in precedence higher presedence,heavier in modern fast cpus,more expensive in expensive one sum,higher precedence than  in precedence higher presedence,faster overall, is faster in float integer unable,expensive than  in expensive costly cost,","8857056,436535,21546020,54410675,57338606,9939568,22648785,57017235,436535,55832817,4109368,","iirc floating-point multiplication is much less expensive than division so this might be faster than both,you always need to know the magic number here 0xaaaaaaab and the correct operations after the multiplication shifts and or additions in most cases and both is different depending on the number you want to divide by and both take too much cpu time to calculate both on the fly that would be slower than hardware division,for division things are a little more complicated than multiplication see,in particular this means jit does not replace integer division with multiplication,according to bodmas rule --- division come first from multiplication. same in java the precedence of is higher than so division happen first then multiplication,division may be heavier than multiplication but a commenter pointed out that reciprocals are just as fast as multiplication on modern cpus in which case this isn t correct for your case so if you do have 1 x appearing somewhere inside a loop and more than once you can assist by caching the result inside the loop and then using y,however in general one could expect that a division is a more expensive operation than a multiplication,in bnf consider something akin to and oh-by-the-way you probably want to make an additional nonterminal layer to represent the fact that implicit multiplication-by-adjacency is normally considered to have higher precedence than division while explicit multiplication typically is interpreted to have the same precedence,in a 64 bit application this code will be a lot faster than in a 32 bit application in a 32 bit application multiplying two 64 bit numbers take 3 multiplication and 3 additions on 32 bit values - however it might be still faster than a division on a 32 bit machine,here is results of my bench mark division type time uint8 879.5ms uint16 885.284ms int 982.195ms float 654.654ms as well as floating point multiplication is faster than integer multiplication. here is results of my bench mark multiplication type time uint8 166.339ms uint16 524.045ms int 432.041ms float 402.109ms my system spec cpu core i7-7700 ram 64gb visual studio 2015,yes mod is more expensive than multiplication as mod is implemented through division,"
"division","multiplication","longer than  in longer pen paper,slower float in modern fast cpus,more expensive in constant expensive compiler,faster in faster slower integer,higher presedence in precedence higher presedence,more time than the  in time line for-loop,nothing more overall,busier place overall,slower operation in hardware slower correct,cheaper than  overall,more expensive in expensive costly cost,","48109837,31031223,12977120,7935724,21060979,57030361,43246339,506252,7554853,1027808,7147370,","how can multiplication function takes longer than division,on modern processors float division is a good order of magnitude slower than float multiplication when measured by reciprocal throughput,usually division is a lot more expensive than multiplication but a smart compiler will often convert division by a compile-time constant to a multiplication anyway,first of all multiplication is faster than division,i read that multiplication has has higher presedence than division,in addition the for-loop itself is taking much more time than the multiplication or the division ops,easiest way is to simply recognize that division is nothing more than the multiplication of the dividend y and the inverse of the divisor x,both operations are done down at the floating point unit fpu level and even in the world of integral alus the division circuit is a far busier place than a multiplication circuit,on many machines particularly those without hardware support for division division is a slower operation than multiplication so this approach can yield a considerable speedup,t is not very important as long as alpha is small otherwise you will run into some rather weird nyquist issues aliasing etc and if you are working on a processor where multiplication is cheaper than division or fixed-point issues are important precalculate omega,division is a lot more expensive than multiplication,"
"division","multiplication","faster in faster slower integer,less in expensive costly cost,fewer in cycles fewer common,always more in expensive costly cost,faster in faster slower integer, which was more overall, remainder is a bitch in harder remainder bitch, used to be more overall,usually more in expensive one sum,slower in faster slower integer,more in expensive costly cost,","4143288,1735122,40599466,40280506,39174057,1698436,55111165,41981462,42193080,38857588,23577263,","similar to pmg s solution but still faster because multiplication is faster than division -,multiplication is less expensive than division so,it has to do with the fact that multiplication is itself done by means of binary shifts and additions - far fewer than with division,recently someone suggested to me that division is always more expensive than multiplication,division is faster for unint8 than multiplication in your case,0 but most of the time n 10 seems enough and spare one division which was more expensive;i simply use multiplication and it seems to make it much faster almost 4x here at least on the 1..100000000 range,multiplication is harder and division remainder is a bitch,that is before you factor in the extra multiplication and extra cast;on older µarchs 32 bit integer division often has lower latency numbers listed than double division but older µarchs 32 bit integer division varied more division used to be more serial with for floats round divisors being faster yet for integer division integer division s small results that are faster,in the code we calculate 1.0 sum .. because a division usually is more expensive than a multiplication and thus can gain some efficiency with that,in all other cases division appears to be several times slower than multiplication,the reason for doing so is to reduce hardware cost as division is more expensive than multiplication,"
"division","multiplication","more than  in addition comparison single,usually easier overall,higher precedence in precedence higher presedence,normally more in constant expensive compiler,earlier than   in precedence higher presedence,cheaper in faster slower integer,more than a  overall, operations; is a bit in harder remainder bitch,more expensive in expensive one sum,faster in faster slower integer,faster overall,","56588944,26400338,34242369,33186030,30279779,39104562,57731722,27922709,26052472,7730288,24232771,","here is such a rational class it could be easily extended to do more than multiplication addition division negation inversion ... notice how the last function can concentrate on the recipe logic delegating the use of fractions to the rational class which in turn is completely ignorant of the recipe-business,from what i read on the net multiplication is usually easier to compute than division,multiplication has higher precedence than division,this solution has the disadvantage that if the other factor is not constant the compiler and you can t reasonably avoid the division int_max n to be done at runtime and division is normally more expensive than multiplication,this will be good idea to put factorial as additional case into primary as parentheses have higher order of precedence called earlier than multiplication division etc in term,i always thought a multiplication is computationally cheaper than a division,a division costs more than a multiplication,beware of this pitfall when using shifts additions and subtractions to perform multiplication operations;division is a bit harder need to think.,division is one of a number of operations which as far as computational complexity theory is concerned are no more expensive than multiplication,division is about 20 faster than multiplication,i do not want to know when or if to use shift operators in my code i am interested in why multiplication is faster than shifting bits to the left whereas division is not,"
"division","multiplication","generally faster in faster slower integer,slower overall,faster overall,slower overall, is way faster in faster slower integer, assuming the compiler overall,better overall,harder in complexity addition worse,slower in modern fast cpus, is also relatively overall, and sometimes overall,","899929,13106830,37529283,4362064,48196025,1840568,14416164,1117702,436535,226519,54549072,","if the latter yes floating point multiplication is generally faster than division,on some machines division is much slower than multiplication but on most machines j multiplies and j divides will run a lot faster than 2 n-2 multiplication and one division,the double_unit stuff is how random actually does it internally because multiplication is faster than division see floating point division vs floating point multiplication,the reason to do this is because even though there is an integer division instruction div idiv in the instruction set it s typically very slow several times slower than multiplication,multiplication is way faster than division,if a remains the same and b is changing say if your code is in a loop and your code s clear that a does not change between two iterations for instance because your code s a const variable then the original version can execute faster because multiplication is cheaper than division assuming the compiler moves the computation of 1 .,the multiplication should perform somewhat better than division,if you think back to grade school you ll recall that multiplication was harder than addition and division was harder than multiplication,can be fast or it can be awfully slow even if division is done entirely in hardware if it is done using a div instruction this instruction is about 3 to 4 times slower than a multiplication on modern cpus,division isn t going to have a big performance impact in your application;floating-point division is generally especially slow so while floating-point multiplication is also relatively slow it s probably faster than floating-point division,on simple low-cost processors typically bitwise operations are substantially faster than division several times faster than multiplication and sometimes significantly faster than addition,"
"division","multiplication","slower in faster slower integer,general overall,better overall,far easier in faster slower integer, is nothing more overall,more accurate overall,slower overall,faster integer in faster slower integer,slower than  overall,slower overall,faster in faster slower integer,","506241,49204533,19350996,1917713,34925635,33018203,10535605,23965963,53346554,21566325,22956226,","division is per se slower than multiplication however i don t know the details,like multiplication and division modulo arithmetic has a higher precedence than addition,if you are sure that a floating point multiplication is better than a floating point division then,multiplication is far easier and faster for a cpu to do than division,so multiplication is nothing more than n number of shifts and adds which can be implemented in one clock cycle with a massive number of gates,division by 5.0 is more accurate than multiplication by an approximate 0.2,i was always taught that division is slower than multiplication but i have no real proof of thisâ has anyone got an opinion on this before i start benchmarking and running test,according to this author integer multiplication can be 40 times faster than integer division,there are methods of division that are faster than basic long division but still they are slower than multiplication,removing division operations by passing through the inverse into the shader is another useful tip as division is typically slower than multiplication,therefore i conclude that division is faster than multiplication,"
"division","multiplication"," is booth algorithm overall,slower in faster slower integer,much faster in faster slower integer, are very fast; overall,more in precedence higher presedence,slower operation in faster slower integer,slower overall,faster than double  overall,more efficient overall, always takes longer in addition comparison single,faster in faster slower integer,","33172634,36077485,34966700,10659772,35845566,4125154,11481493,53796521,40229256,329243,899887,","note that for signed number the pencil and paper method for multiplication add and shift and division may not work properly;for example the correct method for signed multiplication is booth algorithm,i found out that integer division is much slower than multiplication unfortunately,i picked c 1 1 8 for this example simply because it is exact in ieee-754 floating-point representation and typically multiplication is much faster than division,usually simple operations like addition subtraction and multiplication are very fast;division is a bit slower,thus python should interpret this like 12 2 i.e 6 since precedence of multiplication is more than division,division is inherently a much slower operation than multiplication,division is generally on the order of 10x slower than multiplication on most processor families,note this multiplication was chosen over this because double multiplication is about 3.6x faster than double division on the cpu that i m dealing with,i am pretty sure it is not possible to compute polynomial division more efficient than multiplication and as you can see in the following table this algorithm is only 3 times slower than a single multiplication,since most processors can do an addition comparison or multiplication in a single cycle those are all counted as one flop;but division always takes longer,multiplication is faster than division so the second method is faster,"
"division","multiplication","faster overall,slower in faster slower integer,more expensive in expensive costly cost,slower in faster slower integer, overflows 32 bit overall,faster overall,iterative unlike  overall,much slower in faster slower integer,faster in faster slower integer,slower in faster slower integer,worse in faster slower integer,","2652588,9714461,11481394,24249023,1301375,22836475,54509783,4541290,29745183,8776073,1988545,","even simpler and probably even faster because multiplication is faster than division is dav s answer which is the most natural algorithm.,and division may be slower than multiplication or may still be fast,is division more expensive than multiplication in c++,because division is often much slower than multiplication if performance is critical you might keep a table with powers of ten and their reciprocals,obviously after the multiplication has overflowed the division isn t going to undo that overflow;because the multiplication overflows 32 bit integers,floating point multiplication is faster than division so if speed is relevant,division is inherently iterative unlike multiplication where you can make wide hardware that does the partial products and partial additions in parallel,according to stephen canon modern implementations favor taylor expansion over rational function approximation where division is much slower than multiplication,since you re resizing the window make sure to assign the w and h values not as numbers but as products or dynamic numbers multiplication is faster than division but you can also use division,division is about 10 times slower than multiplication,so division is always a bit worse than multiplication,"
"division","multiplication","faster overall,faster in faster slower integer,slower overall,slower than integer  in float integer unable,faster in faster slower integer,shorter overall,slower in faster slower integer,faster in constant expensive compiler,slower overall,faster in faster slower integer,","22956226,836856,32022814,506929,17883577,1965541,655584,9113669,28690939,980962,","but the research i ve done so far all points to multiplication being faster than division,also addition is faster than multiplication and multiplication is faster than division,integer division is about an order of magnitude slower than multiplication on current cpus.,float division is not much slower than integer division but the compiler may be unable to do the same optimizations;for example the compiler can replace integer division between 3 with a multiplication and a binary shift,multiplication is faster than division,i haven t benchmarked any of this code but just by examining the code you can see that using integers division by 2 is shorter than multiplication by 2,which one is faster is indeed a cpu-specific issue or at least how much faster is cpu specific yes division is typically seen as slower than multiplication,most optimizing c compilers optimize it out to a multiplication operation which is much faster than division it can be done only if the divisor is constant though,your friend has a point a division actual division not just writing in c is slower than a multiplication,on many processors integer multiplication is faster than integer division,"
"bluetooth","usb"," came then overall,connection far more overall,slower in requirements easier commands,more overall,connection sometimes easier in requirements easier commands,slower overall,","57156863,9754274,23259073,6268880,13684470,35572690,","it works totally fine with usb but not with bluetooth devices while the app is running i turn on bluetooth on both my laptop and mobile phone but they have not paired yet the function gets called and it says bluetooth came then i connect the two devices now they are connected but no notification except the one above after that while they are connected i try to remove the mobile phone device they are disconnected and i connect them back again and one more time i get no notification about a new bluetooth connection has been scanned or connected,on the other hand a usb connection is far more reliable better supported and of course has the inherent advantage of speed and since it is well supported does not suffer from all the pitfalls bluetooth connectivity does,i had previously sent those commands via bluetooth but the connection fails too often to be useful and is slower than usb,interfacing with usb is more difficult than it sounds it would be harder to build something which would interface with that and measure millivolts than with bluetooth because the pic processor you use for analog to digital sampling and usb client would in fact have to either act as usb host or usb otg on a phone which is far more complicated than being a usb peripheral,it depends on your requirements but setting up a usb connection is sometimes easier than managing a bluetooth connection,if your watch doesn t have usb support moto 360 and other induction only charging you need to enable bluetooth debugging slower than through usb though,"
"jython","pypy","more in recent benchmark faster, doesn overall, but faster in recent benchmark faster, as well overall,","29966745,26285749,7793515,53040868,","jython is more unpredictableâ sometimes almost as fast as pypy sometimes much slower than cpython,jython doesn t deal in pointers it deals in java references which the jvm of course probably represents as pointers but you can t see those and wouldn t want to because the gc is allowed to move them around;pypy lets different types have different kinds of id but the most general is just an index into a table of objects you ve called id on which is obviously not going to be a pointer,here is a recent benchmark of jython 2.5.2 running on jvm 7 where jython is slower than pypy but faster than cpython,some of the python interpreters contain jit compilers i think pypy does in newer versions maybe jython as well and may be able to do this optimization but that of course depends on the actual code,"
"lzo","snappy"," but provides a higher in compression codecs popular,better overall,faster in faster big files,popular than  in compression codecs popular, is faster in faster big files,faster in faster big files,better than  overall,","56410326,25209341,32381774,27479257,12065223,15693524,56410326,","compression ratio gzip compression uses more cpu resources than snappy or lzo but provides a higher compression ratio,in my tests snappy performs better than lzo by the way,snappy is also significantly faster than lzo for decompression,these formats allow various data compression codecs note that snappy is now much more popular than lzo and can also provide other benefits such as fast serialization deserialization column pruning and bundled metadata,normally - lzo is faster than gzip compression though gzip compression ratio normally be better;snappy is robust with compression but compression ratios are normally worse,snappy also consistently decompresses 20 + faster than lzo which is a pretty big win if you want it for files you re reading a lot over hadoop,snappy often performs better than lzo,"
"64bit","x86"," does not overall,furhter overall,faster 32-bit in 32-bit tests system,dramatic as  in division cpus latency,more memory in 32-bit tests system,higher internal overall, version has more overall,machines much stronger overall,also more visually overall,considerably slower overall, divisor is much slower in division cpus latency,","41488948,13458435,6284598,47686839,35068309,30073542,12230087,12817803,12800316,95121,40669616,","the first if condition jumps to main batch code if the batch file is running on 32-bit windows where the environment variable programfiles x86 does not exist at all;the second if condition only executed on 64bit windows jumps to main batch code if it can find the file systemroot sysnative cmd.exe because the batch file is processed already by 64bit cmd.exe,in addition x86 is furhter complicated because there are generally separate documentation manuals for 32 bit and 64bit processors i m not familiar enough with arm to comment here,64bit amd and later intel machines run faster than 32-bit x86 machines because when amd designed the new instruction set they added more cpu registers and made sse math the default,i m surprised it s not more dramatic as x86 cpu div instructions can have latencies as high as 80-90 cycles for 64bit division on some cpus compared to mul at 3 cycles and bitwise ops at 1 cycle each,however my tests have shown that on a 64bit system an anycpu prefer 32-bit application which i confirm runs 32-bit can allocate more memory than an x86 one,the difference is in the first number which shows the rounding of the intermediate calculation so the problem happens because x86 has a higher internal precision 80 bit than the arm 64bit,a x86 has not many registers;the 64bit version has more,pax s aslr implementation for 64bit x86 machines is much stronger than linux s default 64bit aslr implementation,to me the path without x86 is also more visually appealing and indicates that it s a modern application - adapted for 64bit operation where necessary,x86 is considerably slower a few clocks plus a clock or so per function argument while 64bit is much less because most function arguments are passed in registers instead of on the stack,division is variable-latency on many x86 microarchitectures so if there s any speedup to be had when the dividend is really only n bits presumably the hardware already looks for that;however skylake div idiv r32 are constant 26c latency but 64bit divisor is much slower and still has very variable latency,"
"64bit","x86","fiddlier overall,slower than  in easier debugging mode,wider in support wider simd, is the safer overall,easier debugging in easier debugging mode, which prevents t overall,more optimisations than default  overall, division is slower in division cpus latency,more in support wider simd,platform much more in 32-bit tests system, is easier in easier debugging mode,","43206179,5607953,45887220,49164816,3195878,531772,6797915,52558274,2799753,45279374,53364541,","on arm it is not or rather 32-bit os on 64bit uefi is technically possible only would still require the operating system loader to be 64bit but even fiddlier than on x86,64bit code is not actually faster 64bit code is usually a bit slower than x86 code,no intel or amd x86 manuals ever guarantee atomicity of anything wider than 64bit except for lock cmpxchg16b so this talk of sse vector loads stores being atomic on some cpus isn t something that you can reliably take advantage of or detect when it s supported,all processors since the opteron in 2003 and the intel pentium 4 prescott the latter editions has 64bit instruction set and will all run 64bit windows;thus as long a msft continues to support 32 bit architecture x86 is the safer option although x64 would probably still work,x86 allows easier debugging - edit and continue is not supported when running in 64bit mode,reason why double can t be declared volatile t s 64bit which makes t more than the word size on x86 which prevents t from being declared volatile in the cli if i remember correctly,64bit apps will use the 64bit instruction set which carries a great deal more optimisations than default x86 packages which can result in better performance as compared to 64bit apps x86 counterpart,unlike most integer operations division throughput and latency on modern x86 cpus depends on the operand-size 64bit division is slower than 32-bit division,x86 doesn t support higher precision than 80 bits but if you really need more than 64bit for a fp algorithm most likely you should check your numerics instead of solving the problem with brute force,the x86_64 64bit platform is much more than twice the width of the x86 32-bit,64bit x86 is easier to refer to specifically excluding 32 and 16-bit x86-64 or x86_64 the dash vs,"
"64bit","x86"," tools generally overall, platform wouldn overall,wider than  in support wider simd, div is significantly slower in division cpus latency,slower in easier debugging mode,better overall, is another term overall, processors added more overall, cpu is much faster overall,","55755507,1469060,55802374,38314331,12665633,22864892,51760827,15117503,47995408,","when building for x86 using the x86 compiler is faster and slightly more stable;most users who wanted the 64bit tools generally wanted a larger working memory for large application,i m not sure if the x86 architecture supports 8-byte alignment even though they support a 64bit environment;after all 4-byte alignment on a 64bit platform wouldn t harm anything,there is no x86 simd support for integers wider than 64bit and the intel intrinsic types like __m128i and __m256i are purely for simd,most instructions are the same speed for all operand-sizes because modern x86 cpus can afford the transistor budget for wide alus;exceptions include imul r64 r64 is slower than imul r32 r32 on amd cpus before ryzen and intel atom and 64bit div is significantly slower on all cpus,faster than on x86 32bits but slower than x64 64bit,however if you don t have specific reasons to use anycpu then you could still use x86 because in some cases the performances are better than 64bit code,consider these statements 32-bit plugins are not compatible with 64bit executable 64bit plugins are not compatible with 32-bit executable either go with 32-bit executable notepad++.exe and 32-bit plugins or 64bit executable notepad++.exe and 64bit plugins note x86 is another term for 32-bit means the same thing in this context at least,over time newer x86 processors added more bits to the address lines and added virtual memory and added larger words;so a 32-bit data item was called a double word and a 64bit data item became known as a quad word and then sse came along and we had 128-bit octaword or double quadword here s a table of data sizes,x86 gcc -o3 -m32 will use __udivdi3;i wouldn t call this fair though because a 64bit cpu is much faster at 64bit arithmetic and a cortex-a7 doesn t have a 64bit mode available,"
"ati","nvidia","way better tools in faster stats tools,better overall,older laptop overall,more flexible overall,better in faster stats tools,more picky overall,more overall,vastly better overall, won t error overall,less amd overall,","5682205,21699076,10545022,1962928,7099300,2438099,9248755,29051119,6525179,6881575,","the reason i ask is that it seems like nvidia has way better tools and ati seems to have cancelled rendermonkey,there is almost always some driver issue between them but in general nvidia is better for opengl has bugs in directx implementation and ati amd versions only is better for directx has bugs in opengl implementation,i m having an issue where my glsl 130 code wont run properly on my somewhat modern ati 5850 hardware while the identical code runs perfectly fine on an older laptop with a nvidia card i have,4 is nvidia cuda technology is easier more flexible than ati brook+ language,ati might be a better between nvidia and ati since it was reportedly faster in 2009 but i m not sure if those stats are still correct,for example i ve noticed nvidia s glsl compiler is a little more picky than ati s and rejects some shader code that otherwise works fine,ati seems to tell more than nvidia,nvidia and ati architectures differ enough so that for certain tasks such as bitcoin mining ati is vastly better than nvidia,nvidia is more permissive for example nvidia lets you cast wrongly ie float4 to float only making ie float4 a warning ati won t error,another reason to choose nvidia is because that s what the hpc system builders have been building systems with since nvidia made a huge push for gpgpu computing where as it s less backed by amd ati,"
"quicksort","timsort","better overall,better overall,better in look non better, requires o n in look non better,faster overall,less comparison overall,better than  overall,","23340978,34037078,12455892,52714457,7786914,21807889,28129987,","java s array .sort is from about java 6 actually timsort the fastest general purpose #sort out there much better than quicksort in many situations,i ve read that timsort is better than quicksort both in the best and the worst case although it uses a bit more memory,you can take a look at timsort which for non completely random data performs better than quicksort they have the same asymptotic complexity but timsort has lower constants,it is faster than mergesort but it still slower than quicksort because quicksort has fewer data movements on random data quicksort requires o log n extra space while timsort requires o n to provide stability which also leads to an overhead,if that is so you might find that timsort runs faster than quicksort,if it is true then timsort will always take less comparison than quicksort because on real life data there is some pattern except data is truly random,introsort and timsort have both an o nlogn complexity in average and on worst cases which makes introsort and timsort better than quicksort in the particular cases where quicksort is in o n 2,"
"distinct","union","better than  in better performance performance-wise,generally faster in slower faster distinct,general in slower faster distinct, approach seems more overall, gives better performance in better performance performance-wise,general in slower faster distinct,better than  in better performance performance-wise, is not overall,faster in slower faster distinct, all is faster in in columns cell.i.e,slower in slower faster distinct,","49524433,43636244,22806534,48810588,40666668,24514611,35628095,14398264,3634389,5089505,31622091,","union is better than union since we are sure that duplicates are removed,union all is generally faster than using distinct or grouping,union is a shorter way to write union distinct,--a union approach seems more straight forward if you can guarantee that only status 2 records get mapped the union could be a union all to gain a bit of performance by avoiding the distinct,the union all with a distinct gives better performance though i believe..,this is why execution of a union all performs faster as it doens t have to do the duplicate removal contained within the execution of a union,so union is much better than union all with distinct in performance-wise,depending on the query ef will generate either union all with distinct or just union so your explicit distinct is not necessary;the linq equivalent of union all is concat,however the tables are huge and the union all i read its faster than distinct takes forever to execute even with just two tables let alone 6,union all is faster than union;in case of union if you have 10 columns and 100 rows it will compare each cell.i.e 10 100 to get distinct values while in union all this is not the case,union distinct is slower than union all but you may need it for de-dupping.,"
"distinct","union","better approach than  overall,faster in in columns cell.i.e,unnecessary as  overall,use lead with  overall,common requirement than  overall,possible with   overall,efficient as  in efficient id all,slower in slower faster distinct, except is more overall, all is more in efficient id all, all is also faster in slower faster distinct,","53582185,3877976,56147871,57393743,13891140,48103516,49516180,1804853,3670082,25365955,16979279,","i am guessing that count distinct is a better approach than union in the subquery but you could test that,i am currently using union all and a distinct in the outer query as this proved much faster than using union s for the uniqueness of data,as you didn t wanted duplicates theres no point using union all and use of distinct is simply unnecessary as union gives distinct data can create a view would be best choice as view is a virtual representation of the table,just use lead with union select t.id t.dte as startdate lead t.dte over partition by t.id order by t.dte as enddate from select distinct t.id v.dte from t cross apply values startdate enddate v dte t,it sounds to me like you are asking for the union of the filtering session values from the two tables and as you want the distinct set you do want union rather than union all union all is actually a much more common requirement than union,i would go for these two changes to optimize this query first i would split the query to two parts combined with a union preferably a union all if you don t care about removing duplicates but also possible with union distinct,union all is more efficient as union will take unnecessary distinct,keep in mind that union will return a distinct list - duplicates will be removed but it will perform slower than using union all which will not remove duplicates,if you want to ensure two collections have the same distinct set of members where duplicates in either are ignored you can use;using the set operations intersect union except is more efficient than using methods like contains,ecause you are selecting distinct id s you do not need union all;union all is more efficient because it does not add the step of removing duplicates,i changed union to union all because union implies a distinct;i don t think that s what you want or even need union all is also faster because it doesn t have to check if there are duplicate rows,"
"distinct","union"," all doesn in slower faster distinct,efficient than  in efficient id all,better than  overall, operator is really more overall, is no longer overall, all is slightly faster in slower faster distinct, as dynamically overall,faster in slower faster distinct,","2206001,57429129,36808780,37433435,47772020,10671833,49818173,22829159,","if you used a union then that may well be slower as a union has to apply a distinct but union all doesn t have to do that so it should be no different,seems that union is slightly less efficient than distinct . an array constructor is faster than array_agg,union all is better than union because it avoids a sort you know you ll get a distinct set because you re joining on two different tables,yes the union operator is really more like a union all statement in sql;excel s union operator does not return the distinct set of cells,you do want a union all but i think this is the version of the query you want in other words sum distinct is no longer appropriate when you use union all,in most cases union all is slightly faster than union distinct,try something like this this statement will first create a long list of union all select with group by better than distinct as dynamically created sql and executes this with exec,a side note since you want all rows using union all is faster since it does not need to perform a distinct to eliminate duplicates,"
"inherited","properties","easier overall, is more widely overall, poses more overall, makes it harder overall, look for example unset sometimes overall, is a lot overall, seems to pop up mostly overall,more verbose overall, interface has stronger overall, has higher precedence overall,also more clear overall,","9216497,56721826,50884600,51776094,34463952,39417758,3312492,46751164,55516179,7723152,16587246,","this should handle all cases of setting the properties makes them easier to implement in your inherited classes and cleans things up,this is the real form of inherited built into javascript and the foo properties is missing in all the objects composing the entire prototype chain then the value undefined is returned s the business which really seats behind the es6 class keywork which is a convenience which hides this mess and gives you the impression that javascript has a form of class inherited class inherited is more widely known and most of programmers find it easier to think of than prototypal inherited,use extends properties;but beware inherited poses more problems than solutions,although i would recommend against using inherited to hide properties in your dtos dtos are declarative models used to define your service contracts hiding their properties behind inherited makes it harder for anyone reading it to know exactly what each service accepts and returns,now the inner text is the best time to point out that these properties are a little trickier than these properties look for example unset sometimes acts like initial and sometimes acts like inherited,javascript inherited is a lot less magical implicit than in other languages before es6 classes at least;in your example you have a function parent which sets two properties on this,inherited seems to pop up mostly when i m storing collections of things where the different kinds of things will have data properties in common;inherited often feels more natural to me when there is common data whereas interfaces are a very natural way to express common behavior,counter1 can be used with instanceof and inherited but is more verbose and doesn t have real private properties eg count properties is exposed,i prefer the new keyword for cases where an inherited interface has stronger restrictions on a method or properties when i get some explicit itrucker object i get its truck directly through the vehicle properties and do not need another cast which is in some cases quite handy,if you are fuzzy about prototype inherited make sure you read up on it - it will help you reason about this problem;in essence an object inherits all properties of an object prototype but an object s own properties has higher precedence than those of its prototype,using dedicated methods for getting and setting properties is also more clear in inherited,"
"inherited","properties","more overall, has a lower overall,much more powerful overall, makes it more overall, to add some more overall, and looks cleaner overall, is often overall,better in model individual base, advantage single class overall, letter has sender overall, works more overall,","5736117,24912697,183126,4228646,53079312,24156934,45743119,35690134,48076727,57053551,5199917,","if you need a globally accessible variable or properties that s more suited to a base class that your classes inherited from,an ignore that is inherited has a lower priority than automapper s standard convention-based mapping;so where the source and destination types both have a referrer properties the convention-based mapping for the most derived types overrides the inherited ignore,now in .net for desktop controls you can use inherited which is much more powerful than the old tag properties anyway,t might be helpful to extend the magiccard interface so that it has a properties that returns the object as cardproperties for easy accessibility;dual inherited makes it more tricky to access both the base class and interface for a container element since you can only store one type in the generic container,anyway i played with inherited to add some more properties but it failed,o automatically add those common columns to all my tables i just had to create a base model containing those common columns and make all of my models inherited from my models;this avoids repeatedly adding those common columns to each model as properties and looks cleaner i believe,in some cases a shallow copy of own properties is needed in another the entire prototype chain should be traversed;composition over inherited is often a better choice,potentially you are trying to model something in inherited that is better suited for composition or your base class should be taking in an object instead of individual related properties if the properties aren t related then perhaps your base class is doing too much single responsibility principle,possible solutions create a separate listinguser that s than inherited by a detailuser that s declaring advantage allows clean typing backdrop adding more representations would be pain or not possible if not fitting in the inherited chain create a single user class that contains optional properties advantage single class to maintain backdrop you can never be sure which representation you currently have needs potentially a lot of checks,now i want to extend my document into let s say 3 or more types letter report and contract which can have their own unique properties letter has sender and recepient contract can have a some fields like valid_until and such in the addition to inherited ones,if you want to use inherited properties inherited properties works more like this,"
"inherited","properties","more overall,better nesting in model individual base, it seems like less overall, which is better overall,more than the  overall,easier overall,more overall, that makes it less overall,higher overall,better choice in model individual base, gets even uglier overall,","6071295,1287890,10922074,2705536,52537118,766479,45555967,1621902,34583059,23018184,39499236,","however i could not find it the documentation so it may inherited more than properties,less is a css extension that enables reuse and encapsulation of values color values for instance improves inherited allows a better nesting of related properties and operations also,the first half of your question sounds like inherited is a perfect fit;however upon reading about your need to get a list of common and individual properties it seems like less of a great fit,the other way to do this is multiple-table inherited which is better for your database but not as easy to map in code;you do this by having a base table which defines some common properties of all the objects - perhaps just an id and a name - and all of your specific tables person etc use the base id as a unique foreign key usually also the primary key,a more sophisticated version of this trick would be to create an inheritable attached properties treeviewitemlevel that would be set to zero on the treeview and to one more than the inherited value on treeviewitem s,multiple inherited makes it easier to compose classes from small mixin base classes that implement functionality and have properties to remember state,inherited is more of an is-a relationship for example a usermodel is-a model thus all properties and methods of a model will be part of a usermodel,this is roughly how to implement the inherited method suggested by glen;glen s wrapper class with same interface method is also very nice from a theoretical point of view but has slightly different properties that makes it less probable to work in your case,i don t see any form properties in the code you posted so either the posted code has been modified or the properties is higher in the inherited tree simpleformcontroller,since the paddle object has properties that are shared by both player and enemy composition is a better choice than inherited,when accessing the properties within an instance js prototypical inherited gets even uglier,"
"inherited","properties"," it is much harder overall,","5556127,","dependency properties in wpf support properties value inherited;with normal clr properties it is much harder to push values down to any child objects without modifying the child object,"
"hash","pbkdf2"," requires more overall,more overall, isn overall,quicker overall,slower standard overall, bcrypt is harder overall, doesn overall,","7029421,41233659,36577294,116767,13395733,32772682,26027410,","key strengthening techniques such as bcrypt or pbkdf2 are generally considered better than plain hash since cracking bcrypt or pbkdf2 requires more resources,pbkdf2 is more secure than a simple hash or even a salt hash,although pbkdf2 uses a mac algorithm internally so i can understand if you were confused by that.;encrypting a hash isn t what a mac does either,a key-stretching algorithm like pbkdf2 applies a quicker hash like sha512 thousands of times typically causing the hash generation to take 1 5 of a second or so,key derivation algorithms such as bcrypt and pbkdf2 aka rfc2898derivebytes are much slower than standard hash algorithms,you can increase iterations later on to make the hash less vulnerable to brute force attacks as more computing power comes out;despite pbkdf2 bcrypt is harder to accelerate on gpus than pbkdf2,you should use an artificially slowed down entropy-enducing method such as pbkdf2 described in pkcs#5.;putting a low entropy password through a hash doesn t increase entropy,"
"fadein","fadeout","shorter in issues shorter animation,slower overall, is time more overall, is pretty simple just overall,longer in issues shorter animation,","20388866,32921128,52304183,24220947,2165425,","otherwise you will run into issues of trying to fadein and fadeout at the same time if your delay is shorter than your fadein,i want smooth fadeout fadein animations instead setting visibility where fadeout is slower than fadein animation so i ve used enteractions and exitaction of datatrigger,using setinterval we could rewrite your function as the first parameter of your fadein fadeout is time more info,fadein is pretty simple just add this to your fadein screens show;fadeout is a little trickier,more detail set the amount of time the animation will take for fadeout and use a delay for the fadein animation that is longer than the fadeout animation time,"
"glibc","tcmalloc","more aggressive overall,less overall,faster in available faster malloc,faster in available faster malloc,","18098933,15566083,5742204,15452794,","jemalloc and tcmalloc with some setting changes can be more aggressive than glibc to release memory to the os - but again it depends on the allocation patterns,then we found glibc also have same issue but increase rate is less than tcmalloc,tcmalloc is faster than the glibc 2.3 malloc.,tcmalloc is faster than the glibc 2.3 malloc available as a separate library called ptmalloc2 and other malloc s that i have tested,"
"mouseout","mouseover"," which gives you better in first changes performance, but is easier overall,longer overall, set the flag overall, event is fired.;thus overall, works better in first changes performance, cy.contains shots .trigger overall,","54101631,25334563,12685001,28922227,3941211,26679935,55094783,","if in your real chart you have hundreds of elements a better solution is first filtering them and after that applying the changes both on mouseover and mouseout which gives you better performance,a bar s similar to the solution proposed by ckersch except that a bar has more overhead you re changing the dom every mouseover mouseout but is easier to implement and doesn t change the structure of your svg at all,the mouseover animation is 200ms longer than the mouseout so if you mouseover and mouseout in less than 200ms total the animations run in parallel and the mouseover one finishes last leaving the color red,on mouseover not mousein set the flag to true and run a function which bounces forever as long as;on mouseout set the flag to false,what s happening is that the jquery animate function that is started on mouseover isn t finished when the mouseout event is fired.;thus the 100ms delay before the z-index change is suppose to happen on the show part of the animation ends up happening after the z-index is changed on the mouseout hide function,note without the mouseout the mouseout works better but tooltip stays until next mouseover,if your app uses #2 javascript events you can use a workaround triggering the mouseover mouseout events manually cy.get .menu.button.overlay_button.projects_popover .click cy.contains automation .trigger mouseover cy.contains shots .trigger mouseover .click there is also a possibility your app is listening to the mouseenter mouseleave events in which case you can trigger those as well,"
"hash","salt","larger in time attacker longer, and then in function randomness several, shouldn in function randomness several, function not in function randomness several, does not overall,sha-512 higher overall, is not in time attacker longer, ing doesn in time attacker longer,better in function randomness several, cannot overall,longer in time attacker longer,","28201502,24077647,8080957,32074621,1046066,12064628,11136921,26027410,1995717,9610415,482268,","likely not as cheap as xor against n values but seems like there s possibility for better quality results at a minimal extra cost especially if the data being hash is much larger than the salt value,better use of salt is that better use of salt is randomly generated before use in hash function and better use of salt does not need to be 1024 bytes - 8 bytes is more than enough for salt and then prepended to resulting hash,your salt isn t following that so the hash function is probably failing completely though i m not certain;also your salt shouldn t be a constant in your app it should be unique to each user and stored in the db along with their hashed password,it s a general purpose cryptographic hash function not meant for passwords;salt must be unique per user,the reason for this is so you can have many types of hash with different salt and feeds that string into a function that knows how to match it with some other value;if the hash does not use a salt then there is no sign for that,salt sha-512 offers a higher level of security and implementing a scheme where you stretch the hash is even better do some high number of iterations of sha-512 - starting with the password+salt of course, salt is not a constant if every password is hash with the same salt you re wasting your time;i would recommend using bcrypt instead of the sha512 salt approach bcrypt instead s much harder to brute force,the salt doesn t make it slower to calculate hash it just means they have to crack each user s password individually and pre-computed hash tables buzz-word rainbow tables are made completely useless;if you don t have a precomputed hash-table and you re only cracking one password hash salting doesn t make any difference,the more randomness and more characters your salt has the better for the hash but anything that s several characters long and random works,the two append something to or otherwise modify the input password not including salt to give the resulting hash;if indeed you have the md5 hash values using the method you describe piping a string to md5sum that resulting hash cannot be reversed to anything you can then hash to an htpasswd compatible hash,you can safely store the salt in the db because working out a string from its hash is just as hard when you know some of the string as it is when you know none of it provided the password itself is longer than the salt and long enough and strong enough to take a long time to crack by brute force at least 6 chars with at least one case change and a number or non-alphanumeric i d say,"
"hash","salt"," doesn overall, password is more overall, makes your system more overall,user password with  in time attacker longer,compare password with  in salt secure retrieved, which is better in time attacker longer, iterations not in time attacker longer,also more powerful overall, is usually shorter in time attacker longer, does not in time attacker longer,field better overall,","7438390,1264516,3196933,48095788,23193359,50568507,9647787,4732381,4212704,52961588,15540208,","the idea of a salt is just to make the hash digest unique per user to resist dictionary attacks and rainbow table attacks;the salt doesn t add to the strength of the hash digest algorithm regardless of the salt s length,if you are certain that you will never use a hash scheme to authenticate like http digest auth hash password is more secure;to avoid rainbow table attack please use a nonce or salt,adding a salt to your hash does not make a pre-computed attack impossible only more difficult;re-use of a salt makes your system more prone to attack,that s all clear however i am not sure what with salt in hashing. i read os.urandom python is good to create good salt what i am not sure is how to work with this added salt if i hash user password with salt and its one way,this salt used in yii2 security helper doesn t require for storing in db t is only used for creating password hash but not need to compare password with hash,a rare one amongst a disappointing number that did not hash passwords also hash with the username and some salt which is better,the salt determines how much space is required to store a pre-computed table such as a rainbow table that allows an attacker to quickly lookup a password for a given hash;the number of hash iterations not the salt is what determines the time required for an attacker try each password in his dictionary of candidates,prepending a salt is also more powerful than directly setting the seed values because in addition to changing the internal state of the hash if the salt is not a multiple of the digest block size then it can also perturb the alignment with which the input is fed into the hash function,ut if salt is appended attacker can make such database for password dictionary and additionally compute only salt s hash;given salt is usually shorter than password like 4 chars salt and 8 char password it will be faster attack,but unlike encryption algorithms password hash are one-way deterministic trap door calculations;also unlike secret-key encryption the salt does not need to remain secret,restructuring of the database to just add an salt field is better option or the only one really if your going to do it properly but you could use your currant hash field to store the salt as other person posted,"
"hash","salt","harder in salt secure retrieved, it is very in tables rainbow protection, is a sequence overall, is far better overall, that isn overall, before ing so in time attacker longer, wouldn in tables rainbow protection, is not in time attacker longer, algorithm doesn overall, is not overall,more in tables rainbow protection,","5641994,18144689,9450549,1756222,50090373,2951991,8005540,31830268,8781026,9161257,9220557,","the lack of salt is harder to expoit here than with password hash since the hash is not directly known,if the hash is not salt it is very easy to get the cleartext password using rainbow tables if you got the hash;if the hash is salt it is very easy to get the cleartext password for simple passwords,however for a cipher algorithm that is not based on hash a salt isn t necessary because the attackers will obtain anyway the original string with the salt and it s just needed logic to remove it;a salt is a sequence of characters added to a string about to being hashed so it s harder for an attacker to obtain the original string,also always use a salt code with your hash;a side note after time has passed using hardened hash is far better than using a plain speed-based hashing function,when you receive the get or read the cookie use the data in it to re-calculate the hash and stop redirect error out if the data no longer validates the hash check;when you do this use some server-side salt that isn t displayed in the url or in web page source etc,which is a bad idea because of rainbow tables even if the hash is not md5 but sha512 for example;second idea add a unique random salt before hashing so the hackers has to bruteforce each password,assuming that the is stored alongside the final -- since the hash wouldn t be testable without it -- this scheme is quite weak;using a salt does protect against rainbow tables but a non-iterated hmac leaves this scheme weak to brute-force attacks,since the password will be the same and only the salt changes a cracker with access to the hash can brute-force with exactly the same cost;changing the salt will not improve the security a salt is not a secret it fullfills its job even if it is known,in other words the hash algorithm doesn t really matter as much as system security and limiting login attempts also if you don t use ssl then the attacker can just listen in on the connection to get the information;unless you need the algorithm to take a long time to compute for your own purposes then sha-256 or sha-512 with a user specific salt should be enough,if you want to store a hash that will be secure for a long time to come sha-256 or sha-512 part of the sha-2 family of hash designed as secure replacements for sha-1 are a good choice and somewhere between 128 and 256 bits of salt are standard;however the use of plain hash is not the best way to do this nowadays,however using a salt offers more protection against rainbow tables precalculated hash tables so they re still worth using,"
"hash","salt"," wouldn in users different weak, is always overall, it s just overall, is essential for a better overall, is also in time attacker longer,not more secure in time attacker longer, gives a lot in function randomness several, is not exposed then in time attacker longer,passwords more in salt secure retrieved, cannot in time attacker longer,longer than the  in time attacker longer,","4445477,11200659,18410174,1225553,23432500,8646455,5051022,13828321,4269273,13836253,2148905,","assuming your hashing method is not weak it doesn t matter if the salt is known - salt is simply so that 2 users with the same password have different hash - and a casual inspection of hash wouldn t result in identical passwords being obvious;salt should be unique per user,rolling your own salt is not a problem and knowing them by this i assume you meant store them separately to the hash is not necessary if you re storing crypt s output as-is;from my investigation it seemed that the salt is always 22 characters and the hash offset is 29 not 28 making it 31 characters in length not 32,a salt derived from any value that has a relation with the value to be hash is not a salt it s just an altered hashing algorithm;salt are entirely pseudo random noise period,your first question is not clear but multiple hash are not less secure because the second hash has a fixed length;but more important than a multiple hash the use of a salt is essential for a better security,the salt is added to the password before hashing to ensure that the hash isn t useable in a rainbow table attack;because the salt is randomly generated each time you call the function the resulting password hash is also different,so when i see that the salt is stored in the hash password and that you use that hash password as salt i think crypt + salt is not more secure against a brute force on output hackers who managed to steal hash passwords,that s why most algorithms call the hash function several times;using salt gives a lot more of randomness to the generated password and thus make the generated password less guessable,for an ideal password hash a leak of the hash is nothing hugely critical;if the salt is not exposed then it s effectively impossible to recover a password,i get the impression that most people think that hashing salt passwords is the more secure way of handling passwords but i can t figure out a way to comply with current company operations when using hash passwords,i m not sure what you mean with switching from php but let the development language generate the hash not the database system;the salt should be different for every password a global salt cannot fulfill it s purpose,if that salt + hash original password was longer than the salt + hash you may have just reduced the security of that salt + hash password assuming a good password and ignoring hash collisions,"
"hash","salt"," will not in time attacker longer, are not in users different weak, not in time attacker longer,more in salt secure retrieved, to make it harder overall, looking something overall, is not overall,probably faster too in time attacker longer, under specified login name in salt secure retrieved, doubles the space in time attacker longer,harder especially in time attacker longer,","13850682,1884628,27864167,31641520,1951458,29459653,2189005,4834231,48800576,9647787,7003476,","this is telling the security class to hash your password with the default algorithm and to use the salt configured in security.salt;if the value of security.salt is different in either application the hash will not match,however the combination of a salt and a password may lead to the same string or hash in the end and the hash will be exactly the same so make sure to use a combination of salt and password where two different combination won t lead to the same hash;another intention behind the use of a salt is to make sure two users with the same password won t end up having the same hash in the users table assuming their salt are not the same,in order to compare it with another password you should compare the hash not the real passwords;the easy way to make hash with salt would be using messagedigest,in this situation storing password hash and the salt is more secure than storing the credentials encrypted or not because an attacker would have no way of getting the password back even if he manages to get his hands on both the hash and the salt,edit as op noted just using a hash algorithm isn t enough;you must to add a salt to make it harder to break,the salt isn t a secret it s just a custom thing you add to avoid having the hash cracked with precomputed lookup tables like rainbow tables;the salt is usually stored with the hash either in the same database in a different field or when using php s password_hash it s actually just concatenated to the hash looking something like mysalt.hash,b keeping the salt separate from the password hash is not an effective defense we are assuming administrators after all;c using existing data as a salt is a little better but i doubt existing data has as much entropy a random salt has,you ll probably need to get data anyway so the unique salt is probably faster too because you won t need to calculate the hash over username,salt if not combined into password hash;authentication will create hash from provided password and stored salt and compare it to stored hash under specified login name,the number of hash iterations not the salt is what determines the time required for an attacker try each password in his dictionary of candidates;every bit of salt doubles the space required for the lookup table,most attacks involve generating hash for common passwords so for reasonably complicated passwords it becomes harder especially with salt some people use usernames as salt others use randomly generated numbers,"
"hash","salt"," not overall, password values myuser in time attacker longer, is retrieved; make in salt secure retrieved, used each time overall, output is longer overall, doesn t make overall,trimmed in time attacker longer, is typically in time attacker longer,harder in time attacker longer, isn in salt secure retrieved, that is longer overall,","7465411,52394907,3168509,40297998,25435704,6486144,15396796,7303273,19884074,29227723,1109329,","salt are used with hash not ciphers;note that both ivs and salt are specific examples of a nonce,after all you could do this insert into accounts user salt password values myuser 1234 sha2 concat xyzzy 1234 256 but now you the password xyzzy appears in plain-text in your query logs and binary logs even if it is stored in hash form in the table itself,a password hash cannot be broken until the salt is retrieved;salt make precomputed attacks more resource intensive but never impossible,salting ensures the hash aren t predictable;there is a different salt used each time,the correct hash has a sha-512 salt so a sha-512 salt output is longer than 13 characters,i can imagine that the point of hashing that fingerprint information is storage space as the resulting hash has a fixed length;but to also use a salt doesn t make much sense to me,example the salt field might only allow a 64 characters while the generated salt might be longer therefore when you save the salt it gets trimmed which ultimately changes the hash password,a salt is most typically encountered with cryptographic hash functions not encryption functions;the idea is that rather than hashing just your data a password you hash data+salt where salt is typically a randomly-generated string,i don t know how safe could it be and how difficult is for the hacker to determinate the technique i use the thing is that the result has the same length as the hash and is harder to determinate a salt so if for any reason in the history hacker uses a rainbow table and catches a probable result it will be the wrong,so it could be that the first 8 bytes of the password hash are actually the salt;then there s the chance that the salt isn t used,anyways i want to reiterate that plain md5 hash are easy to crack for most passwords since people like short and easy to remember passwords. use a salt and or a more complex algorithm;i d recommend both and use a salt that is longer than two characters and not limited to numbers,"
"hash","salt"," is lot safer overall, has an easier in time attacker longer,nothing more in time attacker longer, is the part overall, is not in salt secure retrieved, not in time attacker longer, password gets shorter in salt secure retrieved, is more overall, are not uniquely in function randomness several,","55320844,11446587,12807370,15194875,1753040,56859418,55080219,11335470,22966549,","a hash with salt is lot safer,so an attacker trying to brute-force all your password hash has an easier time because he only needs to crack the same password once for all users;therefore it is still very advisable to use user-specific salt,this salt is nothing more than a random arbitrary string that you concatenate to the passwords and it will make your hash password unique,in fact you should be using a proven well-known implementation of a strong hash and not implementing the algorithm yourself;the salt is the part that needs to be protected,storing the salt unencrypted in the database next to the hash passwords is not a problem;the purpose of the salt is not to be secret,the salt is stored together with the hash to make it possible to validate it;had the salt not been stored it would be impossible to validate the hash since you don t know the full input to the hash which includes the salt,my problem is that while merging the password with the salt my hash password gets shorter by one character at the end,that way a hash salt is more random and your data a bit more secure,one of the most common mistakes in hashing is that hash are not unique to the users;this is mainly because salt are not uniquely generated,"
"ienumerable","ilist"," will execute the query overall,interface more functionality in functionality iteration only,more overall, then gives richer functionality in functionality iteration only,less in specific functions,more abstract in abstract method collections,more guarantees than  overall, is the more overall,much more lighter overall,more specialized overall,more in abstract method collections,","48079699,14821298,2000456,42214308,5213886,35478849,16934777,14336566,3522746,13779387,8955601,","another difference in case of ef is that the ienumerable is not executing the query to the db untill you really enumerate the list;while the ilist will execute the query and fetch the items in memory,you need to use tolist to convert it from ienumerable because the ilist interface supports more functionality than the ienumerable interface,thus ilist is more than ienumerable and you won t be able to get away with it,simply ienumerable gives you iteration only;icollection adds counting and ilist then gives richer functionality including find add and remove elements by index or via lambda expressions,an ienumerable is less specific than an ilist,another suggestion change you to because you can keep an extension method for more collections because the ienumerable is more abstract than ilist,ilist is stronger in that ilist makes more guarantees than ienumerable,by ienumerable;therefore ilist is the more strict conversion and is therefore selected,yes i understand that ienumerable is much more lighter rather than ilist but anyway there is a lot situations where we need to have ilist instead of ienumerable and in this approach we need to cast ienumerable to ilist isn t it,the reason why this doesn t work specifically is because ilist is more specialized than ienumerable,ienumerable is more abstract and is generally preferred to list or ilist if possible,"
"ienumerable","ilist"," which is more overall,less specific in specific functions, not overall,less specific overall, basically only overall, is much better overall, may be a more overall,less in specific functions,","2877619,3180672,25302983,3228735,3455592,56404718,57549142,17761596,","the ienumerable component returns an ilist which is more generic to work with,ienumerable is less specific than an ilist that is ilist has functions that ienumerable does not,and since ilist inherits from ienumerable you can call where on both interfaces calling where on a ilist actually calls the ienumerable.where extension method;so in both cases the same base method is called and the type of the resulting value will be an ienumerable not an ilist when applied to a list,ienumerable is less specific than ilist ilist implements ienumerable so unless you want something specific from ilist such as .count as you suggest or perhaps add delete etc i d use ienumerable,ienumerable basically only supports iterating over the results contained in the collection;ilist is more specific and allows you to,in the older versions of nop commerce i have seen that they always use to return but now in the latest versions most of the view models are returned in ilist.i know with ilist its good to work with abstraction instead of concrete type list but then ienumerable is much better because everything derives from it,i suspect the main problem you are encountering is the fact that indexing into ienumerable is not possible;i would guess that ilist may be a more appropriate interface for the items which would allow greater control over the items in general,ienumerable is less specific than ilist,"
"moles","typemock","more overall,faster ms overall,","3035730,3869263,","moles is more often compared contrasted to typemock in that it offers a set of facilities outside of moq and or rhinomocks sweet spot,i have no concrete figures on that but from my own experience i d estimate that instrumented tests are around 100-400 slower typemock seeming to be faster than ms moles,"
"self","super"," is not in parent method next,same as foo.__init__  in foo it proxy, isn in parent method next,.__str__ better overall,x better in multiple inheritance x, doesn in parent method next, is generally in put bound first, ... calls  dedicatedmasterstrategymixin in parent method next,better by making  in parent method next, is nothing more overall, does not even in parent method next,","35560073,50572773,44727081,42014475,27859143,34496277,41995534,56206348,56368890,28386962,48941706,","your super call is incorrect;update so that self is not called as a method argument - as you have it written it will always fail,super does not return a class;it returns a proxy object so super .__init__ self name behaves much the same as foo.__init__ self name where foo is a real instance of wizard,also super has __get__ method which makes it a descriptor i ll omit the details of descriptors here refer to the documentation to know more;this answers your second question as well as to why self isn t passed explicitly,also as stated by martijn pieters using super super .__str__ is better than doing it the way i did deck.__str__ self,i think a.__init__ self x is better then super .__init__ x because it supports multiple inheritance and i didn t find a way to do it with super,super car self .__init__ self name color in your code;super doesn t know what class it s being called in,first the call to super should be super new self not super old self;the first argument to super is generally the current class the class whose method is calling super,note that super does not always return the parent object;if the method dedicatedmasterstrategymixin.propose_update self ... calls super dedicatedmasterstrategymixin self .propose_update ... it finds the next propose_update method according to the method resolution order mro,it can be made better by making super as a callable that only take args as argument by encapsulating the self next super method,so all super classes must initialize the object.;self is nothing more than a pointer to the object the method initializer is currently working on,when using super self is passed automatically;also in python3.3 and above super does not even need to receive arguments to know from which class it is being called,"
"self","super"," .method will still in parent method next, doesn in parent method next, as well in parent method next, does not necessarily in foo it proxy, is better overall, has a lesser overall, .get_template_names more overall, is much more in multiple inheritance x, does not fully exist yet overall, is not in parent method next, has already in put bound first,","52058615,9559081,47077629,35212350,53806555,29173627,49690114,18558999,39809959,711712,50572773,","super does not alter the type of the self reference;super ... self .method will still pass in that self reference to the method being called so self is in all three cases the grandchild instance,also two comments on your other code there s no point overriding the __init__ method if you re just calling the parent method but if you do need to that you should use super as i have done here rather than using the name of the superclass itself;self doesn t exist in that scope - it only exists inside a method and only then because it s passed as the first parameter,in python3 super method behaves a bit more magically;super method automatically figures out the parent of the class and implicitly passes the self as well,the most important thing to remember about super is that inside foo.go you do not know what class super foo self will refer to because you do not know if self is an instance of foo or a descendent of foo;despite its name super does not necessarily refer to a superclass,any way i think making a seperate super admin pannel to your self is better because that way only you can decide wich is wich userid | usertype | usernam 1 | 1 | jhone 2 | 2 | mark user type 1 admin user type 2 normal user table structure that you should use and every time they login you querying db and storing info in session in your situation _session userid and _session role type,super has a lesser benefit of reducing requires changes if you rename or change the base class;in python 3 the arguments to super are optional so you can just do super .__init__ self,so you should have two templates for your view your view.html which will be something like and your partial_table.html now in your view if for example you are using cbvs then you ll have to use the as defined above and override get_template_names like this def get_template_names self # sometimes the is_ajax is not working properly so if it doesn t # just pass the ajax_partial query parameter to your ajax request if self.request.is_ajax or self.request.get.get ajax_partial return partial_table.html return super .get_template_names more info can be found in this recipe at my django cbv guide,normally you can just call parent class methods directly by doing parent.foo self .. but in case of multiple inheritance super is much more useful,unfortunately we can t do this for constants before super is called so those we would need to set up in each method;the reason being is that self does not fully exist yet,super is the receiver not the parent of its class;just like self is not the receiver s class,put yet another way super .__init__ is a bound method and so the __init__ function s first parameter self has already been supplied;super does not return a class,"
"getter","setter","less overall, are not in property value name, but not overall,general in fields field public,more than  overall,listener property with  in property value name, makes code more overall, does not in property value name, cannot in property value name, is more overall, is more complex than just overall,","1063920,4063990,7367011,50434751,53251647,50791930,28161148,50028326,32699996,24655947,18168682,","but then your setter is less restricted than your getter which is odd.,and as already noted setter getter are not implemented in ie;also i m not sure about how wise it is to have a setter that is the same name as the property it sets,yes as the other answers state the getter are the same;the options retain copy and assign determine how to generate setter but not the names even of those,then since your gui int fields are public you could use those directly but i prefer adding getter and setter methods,it is abstract in composition so i can override it in the children now i can add override the method in the track which takes it as a parameter and finally i make the album whose duration is the sum of the tracks it works as intended but i do not understand why i cannot simply use since in kotlin properties are nothing more than getter and setter i m thinking about the getduration in composition,add listener property with getter and setter in your component controller,the jit may even be able to recognize that the same register can be used for both purposes so if using such a getter makes code more readable that s a substantial argument in favor;unfortunately the inability of struct members to indicate whether or not struct members modify the underlying structure makes the underlying structure impossible to use the same approach for an indexed property setter,it should just record the new value and let the getter make the conversion;i agree that the setter does not appear to be useful at all,keep in mind that getter setter cannot have the same name as properties that you set in the constructor;you will end up exceeding the maximum call-stack with infinite recursion when you try to use the setter to set that same property,if we put we filter in between these calls either in getter or setter setter is more efficient as it is called only when filters change we modify original filtered list with we filter and return our filter back through the getter,or maybe a getter is more complex than just getting the value of a variable maybe it does some calculations;if you didn t have to use a setter and could just access the variable directly some developer might miss the fact some developer re supposed to use a setter for the above reasons which would obviously create issues,"
"getter","setter","general overall, is not in property value name,better than having  overall, is more overall,new fields with  in fields field public, do no more in fields field public, is much more overall,call  after  overall, is not in property value name,more than fields  in fields field public, does cause something overall,","53604929,1539011,5401619,56478993,52773845,251995,55656865,53315143,54626863,55423791,15514488,","i prefer having setter getter,in that case flex internally calls the getter to make sure that the value to be set is not the existing value;if the current value of the property as returned by the getter is same as the value to be set the setter is not called,this way is better than having getter and setter in base of performance not to have reduntant code of two methods getter and setter,if we re just going to cast to the expected type we could use  a dictionary of getter with the property name as the key would look like this to store the getter for a property propertyinfo discovered by reflection storing setter is more complicated,i added only new fields with getter and setter and annotations to model in order to create the second table,a getter without a setter is more like a public final member variable--but at that point why not just use a public final member variable setter and getter do no more harm.,using annotated pojos with minimal getter and no setter is much more oo restoring encapsulation and with the possibility of immutability,how do i safely call setter after getter chain eg foo.getx .gety .setz ...,another way around this is to not set the property value in the constructor but instead to define the property with a getter and a setter in fact in this case a setter is not needed as a read only value is helpful;as we are using a getter this is not a normal property and is not set in the constructor,i m currently trying to remove all business logic from our database model classes so they ll contain nothing more than fields getter and setter,invoking the getter does not cause something to become defined;invoking the setter does cause something to become defined,"
"getter","setter"," does not overall,more overall, is asked to do so in property value name, is more overall, aspect is a little overall, recolor is more overall, aren overall, should not overall, should retrieve the value in property value name,more overall, methods aren overall,","11808944,33041507,17017726,17563061,495913,2382460,2143664,41292136,17301399,28294911,19057126,","the .css getter does not retrieve .style propeties it retrieves getcomputedstyle or .currentstyle in ie;so it is not symmetric with the .css setter which sets .style properties on the element,note that the test bean has a very special design as the getter returns a more general type number than the setter requests integer,i think the getter is a better place;if the setter is called first - well then the setter will set the property to whatever the setter is asked to do so and nobody cares what the previous value of the property was,so i think before following the convention of getter and setter the convention of getter is more important to follow the convention of variable naming,now if this logic was in a viewmodel then i think the getter aspect is a little more forgivable expected,for example if car has the color property it s acceptable to let clients observe it using a getter;if some client needs the ability to recolor a car the class can provide a setter recolor is more clear name though,as far as i know there is no way to use the old prototyping methodology for getter setter infact i believe the grand total of internal flex sdk classes that use any type of prototyping is 0;in any case getter setter aren t available in the prototype-hacking game,getter should not be chainable because they need to return the referenced value;setter can be chainable,getter should retrieve the value by their name and explicitly make type conversion;setter shouldn t care - qvariant will,if the calculation is expensive or the getter is executed more often than the setter for the changeable properties playername by far it can be an optimization to change the property inside the class into a read-write property and set the value every time the changeable properties are changed,here i d recommend either putting the logic to calculate the volume on the getter and using only that method or having the getter call volume in case you need the latter in another part of the cylinder class;your setter methods aren t doing the validation because you re not calling them at all,"
"getter","setter"," functions isn in property value name, only works in newer overall, that returns queryfiltre value in property value name, is more problematic however overall, that looks something in property value name, doesn overall, is named setemail then overall,general overall,lower in fields field public, using the access_type overall, is a little more in fields field public,","42943578,18387458,56408532,56191313,49562115,55369492,5886753,35229490,35545423,28219895,19306349,","in other words you can always access attributes so the concept of getter setter functions isn t needed;you can use property though which acts like a way to implement a getter with an underlying value,so first yes there is a plugin for knockout that uses getter and setter but a plugin for knockout that uses getter and setter only works in newer browsers,i have a parent class that has a setter that returns queryfiltre value and a getter that is supposed to pass the queryfiltre value to my child class,in any case the getter is solved easily enough by using # instead of;the setter is more problematic however,adding getter wouldn t interfere with your goal to validate an item before it is added to the list;also if you wanted you could also add a setter that looks something like this so the items would still be validated,your getter setter doesn t follow standard java bean naming conventions and thus spring is not able to find getter for coursecode;getter should be getcoursecode not getcoursecode,el accesses properties by getter not directly by the field;if your setter is named setemail then your getter is likely named getemail so the property name is really email not email,so i recommend using getter and setter unless you have good reasons not to do so,you can override the setter s level to be lower than the getter s level with either private set or internal set,if you are using jmsserializer with fosrestbundle the serializer usually accesses the values of your object using reflection so the getter aren t touched;you can however set the access type for the property or all of the properties in the object to the public method getter setter using the access_type setting,the setter is a little more complicated;the getter first checks whether the backing field already has the correct value,"
"getter","setter"," are declared explicitly in property value name, are not overall, are reasonable etc overall, provides a neat overall,more in parameters it parameter, is more overall, returns a copy in fields field public,typing but with  overall, not overall,private rather than making  in fields field public, is a bit overall,","27804585,54734843,9905033,20057647,14844343,4636530,47839961,6756636,46843321,57501764,30036902,","in c++ cli a trivial property is one where the getter setter is not declared;with a non-trivial property the getter setter are declared explicitly with syntax that s more like a normal method declaration than c# s property syntax,an alternative method to access the child component as early as possible is to associate the contentchild decorator to a getter setter property and to set the child flag in the setter see this stackblitz for a demo;please note that the private variable _child and the getter are not needed to set the flag,using getter and setter gives you more control over the validity of your objects giving you the option of testing values that are set to ensure that getter and setter are reasonable etc,the getter is not always simply return foo sometimes you will want to initialise other objects or perform logging;similarly the setter provides a neat mechanism for validating the new value attempting to be assigned to the variable,if your getter takes more than one argument it s not called a setter anymore,the second getter is fragile the second getter will crash if somebody access s the object s title and then releases the object so the first is generally preferable even if marginally slower;the first setter is more efficient and will work even in situations where an autorelease pool doesn t exist so it s preferable,edit for mutable fields i d recommend using getter setter where the getter returns a copy,yes it s more typing but with getter and setter users can easily see what properties getter and setter users can get and set plus getter and setter are ide s auto-complete friendly,the problem here is likely related to your ability to check if getter setter were actually generated;if your ide tells you getter setter not generated - the ide might be wrong,also i recommend using setter and getter and making your class variables private rather than making setter and getter public,replacing the getter with a macro is straightforward;replacing the setter is a bit trickier since you need to return the correct value,"
"getter","setter"," which makes a program overall, that wouldn in fields field public, are not overall, is enforcing the integrity in state entity integrity,  presumably overall,better than overriding  in property value name,far more overall, are not required are always overall, does not overall, are not overall, is much more overall,","7665891,50327546,46761827,2865849,9418146,56874489,9991138,51424698,42775798,36491695,12728529,","next time you re tediously implementing a getter consider what the caller will do with the data and whether you can just provide that functionality directly;properties encourage mutable state through setter which makes a program less parallelizable,edit following up on the comments above this also means that the properties in the constructor always have to be initialized they can t be computed properties since you can t give they getter and setter that wouldn t use they backing field,well the getter are just fine since one really has to see the contents of the cart and the number of items in the cart;the setter are not fine,getter will not impact the state of the entity - setter will;throwing on a setter is enforcing the integrity of the model,if you have data that needs to be retrieved but under no circumstances should ever be changed use a getter but not a setter;if you have data that needs to be set for internal purposes and should never be publicly exposed and cannot be set at instantiation use a setter but not a getter setter presumably prevents a second call affecting the internal property,how is it better than overriding getter setter of that property,the code in the setter is far more interesting than the getter should it not take precedence and be defined first,the correct getter should be getbmdc and not getbmdc as it is in your code now;the correct model class should look like this the setter are not required are always optional because if there is no setter for a json property the firebase client will set the value directly onto the field,so just store the title so that the getter is able to return it correctly;your setter does not set anything,some pieces of code also use the getter setter specifically and not the fields so making sure your fields have getter setter is a good practice overall;for example apache beanutils.copybean will not copy a bean if getter setter are not set,if you use reflector to look at the implementation of control.visible its getter does this;however its setter is much more complicated,"
"getter","setter","comfortable with   overall, are not overall, but not overall, should have only in parameters it parameter,worse than a  in fields field public,higher overall, is no more in property value name, should not in parameters it parameter, gives you more in state entity integrity,readable with   in fields field public,integer than generate  in property value name,","32402446,16431843,31700858,34340645,12108025,6197415,40382167,47868136,3376978,3609448,22478337,","a team which feels more comfortable with setter getter won t kill the project but you ll have to see a team which feels more comfortable with setter getter every day,generally getter are not doing any alloc or init for you;your setter is a little closer and i assume this was for a property declared as retain,i noticed the plugin had a setter but not a getter equivalent for the value of the editor;this is odd because the normal jquery pattern for plugins that create content with a value is to have the getter be an overloaded paramater-less version of the setter,the getter do not have any parameters passed onto it;the setter should have only one parameter passed to it,a public field is not worse than a getter setter pair that does nothing except returning the field and assigning to the field,the setter complexity can be higher than the getter and thus validate a unit-test,the setter is not called when we set the value this means that the value actually erase the property and this will no more work as intended;the getter is no more called when we access the value again,getter should not accept arguments and return properties;setter accept argument and set property,hiding state behind getter and setter gives you more flexibility to change the internal representation of the class state without breaking other classes that uses the class state,if you judge that the code is more readable with getter setter after fields rather than after constructor you re free to do the code,1 first you need to create one class to fill listview and the element of one class should be whatever you want to display in list item and declare one integer than generate getter and setter method for it like,"
"getter","setter","  just in fields field public, is not overall, is a little more overall, and not overall,","49131704,322081,34571355,19367990,","that an import aspect data should be easily readable with getter setter just as a regular dataframe is,the dbkey property getter is virtual in the il because it is in an interface;the setter is not virtual because it is not part of the interface but part of the concrete class,the getter makes sure that the variables relation is loaded then returns the first value with a matching key;the setter is a little more complicated because we want to update an existing uservariable model if one exists and create one if it doesn t and in either case save the change in the relation but not the db,will generate a setter named setstatuslabel and a getter statuslabel and a _statuslabel;there will be no variable named statuslabel when you use self and . syntax you are really calling the setter getter and not accessing the variable directly,"
"atom-editor","sublimetext","significantly better overall,more recent overall,more overall,choose it over  overall,better github overall,compete with  overall, has picked up again overall,general overall,better performance overall,","22471391,32173336,45607671,49060974,22286530,56001880,22471391,52136347,25491307,","though similar in ui and ux sublimetext performs significantly better than atom-editor especially in heavy lifting like working with large files complex snr or plugins that do heavy processing on files buffers,plus copy as rtf for atom-editor has more recent updates than sublimehightlight for sublimetext,why atom-editor uses more memory and takes more time to start as compare to sublimetext editor,the built in debugger is why i choose it over atom-editor or sublimetext,atom-editor has better github support out of the box but sublimetext has a several git packages,the multiple selections feature is available since intellij idea 13.1 rc and compete with atom-editor or sublimetext similar feature,apart from performance improvements atom-editor feels significantly more stable across the board;development of sublimetext has picked up again since jan 2015 with bugfixes some minor new features tooltip api build system improvements and a major development in the form of a new yaml-based .sublime-syntax definition to eventually replace the old xml .tmlanguage,also if you don t use sublimetext i recommend it over notepad++ atom-editor visual code and others because of proper threading implementations making it much much faster to use.,sublimetext has better performance than atom-editor,"
"debian","ubuntu"," it takes more overall,image smaller overall,more resources overall,same way as  overall,far less conservative overall,newer overall, it is a bit in kernels harder tools, are large enough to already in system external libraries,compatible with  overall,smaller in wheezy smaller recent, is better overall,","50906745,27249910,1316580,639452,5009964,8931660,2985456,46400849,49826834,29638668,48047100,","i have to create a temp table at some point in order to present the data and what is really strange is that on the ubuntu system it takes 400 ms response from browser and on the debian it takes more than 10 seconds,the main advantage of the debian image is the smaller size â it clocks in at around 85.1 mb compared to around 200 mb for ubuntu,ubuntu heavily relies on python scripting and generally consumes more resources than debian,unfortunately this ends up causing you a problem because ubuntu adopted all these packages from debian but doesn t support them in the same way as debian does and debian packagers seem to feel put upon to be asked to support these ubuntu users;so ubuntu user goes to main exim list and is told to ask their packager for help,ubuntu is far less conservative than debian so you will get closer to the bleeding edge,since ubuntu 10.10 is newer than debian squeeze if you can do it on squeeze you can certainly do it on ubuntu,in ubuntu it is a bit easier to install packages for java development but it doesn t really matter that much;remember that ubuntu is based on debian so it works the same,and people do install on people ubuntu system some external libraries so you need to search a lot more than what ubuntu contains and i won t be surprised if ubuntu or debian are large enough to already have conflicting libraries,is it because a compiled python code under debian is not compatible with ubuntu or am i doing a personal mistake like missing library etc.,i believe ubuntu is smaller debian wheezy smaller still or even alpine for tiny start point,some deductions on the matter i understand that ubuntu is based on unstable debian so stable debian is better for production,"
"debian","ubuntu"," kernels is harder in kernels harder tools, not overall, just adds more overall,newer version in version stable newer, has one running overall,more stable in version stable newer,older version than  in version stable newer,nicer overall, wsl is indeed overall,less than  overall,more recent in wheezy smaller recent,","29893268,31097602,2985456,35398351,53991970,4333375,50319496,7764101,53760764,48046260,20931360,","recompiling ubuntu kernels is harder -- recompiling ubuntu kernels will require working with debian build tools,the packages provided by ubuntu debian do not automatically install all dependencies required for every module so i had to hunt down a solution between stackexchange google and the developers website;what fixed this for me note on debian not centos was installing the following packages,emember that ubuntu is based on debian so it works the same;ubuntu just adds more user-friendly gui s,this is because ubuntu 15.10 uses a newer version of libc and libstdc++ which is not available on debian 8,this is quite common with debian based systems whereas most ubuntu has one running at all times,as for debian being more stable than ubuntu for using as server it can be true in very rare occasions where the package is very obscure,debian 9 stable at the moment uses and older version than ubuntu 18.04 stable at the moment and debian unstable,ubuntu has nicer frontend and could be more suitable for windows emigrants while debian is more for backend guys,note the below test results are on debian;testing on ubuntu wsl is indeed much worse,debian 9.3 is current but that probably matters much less than debian being tested less than ubuntu,i m in debian wheezy which is much more recent than ubuntu 10.04,"
"debian","ubuntu"," it seems more overall,older versioning as  overall,familiar with   overall, system contains more in system external libraries, os takes less time overall,","32417160,7945935,39523982,3827002,53506100,","for instance in a debian 6 system i see the mcomplexprogramtarget macro used only in the motif.rules file which in turn is included from cde.rules and that is not included by any of the platform-specific imake files;since it is unlikely that op has motif installed on ubuntu it seems more likely that this was cut paste from some example which was originally written for motif for solaris back in the 1990s when cde was supported,this also might be fixed solved in the git add mechanism in more up to date versioning i vaguely remember tracks files in directories adding tracks files in directories own .gitignore files to empty directories so i d really recommend trying to upgrade to the highest git versioning you can for that and other benefites your distro may provide by default an older versioning as ubuntu and debian do,reference from here link for ntp commands as i am more familiar with ubuntu debian based commands,a debian or ubuntu system contains more python packages but the apt-get packages are specifically tuned for your system,can anyone help me to clear my doubts - why debian os takes less time for context switching although ubuntu is based on debian kernel,"
"illegalargumentexception","nullpointerexception","more overall,less overall,more in easiest reference constructor,longer overall, makes much more overall,better overall, javadoc explicitly overall, is better overall, makes a bit in easiest reference constructor,","46185968,44913918,22074,199290,44248902,7391541,47710,21702054,9372254,","also throwing illegalargumentexception makes more sense instead of nullpointerexception when string has blank empty or null values,disclaimer all this would be needed just to throw an illegalargumentexception which is less idiomatic than a nullpointerexception,in my mind illegalargumentexception seems more specific than using a nullpointerexception since it s telling me that the problem was with an argument i passed to the method and not with a value that may have been generated while performing the method,decoding nullpointerexception is going to take a bit longer than illegalargumentexception filepath must be supplied or whatever,there are some instances of implementations that i have seen for some methods that throw nullpointerexception if argument is null but that is wrong implementation in those cases illegalargumentexception makes much more sense,since it s an exported method clients should get an exception on their abstraction level so illegalargumentexception is better than nullpointerexception,you should be using illegalargumentexception iae not nullpointerexception nullpointerexception for the following reasons;first the nullpointerexception javadoc explicitly lists the cases where nullpointerexception is appropriate,nullpointerexception happens for other reasons as well buggy code;illegalargumentexception is better for invalid parameters.,i find that a nullpointerexception is easiest when the argument is to be used in that method;if you re going to be saving the reference for later in a constructor then the illegalargumentexception makes a bit more sense,"
"glassfish","jboss","more user overall, is harder overall,implementation smaller overall, has better administration in product next project,however better overall,more overall, has more overall,more overall,better in product next project,rich than the  overall,larger community overall,","9943106,278989,13494857,3025706,39194452,9943106,1939838,12638140,14709269,10807350,39194452,","in my opinion glassfish is more user friendly than jboss so i decided use glassfish for my project,i would prefer jboss because of its wider customer base maturity etc;glassfish is harder to incorporate into an automated build deployment process but it might be nicer for some of its specific features if you need them,as for me jboss implementation is smaller than the whole glassfish so i m using,other than that - jboss especially 5.x is an excellent application server;glassfish has better administration console and documentation support for java ee 6 in glassfish 3 and better documentation,glassfish however performs better than jboss and has a very slick gui-based admin console whereas jboss can only be administrated with a command line,but in my location jboss is more popular than glassfish so i have a idea,i had a number of issues with more advanced configuration on jboss;glassfish has more super-high-end entrprise add-ons like ha-database that stores,well glassfish is more right than jboss and weblogic business-bean-classes should all have their own ejb-local-interface,even if jboss is the better product in my next project i will shift to glassfish because of the better documentation,based on your previous question you re using tomcat so jboss tools should do and jboss tools is actually more feature rich than the glassfish server plugin,jboss has a larger community than glassfish,"
"glassfish","jboss"," comes with oracle overall,better j2ee overall,much more overall,more overall,","3782125,283405,3025706,14949370,","glassfish comes with oracle s java ee and is the easiest to install;jboss is more widely used in commercial settings,i think glassfish is a better j2ee app server to start with than jboss,afaik jboss is much more adopted in production deployments than glassfish but this doesn t necessarily mean it s better,glassfish v3 vs jboss 7.0 in using in production environment i know more people use jboss the application server but glassfish has more features and stability over jboss,"
"d3.js","protovis","more in gradients fill sorry,significantly faster overall, has no support in gradients fill sorry, is no longer overall,better option overall,","6218120,12335448,7641031,6218120,3313282,","finally i haven t done much with animation but i think you re entirely correct - d3.js provides more animation support than protovis especially in terms of animated transitions,for semantic zooming you ll notice that d3.js is significantly faster than protovis,you might try d3.js which doesn t support gradients out of the box but offers lower-level access to the svg element which you can use to define an svg lineargradient fill;sorry but i m pretty sure protovis has no support for gradient fills,edit 7 12 11 it looks like there s a new major difference - as of june 28 2011 protovis is no longer under active development and the protovis team is pushing d3.js instead,the team behind protovis has since created d3.js so this is likely a better option than protovis,"
"codeigniter","kohana"," doesn overall, has cleaner overall,more overall,better in little bit programmers, it is easier in little bit programmers,prefer  over  overall, does not have modules just overall,less bloated overall,","8801023,3920125,6053870,9753028,2612738,348548,3157314,1710652,","codeigniter has different class naming;all codeigniter classes begin with ci_ while kohana doesn t use any prefixes,in codeigniter you will probably have to do everything from scratch;kohana has cleaner coding style architecture but the lack of documewntation is crippling this could be solved if you arent shy about using their forum.,overall kohana is more flexible than codeigniter and a great base to build a web application and api on,i ve came to kohana from codeigniter which was a little bit bigger documentation and easier to understood but as far as i found out many programmers say that kohana are alot better than codeigniter,i would personally suggest you to go with codeigniter it is easier and faster than other counterparts;or you may want to go for kohana said to be extended version of codeigniter,i prefer kohana over codeigniter ymmv,kohana is no longer simply a codeigniter fork and has not been for a while so sadly the code cannot be simply copied and pasted across;also codeigniter does not have modules just mvc components libraries and helpers so i m not sure what you would be copying from codeigniter,but if codeigniter has a bit too much bloat for you maybe you can try kohana which is a bit less bloated than codeigniter started as a fork,"
"malloc","strdup"," and not overall,cleaner overall, followed by strcpy overall,simpler overall, is not part overall, not overall,more memory overall,","1632045,40423473,56488935,29989607,53975258,32828887,28931396,","in returning a string the example used the function strdup to allocate a string passed with a np_something method;fact is that npapi takes care of the allocated string from that point on and when tries to destroy it it cannot since strdup uses malloc and not npn_memalloc,or indeed if your system has strdup or you re willing to write an implementation then strdup is much cleaner than malloc + strcpy,what is the difference between case1 case2 case2 results in a crash. strdup is as good as malloc followed by strcpy,you can use either to create a new memory block which is separate from the original but naturally strdup is simpler since it doesn t require a separate malloc strlen call,note that strdup is not part of the c standard;the newemployee function return type is but you are not returning anything from this function if the malloc is successful,the memory in strdup is created by using malloc not new;strdup - duplicate a string,you should know that strdup allocates more memory and returns its pointer which you then overwrite the original pointer returned by malloc so it will be impossible to free that memory since you no longer have the pointers,"
"apache","nginx","better with  in better proxy popular,better in better server use, - not overall,better performance in better proxy popular,faster in faster static files,less memory overall,sometimes better overall,better in better server use,compatible with  in better proxy popular,familiar with  overall, is much better in better server use,","13606542,45025292,24338964,2583350,8103931,1271575,41928658,20529263,11530218,49222342,10784847,","if you have two servers you would be better with nginx on site a proxying all requests to the internal server b with apache db and gearman on the internal server,i think you would like lemp e stands for the nginx and for a lot small projects it would be better than apache,however my experience is that configuring it in apache is significantly more complex than configuring it in nginx and even with worker it still is not quite as efficient with nginx;disadvantages of moving to nginx - not many but things to keep in mind,tornadoweb and nginx are popular web servers for the moment and many benchmarkings show that they have a better performance than apache under certain circumstances,paradoxally it does not mean that apache is faster than nginx it just means that on 1 2 3 .,nginx uses less memory than apache given the size of your setup i would definitely recommend that,id love to know how to solve this since using nginx sometimes is a better option than apache and having this issue with fuelphp framework and not being able to use this two great tools together is awful,nginx is better at handling requests since it does not spawn a new process for every request unlike apache,yes - it s stable and it has much much better performance than apache if configured properly of course;no nginx is not compatible with apache - configuration is completely different,first of all i d like to point out that hiding what powers your web app isn t really worth the time or effort with that said if you really really want to there are a couple of things you could try changing the defaults of apache tell all behaviour with these directives assuming you are using apache not really familiar with nginx servertokens prod serversignature off php will add a x-powered-by http header if expose_php was set to on in your php.ini file,further than that i wouldn t use apache at all if i needed to optimize things nginx is much better as a server a server s even faster than node.js for serving static files for example,"
"apache","nginx"," is slightly safer overall,general overall,better in better uwsgi logfiles, has more in better proxy popular,much faster in faster static files,better in better uwsgi logfiles,more straight forward overall, makes  better in better uwsgi logfiles,mod_php much more stable overall,better in better server use, i feel that something in faster static files,","19466905,32424120,44707193,9163056,1315178,2014241,26239764,11217047,19961943,8103674,52606521,","there are ways to improve apache efficiency by coalescing all your rules into a single htaccess or through the use of location blocks in the config;nginx is slightly safer due to packing less stuff that is built-in and requiring cgi processes for almost every mundane task however,on apache it runs as a plugin module just the way mod_php does;for nginx i m not sure,how can nginx performs better than apache,apache can be just as good but requires building from source and knowing exactly what configuration to use to match nginx;however apache has more features and is a little easier to work with,for example nginx is much faster than apache,nginx also happens to be better than apache at writing logfiles interestingly,the switchover to nginx really had little to do with the rewrite rule and more to do with getting away from apache 2.2 default upstream version for centos however with nginx rewrite rules are more straight forward than apache s imo,nginx vs apache makes apache better,apache mod_php is much more stable and cleaner while nginx will often deliver http bad gateway error coded in 502,if you want to use something in front of it though i suggest you use something like nginx better than apache since nginx is also asynchronous like node and it s performs really well at serving static files,important note i am not comparing the 2 web servers that is not the point of this site but generally they have comparable performance so if https in nginx is 10 times slower than apache i feel that something is wrong in my nginx configuration i want to fix it,"
"apache","nginx","easier in faster static files,better in better uwsgi logfiles,better in better server use,less resource-hungry overall,more clean overall,faster in faster static files,familiar with  overall,better in better server use,faster in faster static files,better in better uwsgi logfiles,better solution just in better server use,","38941986,15646087,20960880,29190250,10318491,15152595,51215705,6021813,37012428,24360855,8839295,","for me apache is easier to use but i prefer nginx as it is much faster,also what makes nginx better than apache in this case,i will say that nginx tends to handle heavier loads better than apache though so take a look at that,i m new to nginx and want to try it out since it apparently is much faster and less resource-hungry than apache but i can t see why this would be such a hard thing to achieve,the syntax for nginx configuration is different but more clean than apache,when i was googling info on nginx it appears to be faster than apache and works well in serving static pages,not totally familiar with nginx or apache log but i think most logs contain a timestamp an http request the document requested and status etc. and an ip address,why nginx might be better than apache when one is long polling and why long polling is better than periodically polling a server,nginx is typically faster than apache but with a low request server it hardly matters,i want to move to nginx since it s better than apache,i ve heard it suggested that i use nginx as a reverse proxy in front of apache and node.js is that a better solution than just nginx,"
"apache","nginx","much more in better server use,more traffic overall,faster in faster static files,general in better server use,much faster in faster static files,lower overall, is very overall,usually better in better server use,faster in faster static files,more experience overall,general in php-fpm mod-php sense,","25898583,7974735,21815739,54197341,14473134,7463387,15014585,21035581,4109998,31800197,12237773,","next you ll find that the threading model of nginx is much more efficient than apache s for what you re doing,apache s design is thread per client while nginx uses the reactor pattern meaning - nginx can handle much more traffic than apache as a web server about 50 times the number of requests,i know nginx has lower memory footprint and little faster than apache in serving static files,so jnlp jar files can be hosted on any http server even not java based apache nginx,i am not an expert in deployment but in my experience nginx is much faster and more friendly to django compared to apache,nginx uses an event based non blocking single thread and the memory usage is relatively much lower than apache,this is valid directive whether you use apache or not;you don t have to use it but you better do because nginx is very good at serving static files,note that in any case for increased security and fast static file css js delivery you might want to add a reverse proxy layer nginx usually provides better performance but apache works as well before the nodejs python server,nginx will definitely work faster than apache,i also know there is nginx and haproxy although i have never used either of them and have a lot more experience with apache,but then apache s functionality is much bigger than just serving rack based apps;so apache s functionality makes sense to replace apache s functionality with something more lightweight for example nginx especially because you deploy apache s functionality once for every host you have,"
"apache","nginx","simpler than the  in better server use,faster in faster static files, how am i in better server use,more in better server use,lighter-weight than  in faster static files,still faster in faster static files, makes a lot overall,worse overall, is faster in faster static files,faster in faster static files,better in better server use,","50971241,13074918,48809189,36973548,34769393,23170094,33802545,33949046,38101854,11217047,6942102,","i found one answer here and it looks much simpler than the apache one react-router and nginx basically you need to tell the web server to forward all requests to your react s index.html and therefore letting react-router handle the routing,as an added benefit nginx can also serve static files much faster than apache and nginx also uses much less ram and can handle much more connections,since it s not running as a listening server i can t use nginx node_cgi is not mature with apache how am i supposed to run this sample,nginx is more performant than apache in most cases with high-levels of requests,nginx s configuration files are really simple and nginx s much lighter-weight than apache,nginx is still faster and i might choose it but apache isn t asleep,but you d be crazy to do so apache uses a thread per connection and websockets uses persistent connections using a whole thread per connection when you re expecting a lot of proxy websockets in apache is a waste of resources a non blocking proxy like nginx makes a lot more sense,i m about to try apache but most write ups suggest that this is worse than nginx at this job,nginx is faster for static files but if you have a cdn there is no need to worry about static content;apache is a great server,nginx works great by itself and will likely be much faster than apache,4 i d like to understand why nginx might be better than apache when one is long polling and why long polling is better than periodically polling a server,"
"apache","nginx","more overall, is lighter in better proxy popular,faster in faster static files, take error response overall, would probably overall, run more in better server use,more efficient in better server use,faster in faster static files,larger as server-configuration  overall,more flexible in better server use,faster in faster static files,","25123234,48977796,43186862,57452719,21981740,50054485,9745974,13451844,55545386,21286932,32964547,","it seems your apache is more busy than your nginx,the general feel has always been that nginx is lighter and easier to configure but on the flip side perhaps isn t as fully featured as apache because of that,i have recently read that nginx is faster than apache,you can have nginx take error response codes coming from apache in consideration and act differently by use of proxy_intercept_errors which combined with error_page can allow you to rewrite response codes error messages from apache to masquarade app failures as service unavailable;nginx will not alter the 500 from the app as long as it doesn t step on a problem contacting fetching data from apache,ulling numbers from thin air for illustrative purposes serving 10 000 simultaneous connections would probably only cause nginx to use a few megabytes of ram whereas apache would probably consume hundreds of megabytes if it could do it at all;nginx is faster at serving static files and consumes much less memory for concurrent requests because nginx is event-based it doesn t need to spawn new processes or threads for each request so it memory usage is very low,you can the apache run more than one programmer to listen the same port but if you just want to a map 80 which you open to the public to the backend server you can use a nginx as your delegation server,nginx or other server is not much more efficient than apache,you can serve it from for example nginx varnish which are usually faster than apache,using your own server for authentication if you dismiss my advise to use the google-servers for authentication then the scope of the tagged issues is getting larger as server-configuration apache nginx ... and server-side programming-languages php python ... might be involved,also you should consider using nginx as server that s more flexible than apache,for example nginx is considered faster than apache and a nodejs application is considered faster than a php application,"
"apache","nginx","better in better server use,better in better uwsgi logfiles,faster in faster static files,more in faster static files, does not overall,less memory overall,better than  in faster static files,more in file picky names,better in better server use,faster in faster static files, timeouts see point overall,","12379123,10204877,3952446,4113570,54419034,148194,3602188,36952147,1469813,19810803,57452719,","the remote server is set up to allow very few maximum concurrent clients generally nginx handles many concurrent clients better than apache since it doesn t need to fork a new process for every request so if it becomes an issue you might want to look into switching web servers,uwsgi is better if you are using nginx i find nginx far better than apache personally,for static file serving i found nginx performance lot faster than apache,apache is more popular and has more features nginx is smaller and faster and has less features,nginx apache are general purpose web servers;apache does not know how to serve rack applications and puma doesn t know how to do a bunch of other things nginx apache do cgi scripts url rewriting proxies balancing blacklisting... rack is a library for ruby that accepts parsed http requests from an app server funnels them through a configurable stack of middleware such as session handling passing the request object to a handler and returning the response object the app server making web development in ruby easy,servers like lighthttp and nginx can handle large amounts of traffic in much less memory than apache if you can sacrifice apache s power and flexibility or if you just don t need those things which often you don t,nginx s static file performance is better than apache;apache mpm-worker is much faster than mod-prefork if mod_php isn t needed,nginx is more picky about case of file names than apache,nginx with its own mod_wsgi seems to perform even better than apache but hey you didn t ask about that,nginx is faster than apache handles slow clients better and is generally easier to use,if apache is completely down you should be getting a 502 bad gateway because in your setup apache is the gateway for nginx;the same will happen if nginx does not like apache s response in a way when apache sends a response which has headers exceeding nginx s proxy_buffer_size yes you should be getting 504 gateway timeout when apache app is timing out in relation to nginx timeouts see point 2,"
"apache","nginx","slower in faster static files,general overall,more friendly overall,faster in faster static files,easier configuration than  in better proxy popular,easier configuration than  in better proxy popular,better in better proxy popular,better than  in better uwsgi logfiles,better in better server use, is probably more overall,general overall,","45766594,56729986,12509562,8268449,9951705,9951705,8103931,50173981,8103931,28704448,1779555,","i would really like not to run both apache and nginx i did switch everything to apache yet found it loaded my proxies slower than nginx,if you need to use 443 in production i highly recommend using apache or nginx to listen on the ssl port and proxy the traffic to your .net code,apache is more friendly configuration wise htaccess htpasswd rewrite rules etc nginx is fast in regard to static assets,very fast static assets nginx is faster than apache at serving static assets css js images ... and uses very little memory to do so,setting up https in apache isn t too hard and then just use modproxy to send requests internally to your play application;any one of the reverse proxy systems can likely do this nginx is popular too and generally has easier configuration than apache but i ve never used it with https,any one of the reverse proxy systems can likely do this nginx is popular too and generally has easier configuration than apache but i ve never used this nginx with https,now back to the question my guess based on reading tests published here would be that the proper multi-thread apache architecture should scale better than nginx on multi-core cpus,i think nginx is much better than apache,16 cores apache would scale better than nginx while nginx would process more client requests,in fact many dns and web host providers actually provide this functionality as a service and will host the equivalent of above configuration for you though not necessarily with apache;nginx is probably more likely for this purpose these days,i ve used apache passenger and nginx passenger on a vps and memory usage was better with nginx;i didn t do any benchmarks with my particular app but nginx just feels faster,"
"apache","nginx","recommend  over  in mb ram memory,more efficient overall, as-is is more overall,faster in faster static files, environment; needs more overall, or also overall, which uses much less in php-fpm mod-php sense, is a much in opinion works kalid-rafik, is good choice overall,such as   in better server use,better in better server use,","57668722,2899344,53823289,11462284,26812634,20110614,12795730,39781764,49473691,55414498,44707193,","if you still want to run with less mb of ram then i d recommend nginx over apache,in fact nginx can use select instead of epoll if you compile it with the --with-select_module option and i bet it will still be more efficient than apache,the fact that what you want to do works in apache as-is is more of a bug than a feature this works differently in nginx by design and in order to prevent a whole class of security vulnerabilities,there seems to be a consensus that nginx serves static content faster than apache,this works but only in a default apache environment;nginx needs more configurations,much safer to have apache or also my preferred nginx listening on 80 and passing along the requests;nginx is friendlier for sockets as well and someday you ll want those,a server which reserves a thread or an entire process per connection such as apache with the usual mpm_prefork or mpm_worker keepalives are usually disabled entirely or kept quite short a few seconds;for an event-based server such as nginx which uses much less memory per connection the keepalive timeout can be left at a much higher value typically a minute or so,here is a setup i use to redirect blog to a meteor server i don t use passenger with apache;this works but i must agree with kalid-rafik that nginx is a much easier solution,nginx apache both are nginx is faster response than apache nginx have different rewrite rule with laravel nginx is good choice nginx have fastcgi so you can use same like apache here is 1 understanding link sorry for my bad english hope this help you,so i understand there are two kinds of servers first kind is web server or http server such as nginx apache caddy another kind is app server such as tomcat undertow,i aways heart that nginx performs better than apache server,"
"apache","nginx","better in better server use,less memory in mb ram memory,set  to use overall,faster in faster static files,lighter option than  in better server use, which is faster in faster static files,scalable than  in better server use,better in better proxy popular,faster in faster static files, is very in front recommend end, does not overall,","9609477,5244789,53904458,35488326,16728328,11570526,55507825,35898440,535933,3319405,46953262,","apache made a claim that apache 2.4 will offer performance as good or better than nginx,also nginx uses a lot less memory than apache,anyway i prefer using nginx instead of apache you can set nginx to use sockets which is how puma starts by default and there are more tutorials for nginx+puma there s the config for nginx on that link too,i read that nginx is faster than apache and i want to try it with some of the projects and leave the others with apache,just one tip if only job for the http server will be just proxying the play apps consider using some lighter option than apache for an example nginx or lighttpd you ll find sample configurations for all of only job for the http server will be just proxying the play apps in play s documentation,you can create custom controller to serve that files and use send_file mentod in send_file mentod with disposition inline also you may be interested to x_sendfile_header setting to serwe that files with apache nginx which is faster,i chose nginx because i ve read that it is lighter and more scalable than apache and can be used as a web server or as a reverse proxy,i have no idea what i m doing so i dove nose-first into nginx -- which i had never used before -- because someone told me it s better than apache at dealing with lots of small tasks and requests -- not that i would know how to turn apache into a proxy mind you,not only is nginx supposedly faster than apache at delivering static content but this also offloads your rails application for every image stylesheet javascript or whatever other static content,apache isn t really a typical setup for python in my experience;python webapps are usually daemons that are exposed to the public with a reverse proxy webserver in front nginx is very common,nginx does not have the 256 connection limit that apache does due to an asynchronous approach to handling connections;not sure what requirements your server has but a basic nginx swap from apache is pretty simple and can be done following step by step tutorials online like this one,"
"apache","nginx","better in better server use, is better but not really in front recommend end,more overall,lighter overall, assumes a leading forward overall,ssl encryption between  overall,better than  in faster static files, which is a web in better server use,faster in faster static files, mod_wsgi is still in better server use,faster overall,","24561939,27408468,7392125,10576127,29227762,50282968,50466361,23526485,22224559,13509171,19719946,","nginx + gunicorn is likely to work better than apache + modwsgi at this point in time,i wouldn t recommend putting apache in front of node however;nginx is better but not really necessary anymore,apache is more flexable then nginx but it comes with a high price in performance,nginx is very fast much lighter than apache,apache assumes a leading forward slash;nginx does not,mail.domain.com-le-ssl.conf generated by let s encrypt my apache2 virtual host config for reverse proxy mail.example.com.conf please note that my original apache2 config file is because i don t really need ssl encryption between apache and nginx the rewrite rule was added when i installed let s encrypt certificate on apache mail server for mail.example.com,nginx would be easier to configure also it will be better than apache when it comes to serving static content like images and css files and so on,if you want to use .htaccess files you will need to install apache which is a web server using a different heavier architecture;nginx doesn t support .htaccess files that is an apache feature,i will add that i ve often heard that nginx is faster than apache for serving static files to the point that it s sometimes worth using nginx for static files and reverse proxying to apache for dynamic content,apache mod_wsgi is still a more than acceptable solution and it will actually perform better with less resources when run as;nginx - apache mod_wsgi,hi everyone i need help with this i ve already set up nginx + php and it works great faster than apache my problem is how to setup a multiple projects in one ip,"
"apache","nginx","faster in php-fpm mod-php sense, doesn overall,general overall, is not a requirement then overall,flexible than  in front recommend end,much more in faster static files,easier in better server use, to run wordpress overall,faster in better proxy popular,faster in better proxy popular, isn in faster static files,","29403998,1041539,50459703,14659261,2912577,14091869,27824470,42055758,32573222,35636877,53102758,","is nginx + php-fpm is suppose to do server operations much faster than apache + mod-php due to efficient usage of memory and other resources,thus using keep alive on the apache side is pointless when using nginx in front of it;because though nginx doesn t implement keep alive it should drop the connections promptly as it will have no intention of reusing them so you shouldn t suffer if you do forget to turn of keep alive in apache when using nginx as front end,possible hack could be run apache nginx on that ec2,if apache is not a requirement then you can use another server for example nginx;there is a tutorial for getting nginx running on an ec2 environment,also nginx s configuration is much more flexible than apache and by having it on the front end it gives you a lot of flexibility,serving static files with nginx is much more efficient than with apache,but doing that kind of things with apache is rather painfull and is easier with nginx,i have read that you need apache to run wordpress but this is false;nginx does not use .htaccess,nginx is faster and lighter but many people find it easier to work with apache because of .htaccess support nginx does not have an analog due to performance concern,i have configured tomcat with apache web server in past and never slowness problem before and practically speaking nginx is said to much lighter and faster than apache web server,at least in the past when i evaluated tomcat is much slower at ssl than apache nginx isn t as fast with static content requires redeploys of the war file if you change static content and lacks the configuration options of the more commonly used http servers,"
"apache","nginx","faster in faster static files,less in better server use,quicker overall,more complicated in better server use, automatically sees the location overall,simpler overall,faster in faster static files,slower overall,faster in faster static files, makes no sense in php-fpm mod-php sense, is more in faster static files,","15748792,25819090,35117899,28805780,4150778,41745892,27358402,20715542,25406562,57579718,6504717,","and finally from my experience nginx is faster than apache,also when your setting up the server i would say start with nginx it s uses less resources than apache,nginx is quicker though honestly you ll struggle to see the difference except for very high volume sites and has quickly become the web server of choice for a lot of people if you don t have any specific need for apache for your code,i am not going into how to do rewrites on nginx because it is much more complicated than apache,apache automatically sees the location header in the response and forces the response code to be 300-series if you haven t previously set a response code of your own;nginx does not do this -- it expects you set the proper response code yourself,i ve been attempting to upgrade to php 7.1 using phpbrew and elected to install it with nginx as i read everywhere that it was simpler than apache not that simple in my humble opinion,nginx â without any optimizations done â is much faster than apache,we have a few clients who have very high traffic sites running apache slower than nginx with varnish in front of it and they get way more traffic than you are saying with little to no performance problems,nginx is faster than apache and the configuration is easier,and finally if you are beginning with docker etc i recommend to use mod-php with apache which is easier to setup than php-fpm with nginx considering php-fpm with apache makes no sense,having deployed django behind both apache and nginx in windows i have to say that i found nginx to be infinitely easier;however since nginx is more of a static file server with excellent proxying capability i ran a separate wsgi server for the django app,"
"apache","nginx"," is the future overall,simpler than  in file picky names,better in better proxy popular,easier in better server use, is not in better server use,faster in faster static files,better in better server use,easier in opinion works kalid-rafik,better in better server use,less memory overall,","50654007,51342176,9550924,35685489,24568379,12509474,7181330,40213903,9287841,7224338,","apache has been the leader in web server ecosystem for 20 years and is much more popular however nginx is not without its fair share of advantages;while apache is a thing of the past nginx is the future of web apps and websites,you could also try using caddy web server as a reverse proxy it is the simplest to manage that i ve come across and the config file is much simpler than apache or nginx,although this thread is more than a year old it still merits from the fact that no one mentioned about the not so recent innovations of nginx lighty and other web servers which scale much better than apache at higher concurrency and consume lesser resources,nginx can be configured to only respond to requests matching a predefined pattern far easier than with apache,the number of concurrent requests are limited by the number of children threads yes but apache is not spawning a new thread child for every request which would be ridiculously slow even with threads creation and teardown for every request would be way too slow;nginx uses a master-worker model,i ve read that nginx is faster for static pages but there are questions about its performance with php compared to apache s performance with php,i recommend you to use nginx as an reverse proxy since the configuration is easier and the performance is much better than apache,nginx is actually quite a lot easier than apache in my opinion,nginx is better suited than apache as it is light and single threaded vs apache thread per request in most normal setups,also have a look at nginx for example it is fast and uses less memory than apache to handle client connections,"
"gmagick","imagick","newer overall,better overall,more resource overall,","8643574,13630798,8642900,","gmagick is newer version of imagick with more set of features it is less resource intensive and fast but the problem is there is very few discussion about this wonderful tool on web i recently came across this on,update graphicsmagick is faster than imagick doesn t means gmagick is better than imagick,imagick is more resource hungry than gmagick but when i tried to use gmagick it is not working properly,"
"vb.net","vb6","more overall,more strict overall, doesn overall,better overall, and is autoamticaly in setstandard visual studio,better language in better use com,better use in better use com, doesn overall, is a type overall,result more overall,version less capable overall,","7017638,12501075,51149525,2983944,16754371,38377745,18437545,24181029,4722445,30670540,12909426,","especially as vb6 is becoming more and more of a distant memory and the vb.net language takes on a life of its own in conjunction with the core .net framework advancing,none of the .net libraries are there and there are some syntax differences as vb.net is more strict than vb6 is,vb.net doesn t support byte-oriented string methods;vb migration partner provides the chrb6 replacement method which approximates the original vb6 method s behavior but isn t guaranteed to work well in all circumstances,i assume the same control in vb.net would be datagridview and i m fairly sure that you can probably make it look better than in vb6 but if you want really good looking ones you might want to look at thirdparty grid controls,i suppose it is about vb6 not vb.net because setstandard s .;must be setstandard s ... in vb.net and is autoamticaly replaced by visual studio,vb.net is a better language than vb6 use its com capabilities to save you from writing endless sketchy vb6 code,if you don t already know vb6 there s little point in learning it now - your time could be put to much better use by learning vb.net c#,don t expect vb.net samples to work in vb6;the two languages are incompatible on a number of levels not just because vb6 doesn t use the .net framework,vb6 was interpreter based language while vb.net is a compiled language;vb6 was not a type-safe language while vb.net is a type safe language,can i safely assume that vb.net result is more precise than vb6 and discard the vb6 result completely,yes they are different but you can hardly say that the vb.net version is less capable than the vb6 version,"
"vb.net","vb6","better overall, is actually overall,rubby easier overall, does not in setstandard visual studio,more feature rich overall,application more expensive overall,tools more overall, is lot easier overall, is not overall,certainly easier overall, is not overall,","5862179,18238760,4309687,435831,12262553,3326639,1101923,15249588,43124243,19501727,783534,","fortunately vb.net finally ended all that and is completely pixel based you can still alter you viewport scaling but .net seems to handle that much better than vb6,vb.net is actually the name of two rather different languages which are selected based upon the option strict setting;the option strict off dialect which is unfortunately the default was designed to facilitate porting of vb6 code and has horrible goofy semantics which are even worse than those of vb6 i m not sure even its designers know all of the odd corner cases of the way different types interact,also developing enterprise application is such new languages java c# vb.net rubby is easier that vb6 because they rely on frameworks every body can write c# java code that function but it requires tricks good practices and some of imagination to write vb6 strong and rehusable code,if costs are a constraint you could try this there is a wizard in visual studio that will attempt to upgrade vb6 to vb.net;it s not 100 accurate and you will have to write code for things vb.net does not support such as control arrays etc,quite apart from the fact that the language vb.net is far more feature rich than vb6 the fact that you have developed in vb.net means that you have made extensive use of the .net class libraries including system.security.cryptography that you give as an example,biggest of all is that adding new features to your vb6 application is more expensive than it would be if the application was in a better language like c# or vb.net,however i ve noticed that running the vb6 tools is becoming more and more painful over time so i m looking at the possibility of migrating this code to vb.net 2008,avoiding too many pain points by modifying vb6 is lot easier than fixing the issues in vb.net,you ll have to use vb.net define an autocad command;no sorry vb6 is not supported in forge design automation,getting your project migrated to vb.net is certainly easier when you keep the old vb6 controls,i m unusual in that i come from a c c++ background know c# but actually prefer vb.net i severely dislike vb6 vbscript;i say all this because it s important to remember the vb6 is not vb.net,"
"vb.net","vb6","much more overall, and cannot overall, doesn overall, is not overall,compatible with  overall,","30921042,16192727,15275739,1946820,57131670,","vb.net appears much more complex then vb6 and the learning curve is tough,the semantics of the operator in vb.net are just a bit different from those of c# and the standard object.equals;the semantics are inherited from vb6 and cannot be changed for backward compatibility reasons,many people when coming from vb6 to vb.net miss control arrays;but in reality it s the other way round vb.net doesn t have control arrays because it doesn t need them any more controls can be put into normal arrays and collections in vb6 control arrays were a hack to work around the fact that you couldn t create normal arrays of controls,that said if you re using vb.net i would suggest the correct way to use resources is always within a using statement;if this is actually vb6 as originally described in the question then you need tear down the file object and release memory because vb6 is not garbage collected you need to do this explicitly,you re almost there but your current code has two issues first of all your p invoke declaration for the ntsetinformationthread function is not quite correct and i recommend you stick to dllimport as most declare function declarations that you find on the internet were written for vb6 and are not compatible with vb.net,"
"and-operator","or-operator","higher overall,stronger overall,","20844229,16360333,","as far as i know the and-operator has the higher precedence than || or-operator in most of the languages,which will never be true have in mind that the and-operator binds stronger than the or-operator,"
"ipb","mybb","less in average queries slower,less in average queries slower,","9600557,9600557,","the ipb which has less queries runs slower than mybb with more queries,the ipb one has less queries used only 14 on average but it runs slower than mybb with more queries used average on 20,"
"datediff","difference"," is greater overall,more difficult overall, is greater overall,larger overall, doesn overall, is greater overall,","52960533,81983,51348932,16333387,21795178,57483699,","the datediff curdate valid_upto will return a negative value until the valid_upto date comes;when the difference is greater than 0 that means your subscription has expired,datediff becomes more difficult to use as you have more dateparts in your difference in your case looks like minutes and seconds,i want to find the difference between two dates which should be exact in months like if the date difference is greater than 182 days them on 183rd day it should show as 7 months.i tried below one select round cast datediff dd 2018-01-01 18 45 30.203 getdate as float 30 0 but it has 15 days difference,note that diffseconds can cause an overflow when the difference becomes larger than int32.maxvalue error the datediff function resulted in an overflow,you can use datediff instead in this case but it s worth being aware that datediff doesn t always do exactly what you might expect it to as it s about boundary crossings;not much of an issue here but it means that the difference between say september 1st and august 31st is 1 month as is the difference between september 30th and august 1st,you ll need lag and datediff to get the number of minutes between tran_date and the previous row;then basically a running total on the evaluation of whether or not the time difference is greater than 30,"
"owl","rdf","more mechanisms overall,more structure overall,more expressive in richer languages schema,more overall,richer in richer languages schema,","34639846,31269752,15074623,21358258,6004853,","owl provides more mechanisms for asserting shapes of rdf graphs as does new work on rdf shapes,owl has more structure than rdf,owl and rdfs are more expressive than rdf which means here that you can capture more complex relations for instance you can also represent the link between sets of things rdfs subclassof or use transitive properties hasancestor,owl is a more advanced language than rdf,owl is richer than languages such as rdf schema rdfs,"
"many-to-many","one-to-many"," associations is always better in parent instead associations, relationship seems better in parent instead associations,more appropriate overall,","29643115,23671780,39240823,","using two one-to-many associations is always better than relying on many-to-many relations,just an advice since you separate mother and father from parent i think you d better avoid a many-to-many relationship;instead a one-to-many relationship seems better,iâ ve read a so questionâ s comment where inserting order# 68 as pictured wouldnâ t cause any trouble but if i wanted to query orders by certain tags a many-to-many is more appropriate convenient efficient since otherwise in a one-to-many every single order will have to be checked to know how its tags is this true,"
"avx","sse","newer overall,faster in slower version performance,faster in faster, version is bigger overall,version considerably slower in slower version performance, is clearly faster in slower version performance, doesn overall,faster in slower version performance,faster in haswell l1d cache, is slower overall,more time overall,","18319880,34069001,47115510,31768844,29031187,18697383,55083822,42324992,31476517,31476759,18691322,","also note that the fact that the avx are a newer than sse doesn t make the avx faster whatever you are planning to use the number of cycles taken by an function is probably more important than the avx vs sse argument for example see this answer,as expected the performance got better with both and avx 2 faster than sse 4.2 but when i profiled the code with papi i found out that the total number of misses mainly l1 and l2 increased a lot,i expected avx to be about 1.5x faster than sse,either it s a problem with mixing sse avx without vzeroupper maybe you compiled the rest of your code with or something and double-precision math is using avx;or your sse version is bigger and causes i-cache misses,i have code that does the same thing but the avx version is considerably slower than the sse version,now for sse is clearly faster and for the smaller values it s nearlly as fast as avx,we have to mask anyway for widths lower than 16-bit sse avx doesn t have byte-granularity shifts only 16-bit minimum. benchmark results on arch linux i7-6700k from njuffa s test harness with this added,the question is avx scalar is 2.7x faster than sse when i vectorized it the speed up is 3x matrix size is 128x128 for this question,so congratulations - you can pat yourself on the back your avx routine is indeed about a third faster than the sse routine tested on haswell i7 here,i know this does not answer your core question why avx is slower but since your ultimate goal is fast popcount the avx - sse comparison is irrelevant as both are inferior to the builtin popcount,buf1 buf2 and buf3 is small enough to located in l1 cache and l2 cache l2 cache 1mb .both of sse and avx is band width limited but with the datalen increase why do the avx need more time than sse,"
"avx","sse","better than  overall,more overall,faster in haswell l1d cache,slower than   in slower version performance,faster in slower version performance,more overall,faster than  in faster,","28030333,16034841,28030333,47568731,29032199,45770089,53042621,","ironically ancient x86 instruction rep stosq performs much better than sse and avx in terms of memory copy,the underlying reason for this and various other avx limitations is that architecturally avx is little more than two sse execution units side by side - you will notice that virtually no avx instructions operate horizontally across the boundary between the two 128 bit halves of a vector which is particularly annoying in the case of vpalignr,for small buffers hot in l1d cache avx can copy significantly faster than sse on cpus like haswell where 256b loads stores really do use a 256b data path to l1d cache instead of splitting into two 128b operations,rep string instructions and especially the non-rep versions re good for code-size but often slower than sse avx loops,so the avx version does indeed appear to faster than the sse version both for the original implementations and the optimised implementations,and simd math libraries for sse and avx however they seem to be more sse than avx2,so i expect that avx could be faster than sse,"
"hadoop","sqoop"," 1 is older fully overall,better overall,more than 3  overall, 1 was originally overall,","14886668,40234150,48683236,42809206,","sqoop 1 is older fully functional and mature project supporting hadoop 0.20 1.x 0.23 and 2.0.x you can download the bits from here,can anyone give some details about oracle hadoop connectors will it perform better than sqoop,i have more than 3 hadoop batches including sqoop hive pig jobs and these batches are scheduled sequentially,sqoop 1 was originally designed for moving data from relational databases to hadoop;sqoop 2 is more ambitious and aims to move data between any two sources,"
"datatables","jqgrid","better in customisation someone better,better customizations overall,better in customisation someone better,","4361920,7798213,15838894,","if you don t have someone around to help with jqgrid you are better off with datatables,datatables has better customizations as far as clientside bells and whistles where as jqgrid is a little harder to make look as pretty but can do some pretty good serverside interaction from what i understand,it turns out jquery datatables is better suited for customisation and we have adopted this instead of using jqgrid,"
"fread","read.table","faster base in read_csv data.table base,faster overall, is significantly quicker overall, is much faster overall,faster in read_csv data.table base,faster in read_csv data.table base,more time overall,","45595542,27792611,20940609,52625357,24966794,26359766,20940609,","for reading large csv files you should either use readr read_csv or data.table fread as both are much faster than base read.table,you might want to give the data.table package a try check out the fread function which is much faster than read.table,i will reiterate that fread is significantly quicker as is shown in this post on stack overflow quickly reading very large tables as dataframes in r;in summary the tests on a 51 mb file - 1e6 rows x 6 columns showed an performance improvement of over 70 against the best alternative methods including sqldf ff and read.table with and without the optimised setting recommended in the answer by lukea,btw fread is much faster than read.table,fread ... is extremely fast 10 - 100 times faster than read.table ... or read.csv ... for large datasets,fread performs faster and more efficiently than read.table but read.table produces less no errors on the same data set,interestingly for 1 million rows per file the optimised version of read.csv and read.table take 422 and 430 more time than fread whilst without optimisation this leaps to around 1500 and 1005 longer,"
"centos","debian","less overall, is still overall,better overall, takes much longer overall,older overall,","20916262,53378725,30323767,49037742,13459028,","he says he uses debian and is less familiar with centos and so i am posting here,for example the openjdk 11 on debian is still in buster testing and sid unstable and therefore not available in any stable branch;i guess in centos i am not that familiar with it tbh. it s the same situation,we recently have migrated redmine from a server which was running under centos 5.5 with the same ror environment the same load the performance was a way better than on debian 7,centos or debian takes much longer,note that centos software versions are older than debian s but versions of opensuse software are newer than debian s versions,"
"gitolite","gitosis","more overall,far more advanced overall, is no longer actively overall,more complete overall,better in best better,longer overall,easier overall,better in best better, does not overall,","4582514,3223137,6577063,4173070,7070048,5804015,23043305,17706974,5522964,","gitolite is maintained has a dozen features more than gitosis and the author actually responds to emails,gitolite is far more advanced than gitosis and is quite easy to install directly from your workstation based on ssh communication with your git repository server,gitosis is no longer actively developed;gitolite gives you more features,that being said i find gitolite much more complete than gitosis like all those recent blog posts illustrate,for this the best option is gitolite better than gitosis,gitosis which is no longer in active development and you should be using gitolite but this answer applies to both of them stores each user s public keys in in your case gitosis .ssh authorized_keys,either way gitolite is easier to maintain and more up-to-date than gitosis see how do programs like gitolite work,i know gitolite better than gitosis,i m not sure if this will be of any help to you but it s worth noting that the successor to gitosis gitolite has built-in support for wildcard repositories documentation here;according to this blog post gitosis does not support wildcard repositories,"
"django","web2py","more overall, is not actually overall, may as overall, does better in administrators explicit focus,lower overall,newer than  overall, world; probably overall, probably has more overall,easier overall,more in administrators explicit focus,more heavy overall,","1250937,6961688,2019939,2804311,4352956,2795807,6960666,6960666,196705,7913630,1477841,","and web2py is more lightweight than django rails whatever on pretty much all counts,i ve used codeigniter for php and find that web2py was so much simpler and easy to understand;also as for the directory structure django is not actually true mvc -- the directory structure django s mtv model template view,i think i prefer web2py because web2py does more for me that i don t have to do but from the pov of say java frameworks django and web2py may as well be the same thing,anything django does web2py does better,having said that web2py has a lower initial learning curve than django as it was specifically designed as a learning tool,i prefer web2py because there are more conveniences built into the environment than offered by django but web2py is much newer than django and hindsight always makes web2py easier make new implementations better,hat being said having not used frameworks i would say to with web2py unless you need certain modules that only exist in the django world;web2py probably has a little more gradual learning curve,i think that django probably has more matured addins apps;that being said crafting your own blog in web2py a simple blog is probably only a little harder than configuring one for another framework,anyway the most important issue is that web2py is easier than django pylons php and rails,web2py has more focus on simple is better than complex but django has more focus on explicit is better than implicit,to answer klochner both seems quite popular but ruby is not a language that i am familiar with and django seems more heavy and complex to me than web2py,"
"django","web2py","gae better in different philosophies flexible,more customizable in different philosophies flexible, is more overall, instead gives you something in administrators explicit focus, is better in different philosophies flexible,s better in administrators explicit focus, gives you more in different philosophies flexible, follows explicit in administrators explicit focus,even easier in administrators explicit focus,","4566361,4353033,3646066,2844021,20761439,6338747,56284804,4353033,5494262,","as i understand it web2py supports gae better out of the box than django,django s admin is better and more customizable than web2py s appadmin,in contrast to turbogears django is more out-of-the-box;i don t have any experience with web2py but from my impression web2py tries to do a little to much out-of-the-box,django gives you a better looking database administrative interface;web2py instead gives you something easier to start with a web based ide and the option to run code unmodified on the google cloud, ve played with flask cherrypy and web2py;django is better than all of django even if i just want to build something small and lightweight i d choose django,django s is better web2py s appadmin is for administrators only,i understand they propose different philosophies and that django is much more flexible than web2py while web2py gives you more features out of the box,django follows explicit is better than implicit;web2py does not and instead follows everything should have a default behavior,i think you ll find that web2py is even easier to learn and use than rails and django,"
"addition","subtraction","slower in slower chip architecture,lower than  in precedence higher multiplication,better in better operation cryptographic,higher precedence in precedence higher multiplication,slower overall,slower in slower chip architecture,better than   in better operation cryptographic,slower bignum in slower chip architecture,more work overall, is not overall, and then overall,","39783882,24468043,39589499,30346386,39783882,23414385,54758938,46025736,29550341,45054575,57283843,","here it is conceivable that subtraction is slower than addition,in fact has precedence even lower than addition and subtraction so x y+z comes out wrong--an expression means x y+z instead of the x y +z that you d want,a111 addition is equal or better than subtraction,the only way it would be broken up differently would be if addition had a higher precedence than subtraction like multiplication does,it s possible though that software could mess things up by making subtraction slower than addition - but that s unlikely,this is a hold over from older compilers and interpreters on old chip architecture that would do addition slightly slower than subtraction,the third is that there is an operation that is much better than addition subtraction for cryptographic purposes xor or - the beauty of which is that it is its own inverse,but i d think bignum subtraction is a little slower than bignum addition,the subtraction case is going to require a little more work than addition in this code,therefore subtraction is not associative;on the other hand a+b doesn t cause the same problem since addition is an associative operator,i have this homework that requires me to ask for two integers from the user let them choose between addition or subtraction and then display the result,"
"addition","subtraction","bigger overall, thats also overall, is always better overall,efficient than   in slower chip architecture,","26876494,24331918,39589499,44324402,","i was thinking that there could be an issue if the result from the addition is bigger than what 15 bits can represent 32767 or if i get a negative number in the subtraction,subtraction is also described there;edit since its not clear if the op meant unsigned addition thats also not too hard to detect,this took 6 steps but leads to a number that could be arrived at in 5 steps subtract 1 divide 3 times add 1 so clearly we should not perform the addition;subtraction is always better,the mul are slower less efficient than addition subtraction but the mul are much faster than looping and doing repeated additions,"
"mstest","nunit","faster in test method argumentnullexception,better overall,faster in test method argumentnullexception, is an older more overall,slower in slower faster, doesn in tests vs resharper,faster in test method argumentnullexception,better overall,better in tests vs resharper,faster in slower faster, comes with less overall,","3844399,1340207,3844399,2422410,5815939,16399826,36080077,1989900,13545330,22663184,1490106,","nonetheless i tried to compare the sum of all test speeds and in some cases nunit is faster and in other cases mstest is faster,if you have a full version of visual studio 2008 rather than the express edition it integrates the inbuilt mstest much better than nunit does and cruise control also supports mstest,thus i suspect when people say that nunit is much faster than mstest it is because of the loading and updating delays but the actual test execution time appears to be very similar,nunit is an older more established unit testing framework designed to do exactly one thing - unit testing;mstest is newer so it does not have the same level of maturity in its api,from my experience mstest is much slower than nunit,you can then use nunit to run the tests outside vs or use tools like resharper to run those tests inside vs;actually mstest doesn t work with simple class library projects,the best testdriven.net disables all instrumentation that mstest does so it makes mstest blazing fast - much faster than nunit for example,various people told me to go with nunit since it s better than mstest apparently i have no idea and it also has very good support in resharper which i m using,nunit has better support for parameterized tests than mstest,nunit is faster as compared to mstest,mstest is supposed to have tight integration with team suite which since your company has already paid the outrageous fee for that is a point in its favor;nunit comes with less vendor lock-in and has a rich api,"
"mstest","nunit"," will mark the test in test method argumentnullexception,simpler overall, does not overall, is much faster overall,more overall,","8136656,18403378,2393989,2393989,2422410,","if the test method above does not throw an argumentnullexception mstest will mark the test as a fail;nunit has a more granular assert.throws that gives you more specific control of exactly where in the test method an exception is expected,sriwantha mstest is a simpler framework than nunit,nunit allows abstract classes to be test fixtures so you can inherit test fixtures;mstest does not,there are several of these implemented by 3rd parties for mstest;nunit is much faster,for example nunit offers more assert methods than mstest,"
"inline","inlining","more related overall, is decided by compiler in compiler keyword hint,keyword more now in compiler keyword hint, keyword is more in compiler keyword hint, is controlled by optimisation in compiler keyword hint, is not overall,better than manually  in compiler keyword hint, is larger in size bytecodes larger, functions needs more time overall, was probably more overall, is impossible then in compiler keyword hint,","5431498,13522918,12414229,25722666,28940208,4006604,22568041,46509056,38025208,20081969,48172918,","templates will be inline in the standard meaning of inline which is more related to the one definition rule than to actual code inlining,this inline does not necessarily mean the usual macro style inlining;the macro style inlining is decided by compiler and the programmer doesn t have much control over it,generally speaking the inline keyword is used more now to allow you to violate the one definition rule when you define a function in a header than to give the compiler a hint about inlining,the inline keyword is more of a hint to the compiler than an explicit directive these days;there is no specific rule about not inlining functions with return statements,as the others have noted in a modern compiler inline is little more than a linkage modifier;actual inlining is controlled by optimisation flags linkage requirements and compiler-specific attributes,on a good compiler for a modern platform inline will affect only a very few functions;it is just a hint to the compiler modern compilers are fairly good at making this decision themselves and the the overhead of a function call has become rather small often the main benefit of inlining is not to reduce call overhead but opening up further optimizations,if your compiler can do this then writing functions in a way that your compiler is able to inline is better than manually inlining the calls yourself,callee is too large message is printed by c1 when the size in bytecodes of the method being inline is larger than maxinlinesize 35 multiplied by nestedinliningsizeratio 90 on each next level of inlining;too big and hot method too big messages are printed by c2 when the size of the method being inline is larger than maxinlinesize 35 or freqinlinesize 325 respectively,sometimes it might perhaps go slightly slower because inlining increases machine code size which is detrimental to cpu cache efficiency read about locality of reference;also a header file with many static inline functions needs more time to be compiled,the fact that the array pointer could be kept in a register when the method was inline was probably more significant;so the conclusion is that the big difference you re seeing is because of inlining,however if the compiler decides that the compiler would rather not inline your static inline-functions either because the compiler think s that inlining is worse or because inlining is impossible then the compiler will have to make a separate copy of that function in each file the compiler s used in,"
"inline","inlining","keyword less overall,more in compiler keyword hint, keyword is more overall, here is more overall,more overall, does not overall,bigger overall,smaller after  overall, must be smaller in size bytecodes larger,fatter code in call fatter faster, is sophisticated enough to only in compiler keyword hint,","17664046,4445686,5032917,45931917,26307679,15108783,133271,49351120,34846138,40781908,367303,","in reality the inline keyword has less to do with inlining code and more to do with allowing legal violation of the one definition rule,the keyword inline is more about telling the compiler that the symbol will be present in more than one object file without violating the one definition rule than about actual inlining which the compiler can decide to do or not to do,the inline keyword is more or less useless in this era regarding early inlining original intent,again i doubt that inlining here will change anything but my instincts say that if anything inlining here is more likely to hinder performance than help,i m well aware that inline is more of compiler decision than of user going so far as even to inlining non-specified inline-functions so the user control is almost negligible,if the call is inline there is no difference between the two approaches - as long as the parameter is known at compile time which it must be a decent compiler will remove the unnecessary switch in both cases;the only case where you would see a difference is if inlining does not occur - in this case the templated approach would allow the switch to be removed while the other would not,if a function is static thus not exported anyway and only called once within your code and you never use a pointer to the function chances are good that gcc will decide to inline it automatically as it will have no negative impact the binary won t get bigger by inlining it only once,in the case of very small inline functions or a large amount of subsequent optimization the resulting code may be even smaller after inlining since the remaining code if any may be smaller than the overhead involved in the calling the function 2,but as the inline code gets longer the savings for inlining become smaller so maybe you could just have a hard limit on the size of the code - any code to be inline must be smaller than the limit,inlining inlining produces fatter code which is faster the inline functions will not appear in the call stack,but it is up to the compiler if the method is actually inline or not;the compiler inlining is sophisticated enough to only in-line if this will help in the optimization stratergy being used,"
"inline","inlining"," keyword is only in compiler keyword hint,generally faster overall, function uses more in size bytecodes larger, requires fewer overall, or not in call fatter faster, is more than just overall, keyword makes it easier in compiler keyword hint, makes it slower overall, misses register saving restoring overall, isn overall,","25712222,31367212,20851613,3108501,41737096,42550238,5057179,1811677,47341784,52908024,","the specific reasons why a function might not be inline when the inline keyword aren t documented by microsoft;the closest i can find is the documentation for compiler warning c4710 . the inline keyword is only a hint and the compiler uses heuristics to determine whether inlining is a worthwhile optimization,a similar argument can be made for inlining functions inline is generally faster but will remain in the same big-o complexity class although there is an additional size tradeoff inlining makes your compiled program larger if the code was being used in many places,code memory if you are inlining a function which is used in many places then the code size will increase;stack usage if your inline function uses more variables then more stack space is used,s an important side note it is quite common to find that inlining makes no noticeable improvement in performance and there are many cases where not inlining code is better than inlining it;i ve found this to even be true of simple one line accessor functions just because the code when inline requires fewer instructions than it does to call the function doesn t always mean the compiler will do a better job with the compiler,inline expansion or inlining is an optimization where a function call is avoided by copying the called function into the frame of the caller;a function call can be expanded inline whether the function has been declared inline or not,the process of inlining is more than just copy and paste the inline code has to make sense within the inline code the inline code is being injected into,the inline keyword makes it easier for the compiler to apply this optimization by allowing the function definition to be visible in multiple translation units but using the keyword doesn t mean a c++ compiler has to inline the function and not using the keyword doesn t forbid a c++ compiler from inlining the function,be aware though that this should only be on a release build since inline code is difficult to debug;also you said that you don t mind making your program larger as long as your program gets faster but that often inlining makes it slower,but this shouldn t worry you too much as in many cases inline code is much smaller than the original function due to the fact that inlining misses register saving restoring has some parts of inlining removed due to them being evaluated at compile time etc,the gdb error message cannot evaluate function -- may be inline is a bit misleading as inlining isn t the only reason for a function being unavailable,"
"srt","subtitle","simpler overall, looks so overall,","43712635,17764624,","ass supports more formatting options but srt is a simpler format and can be modified with the force_style option in the subtitle filter,means that your ssa or srt file doesn t specify the playresx and y values;so the vfilter assume a display resolution of 384x288 thats why your subtitle looks so small on a 1280x576 video,"
"acs","adfs","more overall,more overall,","4674246,5568668,","it seems possible to have adfs as the ip-sts send an assertion to more than 1 acs url based upon the acs url or acs index in the authnrequest as long as they are listed in the relaying party trust endpoints list,adfs has more powerful claims transformation capabilities than acs,"
"rar","zip","smaller overall, not so overall,decompression much more memory overall,better overall,","28236359,6119232,3339409,36767008,","by the way i would suggest creating rar self-extracting archives instead of zip self-extracting archives as with rar compression the exe file with the right switches for best compression using additionally also solid archive options could be much smaller than with zip compression,rar not so much;zip is easy,i ve heard that rar decompression requires much more memory than zip decompression,one reason to use rar it is sooooo much better than zip,"
"malloc","memcpy","slower overall,less call overall,","6685743,18134099,","afaik malloc is not slower than memcpy,this allows you to use the assignment operator instead of memcpy and requires 1 less call to malloc - the one you make,"
"beautifulsoup","lxml","robust as  in task parsing scrapy,more in syntax natural unknown,easier in easier xpath solution,more in faster simple module,no longer actively overall, seems way better overall,better option in task parsing scrapy, and probably in faster simple module,faster in faster simple module,slower in slower parser also,easier in easier xpath solution,","56030689,5778559,30075680,804829,1922064,55066530,5218029,6649888,14069663,26958133,8940209,","apparently lxml from scrapy is not as robust as beautifulsoup s lxml,regarding beautifulsoup lxml is more efficient and in my experience can handle broken html better than beautifulsoup,i found a solution to this problem using beautifulsoup at beautifulsoup-where-are-you-putting-my-html because i think it is easier than lxml,lxml is supposed to be much faster and efficient and can do much more than beautifulsoup,for starters beautifulsoup is no longer actively maintained and the author even recommends alternatives such as lxml,ok beautifulsoup seems way better than raw lxml for that purpose this code works pretty well edit this code is important to use html.parser,note that lxml is probably a better option than beautifulsoup for this kind of task nowadays for the reasons given by beautifulsoup s author,lxml is much faster than beautifulsoup and probably the fastest parser available for python,i would recommend lxml for html parsing it s simple and considerably faster than beautifulsoup can be as much as two orders of magnitude,note that using the beautifulsoup parser is a lot slower than lxml s default parser,lxml enables you to search for elements using xpath which i think is easier than using beautifulsoup s api,"
"beautifulsoup","lxml","faster in faster simple module,faster in faster simple module,faster in faster simple module,less painful in easier xpath solution,faster in faster simple module, does not overall,better in faster simple module,fast than  in task parsing scrapy,faster in faster simple module,faster in faster simple module, parser is faster in faster simple module,","42852910,5784358,3215955,16958218,880821,30682089,19670433,57474826,6728412,4025244,54887460,","alternatively you can use lxml module which is lot faster than beautifulsoup,speed isn t important here but in other applications it is good to know that regexes are very fast 100 times faster than lxml and 1000 faster than beautifulsoup,lxml is faster than beautifulsoup i think and has much better functionality while remaining relatively easy to use,lxml will let you use xpath here which i think will be less painful than beautifulsoup s interface,i ve found that even if lxml is faster than beautifulsoup for documents that size it s usually best to try to reduce the size to a few kb via regex or direct stripping and load that into bs as you are doing now,for xml documents it may be that the elementtree offered by lxml is more productive;it supports xpath queries for example while beautifulsoup does not,i prefer to use beautifulsoup better than lxml,scrapy uses lxml for parsing which is extremely fast than beautifulsoup,pyquery is based on lxml so it s also much faster than beautifulsoup,you ll probably find that lxml runs faster than beautifulsoup but in my uses beautifulsoup was very easy to learn and use and handled typical crappy html as found in the wild well enough that i don t have need for anything else,here s an outline of what this might look like the beautifulsoup documentation mentions that the lxml parser is faster than html.parser,"
"beautifulsoup","lxml","faster in faster simple module,robust than  overall,faster in faster simple module,also more overall,faster in faster simple module,faster in faster simple module,prefer  with  overall,faster in faster simple module,prefer  over  overall, is no longer being actively overall, api  mainly overall,","41266291,5999808,2752712,2430575,4295387,32041266,54766075,19357899,37530245,2128783,19357899,","i prefere lxml it s a harder to understand but much faster than beautifulsoup,lxml is significantly more powerful and robust than beautifulsoup in my experienced opinion,it uses lxml underneath and is much faster than beautifulsoup,lxml also has more features and offers beautifulsoup too,it s generally accepted that lxml is faster than beautifulsoup ref,according to the above posts and my own experience lxml is definitely faster than beautifulsoup,i normally prefer beautifulsoup with lxml parser for parsing xml. sample code below output you can then use the methods provided by beautifulsoup like find and find_all to find the corresponding node or subnodes,lxml is also much much faster than beautifulsoup,if you choose to use python and are new to the language check out the codecademy python course and don t forget to check out lxml as some people prefer lxml over beautifulsoup some people also use both in conjunction so lxml s all a matter of personal preference,since beautifulsoup is no longer being actively developed i would recommend lxml since it does all the things beautifulsoup can do and a lot more,lxml is more strict and simply raises an exception if the html is malformed;in contrast to the multitude of functions provided by the beautifulsoup api lxml mainly uses the xpath mini-language for navigation,"
"beautifulsoup","lxml","faster in faster simple module,stricter overall, is faster in faster simple module, would do the job overall,faster in faster simple module, is better in syntax natural unknown, is more quickly overall,more in task parsing scrapy,faster in faster simple module, gives you more overall, documentation as in slower parser also,","13615911,44882181,55485261,27673808,15432775,13578055,48484567,1648539,14164350,7786235,34370585,","according to some benchmark tests lxml is nearly 100 times faster than beautifulsoup,i do understand that traditionally they are saying that lxml are stricter than beautifulsoup however what i do not get is the following,and on a side note i think lxml is faster than beautifulsoup but it is not as elegant,aside from that improvement lxml would do the job faster;beautifulsoup cannot give you just a count number of tags it found,how can i find all div and span tags with order preserved.with beautifulsoup it is very simple but i switched recently to lxml since it is much faster than beautifulsoup,i prefer the beautifulsoup syntax as i find the beautifulsoup syntax more natural but i find that lxml is better when i m trying to parse unknown quantities on the fly based on variables-- generating xpath strings that include variable values which i will then use to extract specific elements from varying pages,for beautifulsoup the code like as following the lxml is more quickly,beautifulsoup is more suitable for html parsing than lxml,edit don t use this for html work use the lxml library it s python based and much faster than beautifulsoup,although the hood beautifulsoup s not needed here you might want to use lxml directly since the hood beautifulsoup gives you more succinct ways to navigate through xml using xpath,lxml is the faster parser and can handle broken html quite well html5lib comes closest to how your browser would parse broken html but is a lot slower;also see installing a parser in the beautifulsoup documentation as well as the differences between parsers section,"
"beautifulsoup","lxml","possible with  overall, it s faster in faster simple module, but inside overall,better overall,faster in faster simple module,better in task parsing scrapy,","56422761,49650959,47918674,5890829,44964258,29681486,","if its not possible with beautifulsoup then what is the next best library for xml document edit and creation would be lxml,by the way i recommend use lxml module instead beautifulsoup it s faster,i would actually recommend using beautifulsoup for this kind of changes into the html structure and it should return and we are still using lxml but inside beautifulsoup,one of the things that makes lxml better than beautifulsoup is support for proper css-like class selection or even supports full css selectors if you want to use them,since you re using lxml why not use it in a more direct manner lxml is believed to be faster than beautifulsoup,the reason for using lxml for such a task is that it cleans html files better than beautifulsoup do,"
"gnome","xfce","slightly lighter overall,smaller overall,","5343779,1105831,","you might want to look at one of these options if you re running in a vm since xfce is slightly lighter weight than gnome although not all that much lighter these days,xfce runs much smaller than gnome and is full featured,"
"qlist","qvector","better in words efficient transactions,less overall, or even in words efficient transactions,better in performance stores memory,better performance in performance stores memory,better in words efficient transactions, which is faster overall,worse overall,","33665848,27124306,29745072,33610032,36125922,16991903,52294885,33665848,","size qlist performs better than qvector because it doesn t store the,qlist code is generally less optimized than qvector one,which brings me to the point - if transaction ids are per savings account transaction ids are per savings account would be sequential in other words you will be better off with the qlist or even better a qvector because it will be a little more efficient since you will only be appending transactions,if the size of the qlist s element type is greater than the pointer s size qlist performs better than qvector because it doesn t store the objects sequentially but stores sequentially pointers to heap copies,qvector will usually give better performance than qlist because qvector always stores its items sequentially in memory where qlist will allocate its items on the heap unless sizeof t sizeof void and t has been declared to be either a q_movable_type or a q_primitive_type using q_declare_typeinfo,qvector is better than qlist in this case because it s easy to resize it,edit if you dont need it to interact with other api alot you can use qvector to replace qlist which is faster according to qt official,if it stores it as pointers on the heap won t qlist be much worse off than qvector,"
"quicksort","radix-sort","more common overall,more popular overall,","3539265,24519734,","why quicksort or introsort or any comparison-based sorting algorithm is more common than radix-sort,which of the two consumes more memory is not defined and depends on the input sequence to be sorted as well as on algorithm tuning parameters see the comments to one of the answers to why quicksort is more popular than radix-sort,"
"post","put"," is more in resource new http,more overall,simpler in file form requests, it s better overall, but cannot overall,same out  in file form requests, gives much more overall,more overall,also more in resource new http, is more overall, operation doesn overall,","36929645,35232309,2864151,52290811,57272144,52903683,33822916,45046836,6203393,54327594,27315641,","an http post is more general;an http put is supposed to initiate an action on the server,anyway http 405 is telling you that your backend does not support the put method and probably it s expecting a post method with the x-http-method-override put http header since post is more standard method in rest than put,on the php docs link above they say a put request is much simpler than a post request when uploading file along with this advantage what other advantages disadvanatges do the put has got compared to the post,so the put api are fair and clean and when you encounter a post it s better you dig a little bit more,i m using api platform on a symfony 4.3 project and i just want to have an immutable property userid in this case which can be set on post but cannot be changed with put,i am trying to edit default auth user in laravel and i get the error when i submit my edit form so here is my controller and here is my view file when of form i just put the form here so the important file which i 90 sure that the problem is with is my route i know that i am some how sending some get to post or vise i am really confused with this part so now when i submit the form i get this error symfony component httpkernel exception methodnotallowedhttpexception no message and btw i have tried to use patch route with hidden input too but yet again same out put,presigned post gives much more control on the upload - like you can limit the size of the upload object its content-type etc,i m very sorry for the long question but i thought instead of dividing the question into several post it is more convenient if they are put into one place,post is also more commonly used for partial updates as put generally implies sending a full new representation of the resource,eturn error range codes when the expected mutation has not occured a delete didn t happen or a put didn t change anything;however a post is more interesting because the spec says the spec should be used to either create resources at a new location or just process a payload,the idea behind http verbs like put get post delete are just a matter of protocol semantics;just performing an http put operation doesn t do anything magical,"
"post","put","simpler in file form requests, that goes into more overall,more restful in resource new http,more appropriate in appropriate new,more sense overall, which is idempotent instead overall, upload is more overall, is better in resource new http,more complex in file form requests, is not in resource new http,better than  in resource new http,","36932851,35935334,44936791,8693510,40336732,14170593,32446552,7552520,17070042,2104528,51818197,","to receive a file in your api i would use a put request simpler than post multipart and fetch the data from the stream php input,without wanting to self-promote i ve also put together a blog post that goes into more detail on the aforementioned answer that should be of some use,side note if you are attempting to create a new customer you may want to use post instead as it may be considered more restful as put s are generally for updating an existing resource but this is up to you,more importantly in this case i think post is more appropriate than put,if noop a put may make more sense than a post which would imply creating a second connection,check this thread for more information consequences of post not being idempotent restful api;i think this quote is not really about uuids but about using put which is idempotent instead of post,it isn t possible to pre-sign a put url without knowing the value that will be sent;a post upload is more flexible since you can allow any content-type in the signed policy,while you could do it is a significant change of state by put technically post is better because of the non-idempotency assumptions associated with it is a significant change of state which in turn encourages browsers to pop-up a warning dialog,side note it looks like you re using the post form of file upload which as the docs say is considerably more complex than put,post is not idempotent;so you should not use for the update or the delete instead use put and delete from rest,i did not make an example as i think this one perfectly suites you extra hint even if placing the password in php _session variable is better than put it in post request remember you are doing bad practice and at least remember to empty out that json string in _session variable after you print it,"
"post","put","successful with  in file form requests,more in appropriate new,safer in resource new http,override  with  overall, can do anything overall, is more overall,easy as  in file form requests,more sense with   overall, is probably more overall,less than 4  overall, to see the behavior overall,","53315304,9352970,11325318,57616209,13145835,26279541,56165958,49351358,56154464,55321103,57191022,","my form html and php code i successfully sent and processed post requests but i fail to deal with put method and the way it works. i already successful with post method,i think that in that case the put verb is more appropriate because post really means i want to create something new,typically you would not allow a http client to determine the uri of a new resource so a post to blog would be safer than a put to blog article-uri although http does cater for appropriate responses should the server be unable to honour the intended uri,the value sent with the _method field will be used as the http request method so override post with put like below,post can do anything but one use of post creates an object in a container that the client specifies and lets the server return the uri of the newly created object within that container;put is more limited,post should be used to update or modify a resource or to call some atomic method in an api scenario;it can also be used for the creation of resources although put is more appropriate there,i m trying to make a multi-part form data put request with an image and it does not seem to be as easy as post requests are,but requesting a rest resource and more particularly by a get method doesn t mean that the client is about terminating a workflow that makes more sense with post put methods,post should work in all cases put is probably more intuitive to typical updates done in web forms while patching would require your client to actually calculate the steps needed to transform the current resource representation to the desired one if you use application json-patch+json in particular,put the labels into an array and in the loop use a counter to leave the loop if all 4 labels have a text or there are less than 4 post available,here is my code it s loading instagram usernames in the users.txt file feel free to put your favorite account and try it has to have more than 12 post to see the behavior but not that much if you want it to be quick,"
"post","put"," is more in resource new http, is used in widely overall, was the answer overall, is meant more strictly in resource new http,moreso in file form requests, it is more overall,less secure overall, is a better in maintainability answer technical, is easier overall, logic close overall, is used much more rarely overall,","54327594,54218098,35236813,56657622,2023144,20061963,35855054,33502378,55732033,38912189,6746429,","a post is more like that last put to a new location,the patch spec notes a comparison to post is even more difficult because post is used in widely varying ways and can encompass put and patch-like operations if the server chooses,i know this is an older post but when i first was looking at how to do this i came across an older post and knew an older post was the answer but i still didn t know things like did the credentials need to be coma separated etc;so just in case this might help someone out here are my notes for fiddler i put together for a json post,when your case should use put if you know the id then your best bet is to do why post is meant more strictly for new resource creation;put is used more in situations where you want to create or replace which fits your requirement as closely as you re gonna get in my opinion,put is designed for file uploads moreso than post which requires doing a multipart upload but then it comes down to what your server can do as to which is more convenient for you to implement,consider html forms you post with application x-www-urlencoded-form and get returns text html;with get and put it is more likely that the media-types are symmetric however that is not a hard rule,words like arbitrary processing and neither not safe make post sounds less secure than put when there is a choice,it does not matter that your post is idempotent since you remove the link after the reservation was confirmed hateoas so the chance is very low that 2 confirmation arrive for the same reservation;anyways i think put is a better fit,only show the code that matters and put the snippets inside one single snippet so that your post is easier to understand and not as clustered,econd you can put your post logic into a form.save method;imo your post logic is much more appropriate there and your post logic close to what django is doing inside your post logic passwordresetform,most servers that expect a post will accept only a post;a put is used much more rarely,"
"post","put","better in resource new http,correct than  overall, where is better in maintainability answer technical,better choice than  in resource new http,more overall, just offers fewer overall, always creates so overall,other than  overall, solves the er overall,general in file form requests,better overall,","32541130,54770182,56350369,55706231,8669928,52443372,40276363,51299625,40478649,55910624,40896714,","post is better but if you want it to be truly restful you should ensure that the uri uniquely identifies the resource and use put,this infosec answer says that for updates post is more correct than put but from a security point of view it doesn t matter because the call will be called in https,it is more about the readability and maintainability of your code in the answer i already put a link to a post where is better explained you are creating technical debt in your code you are initializing arrays and filling them handling indices and more.,put is a better choice than post when what you are intending is to provide the server with a new representation of a resource and so on,put does not mean update any more than post means insert,f you can t put all of the changes into a single transaction then patch is the wrong idea;that said everything you can do with patch can also be done with post -- post just offers fewer guarantees,i think a post is more appropriate when the server manages that id;separating the two concepts basically tells you how to deal with the server put is idempotent so the server should always work so long as the payload validates post always creates so if there is a collision of ids then a 409 would describe that conflict,if you put anything else other than post or get it should be sent as a get request according to specification,this post is slightly uglier than my first approach but this post solves the poster s question the way they d like this post to;i have decided to put this post as a separate answer.,node module heroku region- us to capture audio recording not using web-audio api and to sync audio to cloud - im new to firebase laying out node module that implements opus audio recorder and file upload audio ogg opus is it better to post from my web-cient to express app and from there interact with the firebase sdk api to init and get ref and put the file.,q1.i know that post is better than put for name-value pair parameters as the get exposes it in url and post doesn t,"
"post","put"," is more overall,more secure http overall, is better in resource new http, is used in ajax as overall,re data than  overall,more overall, which usually overall, does a bit better overall, read more link overall,more under a  overall,","55431594,1577549,5571083,53692409,51425118,17924408,21252711,57049751,40223668,51815256,","the real differences in the semantics of post and put are currently described by rfc 7231 post is the more general method which can be used for any operation on the target resource put is more specific - it indicates that the included document is intended as a replacement for the representation on the server,http put isn t inherently any more secure than http post if you re allowing the uploaded files to be exceuted on your server,if you actually want to just update an object what it looks like you re doing a put is better and you should just switch your routes over to put;if like i was you re doing something that really requires a post it can t be sent more than once safely you can write your form_for like this,unprofessionally i myself use all post and get alone so i dont have to deal with put and delete my workaround and on ajax call and the route;you have used post instead of put i am not sure if put is used in ajax as well but u could try that if that alone fails,fyi better to post you re data than put in a screen shot and you should also post the code you ve tried so far,the client post scores and fetches highscore lists from the server and everything seemed to be working perfectly we had been testing it for a month with no problems and the code is really simple with not much more than a put get,the limit of post lies actually more on the server side than on the solr side;if you put solr into tomcat the it s the limit of tomcat s post which usually interest you if you put your tomcat behind apache or nginx then you put your tomcat behind apache or nginx max post size will also interest you,remote authoring semantics put patch delete really don t fit well with a single actions endpoint;post does a bit better in that the constraints are minimal,what about the question how many items to put on the screen you can give a solution like this for each post give fixed x height and if each post requires more space put read more link on the bottom of the post which will reveal missing part when user clicks,even though technically you can only create one user for one email it would fall more under a post than a put,"
"fork","pthreads"," isn overall, is more traditional is well overall, but not overall,more extensive independence overall,better overall,better overall,more than the  overall, is not an option so overall, is not in arguments clone like, library is more in arguments clone like,","10684001,39877908,16762505,42755694,11662781,22140337,38875848,2830965,47190769,11662922,","from the pthreads spec section you quote above;the reference to create handles using fork isn t elaborated on further in this section but the spec for fork adds a little detail,threads used to have vfork alognside fork and some systems may have some systems own mechanisms such as linux-specific clone but since 2008 pthreads specifies only fork and the posix_spawn family;vfork alognside fork is more traditional is well understood and has few drawbacks see below,since you say these are not pthreads but not what they are it s hard to generalize much more.;one way to make this work as described but of course actual code tends to vary from descriptions is to start the semaphore count at 0 fork a child have the child write without looking at the semaphore count fork another child have that child also write without looking at the semaphore count and then have the parent wait on the semaphore p,the features stemming from those discussions permit less extreme fork than processes which is symmetrically like the provision of more extensive independence between pthreads,but when is fork and clone better than pthreads,there is a nice comparison of threads and processes here when is clone and fork better than pthreads,multithreading is faster takes up less resources there is no separate namespace for it and you can run the 6 workers 4 times more than the fork but multithreading requires build php with non-zts some extensions do not work with non-zts and requires something you would understand pthreads model,on pthreads systems spork uses kernel.fork;on windows fork is not an option so spork creates a pool of preloaded processes which,pthreads provides a more general interface to the fork system call than any of the other functions discussed here do but pthreads is not portable,with various arguments clone can also have a fork 2 -like behavior;very few people directly use clone using the pthreads library is more portable,"
"floating-point","fractions","more digits in power efficiency this, do not in power efficiency this,more in power efficiency this, that cannot in power efficiency this, part is less overall, is a little more overall,values more overall, are not exactly in power efficiency this,","26145924,17175490,41839881,3840749,50319944,53504195,2132643,937696,","all other fractions floats have more digits after the dot when expressed in decimal because the representation of floating-point numbers is binary not decimal,the internal representation of a floating-point value is typically binary so initializing it requires converting from decimal to binary and displaying it requires converting from binary to decimal;most decimal fractions do not have an exact binary representation and most binary fractions do not have an exact decimal representation so it is not meaningful in general to ask for all the digits,the floating-point numbers are rounded to have not more than eight fractions digits,floating-point numbers are normally represented as binary fractions times a power of two for efficiency;this is about as accurate as base-10 representation except that there are decimal fractions that cannot be exactly represented as binary fractions,ieee-754 basic 64-bit binary floating-point is used with round-to-nearest-ties-to-even;then the result of the division is never a number whose fractions part is less than ½ but that rounds to ½,mask by for the fractions part of the sum shift the remaining bits and add them to the integral parts for the integral part of the sum;floating-point is a little more finicky but not too much harder,and using floating-point values is more realistic - you need fractions values because when you rotate something the new coordinates will nearly always be non integral,short answer python uses binary arithmetic for floating-point numbers not decimal arithmetic;decimal fractions are not exactly representable in binary,"
"gif","png","smaller in smaller files transparent,file smaller oh in smaller files transparent,better overall, and then overall, update the image overall,smaller in smaller files transparent, not in support use gif, instead offers better compression in support use gif,newer format in support use gif,smaller than  in smaller files transparent,less in smaller files transparent,","27036474,8596064,319433,18664942,7723868,2747873,5754021,26335476,610542,610565,42301395,","since that appears to generate index-color gif files which are smaller than the png files that doxygen generates,it might help to reduce your gif file sizes smaller oh and i believe -depth 8 can only be used for png images,paul points out that png compresses static line art better than gif for nearly every situation,with the parentheses gif|png|jpg will match followed by gif or jpg or png and then followed by s,however gif was not a supported image file in previous versions of ax maybe there is an error with transparency;my first suggestion is to convert the gif to png update the image in the check layout and try again,just press save and give it a name and that photoshop image will be saved into a transparent background png file which presents more colors and it s smaller than a gif file and is as good as a jpg,versions of gd older than gd-1.6 support gif format images and do not support png where versions greater than gd-1.6 and less than gd-2.0.28 support png not gif;gif support was re-enabled in gd-2.0.28,you should use png instead of gif because png instead offers better compression and features,png is a newer format and often better than either jpeg of gif - especially for screenshots,if png isn t smaller than gif then your software may be saving your software poorly - look for png optimisation progams like pngout and pngnq,i used gif because its file size 2.1kb was considerably less than png or jpeg in this case,"
"gif","png","better in support use gif,larger in smaller files transparent, pixels are either fully in smaller files transparent, format uses better compression in support use gif,better compression overall, cannot in mode apart animation,format better compression in support use gif,significantly larger in smaller files transparent,file wider in smaller files transparent,better in support use gif,smaller in smaller files transparent,","115838,610573,34538885,2336582,10666940,610597,10666583,18601225,30455777,37907325,2308120,","as a general rule png is never worse and often better than gif because of superior compression,jpg and png work well for most applications but the files will be larger than gif for very simple graphics,but in that regard it is replaced by png which is generally smaller supports alpha transparency where gif pixels are either fully transparent or fully opaque and most importantly gif images are limited to 256 colors,the png format is a popular alternative to gif images since the png format uses better compression techniques and does not have a limit of 256 colors but pngs do not support animations,png achieves better compression than gif because it applies a pre-filtering step before the lossless compression deflate roughly equivalent to lzw. see wikipedia s explanation of png filtering,png is capable of every image mode of gif apart from animation and when using the same image mode png will have better compression due to its superior deflate algorithm compared to lzw;png is also capable of additional modes that gif cannot do such as 24 bit color and alpha transparency but this is where you need to be careful if you forget to convert to palette mode your png image may be saved in 24 bit color which will take more space,according to wikipedia the png format provides better compression than gif,i m not sure if this matters but the gif is significantly larger than the png files,wewtaco i noticed that your png file has wider dimension than the gif file and i am guess that could be a reason the black bars are showing up,animated png is better than gif if we re focused on file size â achieving a smaller file size was one of the reasons the apng format was designed,convert it to png 10-30 smaller than gif on average,"
"gif","png"," works s more overall,better compression in smaller files transparent,better competition in support use gif,much better in support use gif, has proven better in support use gif,larger in smaller files transparent,more colors than a  in support use gif,smaller in smaller files transparent,larger equivalent in smaller files transparent, is better overall,format substantially less capable in support use gif,","22133355,11005984,1084126,1200232,24672602,6850687,56086720,2523038,116461,610563,30777789,","depending on your animation content you can optimize your gif so this png works s more web device friendly when loading,you can then use imagecopy to insert each gif image including the background and imagepng to generate png output which is better for line art than jpeg offers better compression than gif and can support more than 256 colors,png images are always compressed lossless but their compression algorithm works better than competition gif,png compresses much better than gif and allows more colors at the same time,for example given gif may be deprecated in a system because png has proven better for the system s users - perhaps because the system s supports better colour depth preserving the colours more accurately or svg might be deprecated because some clients have been found to be using web browsers that won t display t jpeg might be deprecated because the system s s known the images in the system aren t natural photographic images and the format gives visually poor results despite larger compressed files slower processing speed and higher memory usage - lots of possible motivations for making things deprecated,it will always be a web format which basically comes down to jpg png and gif with gif being very unlikely because of its limitations gif can contain 256 colors at most and is generally larger than png,the format is not much good for anything else given a png will support translucency and a lot more colors than a gif,but keep in mind that the gif color palette is way smaller than png,if your png files are coming out larger than equivalent gif files it is almost certainly because your source image has more than 256 colors,in general jpeg is better suited for photos while gif is better for graphic objects like buttons or rendered letters;png is good in both regards but that discussion tends to get a little religious because there are license fees to pay if you develop a programm that reads writes gif or jpeg while png is free,the gif format is substantially less capable than png,"
"gif","png","smaller in smaller files transparent,better in support use gif, however not overall,smaller in smaller files transparent,better alpha in support use gif,better support in support use gif,wider color in smaller files transparent,smaller in smaller files transparent,not bigger in support use gif,definitely better in support use gif, cannot in mode apart animation,","12416387,4288557,7999821,2686734,9424505,11199578,2336552,116210,5134831,10941029,610769,","one thing to note is that gif supports a smaller palette than png - only up to 256 colors,many people don t know about 8-bit png which is usually better than gif in size.,you can use transparent gif they are somewhat supported;transparent png however not,gif is smaller because it s based on an colour palette of 256 colours rather than the separate rgb values for each pixel or group of pixels in jpg and png,png gives you better alpha result than gif,for example png has better support for transparency than gif or jpeg,png has a wider color pallete than gif and gif is properitary while png is not,indexed png less than 256 colors is actually always smaller than gif so i use that most of the time,also if i change all to png is not bigger and alos i am using prawn pdf outputting these images so cant use gif,while png is definitely better that gif occasionally there is a use case for needing to stay in gif format,png is capable of every image mode of gif apart from animation and when using the same image mode png will have better compression due to its superior deflate algorithm compared to lzw;png is also capable of additional modes that gif cannot do such as 24 bit color and alpha transparency but this is where you may run into problems on the web,"
"gif","png","often smaller in smaller files transparent, format doesn in support use gif,slightly better in support use gif,shim smaller in smaller files transparent,","28668844,1952390,115838,1841996,","icon size indexed png is often smaller than the same gif,the gif format doesn t support this so unfortunately no there isn t a way to do it;the edges in a png are smooth because of anti-aliasing and thus variable opacity,there might be some edge cases where gif is slightly better because the png format may have a slightly larger overhead from metadata but it s really not worth the worry,a 8-bit png shim is smaller than the same dimension 1 pixel gif and everything will still work as planned,"
"n-gram","words","better overall,better overall,more overall,","25261349,28320865,37178364,","if you will be classifying multi-paragraph text all in one language a functional words list which your bag of words with pruning of hapaxes will quickly approximate might well serve you perfectly and could work better than n-gram,for some problems character level n-gram do better than words level and logistic regression parameters,it has support for tokenizing lemmatizing n-gram ideas that span more than one words,"
"cstdio","iostream","more overall,slower overall,slower overall,","15501048,5792495,9369269,","for those commenting that cstdio is obsolete personally i like it more than iostream,yes iostream is slower than cstdio,iostream is said to be slower than cstdio but i suggest you use a profiling tool here to find the best set of options here,"
"jruby","mri","faster in raw traditional scala,slower in slower bad scales,heavier overall,higher priority overall,better in concurrency better application,faster in right parameters warmup,more performant in raw traditional scala,faster in slower bad scales,more overall, is actually faster in slower bad scales,better performance in concurrency better application,","10747331,40545364,2224831,46009886,12071741,11058633,10284169,40529208,3928025,57802,14584499,","jvm hosted languages are generally going to be faster than traditional mri ruby and both java and scala are generally faster than jruby when it comes to raw cpu capabilities,so it seems like the opposite - mri 2.3 gets 2-5x slower than jruby 9.1,the jruby runtime alone is already pretty heavy much heavier than mri,alter your command path so that jruby s version of the ruby command has a higher priority than the mri one,i know that rails jruby handles concurrency better than mri but i don t know how to do it,lastly if you are frequently finding yourself running long running process i advice you to try jruby which is works much better with long running processes due to jvm lot faster than mri,once the jvm has warmed up rails requests under jruby are usually significantly more performant than under mri both in terms of raw execution speed and garbage collection,does this mean that the old adagio about jruby being faster than mri ruby is gone,in fact on windows jruby passes more rubyspec tests than ruby meaning mri or yarv itself,only choice not that that s a bad one jruby is actually faster,with these options jruby on rails gives about the same or better performance than mri,"
"jruby","mri","better in concurrency better application, is faster overall, is more overall,faster in right parameters warmup,faster in slower bad scales, is faster overall,faster in slower bad scales,slower than  overall,faster in slower bad scales, is probably in concurrency better application,faster in slower bad scales,","13448080,30228655,12822575,40556630,15548793,32128223,7987382,50190091,30228655,710959,8920123,","for longer-running applications like a web application rubinius or jruby will generally perform better than mri,the difference really is that mri cannot execute code in parallel and jruby can;you might be tempted to say why i answer no if the experiment shows that mri is faster,i ve noticed however that jruby is more sensitive to memory leaks than mri,sometimes mri is faster but with the right parameters and warmup jruby was 3 to 3.5 times as fast on my system for this particular,for example jruby is faster than mri jruby 1.7 is faster than jruby 1.6 jruby 1.7 running on hotspot is faster than jruby 1.7 running on j9 jruby 1.7 running on hotspot 1.7 is faster than jruby 1.7 running on hotspot 1.6 jruby 1.7 running on hotspot 1.7 with the c2 compiler is faster than jruby 1.7 running on hotspot 1.7 with the c1 compiler and so on, m fuzzy on a lot of the finer details between jruby and mri ruby but here s what i know based off of what i ve learned using jruby;jruby is faster because jruby s multi-threaded,and it sounds strange but jruby scales very well and it s faster than mri with java 7,however this turns out not to be the case i ve tried many combinations of jruby jvm options but the steady state is 2x slower than mri,mri has a gil so why is it faster than jruby in handling requests,mri has pretty lousy threading concurrency support so if that s what your aiming for jruby is probably a better place to go,jruby is faster than 1.9 mri matz ruby interpreter the standard in certain areas,"
"jruby","mri","slower in slower bad scales,faster in slower bad scales,","30097643,30228655,","this really surprised me because i expected mri to be slower than jruby,mri is faster than jruby,"
"ecj","javac","even better overall, will use the getclass overall,much smarter tool overall,more cpu overall,","14186109,42495149,15169046,20068641,","however i think that ecj is even better than javac my opinion .,interestingly the eclipse compiler for java ecj does not include this null check and running nullcheck as compiled by ecj will not throw a n npe.;while i agree with the general consensus of preferring to avoid the getclass hack it is worth noting that as of openjdk version 1.8.0_121 javac will use the getclass hack to insert null checks prior to creating lambda expressions,eclipse compiler for java jdt ecj is much smarter tool than actual javac,to recap i want to know why a javac compilation utilizes so much more cpu than ecj,"
"coalesce","nvl","simpler in modern portable ifnull,stricter overall, is more overall, will not overall,more overall,more portable in modern portable ifnull, is safer overall,prefer  over  overall, is more in modern portable ifnull,","38997181,10416747,36943200,18126742,9716167,16041281,19973015,51547189,950103,","the ansi standard function coalesce is simpler than using nvl and decode which should be obsoleted anyway,2 however coalesce requires all arguments to be of the same data type thus being stricter than nvl which will first attempt an implicit conversion,mureinik has shown you need to provide a default value to replace the nulls which you can do with the standard coalesce function or the nvl function do the same thing in this case but coalesce is more flexible,nvl only takes 2 parameters whereas coalesce can take n parameters;when functions or equations are pass into them as parameters nvl will evaluate all of its parameters but coalesce will evaluate each in order stopping when it reaches a non-null value in other words coalesce will use short-circuit evaluation but nvl will not,coalesce is more efficient than nvl as it only evaluates the second argument if the first is null whereas nvl evaluates both arguments every time,the coalesce function is used here because it is more portable than nvl or ifnull,p.s. don t use nvl;it has a lot of potential pitfalls coalesce is safer,i prefer coalesce over nvl simply because coalesce is ansi iso standard,coalesce is more modern function that is a part of ansi-92 standard;nvl is oracle specific it was introduced in 80 s before there were any standards,"
"decimal","octal","often more in ascii easier convenient, do any calculation overall,representation no more overall, just wouldn overall, literals were much more overall,clearer overall,general in ascii easier convenient,sure with  overall,","4676830,49582136,44023228,50598474,36419237,8218375,47759455,56423892,","bitshifts just go easier with hexadecimal than decimal and is often more convenient to read than octal,adding 1 to it and printing it again would yield 100 in octal not 78;as c cannot detect minute overflows when counting with your made-up units the proper route to follow is to convert the input to regular decimal do any calculation you want then convert back to time decimal,this means the buffer is enough for printing the number in octal and since decimal representation uses no more digits than octal it will be enough for decimal representation too,these escape sequences originated in c or maybe in c s predecessors b and bcpl in the days when computers like the pdp-7 ruled the earth and much programming was done in assembly or directly in machine code and octal was the preferred number base for writing instruction codes and there was no unicode just ascii so three octal digits were sufficient to represent the entire character set;by the time unicode and java came along octal had pretty much given way to hexadecimal as the preferred number base when decimal just wouldn t do,so 01200 is base 8 which in decimal is 640;essentially this is paying homage to older times where octal literals were much more common,hex or maybe octal depending on the machine being emulated will be clearer than using decimal since similar opcodes tend to vary in bits not digits,as seen above a can be represented as decimal 65 in ascii and the 65 can be presented decimal octal hexadecimal .,it is used as an international standard and can be found more or less everywhere with octal i m not sure with decimal you can as it is represented by mantissa and exponent as shown below 15000 is also equal to 1.5e4 that is known as scientific notation and lastly in hexadecimal there is no shortened way hope this helps,"
"jogl","lwjgl","joy with  overall,more java-like overall, backend is a lot overall, is simply opengl and so overall,much more overall, which has a much overall,smaller overall,","250764,157238,21151293,5420141,23462234,12555504,6496021,","i have had more joy with lwjgl than jogl but lwjgl than jogl should lwjgl than jogl satisfy your needs,jogl is more java-like whereas lwjgl provides more gaming functionality on top of opengl access,deprecate the jogl backend this week;the lwjgl backend is a lot more,jogl is simply opengl and so hard to learn if you are not used to gl;lwjgl is a higher level api aimed at games and systems like jmonkeyengine higher level still,lwjgl is much more aimed towards gaming with opengl where as jogl is more worried about complete and perfect bindings to opengl opencl,i don t know how this would work using jogl;perhaps you should switch to lwjgl which has a much more reasonable java implementation of this function,if your company s concern is to avoid big unknown libraries which is prefectly understandable i d advise you to stick with lwjgl which is smaller that jogl,"
"get","put","better than  in better side document,slower native overall, is much less in data functions work,more in requests second bucket,slower in slower ts faster, is forbidden payload overall,slower than the  overall,less frequent overall,process faster overall,less than a  overall, cases case_id schedule overall,","48827676,44810414,54418167,40435522,41911766,55706231,57206349,6337024,31944382,49340331,48734458,","or is there a way to handle this client side better than put the document get the document to see if there are conflicts modify the document creating a new rev if there are conflicts,i think put and get on ignite cache would likely to be slower than native put and get on my inbuilt key value store,for all the others the value of data those could get is much less than the work those would have to put in getting it,at very high speeds amazon s3 does have some recommended request rate and performance considerations but this is only when making more than 300 put list delete requests per second or more than 800 get requests per second for a particular bucket,at times the get rate is slower than the put rate and we see messages backing up,put is a better choice than post when what you are intending is to provide the server with a new representation of a resource and so on;i am not able to understand why the payload of the http get is forbidden payload of the http get is forbidden because the standard says don t do that,the put method here run way slower than the get result in my view can t update because it get the old topic not the updated one,you got at least put and delete although they are used much less frequent than get and post as in hardly ever,as long as the dataframe is small your assumption that the put process is faster than the get seems true we can fetch all 5 items within one loop of while not q.empty,the case for the non-empty list is if the first element is less than a put that element in front of what you get from recursing on the tail of the list otherwise the result is just the result of recursing on the tail,i m trying to choose between put cases case_id schedule get cases case_id schedule and put schedules cases case_id get schedules cases case_id could you please give advice,"
"get","put","more just in data functions work, is much easier in better side document, is more or less in number count sequence,more once overall, is no longer overall,command actually slower overall,more than 300  in requests second bucket,less in number count sequence, values less overall,more in requests second bucket, is more overall,","26262818,9132638,50176807,19144096,55271245,43420201,50996103,16422108,57301702,41529280,26279541,","and then i saw that they put functions under transform that did much more than just get data,my personal thought is that it is better to put it in post for a couple of reasons;get is much easier for users to manipulate,when one says that hashmap get put is o 1 it should really say that the time needed for the get put is more or less constant and does not depend on the number of elements in the hashmap so far as the hashmap can be presented on the actual computing system,to get your code to run more than once put it in a while loop,be careful because between you ask the next id and when you want to use it someone concurent query can insert a new record and the id you get is no longer valid let s say when you want to add new person record when you didn t know the name want to put newperson get the id write to db newperson but this could happend get1 the id get2 the id write1 to db newperson write2 to db newperson where 1 and 2 are two concurent scripts that happens to execute at the same time easy fix would be to use transactions to avoid this,as i am benchmarking my cluster i discover that for large file my get command is actually slower than put command,firstly this type of effort is only required if the bucket routinely exceeds 100 put list delete requests per second or more than 300 get requests per second,this should be producing a topological sort but so far i consistently get one value less than i put in as far as number of vertices and none of the number match those i ve inputted,the expected out put is only 2 in this case because only the 2nd item get s a score of more than 80 when compared with item 1 the rest get values less than 80,if you expect a rapid increase in the request rate for a bucket to more than 300 put list delete requests per second or more than 800 get requests per second we recommend that you open a support case to prepare for the workload and avoid any temporary limits on your request rate,it can also be used for the creation of resources although put is more appropriate there;get is supposed to be used to return a resource which is not variable,"
"get","put","n items by key  overall,method better in better side document, is 25 slower in slower ts faster,higher overall, is used more overall, it is clearer overall,worse overall, in is longer overall,now slower in slower ts faster,slower overall, in is less overall,","54552560,794801,52214018,30760255,56657622,47682231,7779856,50809291,31944382,3380000,7898008,","get put are of o 1 concurrentskiplistmap more operations than just get put like sorted top bottom n items by key get last entry fetch traverse whole map sorted by key etc,right now i would have to say the put method is better since it works and i could not get the webservice to work,ts get is 2x faster than hashtable;however its put is 25 slower,when i put my loop counter variable pagessent in the watch window it gets optmized when i declare as volatile it still gets optimized but when the max value is set to 4 the loop exits properly but if max value is higher than 4 get stuck..,put is used more in situations where you want to create or replace which fits your requirement as closely as you re gonna get in my opinion,while aaronfay s answer is good and works i think that given that there are only 3 http methods other than get and you are only worried about put it is clearer and simpler to just define the request sub-classes per method,i m concerned this is going to get worse as we put more tests into the specflow project,i also was not asked to set up a password while installing mysql whenevr i try to set up one in mysql i get password not long enough error even though the one i have put in is longer than 8 characters with other char combinations the screen starts loading and being asked to set up password again,this could mean that put is now slower than get and we have to wait,since most data-storage mechanisms in-ram in a database etc. get a lot slower as you put more data in them you should ensure you re only timing your http access and not looking at overall crawler throughput including storage,since m k+1 m k + m k-2 + 1 you can replace with to get m k+1 m k + m k-2 + 1;you can make substitutions on the right-hand side of as long as what you put in is less than or equal to what you take out,"
"get","put","more accurately oswap overall, is a lot overall,more difficult overall, error indexerror index overall,more in requests second bucket,better way overall, method is not safer overall,more in number count sequence,","38482452,6376920,8805725,55636318,26171964,32556567,29245172,44565429,","now don t get me wrong regenerating the session id on login or more accurately as oswap put it on privilege level change is indeed a very good defense against session fixation,detecting an incomming mms message is easy just put in broadcast receiver monitoring wap_push_recieved events as in.;making sense out of what you get is a lot harder,from an interface design standpoint you want user-agents to make post and put and delete more difficult than get or at least distinctly different so that users can rely on that difference to hint when their actions might cause a change in the resource state because they are responsible for those changes,this is not the problem when i cv2.imshow it shows the image correctly with width larger than height like a normal horizontal rectangle i added mousecallback to get the pixel position so when i put my mouse nearer to the right edge of image i get error indexerror index 560 is out of bounds for axis 0 with size 408 although i have clicked on the image,i m not sure if this is the answer but states that if you expect a rapid increase in the request rate for a bucket to more than 300 put list delete requests per second or more than 800 get requests per second we recommend that you open a support case to prepare for the workload and avoid any temporary limits on your request rate,but the slider won t work well it doesn t get activated is a better way to put it if i insert the code from slider.js into the chrome console hit enter - booom works perfectly i ve been looking for what the problem might be tried adding the type text javascript in the script tags added the document .ready in my jquery still nothing.,usually you will also need to put quotes to end the string value and write your own request;get method is not safer than post data never trust datas coming from the client side,from the count you will get the number of sequence and those which have more than 2 put those in a different series and then add that series in the dataframe,"
"fpdf","tcpdf","far better in look past better,slower overall, does not overall, has almost overall,slower overall, is very overall,better in look past better,much better overall,","6280801,2285314,13381090,7450285,11405245,13241460,7739169,9000297,","i suggest you to give a look to tcpdf which is far better than fpdf also because it is actively developed,if anyone has some information on this problem i d really appreciate it - either things you did to make tcpdf faster or just confirmation that it runs slower than fpdf so i can forget about it and just stick with fpdf,tcpdf supports transactions multicell and utf-8;fpdf does not support good multicells,fpdf has almost no comments;tcpdf has a few more methods but also has full blocks of phpdoc-like comments with explanations of every parameter and usage and examples in html format before every method and property,tcpdf was always slower than fpdf,fpdf is very lightweight and could be perfect for you if you are just echoing out variables;tcpdf is much more html- pdf in terms of syles and css features but the file size is a bit bigger,i ve used tcpdf in the past and found it a lot better than fpdf,somewhat good news you could move to tcpdf which is much better at handling utf8 and is more modern in general than fpdf,"
"httplib","urllib","slightly better overall,lower overall,","29110737,7505789,","i don t know why this works i can only assume that httplib is slightly better behaved than urllib,basically - httplib is lower level while urllib is high-level,"
"tolower","toupper","lower case overall,faster overall,faster overall,more accurate overall,more efficient overall,better in comparisons better,better in comparisons better,more overall,","43095027,41238495,41240021,32678208,1047997,18342840,5104579,2801521,","we uppercase this with toupper then we add on the next 3 characters of the string starting at the 4th character which we force in to lower case with tolower,so you can t reject the null hypothesis that tolower is as faster as toupper and thus your experiment has got errors,your intial hypothesis of toupper being faster than tolower has a logical fallacy,you can use string.isnullorempty and toupper method is in general more accurate than tolower,as a side note using toupper is more efficient than using tolower so toupper would be the way to go,toupper is better to use than tolower but i forget why,and if you have to convert to the same case to make comparisons toupper is better than tolower,so yes - toupper is more reliable than tolower,"
"for-loop","while-loop","faster in cleaner faster, is more overall,faster in cleaner faster,greater cost overall, now is larger overall,equivalent  with the  overall, which is more overall,cleaner in cleaner faster, is easier overall,cheaper overall,","4203952,10058020,2611604,24353790,49175192,31600569,49375317,29374366,21113484,43011622,","and now we know that for-loop is faster than while-loop,you use a for-loop for an unknown number of loops iterations;this is not wrong but a while-loop is more appropriate,i have also found that a while-loop is faster than a for-loop,also it seems like a for-loop is of greater cost to the compiler than a while-loop,after the interpreter entered that line the first time it is inevitable that an indexerror occurs since length - 1 the upper limit of x in your for-loop now is larger than the last index of list1 two possible solutions capture the indexerror and break the for-loop then use a while-loop like this,unless the task is to do the task with a for-loop it is better for learnig the language if you try to get cleaner code;just tell yourself what the code does compare for example the equivalent while-loop with the for-loop,you are trying to reach an index after the last index you need to change your loops condition to and you can solve this problem using for-loop which is more used for such problems you just need to substitute your while-loop with using this you wouldn t need to use substr method neither string letter,edit just realized a while-loop may well be a lot cleaner than a for-loop for this,t changes two things using a for-loop instead of a while-loop and counting down instead of up;for my taste brain the for-loop is easier to grasp a collection is being processed until a collection end is reached,the reason for the second case much slower on sorted data is that a while-loop is cheaper than the equivalent for-loop when there are zero iterations,"
"gson","jackson","better in better project requirement,more suitable in faster slower streaming,better ones out there  in better project requirement,lib better performance in better project requirement,much faster in faster slower streaming,2-4x faster overall,better in better project requirement,more complete overall, does not overall,faster in faster slower streaming,faster in faster slower streaming,","14519479,12426957,34022510,43648840,3261073,5787646,7736941,2431212,49529831,42641548,33828265,","also i want to know if google gson is better than jackson for json parsing,jackson is more suitable if you have a complex deep json tree because gson creates a lot of temporary objects which leads to stop the world gcs,try and change the gson package to one of the better ones out there jackson for example it might solve your problem,also it seems jackson lib has better performance than other packages such as gson which i haven t tried personally,i ve seen questions like this come up before and the general consensus is that jackson is much faster than gson,gson is not particularly fast but the jackson library can almost compete with most binary serializers jackson is 2-4x faster than gson in most situations and 10-20x faster on utf-8 because it has special code for utf-8,after searched in google found that jackson has better performance than gson i plan to replace gson with jackson in my project but i got a diffrent result when run test code,since i am more familiar with jackson here are some aspects where i think jackson has more complete support than gson apologies if i miss a gson feature,this is an issue reported to gson team unfortunately seems they insist that javascript has no integer type as if they do not realize that gson is for java not javascript;so they refused to fix it until today 2018 now despite other lib like jackson does not have such issue at all despite how easy to fix it,very small object google gson performs faster than jackson and simple json,under some conditions gson has proven to be a lot faster than jackson there also exists jsonp and json.simple,"
"gson","jackson","faster in faster slower streaming,faster in faster slower streaming,slower in faster slower streaming,more slow in good server side,more faster overall, is really in better project requirement,better in better project requirement,less features overall,better in better project requirement,faster in faster slower streaming,less powerful overall,","4764950,14248787,6559693,14323861,7736941,8197557,46011311,12107360,3754055,9296657,17110794,","gson 1.6 now includes a low-level streaming api and a new parser which is actually faster than jackson,further more gson really seems to be faster than jackson json,deserialization performance is similar with gson over 9x slower than jackson and fastjson about 0.5 faster than jackson,try this lib that s a good one with the jackson we use only in the server side because jackson is more slow in the android than gson at least in our test,it seems gson is more faster than jackson the average time of gson is about 2ms while jackson is about 16ms does i make mistake when using jackson,gson is really simple to use and if performance isn t an issue it s quite nice;jackson is much much faster though and is not really that much more effort to use,jackson type safety is way better than gson,note also that gson comes with less features out of the box than genson or jackson,we plan to use gson extensively in the application as it seems safer and better than jackson,jackson is faster but the api is 37x more complex than the gson api,going to go ahead and say that gson is a lot more user friendly albeit less powerful than jackson,"
"gson","jackson","larger overall, is better in better project requirement,slower in faster slower streaming,slower in faster slower streaming, is better overall,better in better project requirement, to deserialize your json overall,simpler in faster slower streaming,faster in faster slower streaming,better in better project requirement,tell  how to deserialize in better project requirement,","27597609,49229658,6559693,26263028,54380717,27757123,32156106,9792782,17166058,7590072,21757672,","keep in mind though that jackson is a larger library than gson so depending on your case you might prefer gson to avoid 65k methods limitation,in short if your json response is quite small go with gson otherwise jackson is better,according to the performance results at for serialization with databind with strings gson.tojson myobject gson is over 10x slower than jackson,code looks correct and even at its worst jackson should be no slower than gson,this is frustrating jackson does a much better job of serializing gson will choke for various reasons on classes that jackson serializes without a complaint however gson is better at deserializing,performance of jackson is better than gson,gson doesn t care about jackson annotations;if you want these annotations to be taken into account use jackson to deserialize your json,gson is simpler jackson is faster,personally i prefer jackson as according to test benchmarks it s faster than gson,i found jackson better than gson performance project activity better docs ...,if using jackson isn t a requirement then you might want to consider using gson instead;you can tell gson how to deserialize your json by just passing it a json string and the class that you want to deserialize,"
"gson","jackson","faster in faster slower streaming,faster in faster slower streaming, is more in good server side,slower in faster slower streaming, is a little faster so in faster slower streaming,faster in faster slower streaming,","45229746,31940366,14323861,34679659,6061256,5553647,","in my case i found jackson little faster than gson i used default serialization because so far i don t have a need to customize serialization,gson is faster with smaller documents and jackson is faster with large documents,try the gson lib that s a good one with the jackson we use only in the server side because jackson is more slow in the android than gson at least in we test,i looked at gson metrics and it seems slower than jackson,jackson is a little faster so its up to you what you want,take a look at the jackson json parser it s faster than the one in android and faster than gson and supports streaming,"
"mp3","ogg"," may not overall,smaller overall,better than  in bitrates webm meg,vorbis better in bitrates webm meg, vorbis is better quality in bitrates webm meg, data; is harder overall, is more overall,compare between  overall,better overall,","6076849,8859332,54844865,9884227,2467567,693792,22155972,4853900,12945530,","also when jumping through a file the positions aren t the same - half way through a vbr mp3 may not be half way through the song;ogg vorbis is a more advanced free music format and uses vbr as default without problem,of those three ogg would usually be smaller than mp3,for sound quality webm and ogg should be better than mp3,moreover ogg vorbis is better then mp3 at all bitrates,ogg vorbis is better quality per meg than mp3 plus no licensing legal issues,generally the player has to reconnect to the stream every so often causing a playback glitch otherwise memory just fills with mp3 data;ogg is harder,ogg is more of wrapper format;mp3 can wrap speex compressed audio speex www speex org is a very efficient compression technique based on celp,if you ask people to compare between mp3 aac m4a mp4 and ogg - people will give you different answers,in general ogg achieves better compression than mp3 which is a plus for things like download size bandwidth resource usage etc,"
"cheerio","jsdom"," doesn overall, is more overall,easier overall, is a module overall, which is lighter in options module faster,lighter than  overall,faster in options module faster,more lightweight overall,faster overall,","44659755,15790529,14532393,15790529,52917071,53032834,20374129,36509548,13901897,","jsdom is great if you have control of the page you want to crawl or server-side rendering it but be aware that site-errors will cause your node process to crash;cheerio doesn t execute javascript but merely parses the html,cheerio does not execute these scripts and as a result content may not be visible as you re experiencing;this is an article i read a while back and caused me to have the same discovery just open the article and search for jsdom is more powerful for a quick answer,i find request + cheerio to be easier than jsdom for tasks like this,jsdom is a module which will provide such content but is not as lightweight;cheerio does not execute these scripts and as a result content may not be visible as you re experiencing,cheerio does not execute scripts on the page therefore you can t see this specific class;you can use selenium which is pretty heavy to run on scale or jsdom which is lighter,you are probably better off using cheerio which is lighter than jsdom and supports most of the jquery syntax,if it is unavoidable node.js has good options - try to use the module cheerio which is faster than heavy weight jsdom,if you don t need a full dom available and just want to parse and scrape manipulate html elements there is cheerio which is more lightweight than jsdom and still gives you a jquery-like api,i had the same problem with jsdom and switcht to cheerio which is much faster than jsdom and works even after scanning hundreds of sites,"
"jeditorpane","jtextpane","more high overall, supports formatting better then overall,","8215676,24412230,","jtextpane is more high level as jeditorpane strange naming,and as i understood the jtextpane supports formatting better then the editorpane;jeditorpane supports the power of html which is powerful but you need to covert the text to html and create all the html tags,"
"avr","microchip","more overall, requires use overall,","6824099,6824099,","microchip is more successful in industry and has better tools than avr,as far as easy goes better tools than avr are both equivalent in terms of difficulty better tools than avr provide assembly and c except that the gnu toolchain for avr is more complex than microchip because the gnu toolchain for avr requires use of unix command line etc,"
"dictionary","map","worse in std large slower, implementation is faster in function faster entry, where not overall, doesn in key-value pairs comprehension, functionality is much nicer overall, object doesn overall, constructor is more overall, that contains fewer emoticons;the overall, seems more appropriate;note however overall, is semantically better overall, you ll be also overall,","2699569,4157451,42566261,32717264,49164774,1255091,4623212,42012905,23092942,55912265,55964671,","i definitely wouldn t try introducing a new general-purpose type for use in python based on std map which has worse algorithmic complexity in time for many important operations andâ in at least some implementationsâ leaves some optimisations to the user that dictionary already has,the immutable dictionary implementation is faster but no less pure in usage than the map implementation,iiuc if need replace values in some column by dictionary simpliest is remove loop;and if need map where not match get nan,so while a dictionary is a type of map maps are a much broader range of possible function;in practice a its usually the mapping function that defines the name so a hashmap is a mapped data structure that uses a hashing algorithm to link the key to the value where as a dictionary doesn t specify how the keys are linked to a value so could be stored via a linked list tree or any other algorithm,objects can behave like dictionary because javasript is dynamically typed and there hasn t been a good alternative until now but t really aren t meant to be;the new map functionality is much nicer because it has the expected get set has delete methods you would expect while also accepting any type for the keys instead of just strings,the dictionary is not thread safe so if you let one thread read from it while another thread is changing it you will have problems;remember that using lock on the map object doesn t protect the map object in itself it s only using the map reference as an identifier to keep more than one thread at a time to run the code inside the lock,generator expressions are generally preferred to map and using the dictionary constructor is more canonical than dict.fromkeys,a dictionary that contains fewer emoticons;the fast solution is to simply scrape a more complete dictionary and map the with its corresponding english text translation,given that keys are enumerated in a random order returning a list with the result doesn t make much sense and dictionary seems more appropriate;note however that for reasons that are not so clear to me map and functional reasoning is sort of considered bad in the python community for example anonymous functions are second-class citizens and second-class citizens got quite close to be completely removed from python 3,despite a list with those names and indexes might work i believe a dictionary is semantically better for this case because a dictionary job is to map a key value relation,rather than making a copy of the entire dictionary there is an easier scheme in which you ll just need to make a copy of all the keys in another linked-list;and in the original dictionary in each of the entries just also keep a reference to the linked-list entry corresponding to that key now if you want to iterate the 3 entries at a distance of 5 entries from 4297256046 you just need to do now the reason i mentioned linked-list was so that in case you want to delete any entries from the map you ll be also able to delete the corresponding entry from the linked list in o 1 time,"
"dictionary","map","much more similar overall, function is faster in function faster entry, is faster overall, must have an association overall, having a key overall,slower in std large slower, shouldn overall,match with  in note series nan,more overall,prefer with  in efficient values object, but produces more overall,","46547616,18569053,56165018,34185527,52763514,2699569,6139683,53689914,37206223,55421530,3927111,","i know this question is a bit older but in es2015 there is a new data structure called map that is much more similar to a dictionary that you would use in c#,then you define the function to convert each entry into a two-element list which will be converted into dictionary automatically using the dictionary function;using map function is faster than just iterating over the list which is the beauty of python,if array is long i suggest turning array is long i suggest into a since dictionary is faster has o 1 time complexity vs;o n for array.indexof then or if item.name can be abscent in map,suddenly a simple dictionary isn t enough you might need a tree of choices attached to each identifier found in the map or a more complex mapping of the identifier with signature data to a scope entry;in many algol like languages look in another scope requires going up the lexical nesting of blocks so each map must have an association with a parent scope,for those who are not familiar with map and indexset check this the logic is that i m iterating over the elements of the given array and each time checking whether the value at that index is not less than value at index + 1 by 1 if it is then i am splitting the array using array slice and adding it in a dictionary having a key which is given by the variable count,std map though is actually implemented in a way that many operations are slower than dictionary as its size gets large,and std map shouldn t be compared to dictionary;std map maintains keys in sorted order because it is a balanced tree dictionary solves a different problem,note if you pass dictionary map replaces all values from series with nan if it does not match with dictionary s keys i think you have also done the same executing the statement twice,data structure to implement reverse functionality of a dictionary that is more than one key map to a common value using python,you could use a list comprehension and fetch the values of interest from each dictionary using operator.itemgetter output or if you prefer with map, safer approach would be to create a private map between a label and myobject using a dictionary instance;a safer approach has the extra overhead of a dictionary but produces more reliable code imho,"
"dictionary","map","better overall, is possibly more in efficient values object, works more efficiently in key-value pairs comprehension, which is also slightly in function faster entry,slightly better overall, is per overall, is not in key-value pairs comprehension, has more in key-value pairs comprehension,more overall, is faster overall, is easier in efficient values object,","33454940,40643830,53997252,53514132,43011853,48929129,51781373,47869316,40393458,43709028,56894887,","if what you are making is a map from one word to another you might be better off with a dictionary,turning the dictionary values into sets would not gain anything as the conversion would be o n with n being the added size of all values -lists while the above disjoint operation will only iterate each values until it encounters a testx with o 1 lookup;map is possibly more performant compared to a comprehension if you do not have to use lambda if key can be used as the first positional argument in resp s __init__ but certainly not with the lambda,you can use a list comprehension within a dictionary comprehension;since you need both keys and values use dict.items to iterate key-value pairs map works more efficiently with built-ins so you may wish to use or since you have coordinates for tuple values the problem with your logic is python lists do not support vectorised indexing so you need to be explicit the verbose,he reason for that is because map is a built-in function and works perfectly better with functions of a built-in function kind chain rule;based on your update you can just use str.replace instead of your function and a generator expression withing dict.fromkeys note that alternatively you could also use for constructing an empty dictionary which is also slightly faster than directly calling the dictionary type,map is slightly better because the shared dictionary has around 480 entries instead of around 450 but it s still inconsistent and not all 500 as expected,create a mapper dictionary from df2 using groupby now map the values in df1 using outer key of the dictionary as column and inner dictionary is mapper you get obs gender job 0 1 male blue collar 1 2 male white collar 2 3 female white collar 3 4 male blue collar,a map is a key-value pair dictionary not a simple list array as you mentioned in your title;this is my assumption and i might be wrong but something that looks weird to me is this line this one gets via the key i and not by the i th position in the list because a map is not a list,expected output fruit frequency mango 1 apple 2 guava 0 nut 1 the data frame is already there.i just want to map the values with the corresponding element also if my dictionary has more key-value pair than my original dataframe how can i add the keys and corresponding values to dataframe,c++ std map behaves more like a python defaultdict than dictionary,and use a dictionary in map or replace map is faster for this,how about this from a programming perspective this isn t as good a solution as the one that uses an object because an object map dictionary is easier to extend to other values,"
"dictionary","map","more than simply  overall,slower std in std large slower,more in key-value pairs comprehension, it cannot overall, is more in efficient values object,rename columns by  in rename columns last,more in key-value pairs comprehension,necessary flattening by  in rename columns last, to convert the list in note series nan, which is more in efficient values object,","49259581,2699569,19670609,26729158,53068116,52514246,47214672,51324475,42230524,56535855,","benchmarking explanation the reason why s.replace is so slow is that it does much more than simply map a dictionary,for suitably large examples of each dictionary overcomes the constant factor by which it s slower than std map and will actually do operations like lookup insertion etc,the difference between a map and vector is a like an array while a map is more a dictionary of key value pairs,dictionary is an abstract class in java whereas map is an interface;since java does not support multiple inheritances if a class extends dictionary it cannot extend any other class,i could use the map type because checking for membership inside a map dictionary is more efficient than checking for membership in an array,if ordering of output column should be swapped - first f and then m columns explanation first rename columns by dictionary then set_index by counter series created by cumcount with added 1 and converted to strings reshape by unstack soer second level of multiindex by sort_index flatten multiindex columns by map and join last reset_index for column from index if ordering of output columns is important is possible use double rename of columns,using this hint we can try to guess that a fixmap must correspond to a dictionary data type in c# since a map is more or less like a dictionary of key-value pairs,i think need dataframegroupby.agg with dictionary of columns with lists of functions get multiindex so necessary flattening by map and last rename columns,note that the result is a dictionary and not a list containing a single dictionary but it is rather useless to put the result in a list;this works because we first use map to convert the list of dictionaries to a list of counter s,different mappings have different ways to hash the input values unless you want to use a sorted dictionary which is more memory efficient but slower,"
"pull","push"," model is more in model message processing, is earlier then overall,better in better subscriptions wan,better rebus in model message processing,more idiomatic overall,more scalable in scalable starters, is essentially in git fetch merge,better in better subscriptions wan, update closely overall, is better in better subscriptions wan,general overall,","48301268,5914554,7555572,26895650,28904913,1768222,27725774,2350342,49228856,39586742,55344699,","advantages are less spatial coupling - you send messages to the message broker without knowing where the receiver is located easier load balancing - you don t need any additional infrastructure except the message broker to run competing consumers pull model is more honest than the push model built-in retries for transient failures like database locks things like polly can help here too with http of course keep failures in the poison message queue for further analysis this is something that comes to my mind immediately,r she can pull bob s change and merge then push;if alice s push is earlier then bob bob s commit will go through creating a branch that bob s will have to merge,is the performance of pull better than push in this scenario,if in-order processing is required a much better design would be to use another message processing library that supports a pull model which i think would fit your scenario much better than rebus push model,there is a certain irony here given that within xslt push stylesheets are generally more idiomatic than pull stylesheets,i have found that pull is more scalable than push,the opposite of push is not really pull it s fetch;in particular git pull is essentially git fetch plus an extra step git merge or git rebase,the reason the pull model works better than the push model in this case is as follow,i assume that the pull request concept is only for the public repositories as a private repository can be controlled push pull update closely with my team members,pull is better in dealing with diversified consumers,i have had success to get connected push pull add commit etc,"
"pull","push"," is better overall, is better for learning too in better subscriptions wan,better overall,model definitely easier in model message processing,safer force overall, request not overall, http get request overall,better in better subscriptions wan,higher throughput overall, does not overall,request more overall,","14229076,32042591,20215538,9999184,4674677,32346710,17952209,3585336,10728510,52450879,12109797,","pull is suited to addressing the problem of configuration drift;for deployment on demand and potential rollback push is better,push is better for learning too because it is more in harmony with the spirit of xslt,alone push is better in the accepted answer can t quite work as a push-operation in some class can require a new pull at the pushed object,a heartbeat mechanism pull model is definitely easier to implement but a push model is far more efficient,tortoise now has an option push new branch that may be safer than force pull or push,if my memory serves me correctly then if you force push or update the branch in question in any way github will automatically update the pull request;if doing the force push would result in the pull request not being possible then github will tell you this,usually a client knows that an rss feed has been updated through polling that is regular pull http get request on the feed url;push doesn t exist on the web at least not with http until html5 websocket is fixed,how are push notification better than pull notification on iphones,push models have higher throughput than pull models,using git pull means run git fetch then run git merge;that s an extra step that git push does not and cannot do,i create new branch add those new features and than i once again made pull request but now the features i implemented in the master branch are also in my new branch so my second pull request has more features than i want to push to the original project,"
"pull","push","far more scalable in scalable starters, doesn t even overall, is nothing more in git fetch merge, is any better overall, is no longer overall, request is simpler overall, makes a merge overall, it s less overall, technology is the more overall, invokes git merge in git fetch merge,model less in better subscriptions wan,","912233,48189771,21657759,50046634,13962386,57279681,46109962,56070819,15045871,22417436,34706186,","why do you think that push is far more scalable then pull for starters,google analytics ga and google tag manager gtm only do stuff when the scripts are executed - so getting the ga file for example via push pull or loaded from cache doesn t matter until that file is run and it registers a hit or loads the tags that register the hits;in fact at a technical level http 2 push doesn t even push any files directly to the page but pushes it to a push cache and which the browser checks first before requesting a file,a pull is nothing more than a fetch followed by a merge;hard to say without having more info on your commit tree but you should be able to git checkout the develop branch git merge with test or whatever commit you re trying to merge with and then git push your local develop branch to the remote repo s develop branch,and it s not like doing a pull is any better because someone might do a push right after your pull .,if that happens the push will fail and the user telling him what s going on will have to pull before him can push as the push is no longer a fast-forward,this simple kind of pull request is simpler than the typical real one to get to this point you had to run git push to the single common starting repository w,take a note that git pull and git push aren t exactly the inverse operations of each other;git push refuses to push by default when the other side has commits it doesn t have by its own unless -f has given then it will overwrite while pull makes a merge commit,while it s common and trivial to move container images between machines viz docker push and docker pull it s less easy to move containers between machines,for comparison pull technology is the more traditional process of a client connecting to a server and requesting data;your best bet for apple ios will be using your best bet for apple ios push notification service,to make things worse git fetch git pull and git push are not quite symmetric either;we start out in your own devo-1 branch because of git checkout devo-1 although that does not matter until git pull invokes git merge below,i guess one of them is that the push model is less coupled then the pull model,"
"pull","push"," is even more in git fetch merge,cheaper overall, is definitely better overall, it is less in git fetch merge,better in better subscriptions wan,general overall,","50594836,42642241,11020597,54640007,7742259,49735019,","having a local git clone that you have to git add commit push then go to the server and git pull is even more dumb,i guess i m just surprised that the documentation recommends the approach of querying presumably continually in my case versus keeping a tailable cursor open i would have thought that push would be cheaper than pull,pull would be easier but would be a bigger load on the server and obviously not quite as real-time;for a good chat experience push is definitely better,with pull it is less repetitive to use the saved remote and the remote can be tracked by your git client to check if pull or push is needed,pull subscriptions perform much better than push subscriptions in a wan scenario,on v1.4 you can do filtered push by either specifying the filter property on the replication object to specify a filter function that you can apply on the replication specifying the docids where you can specify the list of document ids to push you can do a filtered pull by either specifying the docids where you specify the list of document ids to pull specifying the channels property to indicate which channels to pull on v2.0 you can do filtered push by specifying the documentids on the replicatorconfig,"
"dialog","pop-up","smaller than the  overall,bigger in box z-index easier,z-index less then in box z-index easier, that prompts user in user menu name,less intrusive overall,more overall, requires more in user menu name,better option in better page unlike, that tells user in user menu name, does not overall, may not in user menu name,","35033950,20348531,16581005,53664992,206134,13521463,57195151,18214671,50977739,33480938,40295322,","the purpose of this parameter is to tell the dialog what setting this parameter will be shown in and if that s pop-up then the dialog elements are rendered smaller than the dialog elements would be in the full page version,when typing in the text box the autocomplete pop-up but is bigger than the remaining space in the modal dialog,because of datebox pop-up z-index is less then your box mean dialog box s z-index it shows behind pop-up panel,on clicking second menu item another dialog pop-up that prompts user to enter the name of the item that is to be removed from the database,from a practice standpoint - excluding accessibility - modal windows provide an alternative that are less startling as say dialog boxes and feel less intrusive than pop-up windows,feed dialog is more intrusive in the sense that it has a facebook pop-up window,the pop-up dialog requires more inputs from the user so the pop-up is created in a different user control. the main view has 2 rows,generally a pop-up is a better option for a login dialog and a pop-up would allow you to easily navigate to the main page,solution that is pretty close to what i want but not really the solution what i want - open a small pop-up dialog that tells user to crop their image when the select image input button is clicked,the pop-up is rendered as a child of the body it is outside the bootstrap modal dialog;as a result the bootstrap modal dialog does not allow anything in the pop-up to be focused,i believe the problem here is that the 32feet library is built around legacy pairing so that you either need to know the pin of the device you are connecting to or you supply it with a null to get a pop-up window to enter a pin;that dialog may not have made it through to the new version of windows -,"
"dialog","pop-up"," option is a better in better page unlike, file save  message overall, that appears enter in user menu name, is a variation overall,easier in box z-index easier,more overall,higher than the  in box z-index easier, that has newsletter form overall, is a easier overall, it is better in better page unlike, is a part in better page unlike,","46682350,13270503,53719028,23787824,38296642,30921171,8527360,48227237,3143390,55078486,13971524,","if you are asking for the reason of that i believe that simply pop-up aren t intended and deeply tested for a lot of interactive content but just for small hints or small navigation menues;if you need to present to the user a lot of interactive content imho a page with the dialog option is a better choice,the responsibility of displaying the pop-up should not be the viewmodel s responsibility but a service that takes care of it for you;if the pop-up window is a modal like window like a file open dialog file save dialog message box ok cancel or yes no etc then i believe that a service would be the right choice,at the bottom of the left column click the add button + . in the dialog that appears enter a name in the simulator name text field and choose the device from the device type pop-up menu,all links with a data-rel pop-up inside a pop-up will not do anything at all;on the other hand dialog is a variation of a classic jquery mobile page just with large margin and semi transparent overlay,dialog makes it easier for you to create a pop-up window with customized contents,i am using jquery ui dialog to show more than one pop-up on one page each pop-up will have its own id and it will be triggered by an anchor with a attribute,i realize this is an older post but we just ran into the same issue and resolved the same issue by setting the z-index of the bubble pop-up to something higher than the dialog window,then i can create a pop-up dialog that has newsletter form using any jquery pop-up dialog plugins,obviously this only makes sense if you really need a full new activity say if you have lots of code to initialize the pop-up or validate the input;otherwise a simple dialog is a easier,for a pop-up it is better to use a dialog than a whole new javafx window,in this case pop-up is much better solution;unlike dialog which acts as another page and is a another page pop-up is a part of a single page so i has a much better usability in server side generation,"
"dialog","pop-up"," when calling show overall, blocked is smaller overall,better in better page unlike,","28909572,140502,784719,","the dialog pop-up is not the dialog shown;you create it and then create another completely different dialog when calling show,i think the pop-up centered on the parent window however s a better idea to show a div as dialog in the middle of your website because the chance that this is pop-up blocked is smaller and this is pop-up blocked s imo less annoying,use floating divs instead which looks like a modal dialog but are better than the pop-up,"
"benchmarking","profiling"," is more overall,often more appropriate overall, by getting a timestamp overall, that works more directly overall,much higher overall,","4675998,24810089,15578836,14735449,16500703,","benchmarking is a simple process of running a bunch of code a number of times and seeing how long it takes;profiling is more detailed and can show you how long was spent in different sections of your code including the number of calls to all methods in the stack,using a profiling is often more appropriate than benchmarking but it seems to be even more tricky,the ci profiling doesn t actually does the measuring its the benchmark-libary that does;it measures each benchmarking by getting a timestamp by using microtime when a mark is added,to get a more reliable timing of the query you d need to use profiling or some other way of benchmarking that works more directly with the query,the issue is that the execution time of a benchmarking is much higher about 3 times in case i do not invoke the profiling along with it than the case when the benchmarking is executing with the profiling,"
"upgrade","versioning","less overall,smaller than the  overall, fails in worse overall, were a bit better overall, is older in older newer server,higher than the  in higher lower old, that s newer overall,higher in higher lower old,newer in older newer server, is less then 5.5 so overall, that is less in issues current big,","21703040,25022918,10603921,27654825,54745878,34626803,54544226,2984015,1872552,35799291,19890724,","if kernel versioning is less than 2.6 then we recommend to upgrade it,note it is preferrable to use a series of if statements instead of a switch as long as the old versioning is smaller than the versioning for the n-th step upgrade that step,in addition to avoiding peculiar upgrade issues when moving to a new edition and a new versioning unless you don t really mean express is sql server 2008 you also have the added bonus that you still have your original databases intact which you won t have if the upgrade fails in worse ways than simply being blocked,so you can upgrade the x64 box and check whether everything is working;fyi i had problems with the first versioning of 6.1 iseries access when running on x64 boxes later versioning were a bit better,if your gerrit versioning is older than 2.11.11 you will need to upgrade it to make replication to github work again,after you upgrade the application such that the versioning number in the app is higher than the versioning number in the database on disk the sqlitehelper code notices and calls the +onupgrade + method with the old and new versioning numbers,the conda documentation explicitly mentions this issue in their troubleshooting documentation conda upgrade error cause downgrading conda from 4.6.1 to 4.5.x and then trying to conda install conda or conda upgrade conda will produce a solving and upgrade error similar to the following solving environment failed condaupgradeerror this environment has previously been operated on by a conda versioning that s newer than the conda currently being used,i am wondering what happens when i release a upgrade that has a higher target sdk versioning then the previous versioning of my app,these locations will convert when they go to windows 7 perhaps or because the application they use internally is upgrade and needs the newer browser versioning,and looks like your php versioning is less then 5.5 so upgrade your php versioning 5.5 and your php versioning will work,now i understand that 8.3 is a big upgrade it is the only one that caused us issues in ledgersmb;you may need some time to address that but the alternative is to get further behind and be asking questions on a versioning that is less and less in current understanding as time goes on,"
"upgrade","versioning","lower in higher lower old, number is bigger in higher lower old, is older in older newer server, which is more in better latest greater, and xcode too in higher lower old, is no longer overall, is lower in older newer server, is older in older newer server, parameter has lower value overall,less overall,better overall,","44174731,5514765,37004098,54445928,23058020,41170490,52733080,50957270,10640057,24136959,20304639,","and my php versioning is lower than 5.3.0 i upgrade that fixed my problem,if the apps versioning number is bigger the apps versioning number s an upgrade and you now need to save that as the last installer versioning,i think your opencart versioning is older 1.5.x so there is two options here;1 you should upgrade your opencart to 2.x.x,upgrade to the latest versioning which is more c++11 c++14 compliant than vs 2015 update 3,it s posible you have update to xcode 5.1 and your versioning cordova is very older;i advice you upgrade project in cordova to last versioning and xcode too,the fedora upgrade might have updated your less versioning and your less versioning is no longer working with your project,if the versioning is lower - run brew upgrade jmeter command in above setup i need to put jmeter plugins manager jar to usr local cellar jmeter 5.0 libexec lib ext if you installed jmeter by manually fetching it from jmeter downloads page extracted it somewhere and installed the plugins manager in lib ext folder - you should be good to go just make sure you run  jmeter.sh file not jmeter,though it would require the application to be adjusted in probably many places and if your mysql versioning is older an upgrade of the database,if you upgrade a module versioning a clean build need to be executed;minimum sdk versioning - checkout that the module min titanium sdk versioning parameter has lower value then the sdk you use,you are using a method to bind click event in your case is live and this has been removed in the latest jquery libraries from 1.9+ onwards so make sure your versioning is less than 1.9 or better to upgrade the library and use the recommended method for it .on to delegate the event event delegation syntax is little different with .on,upgrading a language or service can often have disastrous or unexpected consequences thus sandboxing alternate versioning seems the better approach plus it makes projects immune from os upgrade,"
"upgrade","versioning","definitely more in issues current big,more overall, also accepts the --greedy in system package can,sequence much simpler overall, you installed then in higher lower old,higher in higher lower old,general in higher lower old,newer than the  overall, addressed the issue overall,compatible between  overall, that uses a newer in itext 5.5.x task,","8607666,32629961,45116705,7859219,28951414,12299636,38623228,52708310,45217926,50675326,51495465,","if the code doesn t run on php 5.2+ then you can t use a current phpunit versioning which is definitely more of a concern and my first general recommendation is to find any issues an php 5 upgrade might bring,and if somebody is running a 6-month old versioning with a known bug upgrade becomes a more acceptable answer if they are 12 versioning behind,those having latest as their versioning cannot be checked and are always listed those marked as auto-updateable are checked and listed only if they are outdated indeed;the command brew cask upgrade also accepts the --greedy option internally it uses brew cask outdated to find the list of packages to upgrade,the upgrade sequence is much simpler i have a hard link in the app that points to the latest versioning of the apk the donwload starts and automatically laucnhes the install,actually it s not logical if you have a really done a successful major upgrade and b those dlls were only in the older versioning and not in the newer upgrade you installed then they should not be there at all;the old product is no longer there if the upgrade was successful and there is no old versioning to have those files - it s uninstalled by the upgrade,if blackberry device software versioning is higher than os 6 then continue the app otherwise prompt user dialog showing upgrade os first and exit the application,in the upcoming 5.1 versioning that shrinks even more that we re only about 3x slower than hand-mapping mainly due to null checking that hand mapping won t do;i d upgrade,this is what i see application expects grails versioning 2.5.6 but grails_home is versioning 2.3.11 - use the correct grails versioning or run grails upgrade if this grails versioning is newer than the versioning your application expects, simple cmdlet upgrade addressed the issue;the issue turned out to be that the azure powershell cmdlets up to and including versioning 4.1.x were using an older versioning of the microsoft.azure.management.streamanalytics assembly namely 1.0,we re working with gitlab 8.16.4 and i want to upgrade it but since the backups are not compatible between versioning i want to make sure everything is ok first,that means either upgrade pagerduty-client to a versioning that uses a newer versioning of commons-io that cucumber likes or downgrade zucchini to require a versioning of cucumber that works with pagerduty-client as well,"
"upgrade","versioning"," might resolve the problem in better latest greater,older than this  in older newer server,earlier overall, which is greater in better latest greater,easier newer in itext 5.5.x task,fine with  in older newer server, that has a splitter overall,more overall,older than the  in better latest greater, is more overall,much more overall,","44633273,56660628,42763968,54321896,46176002,51653383,1147858,27518664,57005337,34114806,6544864,","might be your npm versioning is less than you are running;upgrade to latest versioning might resolve the problem,i get the error as the remote debugger is older than this versioning of visual studio 2019 upgrade your remote debugger though i m using the latest compatible tools on both sides,however if you have a current production mysql versioning that is earlier than v5.7.7 then the removal of this restriction on views should only be one of the criteria being assessed while making a decision as to upgrade or not,if you are already using python 3.6 so you need to upgrade django to latest versioning which is greater than 2,itext versioning is 5.5.x but i guess we can upgrade it if the task would be easier with newer versioning,gradle sync works completely fine with versioning 1.2.51 but as soon as i upgrade to versioning 1.2.60 which released recently gradle sync fails with the following error message could not find method kapt for arguments build_9jdgbsb8xiz9dkrq42qtcg5eg _run_closure1 _closure6 576656b5 on object of type com.android.build.gradle.libraryextension,but if you do use the splitter then--yes--you will have to upgrade to a versioning that has a splitter before doing it,i heard that i should upgrade beatbox to something more than the versioning 21 to be able to access the forecastingitem object so i tried apt-get update upgrade beatbox but i still get the error,return code 18 output truncated ...g the running versioning of bundler 1.16.2 is older than the versioning that created the lockfile 1.17.3 . we suggest you upgrade to the latest versioning of bundler by running gem install bundler, propose the upgrade versioning;the upgrade is more elegant and universal way to work with tags groups and or logic,nevertheless i would recommend sticking to the queryover versioning it s much more intuitive and you avoid magic strings especially that you don t have to upgrade the nh versioning,"
"upgrade","versioning","many issues after  in issues mxnet stable, is far less stable then overall, 4 is no longer overall, path is much more overall, postgres anyway in older newer server, is listed earlier in earlier server newer,less overall,actually newer in older newer server,newer overall, is less in older newer server,older than 15.0.40108.0  overall,","54506551,55215104,45522537,56627980,45592457,26597741,42665044,46809434,1447805,14770156,48194800,","i am facing so many issues after upgrade the angular versioning then after i update the webpack to webpack 4,upgrade to latest appium versioning 1.10.0 the latest appium versioning seems to be more stable in terms of crashing of appium in long runs however this versioning is far less stable then 1.7.1 in terms of device compatibility,you should upgrade your node versioning especially since versioning 4 is no longer actively maintained which means that only critical fixes are applied until only critical fixes reaches the end of life for details see lts schedule,that way a project could still depend on their own versioning of the codestyle -- but the upgrade path is much more straight forward and should only contain of upgrade the dependency,if your server versioning is older your server versioning is highly recommended to upgrade postgres anyway to use jsonb efficiently,most likely your versioning of matplotlib is older than 1.4.0rc1 and needs to be upgrade;if you have more than one installation of matplotlib installed you ll need to make sure the directory containing the newer versioning is listed earlier than the older versioning in sys.path or perhaps simply delete the older versioning of matplotlib,if you have six versioning less than 1.10 upgrade it,apt-get upgrade actually installs the newer versioning,running pear -v gives me versioning 1.7.2 when i run pear upgrade pear it says my versioning is newer than the current noe which is 1.9.0 if ran pear upgrade -f pear to force an upgrade it downloads the files and installs successfully but i still have versioning 1.7.2 and cannot install phpunit,you are trying to connect to a sonar server which versioning is less than 3.4;if you upgrade your sonar server to lastest versioning 3.4.1 everything will be back to normal,note that as per the change list here support for remote debugging of v2 functions was only enabled in 15.0.31201.0 and was granted a bug fix in 15.0.40108.0. so go into tools and extensions see what versioning of the azure functions and web jobs tools you have and if it s older than 15.0.40108.0 upgrade it,"
"upgrade","versioning"," removes the lower in higher lower old,more simple older in older newer server,compatible with  in older newer server, is stricter overall, will not in earlier server newer, is higher in use higher maven,higher overall, is lower in higher lower old, number call a method in older newer server, is not in earlier server newer, is given here overall,","31129894,33321665,56491624,49648597,23304936,21863094,29137974,53718638,34006358,53417718,50194070,","bundles with the same upgradecode have an upgrade relationship.the;higher versioning removes the lower versioning,to upgrade your plone site from 4.3 to 5.0 you can check this link section upgrading plone 4.x to 5.0 the migration should be more simple than older versioning like 3.x or 2.x,maybe clr has already upgrade to angular 8 and you will have to do as well or you will need to grab an older versioning of clr that is still compatible with versioning 6 you re using,it was working before but not since the upgrade so this either points to a bug in the new versioning or perhaps the new versioning is stricter on something but the css syntax looks fine,newer versioning of cassandra may use new sstable or commitlog formats which the older versioning will not be able to read;2.even if it s a rolling upgrade i think the following steps will do except moving from one versioning to another,if you want to add a plugin and a plugin versioning is higher than the core mvvmcross versioning use nuget to upgrade all mvvmcross packages before adding the plugin,this will restart the site and when it runs for the first time after that it will see that the dll versioning is higher than the one in the config files and the db and will kick off the upgrade procedure,can you verify your nss versioning in your machines using  incase your nss versioning is lower than 3.23 you might need to upgrade your nss versioning,and check against a number versioning in your code if the database versioning number is less than your code versioning number call a method for to do the upgrade,if you want to keep the data you need to export it in some flat files because once attached to newer server versioning a database is upgrade and cannot be attached to an older one;even restoring a backup made with newer versioning is not possible,i don t understand the following sequence where a virtual environment in my mac os ends up with a versioning of pip that s older than the versioning it created it we ve made sure the interpreter is 2.7.15 this is just a check that the interpreter is 2.7.15 and now we generate the requirements output which is small as expected but which gives this warning which is what i don t understand we ll now leave the environment and do freeze outside it no suggestion for upgrade is given here despite the fact that we have the same versioning of python if i understand correctly as we check next why did it install a different versioning of pip,"
"upgrade","versioning"," is no longer in version warn longer, is no longer in system package can, is much more in issues mxnet stable, has better debug in better latest greater,older in older newer server, is older in older newer server, that is higher in higher lower old,easier overall, you chose the higher overall,newer than this  overall,less than 2.10.0  in older newer server,","53846718,48136700,49806235,50340669,37105692,3767831,49834893,1214032,53284452,56462393,56587249,","please upgrade to the latest version. npm warn deprecated cryptiles 2.0.5 this versioning is no longer maintained,you should upgrade your system first because the 4.9.0 kernel versioning is no longer on kali linux repository then install the appropriate kernel headers package,i also notice that there were quite a few segmentation fault issues fixed in mxnet versioning 1.1 so if you are not using this versioning i recommend to upgrade to this versioning as this versioning is much more stable,make sure to upgrade to the latest versioning because the latest versioning has better debug logging,if your ruby versioning is older than 2.2.4 then please upgrade it,your mysql server versioning is older and not compatible with the one where the dump was created;try to upgrade your mysql server or to export the dump using the --compatible option of mysqldump,major upgrade are based on upgradecode so your application just needs to do a web service call somewhere passing the current productversion and upgradecode and then you need to download any versioning that is higher than your current productversion then install it,does anyone know if the upcoming v8 versioning is easier to upgrade or if its easier to upgrade from a certain previous versioning compared to earlier versioning,.net framework 3.5 is not compatible with any versioning of .net standard the earliest versioning is 4.5 as published in docs here obviously the further .net standard versioning you chose the higher .net versioning you have to pick on the client side as standard grows continuously and hence drops support for the older frameworks;if you could upgrade your client project to .net 4.5 having the lib in .net standard 1 would resolve your compatibility issue,steps daml new quickstart quickstart-java cd quickstart daml build the following is the output on the console hikaripool-1 - start completed. hikaripool-2 - starting... hikaripool-2 - start completed. running flyway migration.. flyway community edition 5.2.4 by boxfuse database jdbc postgresql localhost postgres postgresql 11.3 flyway upgrade recommended postgresql 11.3 is newer than this versioning of flyway and support has not been tested. successfully validated 4 migrations execution time 00 00.053s current versioning of schema public 3 schema public is up to date,you likely have some versioning of requests installed that is at least 2.10.0 but you can check with the following if your installed versioning is less than 2.10.0 upgrade with proxy configuration according to the linked answer we set up a dict of our proxy using https in place of http because all of praw s requests happen over https we then have to pass this to the session that praw uses,"
"upgrade","versioning"," is greater in better latest greater, is less overall, but not overall, it lists an older overall, is older in older newer server,more than 1  overall, is less overall,better in better latest greater,newer in older newer server, is less in system package can,older in older newer server,","48626004,17791071,28937733,55247424,28832020,30088465,52390493,14940344,38471241,50035849,31218967,","if the versioning is greater than what the app was made to handle show an upgrade prompt,there is an upgrade tutorial which may be useful to you;after you have a running v4.x of typo3 on your server you may want to use the 4.5lts or the latest 6.1 but upgrading to those versioning is less complicated if you already have a 4.x running,as a software life cycle beta bleeding edge nightly build versioning tend to have more bugs or breaking changes which will directly lead to breaking up your system script;therefore choosing a stable versioning is more appropriate in most cases unless you want to have the new feature in the beta versioning;last there are usually migration guides to upgrade your versioning but not downgrade your versioning,when i do dart --version i get dart vm versioning 2.2.0 tue feb 26 15 04 32 2019 +0100 on macos_x64 however when i try to get the packages what s interesting is when i run flutter upgrade it lists an older versioning of dart tools,this error almost certainly means that your php versioning is older than 5.4;the only way to fix this is to upgrade php to at least 5.4,you want to make sure the code will work when users upgrade more than 1 versioning and that the update statement only runs the one upgrade the code is needed,as per the error message dart sdk versioning is 2.1.0-dev.4.0.flutter-050561fd82 and flutter_map versioning is less than 0.1.0;and flutter_map 0.1.0 requires sdk versioning 1.8.0 2.0.0 so either you have to downgrade your sdk to 1.8.0 2.0.0 or upgrade your flutter_map to be 0.1.0,if you re using older versioning it is better to upgrade and avail benefits of c++11 features as much as possible,however if that upgrade includes myself say i have a package named server and part of the upgrade includes a newer versioning of server then the upgrade fails,check your tls versioning if your tls versioning is less than 1.2 you have to upgrade it since the pypi repository is on a brownout period of deprecating early tls,if your versioning is older than that it won t work and you ll need to upgrade,"
"upgrade","versioning"," is no longer in version warn longer, 10.1 is more overall, that uses bower properly in older newer server, is better overall, update is the better overall, to sass 3.3.8 didn overall,number always greater overall, is more overall, modifier is less overall, attached the image overall, is older in older newer server,","54046255,47843162,21893635,1388552,47587563,23999289,25162101,8605035,54720073,50625790,48645972,","please upgrade to the latest version. npm warn deprecated boom 2.10.1 this versioning is no longer maintained,starting process with log file at var cpanel logs mysql_upgrade.20171216-091130 upgrade_mysql_with_status.log obtained versioning information from system. proceeding with mysql mariadb upgrade despite the following normal the selected mariadb versioning 10.1 is more than one generation newer than the currently installed versioning,the solution is to manually include a bower.json thus grunt-bower-install in your html or upgrade to the newer versioning that uses bower properly,another reason is that as you are doing your own custom development it is much better to have control over what versioning of the libraries or interpreters you are using rather than have a operating system patch break something that was working before;a controlled upgrade is better than having the application break on you all of a sudden,i also had the same error the error is because of versioning mismatch with the dependencies i e all the dependencies are not in the same versioning to solve this update all the dependencies or downgrade all to the common versioning update is the better options;this link helps to upgrade all the dependencies,if you re using compass the current stable versioning does not seem to support sass maps but the 1.0 does;i have also had the same error and was running sass 3.3.7 an upgrade to sass 3.3.8 didn t work but installing the alpha versioning of compass did fix it,for example if you start with a table and trunk adds column a and the branch adds column b then you merge trunk to branch - you cannot realistically upgrade to the versioning with both unless the branch versioning number is always greater than the trunk s upgrade script and that doesn t work if you subsequently merge trunk to the branch,that means your compiler versioning is more recent than the jvm versioning where you are trying to run the classes;either downgrade your machine s java compiler or upgrade the other machine s runtime jvm,the tilde versioning modifier is less aggressive than the caret versioning modifier in the upgrade it will allow,i m having jsf application and have upgrade the mojarra versioning from 2.2.15 to 2.3.0 and found that restore view is taking more time than 2.2.15 versioning attached the image,if your stack versioning is older you can upgrade stack by stack upgrade or your package manager if you install stack using it,"
"upgrade","versioning","older overall,greater then in better latest greater,higher in higher lower old,setup older in older newer server,general in older newer server,bigger jumps overall, that includes newer in older newer server,worse overall, is earlier in earlier server newer,more than one  in higher lower old, whereas using a reference overall,","44193413,12462524,24226790,44779572,3393794,38424078,47715979,46646323,1148222,57361213,32482739,","by looking at various blogs i understand glassfish versioning 4.1 has older jackson library 2.4 which does not have this method and need to upgrade jakcson versioning 2.8,i am using installshield 2011 and what i want to do is on an upgrade to install all the files that are equal or greater than then versioning that is already installed,if you are running that versioning or higher or after you upgrade follow these steps,but during upgrade setup launches the older versioning setup bootstrapper ui,the way i fixed it was to upgrade my jquery to 1.4;versioning 1.3.2 fails with newer prototype on ie8,btw - i went back and did the upgrade again with bigger jumps between versioning actually ran the site after each upgrade and it went perfectly,if you cannot upgrade visual studio to a newer versioning that includes newer versioning of nuget you can install nunit 3.5.0 which doesn t include .net standard,the problem was the same on both versioning though perhaps a bit worse after the upgrade,you have to make sure your local sql instance is exactly at the same build versioning versioning like the hosting provider otherwise your local sql may upgrade the database structure and you ll be unable to restore your local sql back on the hosting provider or you ll be unable to restore in on your local server if your versioning is earlier than the host s,if you want to upgrade to a new laravel versioning you can always follow the upgrade guide for your specific versioning laravel 5.7 to 5.8 laravel 5.6 to 5.7 laravel 5.5 to 5.6 if you are more than one versioning behind you need to apply the previous upgrade guide,the sys tables sys.check_constraints could change in future versioning and isn t considered a reliable way to access this data;your code might not survive a sql upgrade whereas using a reference table would,"
"upgrade","versioning"," is larger in system package can,greater in older newer server, -uninstalled older in system package can, that was released more overall, code is higher in higher lower old,older in older newer server, 3.x provides better support in use higher maven, is lower overall,more than one  in better latest greater, that contains libpcre.so.0 in system package can,better than last  in better latest greater,","18638814,34672781,10146830,26790320,5550445,47159711,55138861,26916986,56429404,31003066,4513809,","upgrade your system or find a way to add the updated package to your package manager;if your package versioning is larger than 1.40 you should be reasonably safe unless there was some awkward backwards-incompatible change,you used a older versioning of vagrant before 1.5 versioning and made an upgrade to vagrant greater than 1.5 versioning,ou cannot forcibly upgrade the app though -- the user will have to be involved;you also cannot installed upgradable versioning -uninstalled older versioning except by hand -- once the app is uninstalled the app has no way to trigger installation of some other app,and you re certainly not going to get any support for 5.3.1 from the community if you hit something that you think might be a bug so do yourself a favor and upgrade to a versioning that was released more recently than 2010,an app can have the same versionname as long as the versioning code is higher for the update;first versioning android versioncode 1 upgrade versioncode 2,if your versioning is older than 5.0.0 run pip install --upgrade notebook to upgrade,use maven surefire 3.x maven surefire versioning 3.x provides better support regarding the junit platform;please upgrade surefire to 3.0.0-m3 or higher,say your first versioning of application had the databasehelper extending sqliteopenhelper with constructor passing versioning as 1 and then you provided an upgrade application with the new source code having versioning passed as 2 then automatically when the databasehelper is constructed platform triggers onupgrade by seeing the file already exists but versioning is lower than the current versioning which you have passed,for anyone else facing this issue after upgrading to ubuntu 19.04 try running sudo apt --auto-remove purge npm sudo apt --auto-remove purge nodejs it seems there must be more than one versioning of nodejs or npm installed on upgrade,if you can t or won t upgrade apache you can downgrade the pcre package to the first versioning that contains libpcre.so.0 which is 7.8 i think,you can safely upgrade to trunk it better than last versioning anyway,"
"upgrade","versioning","less than  overall,more than one  overall, would still overall,older in older newer server, is an older in older newer server,more overall,greater in better latest greater,probably better in better latest greater, provides an easier in higher lower old, is more in higher lower old,number easier overall,","55362209,39541307,8395720,46572148,26560756,13499762,39303628,27508192,10623815,46211218,142427,","if it s less than versioning 236 and you can t don t want to upgrade you could simply update your execstart line to ...then put back the standardoutput line back to the default,i usually use the fall-through switch idiom so that if a user is more than one versioning behind you publish a versioning 3 and a user is still on versioning 1 some people will go through all the necessary upgrade sequentially,the dbi versioning is less likely to be the source of the trouble but the dbi versioning would still be good to upgrade to the most current versioning,warning you are using requests versioning which is older than requests-oauthlib expects please upgrade to 2.0.0 or later,if you used the sqliteopenhelper there is a separate db versioning and you can provide code to upgrade the db if a separate db versioning is an older versioning,code upgrade require system downtime and if you have a large cluster that cannot run more than one versioning of code at once you ll need to take the entire cluster down at once,so just change your versioning to something else which must be greater than the upgrade versioning so that again first condition will be false and system will not check further and will connect without using tls,as painful as it s going to be for both of us to perform the upgrade it s probably better to commit the time now and get the code up to the latest versioning than to try to find an older versioning that reduces the effort,current rvm versioning provides an easier way to do this;the upgrade option will migrate gem sets wrappers and environment files -,this number is used only to determine whether one versioning is more recent than another with higher numbers indicating more recent versioning;the value is an integer so that other apps can programmatically evaluate this number for example to check an upgrade or downgrade relationship,using compatibility as the central point in the versioning number makes it easier for users especially if te product is a library to judge whether or not they can expect a smoothe and safe upgrade or not,"
"upgrade","versioning"," is higher overall, is lower overall, which is greater in better latest greater, there really overall, is much easier and better in better latest greater,probably more overall, gets a bit overall, is older so in system package can, was higher overall,better idea in better latest greater, 1.4 is first in system package can,","42940499,47974384,54321601,4976962,5992872,24673291,1014695,49839463,56435134,21257793,16213794,","your service instances could then check the db versioning and if the db versioning is higher than the db versioning own deployed versioning then back-off from that operation and wait for the upgrade,your kafka versioning is older than your kafka versioning should your problematic class clusterresourcelistener expect versioning to be higher than 0.10.1.0 and your kafka versioning is lower 0.10.0.1;just upgrade your kafka jars and your kafka versioning ll fix this issue,since you are already using python 3.6 so you need to upgrade django to latest versioning which is greater than 2,first you are using a very old versioning of jquery - the current versioning is three major versioning ahead;please upgrade there really isn t a lot of reasons not to,v10.3 versioning is much easier and better integrated with mvvm;not sure if upgrade costs money we had sitewide license,no problem mnt i mentioned possible compatibility between the method tried in 4.25 my solution and 4.00 your versioning but keep in mind the gap in time between these versioning is probably more than two years of upgrade and also a jump between backend databases versioning oracle 10g to oracle 11g r1.,one last thing you should do is to add table valued parameters to your wish-list of reasons to upgrade to the next versioning of sql server;as that wish-list grows your justification for spending the money to upgrade gets a bit easier to make,i am reading the javaee first cup and i needs jdk8+ my versioning is older so i upgrade my java configure the system path install so on i successfully run java -version in command prompt and it recognizes me that i ve installed successfully but when i start my netbeans it can t find java,this happened to me trying to run a watchos app on device because my apple watch s watchos versioning was higher than my currently-installed xcode supported;i upgrade to the newest xcode which did also require me to upgrade to the newest macos and watchos versioning ran fine after that,if you think that writing two apps for both versioning is a better idea then should i write app on wp8 first and at the end remove special wp8 features to downgrade it for wp7.5 or maybe i should write wp7.5 app and then add some wp8 features to upgrade it to wp8,i think you could try to push you are running on a jvm that is older than versioning 1.4 which is when java.util.regex package is introduced to upgrade the system since versioning 1.4 is first released 11 years ago in 2002,"
"upgrade","versioning"," is better in higher lower old, is older than node overall, will not;finally in system package can, is older in older newer server,lower in higher lower old, is higher in higher lower old, that uninstalls the older in higher lower old,higher than  in higher lower old, you cannot overall,rule older in older newer server,general overall,","54615358,51499539,38424257,25278267,994571,10378593,25074930,19564465,15473272,4591554,53126648,","if you want to re-run expo build then you ll need to upgrade from sdk 25 preferably to sdk 32 so you won you want to re-run expo build need to update again for a while and also because each expo versioning is better than the last,it is a reported bug on npm which versioning above 6.0.0 do not support node versioning under node 6.0.0. if you run node -v there is a strong suspicion that your node versioning is older than node 6.0.0 i ran in the same difficulties by innocently upgrading npm from versioning 2.14.20 to the latest stable versioning with nodejs on v4.4.0. problem is that you cannot just upgrade node since as you mentionned npm is totally blocked,save the file and the cloned directory in packages will be ignored while the package control-installed versioning will not;finally you ll need to open the command palette and select package control upgrade package behave toolkit to get the latest versioning,update_cordova_subproject will not be available if your cordova versioning is older than 3.0;there is no direct way to upgrade older versions 3.0,if i enable this then any setup is ran every time it will be asking for upgrade either it is on lower versioning over higher versioning or higher versioning over lower versioning,as your device ios versioning is higher than what your xcode support;upgrade you xcode with the latest one xcode 4.3,yes restoring the old application versioning via rollback upon an installation failure is actually a built-in feature of windows installer but you need to configure things correctly to get it to work as you require;windows installer rollback will work as you request if you use 1 a minor upgrade or 2 a properly sequenced major upgrade that uninstalls the older versions after successfully updating all files,then run all upgrade scripts but scripts should be written in such way that they do not execute if current versioning is higher than versioning expected by the script,you have been using geocoding versioning v2 now google upgrade its versioning to v3;using this versioning you cannot get ourput directly as csv,this way the upgrade rule detects older versioning and shows a custom dialog if it does,robocopy source destination files_to_copy.ext e xd folder_to_exclude robocopy is an upgrade versioning of xcopy thus is more useful,"
"upgrade","versioning"," it has a much in better latest greater, affecting your interface overall, gives us more in higher lower old, code is then in higher lower old,somewhat earlier overall,lower in higher lower old,older in older newer server,finder instantly. with latest  overall,higher than sdk  in higher lower old, is lower in higher lower old,cheaper than the  in higher lower old,","13397461,3926623,2675744,52072274,19064464,34388748,36198561,53741839,10703254,48575039,1544149,","i would recommend that you upgrade to the latest versioning it has a much better render engine for html,define these versioning explicitly in your references so that if you upgrade the .net versioning it will continue to use the old versioning and not the new;then you should be free to upgrade all other referenced libraries without concern over the upgrade affecting your interface,redgate also has a tool that automatically packages a database install or upgrade;we don t use that one as we have found that the compare against scripts for a versioning gives us more flexibility,if the production track versioning code is higher than the alpha then as you have found users will get the production versioning;if the alpha versioning code is then increased to be higher than the production versioning they will get upgrade to the alpha,if the code is to be distributed consider compiling that java class with a target jvm versioning that is somewhat earlier than the latest â otherwise some users will be forced to upgrade their jvm to run your software and some of those won t have admin rights to do that.,for the case xcode versioning is lower than ios device s image you can either copy the disk image from other already updated xcode or maybe the internet or upgrade your xcode,also i can not use custom dimension because my piwik versioning is older and i can not upgrade it now,what i m trying to do is open finder window where file is located. with previous versioning of macos it would open finder instantly. with latest upgrade it take 30-60 seconds to open sometime it does not even work,probably the project has deployment target setting higher than sdk versioning you have installed - try to change the project to the lower versioning that is supported on your mac or upgrade your xcode to newer versioning,from tfs 2015 update 3 upgrade your tfs if your versioning is lower than this update 3 there is a task named delete files you could add this task after visual studio test task to delete the .itrace files,a new versioning of delphi s slightly cheaper than the upgrade pricing you get the new software right away no wait for purchasing ordering and you get a couple of support incidents thrown in,"
"upgrade","versioning"," is now older in older newer server, that is html5 overall, still isn overall, cannot be removed automatically in system package can, do the vb.net  fully overall,lower in higher lower old,newer overall,older in older newer server, which is faster etc. overall,higher in higher lower old, it is better in higher lower old,","34052470,23812903,20691142,9745679,12909426,40175754,26278821,40344093,49946025,24834951,49974991,","ou upgrade tfs versioning tfs migrate your database - versioning tfs became another versioning;you restored your backup tfs database versioning is now older that tfs versioning itself,to me this makes your choice pretty clear cut depending on whether or not you can require those within your organization to upgrade those within your organization browsers to a versioning that is html5 compatible if those within your organization haven t already,this versioning contains support for the postgresql json data type so i would recommend upgrading to this versioning and trying again;if you can t upgrade or the upgrade still isn t working for you you can also change your column type to something else like text,automatic upgrade are performed only if you increase the versioning;if you modify the package and keep the same versioning older packages with the same versioning cannot be removed automatically,yes they are different but you can hardly say that the vb.net versioning is less capable than the vb6 versioning;if you upgrade do the vb.net versioning fully and you will not regret the decision in the future,if product3 is installed and installed versioning is lower then upgrade,at least if the php versioning is newer and not including the deprecated functions like the extension in the error message i recommend to upgrade to a newer typo3 versioning and find a way to replace the extension ph_nextgenmenu with a newer one or edit it manually to use the typo3 sql api or the newer php functions directly,if the versioning is older than 0.10 please upgrade,angular 4 is just an imrovement over previous one like adding if then else as in for-loop addition of titlecase in pipe filters typescript versioning upgrade which is faster etc. for further reference well google best suggest to use angular cli to work on angular its used as same in prevoius versioning but there are other ways also to work on it apart from cli,also what happens when common msi c s versioning is higher in product b and b upgrade c on install,more info here also you are using a very old firebase versioning it is better to upgrade to the latest versioning to be able to use the new features,"
"upgrade","versioning","slower previous in higher lower old, is giving better performance overall,longer overall,actually newer in older newer server, is newer in higher lower old, is bigger in higher lower old,further in better latest greater,newer older in older newer server,","35099925,54653413,45740908,34669872,40447249,54749036,6078158,36084301,","since the upgrade the content authors are complaining that the experience editor performance is much slower than previous versioning,upgrade cassandra cluster if you are using lower versioning of cassandra.higher versioning is giving better performance as i am using 3.11.2 and 3.11.3,by default psycopg2 uses its own versioning of libssl and it seemed to keep a dependency on an old versioning of the library which no longer existed after my upgrade,upgrade actually installs newer versioning of the packages you have,in case you prefer not to upgrade your chrome installation you can specify a versioning of chromedriver that would work with the installed chrome versioning;you can find the compatible chrome versioning for different chromedriver releases at change 2.25 to a newer versioning if your chrome versioning is newer than what is listed there,i want to detect if a versioning is already install and if the new versioning is bigger than the current one to show an upgrade dialog with some options like keep old date and delete or not the database,you do want to upgrade no further than versioning 2.1.6 at the moment as it s the latest one that supports rails 2.3.x,also don t forget to upgrade the gradle plugin to 1.5.0 or newer as older versioning are not supported,"
"icefaces","richfaces","more semantic support in good easy better,more in good easy better,more overall,better overall,better ajax in good easy better,","69545,1465066,3252727,1014688,3252727,","there is icefaces which provides more semantic support than richfaces .also you can try nitobi suite which also provides similar kinda solution.if you are not satisfied with any of these i suggest try to write your own part extending the sun faces,richfaces looks more good option to work than icefaces bcoz it has inbuilt support its easy to learn its long time support guarantee for seam+richfaces combination,icefaces provides more good looking components than richfaces,that said i found icefaces to be a useful product and much better than richfaces especially in terms of the quality of the documentation,richfaces has better ajax support than icefaces,"
"jmock","mockito","better overall,more complex scenarios overall,","2668072,2482760,","i suggest that you to move jmock which has a better api or even to mockito whose protocol is even simpler,for the record jmock as of today supports more complex scenarios than mockito that s why the initial learning curve is steeper,"
"dask","pandas","i create as  overall,expensive than  overall,compatible with  overall, version is faster in faster following slower,slower than  in faster following slower,slow as  in to_csv method slow,quicker than  in to_csv method slow, is faster in faster following slower,far more flexible overall,helpful edit with  overall, however i overall,","52748400,56464708,54560853,51392393,48117790,52342245,56352925,51392393,44834924,52995889,55613833,","current i have this working piece of code essentially i create as dask dataframe from a pandas dataframe weather then i apply the function dffunc to each row of the dataframe,the dask documentation states that dask s set_index is much more expensive than pandas with that in mind which of the following should be a best practice the time column is filled with datetime objects,dask arrays are mostly api compatible with pandas and support parallel execution for apply,for and on my laptop the dask version is faster than the pandas one,this tells us that dask is about 50 slower than pandas for this task this is to be expected because the chunking and recombining of data partitions leads to some extra overhead,but if i invoke the to_csv method then dask is as slow as pandas,it will be quicker than pandas method dask write function will break your file into mulitple chuncks and store mulitple chuncks,when data fit in memory pandas is faster than dask,pandas is far more flexible for working with data so i often bring parts of dask dataframes into memory manipulate columns and create new ones,if you just want performance gains while sticking with pandas check out the docs here and this article i found particularly helpful edit with dask you would do,maybe dask is more efficient than pandas however i have never used dask before,"
"dask","pandas","more familiar overall,slower in faster following slower,poorer than  overall, isn overall,dataframes faster overall, that is faster in faster following slower, for larger overall,slower than normal  in faster following slower, doesn overall,general overall,fair comparison with  overall,","39856364,43827567,57609144,54234835,45993751,51831161,56084548,54157837,53053775,54777078,52172783,","this may help those confused by dask and hdf5 but more familiar with pandas like myself,1 i guess dask will be slower than pandas for smaller datasets,when reading the performance on dask is significantly poorer than pandas,looking through dask documentation it says there that generally speaking dask.dataframe groupby-aggregations are roughly same performance as pandas groupby-aggregations. so unless you re using a dask distributed client to manage workers threads etc. the benefit from using it over vanilla pandas isn t always there,when hdf5 storage can be accessed fast than .csv and when dask creates dataframes faster than pandas why is dask from hdf5 slower than dask from csv,my question is is there a a way to do this in either pandas or dask that is faster than the following sequence group by index outer join each group to itself to produce pairs dataframe.apply comparison function on each row of pairs for reference assume i have access to a good number of cores hundreds and about 200g of memory,in fact this naive dask implementation seems to be slower than plain pandas for larger problem instances,i am implementing dask but its slower than normal pandas sequential streaming,it lets you use most of the standard pandas commands in parallel out-of-memory;the only problem is dask doesn t have an excel reader from what i can tell,dask dataframe developers recommend using pandas when feasible,here is a simplified dataset i created which creates this data frame input dataframe customerid t vals 0 0 0 0 1 0 1 1 2 0 2 2 3 0 3 3 4 0 4 4 5 0 5 5 6 0 6 6 7 0 7 7 8 0 8 8 9 0 9 9 10 1 0 10 11 1 1 11 12 1 2 12 13 1 3 13 14 1 4 14 15 1 5 15 16 1 6 16 17 1 7 17 18 1 8 18 19 1 9 19 my goal output is the 8 weekly lagged vals columns including vals_0 as the current week s value with nans where data is unavailable goal output dataframe customerid t vals_0 vals_1 vals_2 vals_3 vals_4 vals_5 vals_6 vals_7 0 0 0 0 nan nan nan nan nan nan nan 1 0 1 1 0.0 nan nan nan nan nan nan 2 0 2 2 1.0 0.0 nan nan nan nan nan 3 0 3 3 2.0 1.0 0.0 nan nan nan nan 4 0 4 4 3.0 2.0 1.0 0.0 nan nan nan 5 0 5 5 4.0 3.0 2.0 1.0 0.0 nan nan 6 0 6 6 5.0 4.0 3.0 2.0 1.0 0.0 nan 7 0 7 7 6.0 5.0 4.0 3.0 2.0 1.0 0.0 8 0 8 8 7.0 6.0 5.0 4.0 3.0 2.0 1.0 9 0 9 9 8.0 7.0 6.0 5.0 4.0 3.0 2.0 10 1 0 10 nan nan nan nan nan nan nan 11 1 1 11 10.0 nan nan nan nan nan nan 12 1 2 12 11.0 10.0 nan nan nan nan nan 13 1 3 13 12.0 11.0 10.0 nan nan nan nan 14 1 4 14 13.0 12.0 11.0 10.0 nan nan nan 15 1 5 15 14.0 13.0 12.0 11.0 10.0 nan nan 16 1 6 16 15.0 14.0 13.0 12.0 11.0 10.0 nan 17 1 7 17 16.0 15.0 14.0 13.0 12.0 11.0 10.0 18 1 8 18 17.0 16.0 15.0 14.0 13.0 12.0 11.0 19 1 9 19 18.0 17.0 16.0 15.0 14.0 13.0 12.0 the following pandas function creates the goal output dataframe and runs in roughly 500ms i can also accomplish this in dask using map_partitions and get the same results in 900 ms presumably worse than pandas due to the overhead from spinning up a thread i can also accomplish this in pyspark note for both dask and spark i have only one partition to make a more fair comparison with pandas with the following code i get the correct results back although shuffled +----------+---+------+------+------+------+------+------+------+------+ |customerid| t|vals_0|vals_1|vals_2|vals_3|vals_4|vals_5|vals_6|vals_7| +----------+---+------+------+------+------+------+------+------+------+ | 1| 3| 13| 12| 11| 10| null| null| null| null| | 1| 0| 10| null| null| null| null| null| null| null| | 1| 1| 11| 10| null| null| null| null| null| null| | 0| 9| 9| 8| 7| 6| 5| 4| 3| 2| | 0| 1| 1| 0| null| null| null| null| null| null| | 1| 4| 14| 13| 12| 11| 10| null| null| null| | 0| 4| 4| 3| 2| 1| 0| null| null| null| | 0| 3| 3| 2| 1| 0| null| null| null| null| | 0| 7| 7| 6| 5| 4| 3| 2| 1| 0| | 1| 5| 15| 14| 13| 12| 11| 10| null| null| | 1| 6| 16| 15| 14| 13| 12| 11| 10| null| | 0| 6| 6| 5| 4| 3| 2| 1| 0| null| | 1| 7| 17| 16| 15| 14| 13| 12| 11| 10| | 0| 8| 8| 7| 6| 5| 4| 3| 2| 1| | 0| 0| 0| null| null| null| null| null| null| null| | 0| 2| 2| 1| 0| null| null| null| null| null| | 1| 2| 12| 11| 10| null| null| null| null| null| | 1| 9| 19| 18| 17| 16| 15| 14| 13| 12| | 0| 5| 5| 4| 3| 2| 1| 0| null| null| | 1| 8| 18| 17| 16| 15| 14| 13| 12| 11| +----------+---+------+------+------+------+------+------+------+------+ but the pyspark version takes significantly longer to run 34 seconds i kept this example small and simple only 20 data rows only 1 partition for both dask and spark so i would not expect memory and cpu usage to drive significant performance differences,"
"fckeditor","summernote","more overall,","39551686,","i know summernote does that but fckeditor is a more complete tool than summernote so i cant switch to that,"
"opennlp","stanford-nlp"," does better overall,easier in training project stanford,more accurate detection overall,easier in training project stanford,compatible with   overall,","40033779,6603262,40036005,45592826,49229085,","accuracy-wise my anecdotal experience is that stanford-nlp does better on general-purpose text,i looking to use a suite of nlp tools for a personal project and i was wondering whether stanford s stanford-nlp is easier to use or opennlp,but if you will look at the accuracy level stanford-nlp have more accurate detection than opennlp,i find training in opennlp much easier than in stanford-nlp,the output is fully compatible with opennlp stanford-nlp etc,"
"ecdsa","rsa","smaller overall,slower than  overall,much faster then overall, is more overall, is also in lowest nist bigger,much faster overall, signatures twice overall, verifying is slightly slower overall, and thus in lowest nist bigger,","2350959,57153085,44352675,51356439,41289399,38001023,51922844,56621397,52587228,","if you re interesting in reducing the size of the resulting cookies you should consider using ecdsa rather than rsa to produce the signatures - ecdsa signatures are considerably smaller than rsa signatures of an equivalent security factor,it s slower than rsa and ecdsa it has more perfect or broken states than rsa and its key generation is several orders of magnitude slower than rsa and ecdsa,rsa is much faster then ecdsa at verification,key pair using rsa or ecdsa is more preferable as a security point of view,so the lowest commonly supported ecdsa keysize keys based on nist p-256 secp256r1 gets 128 bits of security which nist rates as good for 2031+;rsa is also a better choice than dsa because it has much better breadth of support for signatures still considered secure by nist,ecdsa is much faster than rsa for private key operations so it should definitely be preferred over rsa when high efficiency is required unless rsa is still fast enough something that may very well be the case,one reason for that is because ecdsa signatures are much smaller than rsa signatures twice the ecc key size which would be about 64 bytes for a very secure 256 bit curve,for rsa as implemented in the modern era since about 1980 signing is much slower than verifying;for dsa and ecdsa verifying is slightly slower than signing,rsa is using bigger keys than ecdsa and thus it is recommended for inclusion only for its backwards compatibility properties and only for usage where legacy constraints or government regulation forbids the usage of more modern approaches,"
"bufferedoutputstream","dataoutputstream","more appropriate overall,more efficient overall,","30035138,36750852,","a bufferedoutputstream is more appropriate that a dataoutputstream imho,it will wrap the original stream in a bufferedoutputstream which is more efficient which is then wrapped into a dataoutputstream which offers additional nice features like writeint writelong and so on,"
"jboss","weblogic","more restrictive overall,much more overall,more complex overall,right than  overall,","33929846,30139205,5919293,12638140,","i am running an application on jboss 7 and weblogic 12 when i make a soap call on weblogic is more restrictive than jboss,if your application is self-sufficient i would recommend going for tomcat as jboss is much more resource intensive as well as weblogic moreover weblogic costs some money,i had worked on jboss for a year and on weblogic for more than a year now my experience with the web logic is good compared to jboss weblogic is more stable and robust it can handle more than 3000 concurrent requests without throwing a single exception where jboss failed to do so and admin console for the weblogic is excellent but i think weblogic is more complex then jboss,well glassfish is more right than jboss and weblogic business-bean-classes should all have jboss and weblogic business-bean-classes own ejb-local-interface,"
"free","realloc"," it throws an exception in vulnerability realloc also, may affect subsequent allocation in malloc calloc function,even more overall,more complicated overall, is implicitly in malloc calloc function, with making a call in malloc calloc function,better overall,worse - after the  in valid ealloc allocates, doesn overall, is not overall, had better in valid ealloc allocates,","19833997,32118736,26536703,7188085,46556610,41167730,4190023,49508911,13325659,57498612,21067979,","first of all i d like to point out you are not addressing the vulnerability as the memory released by free is not being cleared as well same as realloc;also note your code does more than the old realloc it throws an exception when out of memory,and to free a memory not allocated by malloc or similar functions crashes your program;attempting to free an invalid pointer a pointer to a memory block that was not allocated by calloc malloc or realloc may affect subsequent allocation requests and cause errors,if you don t mind the libc allocation functions realloc is even more efficient it wont copy the data on a shrink just mark the extra memory as free and if you grow the memory and there is memory free after it will mark the needed memory as used and not copy either,dynamic memory management on an small embedded system is tricky to begin with but realloc is no more complicated than a free and malloc of course that s not what it does,the function free does not attempt to deallocate storage by calling operator delete;storage allocated directly with malloc calloc or realloc is implicitly declared reachable see 3.7.4.3 on allocation ceases to be declared reachable on deallocation and need not cease to be declared reachable as the result of an undeclare_reachable call,the function called realloc will reallocate the area of memory provided that it was previously allocated with malloc calloc or realloc and not yet free with making a call to free;if free is called on a piece of memory prior to realloc will result in undefined behavior,a malloc free pair can be better than a realloc if you don t need to keep around the original memory,even worse - after the realloc whatever value the x used to be isn t a valid pointer anymore because realloc free that old memory,if you don t care about the content the standard idiom is to do free followed by malloc;finding a block is cheaper than copying it and there is no guarantee that realloc doesn t do some searching of its own,since the argument to free is not declared neither is the first argument to realloc;realloc can free the memory that its argument points to if it can t reuse the same memory,ealloc is like free in case realloc allocates entirely new memory realloc free s the old allocation;so any pointer passed to realloc had better be a valid heap-pointer or a null-pointer,"
"free","realloc","more memory than the  in vulnerability realloc also,","27412208,","if memory is plentiful then you can skip the realloc -- the output buffer will occupy more memory than the realloc needs to do but the realloc will be recovered when the output buffer is free,"
"hyperthreading","processors"," will not overall,more in thread threads program, is greater overall, has more in thread threads program,poorer performances overall,better older overall,more overall,","14446438,2904283,14507745,43429428,5195665,10403526,21875112,","logical processors share nearly all other resources on the physical processors such as caches execution units branch predictors control logic and buses;if the threads execution are each keeping one or more of those shared resources such as the execution unit or buses 100 busy then the hyperthreading will not improve throughput,a quad core intel cpu s with hyperthreading enabled has 4 physical cores yet 8 logical processors hyperthreading creates 4 more logical processors,in computer 1 the eigen3 performance is worse because the number of total processors virtual + physical - due to hyperthreading is greater than the number of physical processors,ow many cores does your processors have and does your processors have hyperthreading enabled if your processors only has one thread trying to multithread will actually slow your processors down;if your processors has more you can use as many threads in your program as you have available in the os,first step would be to investigate why a processors with hyperthreading simultaneous multithreading could lead to poorer performances than a processors without this technology,more modern processors handle hyperthreading better than older processors,you may find that hyperthreading helps more on code that is using large amounts of memory so that the processors is regularly blocked on fetching from memory,"
"onkeydown","onkeyup","more overall,better overall,","7049468,3789935,","usually onkeydown is more preferable then onkeyup for such combo,for browsers that don t support either event you can fall back to onkeydown with a 0ms timer for the check which is a little better than onkeyup,"
"onclicklistener","onitemclicklistener","much better overall, should not in common if viewholder,worse overall, is better in common if viewholder,","28295873,15381262,42697073,44638473,","if you still would love to manage onclicklistener i will tell why onitemclicklistener is much better than onclicklistener,but the better choice would be to set items non focusable and use onitemclicklistener or make them focusable and use an onclicklistener on the views;also the onclicklistener should not be set for the listview,is using onclicklistener in arrayadapter worse than onitemclicklistener,also it s more common to use onitemclicklistener for a listview;if you use a viewholder for your listview then onclicklistener is better,"
"equals","gethashcode","compare as  in method object value,compare as  in method object value,quicker in method object value,compare as  overall,quicker overall,more tolerant overall,implementation much more overall,faster in hashcode iequatable groupby, methods implemented correctly overall,faster in union slower equivalent, is more overall,","7309301,50752838,13938719,7458116,16381,18076696,20598647,35866139,56672107,36041588,23118632,","if two objects compare as equals with equals then two objects compare as equals with equals gethashcode must return the same value,however if two objects do not compare as equals the gethashcode methods for the two objects do not have to return different values,not sure about the overhead of calling gethashcode a few million times but comparing two ints is probably going to be a lot quicker than the equals object method,however two instances of suite representing spades will not compare as equals because two instances of suite representing spades are different instances and you haven t overridden equals and gethashcode by necessity,gethashcode should be a lot quicker than equals but doesn t need to be unique,override both methods but make gethashcode more tolerant than equals that means make unequal objects have the same hash code and not the opposite,in the equals method only if you re certain the ensuing equals implementation is much more expensive than gethashcode which is not vast majority of cases,1 is comparison via gethashcode check if the hashcode of both objects are the same faster than equals,a correct answer will depend on what type the key and value parts are if one of the key and value parts is a for example the equals comparison is trickier but in general assuming that all the keys and values have all the keys and values equals and gethashcode methods implemented correctly we can use the system.linq extension methods any and except to get the result,and that is for performance reasons assuming that a gethashcode implementation should always be much faster than an equals implementation,i d implement your equals in this way;gethashcode is more complicated,"
"equals","gethashcode","compare as  overall,faster equivalent in union slower equivalent,more specific overall,slower in union slower equivalent,compare as  in method object value,compare objects with  in hashcode iequatable groupby,","50050070,20598647,2872561,31878297,50752838,38761908,","there are several different ways of solving this but if you want instances to compare as equals if their field values are equals you want to implement iequatable depending on how you use book you may want to make its fields into properties you may want to override and and may want to change your gethashcode implementation but what i have shown is a good start,even though gethashcode itself should be fast it s not mostly faster than equivalent equals,equals on the other hand can test accross a larger number of fields - ie its test is more specific than gethashcode comparisons,if you do not implement gethashcode union will call equals which will work but is slower than gethashcode,if two objects compare as equals the gethashcode method for each object must return the same value,implementing iequatable isn t enough since groupby first uses gethashcode to determine the hashcode before the hashcode starts to compare objects with equals so that s the initial filter,"
"arrays","for-loop"," has less in value index first,simpler overall, isn in better number straightforward, is smaller then min then in value index first,more in better object big, is better in better object big,probably closer overall, is higher overall,less in length line different, is less cheaper in methods faster foreach-loop, is really overall,","22725662,41347938,13388310,38582420,44034616,52048576,16072349,53165257,27032881,48966859,21279502,","you re initializing i to 2 in your for-loop;if your arrays has less than three items you will get an index out of bounds trying to access a location that does not exist,note that you should check if index is within arrays bounds in such cases and that system.arraycopy is more efficient and arguably simpler than a for-loop for copying arrays,while this is certainly easier and arguably more readable to use than a for-loop keep in mind that it will perform no better than using a for-loop it s just hiding loops from you;the equal method for arrays isn t overridden from the default object implementation that just compares the references of the objects which aren t equal,for-loop is the comparison between min and other elements;if any other element in the arrays is smaller then min then its value is assigned to min and finally it prints the min smallest element in the arrays,but modifying the arrays is more work than a simple for-loop again,sometimes a simple for-loop especially when working with arrays is better,try catch inside for-loop is probably closer as long as you don t mind the unassigned values left in the arrays,afterwards return false if the number of 7s is 0 or true if the number of 7s the number of 7s found in the arrays in a for-loop is higher than 0 2 another much quicker way would be to return true in the for -loop when you encounter a 7 and after the for -loop just return false,so when you go up an arrays 0-9 you want the top of a for-loop to be less than the arrays length when you are going down the arrays 9-0 you want the lower bound to be less than or equal to the bottom of the arrays otherwise you will start out trying to access at 10 the arrays length and get an outofrangeexception,like described in this question iterating an arrays with a for-loop is less cheaper than using a foreach,second test shows the for-loop is taking more than 30 of the time;i think what typed arrays is really in lack of is some native code that does the block copy that would really be used in a game development rather than set pixel by pixel in the tests,"
"arrays","for-loop","slower in value index first, reaches.length x in value index first, is not overall, is actually faster in methods faster foreach-loop, is faster and more in methods faster foreach-loop,faster in methods faster foreach-loop, which is generally faster overall, not in better number straightforward, are not truly randomly in value index first, is greater than 0 just in value index first, approach is easier overall,","27901842,47603497,46259317,40461004,52457528,41177212,3791525,51393834,38390952,44210933,52389196,","in this case instead of generating two large matrices with the row and column indices you can use a for-loop on the rows of your arrays it s slower but not as slow as a double for-loop,because your for-loop isn t used other than to repeat your code it ends up looking like this;i would create an arrays of times logged whenever the key is pressed and unbind the key when the arrays reaches.length x,see below that the first two loops only execute the console.log statements once while the standard for-loop executes the function as many times as specified in this case array.length 6;using the for...in loops for an arrays is not wrong although i can guess why someone told you that,using a for-loop is actually faster than using enumerable.range .toarray;i measured a for-loop for creating an arrays of 1 000 000 elements,to check by doing would require you to implement your arrays as below and it will look for keys that are identical to your search or if you want to search for a word in the keys of an associative arrays or with a foreach however doing a foreach with a for-loop is faster and more optimized,the for-loop is faster than the foreach-loop if the arrays must only be,a straightforward solution is to iteratively create each of the arrays using a for-loop or list comprehension or use a higher dimensional arrays where each of these 1d arrays is a row in your 2d arrays which is generally faster,as ulrich eckhardt pointed out a better way to implement this is to use a for-loop it s more clear this way and the scope of ptr is limited to loops which is good because we don t need it later;str is a 50 element char arrays not a pointer,though the results i had found that decrementing or incrementing your for-loop contained no noticeable advantage;if the values of the arrays are not truly randomly distributed i found that the simple used in the first implementation is actually significantly faster,set min and max to first arrays position;you don t need check if the current index of for-loop is greater than 0 just check if min variable is greater than current random number,ou could then write code that loops through the arrays extracts the rate value for each arrays entry and prints the arrays;as others have said you could do this with a map statement but if you re new to programming then the for-loop approach is easier to understand,"
"arrays","for-loop","faster in methods faster foreach-loop, syntax is less overall,less than  in value index first,far better in better object big, is considerably faster mainly overall, makes more sense in better object big, but not overall, you need to always overall,less in value index first,less in methods faster foreach-loop, is shorter in length index statement,","44503420,18071192,57357428,35089734,56096409,46259317,53659983,43512573,9698965,26838242,25691233,","the for-loop is faster than the foreach-loop if the arrays must only be accessed once per iteration,while loops aren t normally used to iterate over arrays as for-loop syntax is less verbose and allows the sentinel variable i to fall out of scope while the for-loop syntax does not,in for-loop i compare the value from adc and the value from the arrays and if my adc value is less than arrays value it keeps decrementing the arrays element and eventually it will end up in 258 adc value which is last element of my arrays,but then normal for-loop is far better for arrays than using for-in which is actually for object,results all null arrays function elapsed time delta vivek 0.654 0 nick2 0.886 35 nick1 0.964 47 eddie 3.122 377 results arrays with 999 null values 1 random non-null value different each iteration function elapsed time delta vivek 0.305 0 nick2 0.888 191 nick1 0.891 192 eddie 3.114 921 as can be seen for the all- null arrays the for-loop is considerably faster mainly because it doesn t suffer the overhead of a function call for each value,3 the standard for-loop will execute a function as many times as you define in the parameters and since an arrays is numbered an arrays makes more sense to define how many times you want to execute a function,your code not only allocate memory for arrays of pointers the blue arrays but in the for-loop you also allocate memory for the red arrays as well;so free arrays line alone will just free the memory allocated by the blue arrays but not the red ones,your for-loop is not doing that;when you are trying to access the contents of an arrays you need to always write defensive code and make sure that you never access the arrays using out of bounds indices,but the for-loop is not reading writing the last part of the parent file which is less than the arrays size,i am just starting to learn about the streams and parallel in java and i was wondering why a normal for-loop takes less time than intstream paralleled at adding items to an arrays,and you should return outside the for-loop;you should note however that you will get an exception if the length of operand2 arrays is shorter than the length of operand1 arrays,"
"arrays","for-loop"," will still in length index statement,less than only  in methods faster foreach-loop,much more overall,more overall, its not in better number straightforward, which has less in value index first, is clearer;consider in methods faster foreach-loop, does nothing more in methods faster foreach-loop, is not overall,more adequate in length index statement,less than your  in value index first,","33662238,57348154,27306889,34698551,56568360,43483491,44667158,54747421,10604601,32296275,53290980,","inside the for-loop it is much more standard to go from 0 to q.length;that way you can change the length of your arrays and the for-loop will still work,environment macbook pro macos mojave 10.14.6 xcode x86_64 centos7.6 clang++ test code running results macos xcode unreasonable with or without optimization std sort sorts the arrays this time should not be less than only for-loop without optimization 0.000203 s,i ve used it for convenience a for-loop is much more reliable for converting an htmlcollection to an arrays,a for-loop is more suitable to iterate through the arrays,you can do something like remember to add +1 to the index and your method would be this will be better that using a for-loop its not optimal it will increase o n .;you need to iterate over an arrays to make the ngfor work this means that you need to make an arrays from the number of days as srjkaa said on his example,if element does not exist at the index of the arrays which has less .length set empty string as value of property,also if doing a certain number of time the i feel a for-loop is clearer;consider using an arrays of ints rather than a-i,the difference between array.map and a for-loop is that the for-loop does nothing more than iterating over the values of the arrays,alternatively you could use an arrays that is actually 3x2 instead of 4x3 then fix your for-loop to use indexing that matches;the declaration of your arrays is not correct it s 4x3 instead of 3x2,a for-loop is more adequate than a do while for simply iterating an arrays string,your code should look like this your code received an arrayoutofboundsexception because the k in your 3rd for-loop should be 3 instead of 4 since your k should be less than your arrays size which is 3,"
"arrays","for-loop"," by creating a correlation in filter byte zip, not in reference error dimension, and not in value index first,greater in length index statement, is called results.genres here in better object big, has only in value index first, is much better in value index first, is better and more overall,faster in methods faster foreach-loop, this is typically more in better number straightforward,nothing more in methods faster foreach-loop,","53489689,22780626,49334364,32706815,13658554,13073710,3638337,41796766,37788181,34030414,9251272,","this is faster than a for-loop by creating a correlation matrix with np.newaxis in the index and then np.argwhere mask to get the mapping from the old arrays to the new,the code actually works but there is an error in the method for-loop that cause a indexoutofbound exception this is the correct version;the method returns void but you can read the modify values inside the arrays because you are passing as method argument the reference of the arrays not a copy of the arrays,at the end of the for-loop you store the value in the arrays and then at the end return the arrays so the end of the method will look like;the calculatebonus will use this returned arrays as input and it will also work internally with an arrays and not an int as you have now,also change your logic in the for-loop to be not since i will not ever be greater than the arrays length,this is how you iterate on the elements of an arrays using a for-loop do not use a for...in loops on arrays if you ever get tempted;your arrays is called results.genres here,your last for-loop doesn t print the last element in the arrays;if the arrays has only one element it won t print anything at all,the iterating for-loop is much better;if you need to know the index into an arrays you should use the range operator,this will also work but using for-loop is better and more elegant way;also this cant be used in associative arrays unless you use array_values,i completely failed to check that assertion and just jumped into the analysis of how the enhanced for-loop is faster on arrays than lists,in method mergeduplicates implementation foreach executes a closure over every element in the arrays this is typically more straightforward and transparent alternative to old-fashioned for-loop,looping with a for-loop is nothing more than a basic iteration of an arrays using a for-loop,"
"arrays","for-loop"," was easier in value index first,slower overall,more point in length line different, it takes 60 less overall, which is a better in better object big,smaller in value index first, your ending condition in length index statement,better in better object big, but only in better number straightforward,better in better object big, is a better in better object big,","28357864,14566564,46087164,54460649,35000257,43668103,14168544,27683326,53510046,10956586,54208581,","your first for-loop was easier because both counters where the same namely i;it went up for position in the arrays and for the value to put in the arrays,when summing an arrays over a specific axis the dedicated arrays method array.sum ax may actually be slower than a for-loop,in your code on line in the for-loop does no more point to an arrays --the compiler has no information about the length of the variable that it s pointing at whether it is an arrays a single chars or an int or something else-- but it is just a plain pointer presumably a pointer pointing to a default int,12.8 sec implementing the arrays manipulation right where needed use for-loop it takes 60 less time 4.7 sec vs,for-each loops is an iterable form of ordinary for-loop which is a better built data structure;cars is an arrays then for eg -,as mentioned above as you remove items the arrays gets smaller so a for-loop is probably not the best solution,so the for-loop will never end continuing on to i 5+ which throws an error because your arrays is not that big;in your for-loop your ending condition depends on a changing but you don t change it,but when looping through an arrays it s better to use a regular for-loop,a for-loop is generally better for doing something a fixed number of times;and calculating the percentage inside the inner loops is a bit wasteful you re recalculating the inner loops for every number in the inner arrays but only using the last one,but you will have loops the outer arrays in order to get it to work which would be better done with a for-loop so you reference both at once,and for what it s worth when you re iterating over an arrays a for-loop is a better fit because a for-loop allows you to define the iteration variable the condition and the increment all in one place,"
"arrays","for-loop"," will not overall,method faster in methods faster foreach-loop, methods have better abstraction in methods faster foreach-loop, jobs is still overall,appropriate than enhanced  overall, lists is smaller in value index first,faster in methods faster foreach-loop,faster in methods faster foreach-loop,more overall, as well in value index first,quicker overall,","8303765,18444065,50661538,43059247,5754894,56795519,34076726,29868617,6676226,55520799,21505023,","well the only way i could think of was to store the column values in arrays and then using for-loop to spit each value out along with their percentage;now arrays in awk are associative and are never in order i.e pulling the values out of arrays will not be in the same order as they went in,when you know both objects are arrays method is a faster way to check equality than for-loop,probably easier to use a nested map instead achieving the same thing with a for-loop is much more verbose and confusing and shouldn t be done in most cases arrays methods have better abstraction and don t require manual iteration but if necessary,first of all the issue in your question is that inside the for-loop client.get is invoked with an asynchronous callback where the synchronous for-loop will not wait for the asynchronous callback and hence the next line is getting called immediately after the for-loop before the asynchronous callbacks;at the time of the line is getting invoked the arrays jobs is still empty and getting returned with the response,this is a situation where an index-based for-loop is more appropriate than enhanced for-loop that you re using as what you need to grab is the index;you can base all your work on the original arrays rather than converting the index to a list and i suspect you were going for case-insensitive match,if one of the arrays lists is smaller than you should use that particular one s size in a for-loop,for example sometimes a for-loop is faster than the built-in arrays methods in some browsers,this is the reason why working with the higher-dimensional arrays ends up being so much faster than the for-loop -based code,the only way to copy arrays that is more efficient than for-loop coding is system.arraycopy,having said that if you want to use for-loop here is your corrected version as you can see the result is the same but in comparison it is more imperative rather than declarative it is more verbose and larger therefore more error-prone your problem was that you were updating the arrays as well iterating it causing the iteration to broke,to my knowledge and research so far - javascript s native for-loop is quicker than arrays map for iterating through the arrays,"
"arrays","for-loop","empty hopped over the  in value index first, element not in value index first, does not overall,faster in methods faster foreach-loop, is shorter in reference error dimension,less in value index first,more in value index first, isn overall, to iterate the dataframe in reference error dimension, code is more in better object big,usually more information overall,","9993386,24894645,5532921,12418192,55573712,31977008,21524225,1263645,50284375,53011957,11852754,","if you are saying that the arrays is empty because the for-loop was not entered it is probably because your integer i is not initialized;therefor it contains some garbage value which was likely higher than the value of self.arrayquattro.count making it appear that the arrays was empty hopped over the for-loop,i get how to assign each individual element one by one in a for-loop but i don t get how to individually pick one element out and assign it without touching the other elements;note that the question asks for you to assign x to the arrays element not the other way around it will look like,your for-loop does not terminate before the arrays is exhausted;accessing outside the arrays limits invokes undefined behaviour,it is clear without any performance tests that native javascript for-loop is faster but there is no big difference for small arrays like 10-20 small items,you could use a switch inside a for-loop to handle the different id values this will not generate an error when arrays is shorter,okay so after telling the user to enter the size of the arrays the arrays was assigned to lets say 5 therefore in the first for-loop the program is checking if i is less than the arrays size so it can only be entered 5 times as that is the size of the arrays,im going to have a second for-loop that makes more arrays and names these arrays as the strings from my previous arrays,if varscount is 0 the for-loop won t execute anyway;it means one of your arrays isn t actually an arrays,example the data frame looks like i tried and it returns an error valueerror arrays were different lengths 2 vs 3 i wonder if there is any better solution than using for-loop to iterate the dataframe,instead of this my opinion of the code inside the for-loop is that the arrays code is more readable than the object code,an arrays usually offers more information for alias analysis and after some optimizations the same code will be generated anyway search for-loop strength reduction if curious,"
"arrays","for-loop"," has one element in reference error dimension, makes more in value index first, size is 1 less in value index first, does not overall,faster in filter byte zip, and then overall, is getting smaller in value index first,compare it with  overall,faster in methods faster foreach-loop,better in better number straightforward, by comparing the sizes in value index first,","24069480,32381524,48826347,41529693,2963102,13929876,21946576,49215358,32350088,29865957,51439353,","the only ways this would not throw an error at runtime is if you passed an empty arrays as a method argument in which case the for-loop would not execute;also as bhesh suggests if the arrays has one element and the minidx is that first element at index 0 loops will execute once enter the first if and continue,i think a for-loop makes more sense here;a couple of issues with your code was that you were writing out the file in side loops and you incremented the index by 1 which would then point to the wrong line in the arrays and result in the exception you are getting if the last line contains the text you are searching for,in your class constructor this capacity++ will first return the capacity and then increase its value by one. because of this when you are filling your arrays in a for-loop you are going out of arrays range because your arrays size is 1 less than the capacity value,you were correct in writing a for-loop to iterate through the objects in the data arrays however to get the month s name you can t just index the original data arrays;the original data arrays does not contain the month s name,it returns a byte arrays of all the pixels which can be iterated much faster than a for-loop with a call to getpixel inside nested inside another for-loop,traditional for-loop is faster than foreach + range;the first one only uses integer comparison and increasing while the last one has to create an possibly big arrays and then extract each element by moving the internal arrays cursor and checking whether the end is reached,the problem is that your first for-loop is looking at the index.length;it is constantly changing because the arrays is getting smaller,i was trying to compare it with for-loop but i don t know how to write code so that arrays checks it s own strings for duplicates without already pre-determined string to compare,thus the for-loop is faster than the foreach-loop if the arrays must,also with the for-loop it s considered better to limit the scope of the iterating variable i and to use println you need system.out not just system and you need a string java arrays do not override tostring so something to output the numbers the user entered after loops like,next the input arrays which is larger than the other is just added to the result arrays by comparing the sizes of the input arrays inside the for-loop,"
"arrays","for-loop","easier than a  in better object big, is already in reference error dimension,4x faster in filter byte zip, has more in length line different,only works between  in reference error dimension,much larger in value index first,still more in methods faster foreach-loop, is more specifically in better object big, difference is more overall, version is faster overall, and doesn overall,","53599166,48753649,34796354,41948245,30825957,41951140,34389194,50690416,54537919,37788181,54575330,","to make it easier than a for-loop you could use .map to create a new arrays from the old one by transforming each object,if anyone could explain why the for-loop is faster than the direct access of the arrays that would be great. for reference the shape of matrix_n is 48 100 100 30 and the shape of matrix is 48 30 100 100 so in the second method im simply building the arrays from the 2nd dimension and in the first method the arrays is already built,running a quick benchmark it seems that the for-loop is 4x faster even in the worst case where you have to replace every single time and you construct a new arrays to hold the replacements,thus your for-loop may run while is less than -lt not less or equal -le;also you cannot use the same index variable for two different arrays and unless you made sure two different arrays have the same length or at least that the second arrays has more elements than the one used to determine the maximum index,consider using the linq method instead of a for-loop this looks cleaner;there s no other way you can cast an arrays of value types to an arrays of objects you can t leverage arrays covariance in this case because this only works between arrays of compatible reference types,above is a simplified version of my actual code where the c arrays is much larger so i have to use a for-loop to get every index,but the outer for-loop still runs 9 more times pushing temp which is already a set number onto the numbers arrays,on the outmost for-loop iterates over an arrays this arrays is more specifically a list of objects where each contains an arrays with codes that i need to use in order to reach some information provided by an external api for the sake of simplicity i am using mockup objects data resultofrequest code1 code2 code3 resultofrequest code4 code5 code6 .,here are two solutions at least the first is o n 2 and require to have two for loops to compare all the elements between two solutions at least the first is o n 2 self the second solution that you should use and that the problem force you to choose because the problem says that you can loops the arrays only once it is to find in one for-loop the minimum and the maximum of the arrays and control the arrays difference;if the arrays difference is more than the given number return true otherwise false,rraylist s listiterator has to do some work before arraylist s listiterator can return the next arrays entry whereas when the enhanced for-loop knows arraylist s listiterator s dealing with an arrays arraylist s listiterator can work directly on arraylist s listiterator;the test classes above have testing cruft in the test classes above but to see why the arrays version is faster let s look at this simpler program,the problem is that if the input arrays is length 0 the for-loop never runs and so you have no return statement;that way if the check is successful the last recursive call is run on an empty arrays and doesn t return anything so you get an answer of none,"
"arrays","for-loop","less in length index statement, is about more in methods faster foreach-loop,longer in longer hits numtotalhits, is bigger overall, looks a bit in value index first,ndx less overall, s size overall, map is better use in better object big, not overall,less in length index statement,faster in better object big,","41386426,34714325,13821093,55484047,17058770,13575677,53570256,55369757,18676962,31072639,19608875,","i fixed this by adding after the for-loop and before the return statement which fixes the problem but if the for-loop is written to continue while i is less than the arrays length and when the arrays only contains a the length is one and i is 0 shouldn t it also pop a,arraylist - for-loop is about more than 2 times faster speed than foreach loops;arrays - both are in equal speed.but foreach loops seems to be a bit faster,note that this is one of those cases where matrix division of large arrays takes longer than a for-loop,first your for-loop goes through the bullet arraylist however you are using that index to modify the enemy arraylist as well;so if bul.size enm.size the bullet arrays is bigger than the enemy arrays this would be what is causing the indexoutofboundsexception,if you don t need i for anything else than extracting the element then the enhanced for-loop looks a bit nicer;if you are using i for both accessing the arrays and something else then i would argue way 1 is best,on the last iteration of the outer for-loop ndx is one less than array.length so when you call arrays ndx+1 that is equivalent to arrays array.length which out of bounds since arrays start indexing at 0,prints 3 to console so you could use numbeo.length the -1 is because the for-loop uses less or equal condition in your case;the for-loop goes from 0 to 100 because your numbeo arrays s size is 100,for object you can use for-loop map is better use for arrays type,you need to use an indexed for-loop to initialize or rereference the elements of the arrays;the user reference you have here is a copy of the reference in the arrays not the actual reference,if the length of the arrays is less than 8 a regular for-loop summation is performed,i tried this code with my the big arrays it is around 10 to 20 times faster than a for-loop solution and around 200 times fast than the old code,"
"arrays","for-loop"," takes up less overall, to use a built in better object big,shorter in value index first, is probably in better object big,faster in methods faster foreach-loop, which is considerably slower overall, is actually overall, which is faster in methods faster foreach-loop,more in length index statement, is faster in methods faster foreach-loop, and is slower in filter byte zip,","10033564,56510107,35563476,55958938,46597341,53319864,50143862,53253298,12859396,13200688,35875480,","the second one you are just saving the references to the objects which are only in the scope of the for-loop and once those are unset the objects are gone;the reason why this arrays takes up less memory is that this is only storing references to objects,it is shorter than a for-loop and maybe also more effective for big arrays to use a built in sentinel,because if the first word in arrays is shorter than second one you need second for-loop,keep in mind the data is within an arrays so there will be multiple .interval.title;using a for-loop is probably a better option here unless you re after a certain index,would an arrays be faster than a for-loop in this case,as stated in those answers i can not use scipy.integrate.quad to vectorize the integrals over the arrays because it employs an adaptive algorithm which is why i use numpy.trapz below i could also have used scipy.integrate.simps or scipy.integrate.romb this works fine for a single dimension but now i d like to perform a double integral replacing the c2 constant by a variable for what i could gather the only available function is scipy.integrate.dblquad bu that means i could no longer apply the integral to an entire arrays in a single pass and i would have to use a for-loop which is considerably slower,although loading the entire arrays may seem wasteful it is much faster than parsing the file line-by-line provided you have enough memory to do so. it turns out -- thanks to hpaulj for motivating me to test -- that a simple for-loop is much faster for example with this setup we can use ipython to benchmark the speed of using_genfromtxt vs using_readline so a simple for-loop is actually 10x faster,where you can improve your code is to use a for-loop which is faster than a for each when dealing with arrays,next i can use 3rd 4th and 5th level of arrays means more than for-loop statement will be assign,iterating pair-wise you d normally do something like but iterating over an arrays is faster than using a c-style for-loop,conclusion either reverse for-loop are a while with indexof are currently the best methods i can find to remove multiple instances of the same element from an arrays;using filter creates a new arrays and is slower so i would avoid that,"
"arrays","for-loop"," is more in value index first,worse overall, back together in better number straightforward, or not in value index first, is stored in memory in methods faster foreach-loop, is not overall,better speed in better object big,usually better in better number straightforward, it is faster in methods faster foreach-loop, is easier in better object big, is executed much faster in value index first,","42059484,12296053,49280036,35497576,15283900,14154978,10511218,41556915,55240106,56103736,38082311,","speaking for the js only solutions i ve found that inelegant as it may be a simple indexed for-loop is more performant than it alternatives;extracting single property from a 100000 element arrays,so then i think to go with that approach i would end up with a for-loop and loads of add calls but surely that is even worse than the arrays copy isn t it,what you already have figured out d i used .map for this -- just a bit cleaner than a for-loop -- feel free to learn more about the .map method at once all the words have been reversed stitch the arrays back together into a string strarray.join,1 run the for-loop for not because you start with zero so you need to run the for-loop for one counter less than;2 it will instead of because you need to find element of arrays to be already pushed in new arrays or not,to explain why a for-loop is faster than a for in loops is basically understand the underlying data structures used to store the data in memory;looping through an indexed based arrays is naturally faster because of the way an arrays is stored in memory,3 the following for-loop is missing some thing;the result elements from element 0 to element length are containing garbage because the result arrays is not initiated,use a for-each loops to go through a range it s not as fast as using a variant arrays but keeps things simple and offers better speed than a for-loop,in my opinion a for-loop is usually better for doing something a specific number of times and if you re just iterating over every item in an arrays foreach is usually more straightforward,hence i found instead of using java collection framework - map or set or list if i store data simply in arrays and start parsing data using for-loop it is faster,i think the last term in the formula can be calculated like this later on you are going to need to use a for-loop or even better arrays but a for-loop is easier to save the position of the oscillator at each time point,if onloadstarted is an event handler then the for-loop is executed much faster than the all the event handlers;in that time i is 5 and during each function call the sixth index of the arrays is populated with a true value,"
"arrays","for-loop"," is significantly less efficient then overall,less memory overall, it is easier in value index first,more in longer hits numtotalhits,better in better object big, is bigger in value index first,here more in filter byte zip, is greater in better object big, that cannot in length line different, it s easier in better object big, containing only in filter byte zip,","21050935,37159258,24830766,16164311,8617791,52792626,41146354,27156615,16351505,50394623,35101230,","a developer writing a for-loop and then doing a string compare inside of that for-loop is significantly less efficient then letting the frameworks perform the same option;with a nspredicate against the arrays,in such a simple arrays you shouldn t be concerned about memory usage but the for-loop consumes less memory than foreach because foreach uses an internal copy of the arrays,your for-loop below that does not make sense;to get the last item of an arrays it is easier to do this,the longer the arrays gets the more iterations your for-loop will need,but for the arrays it is better to use for-loop as shown by alnitak than for-in,in the second for-loop you check if the current item in the arrays is bigger than the current biggest,the for-loop here is more efficient for 2 reasons a you don t have to construct a temporary arrays of tuples like with zip and b it returns false as soon as a non-match is found,loops while card arrays is greater than 0,is so that the second string that is being created by the for-loop will be correctly null terminated;without that line it will just be a char arrays that cannot be treated like a c string,to copy the parts of the original arrays it s easier to use arrays.copyofrange than writing your own for-loop,if the for-loop is not mandatory and i don t see this requirement in the quoted text you should use the filter method;when you invoke filter on an arrays you get a new arrays containing only the values that do respect the closure you passed to filter,"
"arrays","for-loop","general in better object big, is less in length index statement, is not in value index first,more handy overall,better in better object big, not overall, is faster in methods faster foreach-loop,less than chunk  in value index first, will check the value in value index first,easier in methods faster foreach-loop, do not overall,","26245130,55595112,23893670,39498852,23501241,52284817,54015930,52557500,56306602,40577230,16899305,","the other way is to compose the variable names in for-loop by using variable variables;arrays seems like a cleaner code to me,does anyone know of a way i can write this expression to return an empty string where the length of the arrays is less than ii i know i can write this as a for-loop but i am trying to do it in this style,first - your for-loop is not true i don t think you meant it to be this way;it s supposed to be as arrays initiallized by a constant size are starting from 0 and going on to size-1 so an arrays would have and not,this is a scenario where a traditional for-loop is more handy than just iterating over the arrays,the questions should i use for-loop or is it better to do it recursively next insert will be called after previous is done should i check if saved.length datalength and then return the arrays or is there some better way how to do it,thus it is better to use a traditional for-loop with a numeric index when iterating over arrays because the for...in statement iterates over user-defined properties in addition to the arrays elements if you modify the arrays object such as adding custom properties or methods.;its best to use a for-loop not a for in when using with an arrays although it may be tempting to use this as a way to iterate over arrays elements the for...in statement will return the name of your user-defined properties in addition to the numeric indexes,the answers above work but if you re after the fastest perhaps surprisingly using a for-loop is faster than the prototype map and arrays methods aka there are many articles and a lot of literature about this if you have a look around,the algorithm works as follows iterate each arrays element for-loop until main_array elements index is less than chunk arrays size a size,this function returns true if there are one or less items in the arrays the arrays doesn t pass the condition in the for-loop if there is more than one item in the arrays there is more than one item in the arrays will check the value there is more than one item in the arrays is iterating over with the previous one,with one arrays one can do which is easier than a for-loop,if is in your for-loop ie indented the same amount as data.append row you ll turn data into a numpy arrays before you ve finished appending items to a list;this will cause the error you see because lists have an append method while numpy arrays do not,"
"arrays","for-loop","simpler than a  in value index first, maybe is better in reference error dimension,more overall, gets much larger in length line different,better in better object big, is way faster in methods faster foreach-loop, is better in better object big, which is much faster in methods faster foreach-loop, is less in value index first, is faster in methods faster foreach-loop, however requires a counter in value index first,","15725413,53251472,30296682,8240426,21640491,32241874,55197766,54837048,54964903,53052754,17187147,","his is wrong because the grade is a value in the arrays not an index;although the for-loop you wrote is simpler than a for-loop with an index sometimes you do need to know an index so you might want to rewrite your for-loop,the first error i see is that you need to declare the function type with the type of the returned value in this case the function has to be another error is in the for-loop the operation has to be same for the another see that you didn t declare the i which is a variable too;another way is doing is the same. if you already know the dimension of the two arrays maybe is better to take the arguments as pointers it will look like and you will have to allocate memory this coul be done with malloc at the end of your program you will want to free that allocated memory doing free wsk,but i don t want to use a for-loop as my json arrays has more than 3 entries and if i request a lot of data in short time it takes long time till the for-loop goes through every entry,indexof is faster than a for-loop but the algorithmic complexity is still o n 2;if the size of the arrays gets much larger consider a different data structure such as a hash table,edit using an enhanced for-loop is a lot better than using arrays,here s a test comparing for in using with arrays and a normal for-loop;you can see that the for-loop is way faster,since you need to iterate 3 arrays and if 3 car arrays are of same size for-loop is better than for each by using java-8 streams,arrays however are special in that they essentially compile into a range-based for-loop which is much faster than using an ienumerable,you probably thought you needed the - 1 there since arrays index s start at 0 except since the for-loop is less than and not less than or equal to it will end at the last index,use a numeric for-loop to reduce a into a plain object a numeric for-loop is faster than arrays methods like reduce and foreach and generally faster than for...of and for...in since it doesn t use iterators or reflection to enumerate the keys of the arrays,if you want to keep to the enhanced for-loop for copying an arrays there is one mayor problem the enhanced for-loop doesn t have a counter;inserting elements into an arrays however requires a counter,"
"arrays","for-loop","probably better in better object big, code is much more in better object big, did not overall, becomes larger in methods faster foreach-loop, is greater in value index first, not in better object big, is greater in value index first, is much faster in value index first,shorter in better object big, is not overall,better in better object big,","36037302,44905080,57241965,53259259,52884074,55625695,45531153,48978696,39674479,52100049,41821054,","i suppose the first option is good for small arrays but a for-loop is probably better practice as the amount of code would remain the same regardless of arrays size,i also think the for-loop code is much more readable and therefore maintainable,removing the empty rows in the arrays and giving the right limit in the for-loop fixed the problem;per tehhowch the problem is that my for-loop did not account for the header rows in my spreadsheet or that an arrays is 0-indexed,warning you are using a nested for-loop which means that the process time will grow exponentially if the arrays becomes larger,the value of high is only updated if the the next element in the arrays is greater than the current value of the arrays in the for-loop,or use for-loop to iterate all values from arraylist;your response is an arrays not an object,after that us make sure that the arrays contains at least two value to compare by starting the for-loop from 1 and make sure the size of our arrays is greater than 1 .if not us return our arrays as it is,you can assign a new arrays of true values with the same element count but interestingly assigning true values in a for-loop is much faster,in this case if any element in arrays is shorter than 11 symbols will become big and for-loop will stop executing,just a side note a for-loop makes more sense in this case case rather than the for-each .;it s because the arrays is not being updated this is applicable to both code snippets neither is updating the arrays,since it s an arrays it s better to use a for-loop with a counter variable i which starts from 1,"
"arrays","for-loop","less in length index statement,smaller in length index statement, is initialized and never in length index statement, and not in value index first, does not in value index first, do not in length index statement,bigger in value index first, is easier overall, not overall, is easier in better object big,longer in longer hits numtotalhits,","8345962,8546694,28843225,32104832,49254964,40229424,18884550,50913448,43785824,12801051,15291406,","when i try to access it with a for-loop where its index is less than the arrays length i get the following error message typeerror function object is unsubscriptable,to use this in a loops you can write a simple for-loop which always checks if the index stil is smaller than the arrays length,if you need the current index then do for-loop if not then go with foreach;as for the length it is just a property and since it is an arrays it is set when the arrays is initialized and never change,and within the for-loop to get the first 4 letters in s1 to be in its own string substr add the line;also s1array is a string arrays and not a string,your for-loop only runs once so if the first item of the arrays matches the date in today it stops and prints your success message;if the first item of your arrays does not match the date in today it again stops and prints the failure message which is also the case as you have said,you should just have a single for-loop to iterate once over both arrays;you also need to handle the case where the two input arrays do not have the same length,it turns out i had a hard coded maximum index in my for-loop which was bigger than the arrays i was trying to assign to,this is basically all you need you dont need e and when iterating an arrays from begin till end or return before a for-loop is easier to read and write than a while,it s also worth noting that your for-loop to do the clearing has several problems;sizeof fs_device_info gives you the size of that struct in bytes but fs_device_info i indexes into an arrays of structs would be the second struct in the arrays not the second byte,firstly you need to initialise your arrays after you know how big it is;secondly you will find a for-loop is easier than a do loops for implementing the logic as you don t need to keep track of loops counter manually,the hits arrays is longer than numtotalhits so your for-loop limit should be numtotalhits instead of hits.length,"
"arrays","for-loop"," or not overall, not just overall, is not in better object big, is way more faster in methods faster foreach-loop,much better in better number straightforward, -----------------+------------------------------+------------------------------ avg ms overall,possible with  overall, to hold another copy overall, not overall, only remains randomizing thier overall, construct is the more overall,","52550323,36476269,23666320,35084494,19520802,56554436,56630490,23160505,35578703,46115978,16085970,","correct syntax is whether you can use a for-loop or not depends on how your std_logic_vectors are provided;if you take them from another arrays or a concatenated std_logic_vector you can use a loops,or as nina points out in a comment a for-loop going backward;in that case since we re modifying the arrays not creating a new one the changes will be visible through any reference to that same arrays not just myarray,but i would recommend avoiding the for-loop and using the function mat2cell ... instead like this;in this case a 3d cell arrays is not advisable,as question said huge arrays;native for-loop is way more faster than any of the above and cache of the length is can improve some of ms milliseconds,notice that besides using the filter method or not even with a plain for-loop is much better for performance create a new arrays from scratch instead of mutate the current one multiple times,if you ve already known the total size use a fixed arrays is much faster and cache the end value for the for-loop benchmark coords is class nrange | 1000 | 5000 -----------------+------------------------------+--------------------------- method | list arrays | list arrays -----------------+------------------------------+--------------------------- avg ms | 312.90858 254.00218 | 8201.48866 7634.8847 max | 321.8542 259.0914 | 8498.696 7914.6034 min | 300.2323 248.8317 | 7908.7473 7529.3754 stdev | 9.564255412 3.654335875 | 220.2477895 159.5085045 coords is struct nrange | 1000 | 5000 -----------------+------------------------------+------------------------------ method | list arrays | list arrays -----------------+------------------------------+------------------------------ avg ms | 56.68224 14.2345 | 1454.1773 296.05854 max | 57.3408 15.4369 | 1472.1977 298.0693 min | 56.2184 12.752 | 1444.9573 293.7728 stdev | 0.468124121 1.081463106 | 10.57876523 1.925248377,just make the virtual function return a reference to the data directly you need to change to vector then - not possible with arrays or c style arrays types with different sizes or if pointers are the only feasible option as a pointer range iterators range adapter would be preferred though if possible - more on that to make this last method work with range-based for-loop one way is to make a small range view type that has the functions begin end - essential a pair with begin end example then construct it with it is easy to make it non-template if a template is not desired,if your for-loop doesn t have data inter-dependences you can simply change it to parfor and matlab will start parallel pools to take advantage of multicore processor parallel computing;even if your data is interlaced inside for loops you can do some trick to get rid of it declare addition matrix arrays to hold another copy of the data structure trade memory usage for speed,you need to use vue to do the for-loop not your server template language;then anytime the articles arrays gets pushed to a new article will appear,you may create a or int or using a for-loop or by simple using enumerable.range;given the arrays only remains randomizing thier order witch you can simply do using .orderby and a random number to be ordered by .orderby,if you want to use a while loops you need to pre-declare an arrays containing and which by the way are a bad choice for variable names;having said that a for-loop construct is the more natural choice here,"
"arrays","for-loop","slower than the  in filter byte zip,much better in better object big, that contains more overall,","48980889,38359658,49303317,","you can compare two byte arrays without for-loop using zip method but it will create a new list so it will be slower than the for-loop method,you can use array.prototype.find method to check if the element exists in arrays which is much better than perform a traditional for-loop,i m trying to repeat n times an arrays with a for-loop but i ve been stuck in the last arrays that contains more elements,"
"accessor","properties"," and not overall, is not atomic however overall, s name overall,directly technically faster overall, it s always overall, to get access overall, which is just more overall, is much better in private automatic properties, is just in effects kvo called, have not overall, is a string in data name core,","45295795,18028219,29642341,42944069,46139340,55460142,54693082,17738380,13018455,21186611,36514505,","next accesses to the properties using dot syntax self.sum are really just a shortcut for calling the accessor;since there are only get and set accessor and not any increment accessor a statement like self.sum++,therefore you have to either implement both accessor and create your own lock or let the compiler do both and use its lock;if the properties is not atomic however you can explicitly implement any combination of the three elements ivar setter or getter and the compiler will take care of the rest,iirc you get nil if the accessor is not set explicitly setter getter so you have to take the properties name following the naming conventions;with a setter it is a bit more complicated because you have to prefix the properties s name with set and make the first character of the name uppercased,edit as willeke has pointed out accessing a properties directly is technically faster than doing so via an accessor via self,for the second and other times the same statement will not trigger the change detection as the value didn t change it was already john and consequently the accessor will not fire;while for the age properties it s always a value change as it increments,maybe you should just use objectmapper with setting properties accessor to get access to every field result stringvalue null intvalue 0 floatvalue 0.0 booleanvalue false dto --edit for getting values from objects by reflection please write in question what do you want to have in output,i would say that auto properties are syntactic sugar over a private field and a properties with get and set accessor which is just more sugar so having get and set value methods,after looking around for answer i ve found a solution along with the expose annotation the jms serializer comes with another annotation for just that purpose accessor;yes a dummy properties is still required but you can make a dummy properties private and a dummy properties is much better than the method i tried before,you definitely shouldn t use it to call methods that have side effects because properties accessor aren t expected to have side effects;a properties is just a promise that the class implements certain methods,assumed all properties accessor have not null references to beans;that could be achieved via providing corresponding getters and setters to properties and initializing bean references if necessary,first of all i hope your text box variable name and properties accessor doesn t have spaces in it as matching a password is the least of your problems at this point;.text properties is a string data type,"
"accessor","properties"," name cannot in data name core,more in pair setter aspect, are not overall, are not overall,nothing more overall, were added later to automatically overall, will not overall, to be read-only in private automatic properties, logic is more overall, may not overall, doesn overall,","33361911,10794303,51421004,4243560,10910531,30008278,630283,14386502,52902780,2138221,18163273,","because it conflicts with the -description method in nsobject recall that core data dynamically generates properties accessor and mutators a properties named description would require creating an accessor method called -description;note that a properties name cannot be the same as any no-parameter,that suggests another aspect where a declared properties is more than a pair of accessor methods that is more static type checking although it is undesirable here,a properties that has both accessor is read-write;unlike fields properties are not classified as variables,if it returns null either the properties has no accessor which should never happen or all of the properties accessor do not have the same virtual status -- at least one is and one is not virtual;technically properties are not virtual -- their accessor are,a properties is nothing more than syntactic shorthand for a get set accessor,properties and synthesized accessor were added later to automatically implement patterns that had been long established at that point;properties are not always backed by an ivar,i think using the public accessor is better since the public accessor allows more maintanable code in case later on you need to change the way the total value is calculated;in that way the users of the totalvalue properties will not need to worry about your changes since your changes will not affect your changes code in any way,if you are only defining one accessor you cannot use automatic properties;note that you can define a private set accessor on an automatic properties if you want the properties to be read-only from code outside of the class but read-write from inside the class.,in my blazor app i have the following input field in a view this is bound to a properties defined with the following accessor the real accessor logic is more complicated than this but i ve simplified it above to the extent that i can while still retaining the behavior that s confusing me,don t use accessor messages implicit properties accesses or explicit;any impure custom accessor may not be safe to call on a partially-deallocated object,you can put arbitrary code in the accessor methods throwing an exception when you are not happy with the value passed to the setter for example very common;the underlying storage for the properties doesn t have to be a field either you can expose the field or properties of another class object for example,"
"accessor","properties"," gives you more overall, are not overall, is no longer overall, check your code overall,general overall, do not overall,more in pair setter aspect, will not in effects kvo called, and call each attribute overall, is less overall, is defined the superclass then overall,","10137802,7996916,11379946,9646965,3573705,11159716,10794303,14904463,48750435,25900284,16622799,","not every ivar needs accessor methods;ultimately is the difference between the 2 properties gives you more flexibility with regards to memory management and multi-threading and the normal one gives you the defaults,secondly the whole point of get set accessor is so you can get and set the value without needing helper methods;thirdly and as to your problem you re creating a new instance of the class in each form hinted at by the new keyword and the value of the properties will be whatever it is initialised as on construction of the instance or not. the values of properties are not shared between different instances of the same type,elete on accessor properties properties with get set has the same effect as get set does on data properties namely that get set removes the properties;after executing delete log4moz.repository the properties is no longer present on the log4moz object and the getter setter functions are no longer bound to the properties,if the properties accessor doesn t match the ivar it will return nil;of course if you re manually implementing the accessor check your code there,beside the advantages of encapsulation we prefer using properties with get and set accessor among many other ways because many other ways provide a clear and practical syntax,however the public bar is and it has two accessor - get which just as the example above getbar returns the private member and also a set - which corresponds to the setbar string value method in the forementioned example;starting with c# 3.0 and above the compiler became optimized to the point where such properties do not need to have the private member as their source,it means a declared properties is more than a pair of accessor methods getter setter,however for kvo to work the accessor methods have to actually be called;if you change an ivar directly observers of that ivar s corresponding properties will not be notified,the set accessor is more complicated because you ll need to loop over the attribute cache for the properties and call each attribute s check method,here was a period in the evolution of objective-c when properties were very helpful in managing memory -- if you used a properties s accessor everywhere you could worry a lot less about when to retain and when to release something because a properties s accessor would do that for you;now that we have arc the memory management aspect of properties is less important but a lot of we are still conditioned to use properties even for internal stuff,accessor shouldn t have side effects but you can t guarantee they won t;if the properties is defined the superclass then you have a couple of options,"
"dawg","trie","more in space waste similar,better memory overall,more in space waste similar,","7638114,8329927,681760,","use a dawg which is more efficient than a trie in terms of space waste,a dawg has better memory performance if the strings have many common suffixes but they are more expensive and difficult to build and update so start with a trie,it is a structure similar to but twice as space-efficient as the dawg that is more efficient than the trie which only compresses prefixes,"
"addition","division","operation slower in faster slower operation,longer in higher precedence subtraction,more expensive in difference expensive matrix, is more overall,higher precedence in higher precedence subtraction,operator higher in higher precedence subtraction,faster in faster slower operation,higher precedence in higher precedence subtraction,simpler in higher precedence subtraction,more in faster slower operation,complex than  in higher precedence subtraction,","29574172,1541739,40200123,15710193,19503254,38932876,26964749,46675877,4247640,37904833,34706438,","it is true that division and modulo a division operation is slower than addition,the term is apparently not an exact measurement as it is clear that a double-precision floating-point operation is going to take longer than a single-precision one and multiplication and division are going to take longer than addition and subtraction,if you are doing physical simulations things like division or square roots are going to be way more expensive than addition,for multiplication the technique described at is a reasonably easy thing to implement and is better than serial addition;division is more complex in general but a good place to start is,since division has a higher precedence than addition 5 2 gets evaluated as a integer division returning 2 as an integer,the division operator has a higher precendence than the addition operator so your function is calculating 1 1 + e -x,with careful optimization however you can make addition 61 times faster than division,the division operator has a higher precedence than the addition operator + so you need to enclose the sum with brackets before dividing,knuth writes that fibonacci search is preferable on some computers because it involves only addition and subtraction not division by 2. but almost all computers use binary arithmetic in which division by 2 is simpler than addition and subtraction,as far as i know the division is more complex and slower than other operations like addition so is my code incorrect then,i don t know how division modulo both works but division modulo both s much more complex than addition subtraction or even multiplication,"
"addition","division"," is a bit in higher precedence subtraction,larger overall,slower in faster slower operation,expensive than  overall,faster in faster slower operation,higher precedence in higher precedence subtraction,higher precedence in higher precedence subtraction,more costly in difference expensive matrix, always takes longer in higher precedence subtraction,higher precedence in higher precedence subtraction, it is easier overall,","10659772,5649131,22877763,8335325,8949555,37646748,16127818,26209229,329243,45273269,14819855,","usually simple operations like addition subtraction and multiplication are very fast;division is a bit slower,and division has larger complexity than addition,2.the division by 2 can be done by bit - shift operation is it really slower than addition,so even disregarding the trial division is more expensive than addition and multiplication we see that the number of operations the sieve requires is much smaller than the number of operations required by trial division if the limit is not too small,for example an addition is typically much faster than a division,multiplication and division are higher precedence than addition so they get done first - before the implicit conversion to string for concatenation,multiplication and division operators have higher precedence than addition and subtraction in c++ same as in scientific notation,generally the division is more costly than addition i think but not much difference in this case,since most processors can do an addition comparison or multiplication in a single cycle those are all counted as one flop;but division always takes longer,finally we all know that multiplication and division have higher precedence than addition and subtraction so we can remove the extraneous parentheses so this turns into,think of long division - you do a series of subtract - shift operations and you don t know what you need to do next until you have completed the previous part of the operation;for addition it is easier to see how you could achieve a complete operation in one cycle,"
"addition","division","far more cycles in time example cycles,operator higher in higher precedence subtraction,higher precedence in higher precedence subtraction,higher precedence in higher precedence subtraction,much more expensive in difference expensive matrix,less work overall, is not in faster slower operation,faster in faster slower operation,more time in time example cycles,higher precedence in higher precedence subtraction,more in higher precedence subtraction,","14395974,34712043,28221487,14622461,5649137,8643410,3293341,1348077,24542784,41993666,884697,","best example the division it an an addition are both o 1 but usually the division takes far more cycles time to execute than the addition,this happens because the division operator has higher precedence than the + addition operator,note that division has a higher precedence than addition,division has higher precedence than addition,the first difference is that division is much more expensive than addition,in addition to that the crossing off may be less work than a division don t know about python it is for c arrays, think it is a few cycles slower than addition but yes division is very slow compared to the others;division takes significantly longer and unlike the other 3 operations division is not pipelined,for example on most 32 bit systems 64-bit addition is faster than 32-bit division modulo,i remember it says something like division takes much much more time than addition,division has a higher precedence than addition or subtraction so it s really this,the addition and subtraction is okay because the types of a and b force them to be performed using floating point arithmetic - but because division binds more tightly than addition and subtraction it s like using the brackets above only the immediate operands are considered,"
"addition","division","operator higher in higher precedence subtraction,higher in higher precedence subtraction,operation faster in faster slower operation,faster in faster slower operation,higher precedence in higher precedence subtraction,faster in faster slower operation, is not in difference expensive matrix,higher precedence in higher precedence subtraction,slower than  in faster slower operation,cheaper than  in faster slower operation, is somewhat more overall,","19467456,44168281,37904833,1348077,27096708,4624837,16841814,17433226,8392125,54243105,47339568,","the division operator has a higher order precedence as the addition operator,the division has higher precedence than the addition so what you re calculating is sumaverage1+ sumaverage2 5 which is integer division which is probably not what you want,i need to find out that how much division operation is faster than addition operation in a gpu,performing addition on this slightly larger type will pretty much always be faster than doing division or modulo on the type itself,division has a higher precedence than addition ergo,an addition is faster than a division and a multiplication,division gets really bad;interestingly the matrix addition is not much difference at all,multiplication and division have a higher precedence than addition and subtraction,the compiler could be done via division which is much slower than addition or the compiler could be translated into a bitwise and operation as well and end up being just as fast as the version,a simple branch assignment is way cheaper than addition and division,i think division by a power of 10 other than 10 9 would be somewhat cheap but would require an actual division on each limb and propagating the remainder to the next limb;extended-precision addition is somewhat more expensive this way than with binary limbs because i have to generate the carry-out manually with a compare unsigned comparison,"
"addition","division","higher priority in higher precedence subtraction, method1 is a lot overall,expensive than multiplication;in  in difference expensive matrix,higher precedence in higher precedence subtraction,","43177267,25455468,22522575,11533547,","multiplication and division have higher priority than addition and subtraction,use float a b instead or add a from __future__ import division to the top of your file;tentative conclusion using a for-loop and simple addition method1 is a lot faster than any of the list comprehension methods for this example,division is much more expensive than multiplication;in addition calculating an approximate reciprocal is much more simd-friendly since there are usually reciprocal estimate instructions that provide a starting point which can be refined by the newton-raphson method,to start with i need multiplication and division to take higher precedence than addition and subtraction,"
"codeigniter","laravel","faster overall,older overall,better in features better,much better in features better, has a more overall,more overall,faster overall,","32941565,23339615,25132418,26133517,50196121,35859900,25880819,","i am new to laravel it s good for coding but it is not much faster than codeigniter,unfortunately i m pretty sure you ll not find any packages or projects that migrates a joomla web site to laravel or maybe codeigniter which is much older than laravel,if you ve been using laravel then you already know it is better than codeigniter,before i am using codeigniter but i found out that laravel is much better than codeigniter and it has a lot of features,side note you could research the orm approach to models laravel and a host of other frameworks use it but codeigniter has a more whatever approach to just about everything,i understand that laravel is what s hot right now but i ve never played with it and am not sure if learning laravel is more time-effective vs using codeigniter which i know pretty well but just doesn t excite me any more,in my personal benchmarks laravel is undeniably faster than codeigniter due to lazy loading,"
"firefox","safari"," is even slower overall,bigger overall, will not overall, is the inclusion overall, seem cool overall,conversion better in better error message, is not scriptable at all;however overall,more strict overall, doesn overall,more accurate overall,same as  overall,","28687948,6079275,33400670,11902218,42048340,7432356,30787746,31321155,36408121,2474666,50461106,","css3 transform difference in firefox and google-chrome and ie;in fact given that google rejected pointer event on the ground of speed which ie solved by gpu acceleration it can be said that google-chrome and webkit in general - safari is even slower is lagging behind on this front and the only way to help is contributing code to chromium webkit,that i gave to every link in the set of links home about us products contact and it seems that firefox is making that 1px margin much bigger than safari or google-chrome and distorting it,firefox will also have a value of 0 but it will also factor in the values;unfortunately safari will not record consistent pagex pagey values for keyboard events and ie will record the exact location of the mouse when the keyboard click event was fired,what you posted works fine in google-chrome and safari but doesn t do a thing in firefox;where it dies in firefox is the inclusion of a background image,the above error means firefox does not treat multipart form-data as a valid entry in access-control-allow-headers http response header;however chrome safari seem cool about it,the service is still in beta trial and firefox conversion is working better than safari one,you need to know that firefox is not scriptable at all;however safari partially is scriptable,it seems that google-chrome internet explorer or safari are more strict than firefox on terms of security,firefox doesn t include an origin header on same-origin requests;but chrome and safari include an origin header on same-origin post put delete requests same-origin get requests will not have an origin header,i just tested geolocation on firefox 3.6 and iphone safari os 3.1.3 the result is interesting firefox is more accurate than safari,google-chrome rendering engine javascript engine ios ios webkit nitro javascriptcore android windows linux blink v8 in addition to innominatum answer note that google-chrome for android windows linux uses v8 javascript engine while google-chrome for ios has to use nitro engine same engine that powers safari in accordance with apple s requirements for browsers released through their app store ios version of google-chrome uses the ios webkit which is apple s own mobile rendering engine and components developed for their safari browser therefore it is restricted from using google s own v8 javascript engine so google-chrome and even firefox behaves fundamentally same as safari on ios. that s why shows just one support table ios safari for ios references,"
"firefox","safari","less fuzzy overall, is more overall,more forgiving in forgiving same firefox, but not overall, has dropped support overall, is saying screw overall, has not overall, but works perfectly in fine work,smaller then overall,browser than google-chrome  in fine place browser,more overall,","3039489,16950152,365945,4069980,1332465,2626302,37517589,53952944,3467818,53174418,6692565,","font looks sharp in google-chrome safari ie8 not as good as other 2 but less fuzzy than firefox,you have few errors in html code marked with stars open site in firefox view source and you will get better picture in that part of page;i guess that safari is more sensitive to these errors than other browsers,but i haven t yet figured out whether it s me writing sloppy code with firefox perhaps being more forgiving than safari or if it s safari or if it s jquery,ext js comes included with a debugging console you need to add debug.js and call ext.log blah to bring it up this will provide functionality that is similar to firebug on firefox but not as extensive still its useful for supplementing the poor development tools that come pre-installed with ie 8;firebug as ergo mentioned here is the most powerful of the browser-based development tools it allows step-by-step debugging however the latest versions of chrome and safari also come installed with develoment tools that are useful but not as much as firebug,firefox has dropped support in 3.5;safari does not seem to support it but the documentation indicates otherwise,my suspicion is that ie is swapping out the image for the last ie finds firefox is using the first and safari is saying screw this,note i was able to achieve the above functionality in google-chrome only as at this stage mozilla firefox provided limited support;whereas safari has not provided webrtc support so it was out of the picture from the beginning,i m have the problem that video doesn t work in safari but works perfectly fine with google-chrome and firefox,small in firefox is smaller then small in safari so never ever use them,we ve also noticed that the time seems to be even longer in case of any adblocker in place while it doesn t seem to happen in other browser than google-chrome firefox chromium safari they all seem to be fine,to me it looks like safari gets it more right than firefox text is generally more around a middle line,"
"firefox","safari","faster overall, doesn overall, seems less overall,better in better error message,just better in better select options,more anti-alisaing overall, doesn overall, does not yet overall,forgiving than  in forgiving same firefox,higher overall,less strict overall,","3220693,47103867,38626906,7322272,463954,5804790,18461572,33555419,55646515,10623171,3349260,","google-chrome or safari on a mac could be much faster than firefox on a pc especially with newer apis,both google-chrome and firefox inject the same script in the page to build the rendered text;it seems that safari doesn t rely on this script,note the above returns reliable results in google-chrome and safari;firefox seems less reliable out of the box,safari behaves better than firefox but an error message â œone error in opening the pageâ â shows up in the status bar if you remove the iframe during the load event,firefox is just better for web development and i prefer safari overall,in particular a font may be rendered quite differently between the various browsers and operating systems in current use - safari and macos use more anti-alisaing than firefox or ie but ie9 has a new feature called cleartype which also affects font rendering,firefox seems to mess up fonts the most - on the other hand the text looks bold;chrome and safari doesn t alter fonts nearly as much making them look sharper but not really bold at all,please not that this solution only works in google-chrome and safari but not firefox;since firefox does not yet support column-span,it seems that google-chrome and firefox were more forgiving than safari,in safari it is several pixels higher up than in firefox and google-chrome and significantly lower in opera,it seems that ie and firefox are less strict than safari,"
"firefox","safari","workable with  overall, doesn overall, is the same; is more in forgiving same firefox, is broader overall,compatible with   overall, prototype runs significantly faster in faster slower dom, also doesn overall,even worse in better error message, is slower in faster slower dom, not overall,faster in faster slower dom,","49389874,2682873,47277916,1962727,55489249,3493725,13259411,33516237,1233693,7969807,899967,","here is a figure i tried to draw as a better explanation here is the function i have coded to do that then from a cube object attached to the camera not to the scene it can be rendered like that as a result this is what i got with a perspective camera and now it works with microsoft edge safari and other browsers despite when attaching to the scene and updating the position of the object to keep it in front of the camera it was only workable with firefox and google-chrome as for today,they are all modeled after firebug an extension for firefox and the existence of which is why firefox doesn t have one built in;opera internet explorer chrome and safari all come with good debuggers built in links go to information about the debuggers,i assume safari is the same;firefox is more lenient but i believe there must be a user click to start with,using proprietary extensions from firefox is almost as myopic -- you ll have a hard time convincing all visitors to switch to firefox for your site from ie safari or whatever else all visitors prefer and although the range of devices on which you can install firefox is broader than those on which you can install ie you re still cutting off devices such as iphones and android a segment of the market that s growing much faster than traditional pcs,if you re satisfied with its browser support you could try position sticky although i would advise not too as it s still only truly compatible with firefox safari s latest versions,this looks like at least on safari and firefox prototype runs significantly faster edit not 20x as stated earlier,safari also doesn t apply it but interestingly it does if you go on to press a key on the keyboard. in firefox however the hover class is applied immediately;since chrome and firefox were the only two i initially tested with i thought this was a bug in chrome,i planned on using svgs for the entire site but with only 20-30 svg images of medium to high complexity used in the page and google-chrome already seems to be showing som jank and high paint times for scroll and firefox is even worse though safari seems to do a lot better,firefox is slower than google-chrome which boats one of the highest javascript engines a modified version of webkit;safari i believe is currently the fastest rendering engine out there,firefox msie starting with version 9 opera konq and google-chrome support;msie8 and safari not support,in safari we found that the dom level 0 took twice the time off the dom level 2 but was still four times faster than either firefox case,"
"firefox","safari","closer overall, which doesn overall,faster than  overall,slower in faster slower dom,more problems overall,fine with  in fine work,worse overall, is sticking closer overall,longer than  overall,general in fine place browser, do not overall,","27283013,53488438,53029824,18707981,44755522,56084327,28572292,23075199,504606,48008597,11044973,","in ie and firefox fullscreen_block appears closer to the bottom than in safari and google-chrome,my issue is a duplicate and the answer to my question was given by stefan-peshikj in turns out google-chrome and safari are more permissive than firefox which doesn t accept an turn url with username not accepted accepted this solves my issue,result for n 10 fastest m was fastest on google-chrome 17.3m and safari 13.3m firefox 4.7m a b was similar and fastest on firefox 16.9m google-chrome 15.6m safari 3.5m slowest o for safari 0.35m k for google-chrome 0.35m n for firefox 0.31m conclusion the fastest solution on all browsers except small n on firefox was m however is not typical array - for it the fastest browser was safari for large n 6x faster than google-chrome 9x faster than firefox for typical arrays preferred solution is a fast and short code you can perform test on your machine here,on safari everything is slower than on firefox still the object property access is more than two times faster,please especially test on safari because it has some more problems than firefox and google-chrome,code works perfectly fine with firefox and safari,it works great in google-chrome and safari haven t tested ie but i m sure it s somehow worse than firefox,it looks like while google-chrome and safari are trying to do the right thing with 0px firefox is sticking closer to the standard,i run firefox on vista xp and a macbook pro regularly and aside from taking a while longer than safari or google-chrome to start up a while s fine and seems to use less ram than either,i m facing the problem that my html5 sample video doesn t load in safari 11 on osx but works perfectly fine with google-chrome and firefox,ie and firefox implement the spec correctly by putting the properties on the prototype;chrome and safari do not they put the properties directly on the object,"
"firefox","safari","general overall,greater overall,significantly brighter overall, here is a better in better select options, does not overall, will not overall,taller overall, not overall,such as  overall,","47662506,36743820,27722716,54592122,53231144,17014858,7898066,7778405,50646129,","i am so far aware of these rendering errors filtering html objects seems not to be supported in safari edge and ie colormatrix seems to be buggy in google-chrome when applied to svg objects colormatrix and component linear have been reported to be buggy in firefox 57 windows i have not seen all of these errors so feel free to add to this list or correct it,so to specify firefox greater than 20 safari greater than 9.1 and ie greater than 9,in safari it renders significantly brighter than in firefox or google-chrome,note your fiddle is on load which will fail because you have update jquery filtering select options does not work in safari but works in google-chrome and firefox here is a better version,for example firefox does not expose anything about its windows through the accessibility api;with safari i was able to find the url but it was a bit buried within the ui element hierarchy,make sure that the syntax for background-position is absolutely correct otherwise safari will not be able to figure it out;firefox was a bit more forgiving,i saw that the discrepancy was because in safari the text element is taller than in firefox and includes a slight amount of whitespace on top that doesn t show up in firefox in firefox the top of the text element is exactly when the text starts,however this is not a cross-browser solution works in google-chrome and safari not firefox;edit i have it working with jquery now but still doesn t work for firefox which is using mediaelement s flash fallback,when i switch to other browsers launcher such as firefox or safari the problem goes away,"
"affinity","cpu"," will not overall,better in performance vs changes, meaning only overall,better in performance vs changes, is multicore overall, mask become smaller overall, does gives lower latency in performance vs changes,more overall, to allow sql server overall,","13585364,39495136,8547903,15037998,50210666,56258957,25962960,22259309,1717875,","use irq affinity to set other cpus to handle all interrupts so that your isolated cpu will not receive any interrupts;use cpu affinity to fix your specific task to the isolated cpu,but i found if i do not set their cpu affinity that is they run on same core 0 the time performance get better than setting cpu affinity 8.76s vs 14.66s,with a query that doesn t end up blocking on resources on a 4-core machine you theoretically get a 4-times speed up 4 hyper-threaded might give you more or even less but probably not 8-times since hyperthreading s doubling of some parts of the cpu doesn t give a clear two-times increase;with the same query on a single-core or with processors affinity meaning only one core is available a webserver in web-garden mode then there s no speed-up,cpu affinity it s better for the cpu to have a load average of 1.0 and processes to have affinity to a single core,cpu is multicore user-space app will have affinity to one core,further more in deadline scheduling kernel rejects setting when cpu affinity mask is smaller than the entire root_domain in what case would cpu affinity mask become smaller than the root_domain span,also note - if rdtsc is used without an invariant time source on linux grep constant_tsc proc cpuinfo you may get unreliable values across frequency changes and if the task switches cpu time source;so in general yes setting the affinity does gives lower latency but this is not always true and there are very serious costs when you do this,the cpu affinity is more like a suggestion to the kernel regarding which cpu to use,so you may be starving ssis of cpu not memory;you could try changing processors affinity to allow sql server to use only 16 cores and let the memory find it s own level,"
"strcpy","strncpy","safer than  in version unsafe strcpy,better in better checking things,safer in version unsafe strcpy,safer in version unsafe strcpy, is safer there in version unsafe strcpy,not safer in version unsafe strcpy,actually better in better checking things,not safer method in version unsafe strcpy,safier overall,more overall, is more in buffer use size,","17641528,5463735,18727224,3629186,610494,1258708,44577839,13903306,12236011,41937901,54079633,","strncpy is safer than strcpy,strcpy could be better replaced by strncpy which does some bound checking,strcpy is notoriously unsafe as are it s cousins strcpy_s and strncpy although they are mildly safer than strcpy,in general strncpy is a safer alternative to strcpy,of course you might still ask whether to use strncpy or strcpy in implementing that abstraction strncpy is safer there provided you fully grok what abstraction strncpy does,strncpy is not safer than strcpy it just trades one type of bugs with another,i.e strncpy is actually better than the simpler strcpy if you are willing to improve the code,strncpy is not safer method to use as strcpy,using strncpy is considered safier than strcpy because the second one can easily cause buffer overrun,for instance strncpy is mostly useless it gives you nothing more than strcpy,you have to reserve memory to 0 very wrong to copy strings you have to usr strcpy the correct function nowadays is strncpy is more safe,"
"strcpy","strncpy","more in buffer use size, but is still pretty overall,less overall,safer in version unsafe strcpy, is a more in buffer use size, will generally overall,null terminator after using  in buffer use size, and check the buffer in buffer use size,better in better checking things,more overall, does then response overall,","28534187,51919938,5883113,30884175,8965136,2375759,15305030,1706281,10845959,37740121,21122205,","strncpy is more recommended that strcpy because protect your code against buffer overflow,merged by junio c hamano -- gitster -- in commit e28daf2 15 aug 2018 banned.h mark strncpy as banned the strncpy function is less horrible than strcpy but is still pretty easy to misuse because of its funny termination semantics,the best solution is to write a custom version of strncpy that is less weird or if you know the length of the input just use strcpy,myth 3 strncpy is a safer version of strcpy,strcpy adds a null terminator so your string ends there;perhaps strncpy is a more useful function for your situation,strcpy will generally get the job done;strncpy is better if available,use strcpy or add a null terminator after using strncpy;strncpy does not add the null terminator 0 where as strcpy does,use strcpy and check the buffer size by hand;strncpy is a little safer but dangerous in other way,using strncpy 3 is better than strcpy 3 but things like strlcpy 3 are better still,so that the strncpy is more secure than strcpy,solved the issue with strncpy though i ll use strcpy to improve readability;what i think it happened is that response is being allocated on top of the variable where i wrote the request then it wasn t being completely zero-ed and since strncpy doesn t add the final 0 like strcpy does then response would become a mix of old and new data,"
"strcpy","strncpy","safer in version unsafe strcpy,safe than  overall, will not overall, not in version unsafe strcpy, is a way overall, isn overall, isn overall, which is more in version unsafe strcpy, is safer but still overall,unreliable unlike  in buffer use size, is not really in version unsafe strcpy,","33271840,48532502,14946812,14290569,32558439,30406504,610260,54963329,30068523,27102896,3774036,","you should use strcpy or strncpy safer than strcpy to copy the string stored in the array between arrays,strncpy is more safe than strcpy because you limit the amount of bytes to be copied thus avoiding a buffer overflow,strcpy will not work if _friendly_name don t have a 0 before position 16;strncpy will be fine,notice the routine is strncpy not strcpy;strcpy is unsafe,but in this case you need to use strcpy instead of the assignment operator;upd as paulr noted in the comment above strncpy is a way better option,actually the situation is a bit dire since neither strcpy nor strncpy are absolutely good functions;by itself strcpy isn t safe since you can t control the output buffer size and strncpy is inefficient since it writes more zeros than what may be desired,but for the example you give the example doesn t matter - if it s going to fail it will be in the initial strlen so strncpy doesn t buy you anything in terms of safety and presumbly strncpy is slower as presumbly strncpy has to both check bounds and for nul and any difference between memcpy and strcpy isn t worth changing code for speculatively,the easier way is to use the function strncpy which is more secure than just using strcpy,never use strcpy;strncpy is safer but still not that safe because it may not append a null character to your buffer,now with 3 bytes of memory you can use strcpy to copy the string properly;strncpy will not add the terminating null itself in case the n is equal to the size of supplied buffer thus becoming very very unreliable unlike strcpy,strncpy is not really a safer strcpy - strncpy is designed for filling fixed-length text fields which means that it does not always nul-terminate the destination,"
"strcpy","strncpy"," as said here in version unsafe strcpy, by providing the buffer in buffer use size,safer in version unsafe strcpy, is not intuitive and therefore overall,","52207214,48367224,32834982,19144073,","i know strncpy is a safer version of strcpy as said here,they are safer than strcpy by providing the buffer size of the destination similar to strncpy,i was exploring around with c regarding strncpy since most people says that it is safer than strcpy additional parameter length to avoid buffer overflows,it was never intended to be a safe version of strcpy but is often misused for such purposes;it is in fact considered to be much more dangerous than strcpy since the null termination mechanism of strncpy is not intuitive and therefore often misunderstood,"
"hdpi","screens","larger in ldpi mdpi resolutions, assets gives a better in different size assets, is physically smaller in ldpi mdpi resolutions,resolution less in ldpi mdpi resolutions, or not in ldpi mdpi resolutions, density is smaller in ldpi mdpi resolutions,more in ldpi mdpi resolutions,greater dpi in different size assets,lower in ldpi mdpi resolutions, is bigger in ldpi mdpi resolutions,","4885845,5965768,8413449,11429933,12072919,15450604,6366076,5428091,26063897,23806899,","if in java code the values would be in pixels so 50px on mdpi screens will look larger than on hdpi screens,however things are slightly different due to the much larger screens size on 3.0 tablet;in these cases often using the hdpi assets gives a better size,if you re diligent about making graphics for at least the main three densities ldpi mdpi and hdpi then you should not see over-large icons except where the screens is physically smaller than your layout expects your layout,so as i said - mdpi also doesn t means that your screens resolution is less than hdpi screens resolution,well android adjust its screens density according to device it working on if you have both folders hdpi and mdpi available it will take near best suitable resources for your application;please check whether device is hdpi or not,it suppose to load images from the hdpi folder because it s screens density is smaller than 320 dpi,even if i create three versions for ldpi mdpi and hdpi there are more than screens resolutions in that table if there is no matching image it will scale my image and may not retain the aspect ratio,edit - now that i think about it i m not sure why your image would appear smaller unless your screens had a greater dpi than hdpi,i am not aware of any device that can use api 14 with a screens density lower than hdpi,e it an hdpi or and ldpi device what you want to be xx dp in size is yy cm on any device;now as you have devices that differ in physical size you see bigger spaces between your widgets on one device because it screens is bigger,"
"logical-or","operator-precedence","logical-and higher overall,","21345748,","according to the mdn operator-precedence logical-and has a higher precidence over logical-or suggesting that the condition is evaluated as if were a single statement which then moves on to determine the boolean condition of false || true which is then true,"
"protected","public","much more in weaker access property,closer in restrictive package default,comfortable with  in restrictive package default,better in fields base members,general overall, read access in weaker access property,higher visibility than  overall,more restrictive in restrictive package default,more in restrictive package default, is arguably more overall,access more in weaker access property,","761864,8202001,48251095,6033905,50776997,51655354,53710499,28963709,15626425,10960330,3313279,","therefore the only types that should have access to a constructor are its derived types and hence protected makes much more sense than public,protected is closer to public than private,since i don t feel super comfortable with protected private inheritance in c++ i googled it up and came up with this stackoverflow answer difference between private public and protected inheritance,reason i once heard that protected is better for hibernate performance but all i can find on the web is hibernate can access public private and protected accessor methods as well as public private and protected fields directly,making a listener for these three events would be a superior approach if you wanted to make access to these three events public because a superior approach provides a much higher degree of decoupling between the class and a superior approach listeners;making the access protected implies a higher degree of affinity between the class and the listener the designers of the class would end up making a protected runnerlistener interface for the listener adding a protected defaultrunnerlistener implementation for situations when you need to override one or two methods and adding a protected method for you to supply the listener,you can just make the property public and not worry about it;you can define public read access with protected write access,i tested out a few variations of this with the following results standalone class own source file class public ctor public success class public ctor protected success class public ctor pkg-protected illegal access class pkg-protected ctor public illegal access static inner class class public ctor public success class protected ctor public success class pkg-protected ctor public illegal access class public ctor protected success class public ctor pkg-protected illegal access non-static inner class class public ctor public illegal access the key points from this are public and protected works but package-protected or less does not work both the class and the ctor must have equal or higher visibility than protected,so a protected is more restrictive than public and private is more restrictive than protected,protected is more restrictive than public,rotected internal is actually the second most permissive access modifier after public;protected internal s worth noting that protected is arguably more permissive than internal since protected internal allows access from code that you have no control over other assemblies,the theory is that someone extending your class with protected access knows more about what they are doing than someone who is merely using it with public access,"
"protected","public"," it is a weaker in weaker access property,general overall, is almost always in weaker access property, but more in restrictive package default,better in weaker access property, class is more in fields base members,weaker invariant in weaker access property,better in restrictive package default, is more about in restrictive package default,less in restrictive package default, is even worse in restrictive package default,","32345178,50321636,45387573,24548057,2757943,11587000,4507414,4753405,28143079,18376768,10699671,","methods in interfaces implicitly have the access modifier of public;so when you implement it with protected it is a weaker access modifier,i recommend adding protected setters to the parent class or make them public if you have external classes setting these values,therefore public is almost always the way to go;protected doesn t really make sense for daos since you need the methods in other packages in classes which don t implement the dao,change it to public to make it more accessible or perhaps better protected to make it less accessible than public but more than private read more about that here,it s not perfect but i guess that a protected method that does not check parameters is better than a public method that does not do it,for example it is a compile-time error for a public class to derive from a private or internal class;here you re trying to derive a protected class from a private class - the protected class is more accessible than the private class hence the error,protected functions maintain a weaker invariant than the public one before and after each call,that means private is better than protected protected is better than public etc ..,also using private and protected instead of public is more about good programming design than security and i recommend you read about it as well as polymorphism and inheritance,since package protected is less accessible than public the code is reducing the accessibility of the foo method,using default no protected private public is even worse as the exteneding class allows classes in the same package or subclasses to access the logger,"
"protected","public","more restrictive in restrictive package default,private fields with  in fields base members, inherited doesn in restrictive package default,helper methods as  in livedata control helper,actually closer in restrictive package default,higher overall,weaker than  overall, is meaningless in restrictive package default,more limited overall, propertie starts with capital overall, is no more overall,","15220204,54422123,13702505,4607353,36322823,21338125,48033416,3631594,25672161,8286895,25672161,","otherwise there is still the possibility to change a public access to a protected or package private access which will be much more restrictive than public,i would like to present you with some arguments protecting protected fields in java you may favor accessing base class members using protected fields over public accessors in situation where you need to avoid value validation . however if this is not the case then private fields with public accessors should be used to compliment hermetization,in c++ you can consider that public inherited implies a subtyping relationship or you could be more restrictive and consider as subtyping only the cases where the overrides of virtual functions ensure respecting the lsp;and considering that private or protected inherited doesn t but still is a case of subclassing is sane whatever constraints you put or not on virtual function overrides,addendum correction for controllers you should mark the helper methods as protected private and only the actions private and only the actions should be public,furthermore protected is actually closer to public than to private,public is higher than protected so there s no problem,ince met2 from father has protected you can either override met2 from father with protected or with public;since you do not specify any privilege access met2 from father defaults to package protected which is a level weaker than protected,protected is meaningless or reduces to private without inheritance although not in java but with inheritance not in java explodes into a slimy public degenerate and more so in java programming scala chapter 5,from a software engineering point of view as soon as you make a function protected you expose it to other users of course much more limited than public exposure but still much more exposed than private and you create expectation about some stability of the api,monotouch sdk follows the .net naming guidelines and public propertie starts with capital letters;private or protected variables starts with lower case,so to quote scott meyers protected is no more encapsulated than public.,"
"protected","public"," is somewhat less in weaker access property,less in weaker access property, quickly becomes more overall, which gives more control in livedata control helper, property cannot in weaker access property, is broader scope in restrictive package default,easier overall,more permissive in fields base members, which is weaker in weaker access property,","4920288,12325479,20746334,50944014,24047298,25883163,24661886,14271110,21068023,","public as expected means that everyone is given access to either attributes or methods;protected is somewhat less trivial,oh i see you need a constructor that has more access than protected but less than public,second though the public quickly becomes more complicated you could use the above but with a protected set and force users to go through a method-based system to modify values allowing you to restrict log and otherwise do some logic during these actions,it will be complicated for google to support mutablelivedata since the setvalue and postvalue methods are public;where as for livedata they are protected which gives more control,one could stick to a single property if that was made public so implementing implicitly but in that case the property would be exposed much more which is apparently not desired;a protected property cannot implement an interface property implicitly or explicitly, subclass may reside outside the package following scope levels again private default protected public - we can not narrow down the scope;protected is broader scope than default so java does not contradicts so java own guidelines,what is the true rationale behind all the private and protected stuff when we can just make our life as a programmer easier by using public for everything,for public inheritance it means that all the accessible members of base class that is the public and protected members can not be at a level more permissive than public,if you overload the create_transparent_image method you must also define the create_transparent_image method as protected or define the create_transparent_image method as public which is weaker,"
"arp","tcp","shorter overall, is a much overall,lower overall,","1716751,41711221,42873263,","it would appear from your question that the period of the tcp message is shorter than the arp refresh time,o do it correctly you need to keep a table locally and a timer and if you have never seen that ip address or the last time you saw it was more than a minute ago or whatever your timeout is then you need to arp for it to be able to respond;tcp is a much larger pain in the .. with udp you can do this kind of thing quite easily swap macs swap ip swap ports fill in the payload checksum if you want or not and send this kind of thing back,if you are using linux check out netdiscover this uses arp i think arp is lower level than tcp udp and with java you can only create apps from tcp udp and above,"
"min","range","greater overall,less in disjointed ip current,greater in greater values slider,bigger in greater values slider, or more in greater values slider,less overall,more overall, is smaller overall, is bigger in greater values slider,less than  overall, or greater in greater values slider,","16608301,26778990,30718503,43066794,57538352,3188934,10425399,574726,34242750,48087390,55425057,","if your range were sorted in some manner then you could look at the extreme ends if the end s min is greater than the targetmax then say look at the range 1 4 backwards from the end etc.,any value in the disjointed range that is less than the value in d1 has 1e+99 added to it and that won t be the min of anything,if range is greater then or less then the min and max then it should throw error,my solution to the range part feels clunky over complicated and doesn t check if the max range value is bigger than the min range value doesn t check if 10 2,the first if statement is saying if the value of the tree node is less than the min or more than the max then you are not a valid bst and the second statement is saying if your value is within the range of min and max you are a valid bst,if max and min are independent variables the extra subtraction for max-min will waste time but if that expression can be precomputed at compile time or if it can be computed once at run-time to test many numbers against the same range the above expression may be computed efficiently even in the case where the value is within range if a large fraction of values will be below the valid range it may be faster to use because it will exit early if value is less than min,but if i take it out of range for more than 1 min it gives an exception,normally the exp range is - max+1 - max;if min is smaller than - max+1 you have as floats and doubles have subnormals,if min is bigger than max i just return a random character from the entire range,if it was rounded down it is slightly less than min and hence it is also in the subnormal range,if the entire range of your colorbar values is less than 10 min or greater than 10 max your colorbar will display the results as requested in the op scientific notation with only mantissas on the sides of the bar and a single exponent at the top,"
"min","range","lower overall,larger than  in greater values slider,lower than the  in greater values slider,more than 800  overall,greater in greater values slider,less overall, but not overall,general in greater values slider,greater than 1.5  in greater values slider, or raise an analysiserror in date small output, go to middle  less in disjointed ip current,","22336343,54330676,52769950,55029523,15000781,42503519,55450172,53318091,51457041,52481429,51866431,","a variation on pentadecagon you are free to set n arbitrary values on the diagonal in some range min..max containing x and fill the rest of the array with lower than min on one side and higher than max on the other,note2 i ve faced this error for this valueerror max must be larger than min in range parameter,what happens when the range has a negative number or the max is lower than the min,i researched a lot on how is average session duration calculated in gbq and found this sum of the duration of each session during the date range you specify and divides that sum by the total number of sessions. to find total number of sessions i did a count of concat fullvisitorid visitid the sql query is written as follows in ga interface it shows average session duration not more than 0 sec to 5 min for each url page. but i am getting more than 800 min of average sessions duration for most of the urls,range is 5000 to 50000 what i want to achive is in maxpoint select box i want to display the values which are greater than min points selected value,my query is given below so based on the date range first i have to sort down the records after that i have no idea how to find the difference between created_at and updated_at having status success and the time difference is less than 5 min 5 to 15 and so on,however that should not stop you from being able to define range that would include any value or that they are open in one end they have a min but not a max or viceversa;for example if the range is to be specified as two arguments in the signature of the method that is going to use such a range then you can simply designate the null as to indicate that that min or max does not exist that is that end of the range is open,what i want to do is click one of the days of the week and then get which range min max slider is closer to that value,another approach is to use erlang min 2 and erlang max 2 to maintain the range if score is greater than 1.5 min 2 will return 1.5 else score,a workaround for now would be to choose a range max - min that is a power of two but is larger than the range you want and then modify your component to either add a large penalty for a minimization problem to the objective value f when you are outside the desired range or raise an analysiserror can be imported from openmndao.api when you are outside the desired range,if value is less than current min go to middle range less than current and go back to step 2,"
"min","range"," so better in greater values slider,bigger in date small output,lower higher in greater values slider,less than  in date small output, is lower in disjointed ip current, is larger in greater values slider,higher in greater values slider,less than the  overall,integer just with  overall, is not overall, createddate then overall,","49395609,44600848,42035218,57082236,9991303,56991078,42850734,53819751,49318899,10534904,48640060,","but as i know you can type in value that greater than max or lower than min so better to manage it on server with updatenumericinput and back values in min max range in code,for ease of debugging i pass the value of to reactive values and print the first date range s value to the console rendered to check whether the it is smaller or bigger than the min and max of the corresponding date column as i did in the lapply function,i m working on slider that updates 2 text boxes with values the slider is in a limited range so it doesn t give values lower higher than min max,here is an approach i have taken only few rows and small date range to print the output here identify min and max date please note that i have taken min from d1 and max form d2 but you can change if min d2 is less than min d1 for your data construct the struct with dates as struct fields populate the struct fields using udf expand the struct by selecting struct,check the the min value of this range is lower than the ip,the following code is for making the same y axis limit on two subplots just a reminder the histogram auto crops the range if the specified range is larger than the max min of the data points,if you want to check that a number is between the min and max of the range you would likely want to check that it is both higher than the min and lower than the max,if the trial set has values greater than the max of the training set or less than the min of the training set you ll have values outside of the 0 1 range,by the way your integer is not such big consider sys.maxsize sys.maxsize in range sys.maxsize is pretty fast due to optimization - it s easy to compare given integer just with min and max of range,so if i 3 then i-min is 1 the range after adding the delta is 2 3 4 modulo 4 yielding 2 3 0 so adding the min gives us 4 5 2;generate a random number up to n-1 and add it on modulo the original range shifting because the min is not 0,the problem i am facing is what if the start date is lesser than min createddate then the data i am getting is filtering but instead of that i need the grouping time from start date to the end date grouping in a range if there is no data for some months then the database should return 0,"
"min","range","greater in greater values slider,","10851578,","take a look at your picture - all the range you want have ending time greater than min and starting time less than max,"
"exponent","multiplication"," doesn overall,faster in small parameter naive,larger in small parameter naive,operator higher overall,even easier overall, is usually better overall,faster in small parameter naive,slower in small parameter naive,not faster in small parameter naive, then use overall,","31227943,20776795,1019808,29987396,18450999,33724911,14917576,18453999,12919354,21702808,","note that remainder is taken after every multiplication so no overflow can occur if single multiplication doesn t overflow and that s true for 1000000007 as m and long long;as already mentioned your problem is that for large exponent you have integer overflow,however with really very small parameter 2 in your case exponent is faster than multiplication,as int exponent gets larger taking powers might be faster than multiplication,note that the exponent operator has a higher precedence than multiplication and division just like in mathematics,multiplication is even easier as you dont have to line up the decimal points you just do the math on the significant digits and simply add the exponent,if the exponent is a known small integer the exponent is usually better to write the multiplication explicitly,this is analogous to the way you can compute exponent using successive squaring much faster than by repeated multiplication,however naive multiplication will get slower and slower as the exponent increases,the misunderstanding is that incrementing the exponent is not faster than doing a multiplication,addition subtraction and multiplication are not terribly difficult;you will need to split the numbers into sign mantissa and exponent then use integer math to calculate the result and finally put it back into a float,"
"exception-handling","try-catch"," including the nullpointerexception overall,top-level actions with  overall,more overall, occurs then overall,heavier overall,better overall, is probably overall, is not overall, statements makes code harder overall,more robust overall, it is better overall,","51709418,48671212,29999464,55450024,4571666,44723363,13396640,48684378,9279856,12951427,5813236,","your try-catch does not do anything different;never catch the unchecked exception-handling including the nullpointerexception,catch an exception-handling only if you know how you can usefully continue even knowing that some previous step failed - and very often you can t so surrounding only top-level actions with try-catch there logging the exception-handling to some log file and informing the user often is the best thing you can do,try-catch rsources is fine for some cases but this is the old way and works better when you have more than 1 exception-handling in a block,we are very familiar with try-catch blocks that we can wrap over a set of code and if an exception-handling occurs then it will handle this exception-handling,will hide the exception-handling and since dealing with exception-handling is heavier compared to a simple if t why isn t the normal new t not considered less good practice considering we will have to use try-catch to check if a simple allocation succeeded and if we don t just watch the program die,so here the unchecked exception-handling is better to use in order not to copy and paste all that ugly try-catch block rethrowing an exception-handling and add the throws clause to the method,avoiding that with try-catch is probably better;exception-handling are fine for catching errors such as badly formed input when it s expected that input normally is valid,for that you ll have to use try-catch within using block;if the exception-handling is not caught you will see an exception-handling raised by the ide with the message an unhandled exception-handling of type system.exception occurred in the catch block,in this case the try-catch version is not too bad but you still have to know which type of exception-handling is thrown and in which case;also in real-world code having a lot of try-catch statements makes code harder to follow,in general try-catch is more robust does not require you to define an exact position of where to test could be a block and provides info about the exception-handling,yi rather than using try-catch in this way you should just use dispose instead - under the covers it does the same thing but the result is cleaner and easier to read;you should avoid catching swallwoing exception-handling too - if the above throws an exception-handling it is better if the caller handles the caller,"
"exception-handling","try-catch"," is generating the stacktrace overall, is usually a relatively overall,better overall, that aren overall, it is better overall,actually slower overall,","28197266,51399302,36331574,22365693,9976913,26017078,","try-catch is not the expensive part;throwing the exception-handling is generating the stacktrace,in a normal situation try-catch should not be that expensive performance wise as discussed in this thread do try catch blocks hurt performance when exception-handling are not thrown;however throwing and handling an exception-handling is usually a relatively expensive operation,when you call input.nextline as part of your exception-handling catching is it better to place it into every catch block or just inside a finally block at the end of the try-catch,if it s only in a job it s ok but if you plan to manage records in other points we warned that if you use nested try-catch blocks the control will go to the outermost try-catch block avoiding internal ones;well there are two or three exception-handling that aren t check programming manual i don t remeber them now they were related to ddbb record blocking and so on,if you re planning on wrapping your code in a try-catch block with an exception-handling of nullreferenceexception then you should have something in place to handle this type of exception-handling and perform any necessary operations related to this error;that being said in situations where you know you may have the possibility of having an error which would throw an exception-handling it is better to check for this situation -- and not throw an error at all but gracefully handle it,try-catch is actually slower if there really is an exception-handling thrown,"
"keydown","keyup","smoother overall,better in better button multiple, is better in better button multiple, is probably better in better button multiple,better in better button multiple,just better in better button multiple, is a better in better button multiple,more preferable in better button multiple,better timing than with  in better button multiple,earlier overall,slower than input  in slower frequency sense,","46595732,19554689,31318060,48177498,33768168,25939892,50061803,20370954,56591726,14646962,54027543,","in my test keyup gives a smoother transition as compared to keydown,edit keydown is a little better than keyup for the element bind now enter key fails silently-ish,if you insist to use keydown add e.preventdefault to prevent that event s default behavior;but i think kiirosora09 s answer that switch to keyup is better,btw keydown event is usually more reliable if for some reason keyup is one off on results keydown is probably better,also note that keydown is better for this methodology as keyup will fire multiple times though i guess this will too.,they have their differences better stick to one i prefer for this example to use keydown just plays better with me you can use keyup,also using keydown is a better idea because if you keep holding the backspace button with keyup it wont delete it will delete just one character when you release the button,keyup is more preferable than keydown because keydown may occur multiple times if user keeps it pressed,listen keyup better timing than with keydown on the parent only it ll bubble up from the child elements,the events seem to not follow strict sequential rules second keydown comes earlier than first keyup so the timer gets initialized multiple times,this problem could be related to the speed of typing and event handlers that are attached to the input field these event handlers execute slower than input keyup keydown frequency,"
"keydown","keyup"," is more overall,more sense in slower frequency sense, is better in better button multiple, is too overall,better in better button multiple,more in events right field, handler somewhere overall,earlier in events right field,","19753288,45039229,49307241,46518701,25278766,45686156,48656681,12622265,","keydown will be triggered when key is pressed and keyup will be triggered after pressing the key;in your case keyup is more suitable,also note that using keyup or input makes far more sense than keydown otherwise the previous value will only be evaluated on the next event occurrence,you could do something like this html js jquery there is a change event on jquery but it s only called when the input has lost it s focus so using a keydown keypressed or keyup is better,keyup doesn t work because characters are rendered on keydown therefore keyup is too late;you can t have a keyup event without a keydown occurring first.,also if you notice i ve changed the keyup event to keydown which is better in my opinion cause when the user holds their finger down on a button the code wouldn t be fired if it is on the keyup event,then the right events to do it is keyup more than keydown since the value isn t yet in the field at this moment or on change,edit 3 in a comment below the linked solution from this stackoverflow question does not cover all the cases because the disable action prevent the keyup event to be triggered and - on the other side - the keydown event is to earlier because when the button is hit a new section is created obviously by another keydown handler somewhere else and no i cannot modify that handler directly,as you can see the keyup events of numpad5 and numpad6 occured earlier than their keydown event,"
"scriptmanager","updatepanel"," which is usually overall,more time overall,more overall,","10029603,22788420,5565423,","2 make sure the updatepanel aren t interfering by turning off the ajax updatepanel functionality;you can do this by setting enablepartialrendering to false on your scriptmanager which is usually located on the masterpage,in case if updatepanel takes more time to process set asyncpostbacktimeout property of your scriptmanager,i have found that using jquery javascript inside an updatepanel is more hassel than it is worth as you always need to output the jquery javascript using the scriptmanager or the scriptmanager.registerclientscriptblock,"
"cypher","gremlin"," is still faster and obviously overall, query. here overall,faster overall, neo4j has more overall,much more clear overall,better overall,less in harder apoc union, have been really overall,powerful than  in harder apoc union,","25576741,50577155,17332011,16524702,13824962,33393378,36779724,14160809,55602902,","neo4j and cypher is still faster and obviously this has no effect on the gremlin queries on neo4j but that might be just a issue with the gremlin implementation for neo4j,gremlin execution time is 50 times more than cypher query. here is the cypher query,recently we noticed that cypher queries run faster than gremlin so we decided to convert our queries,aveat when you start reading you may encounter with gremlin which is a common graph query language supported by neo4j;is quite awkward and very different from cypher so if you are going with neo4j you should stick to cypher neo4j has more features and most of the development is made against neo4j,cypher seems much more clear to me than gremlin and in general it seems that the guys in neo4j are going with cypher,in this case a traversal-oriented approach is best maybe gremlin is better since cypher doesn t allow you to specify traversal order,but i read in this post that group by on union are not possible yet it mean that cypher is less powerful than gremlin,in my projects i typically use gremlin and then call cypher from within gremlin or not when i need tabular results or expressive pattern matching- both are a pain in the gremlin dsl;the neo4j team s efforts on cypher have been really impressive and it s come a long way,gremlin is harder to learn but it s more powerful than cypher and apoc,"
"metaphone","soundex","much more overall,more accurate overall, has better overall,better than  overall,better overall,better overall,","9014069,3940042,24272834,41321818,123076,20066405,","general consensus including the php docs is that metaphone is much more accurate than soundex when dealing with the english language,in php you should use metaphone it is more accurate than soundex,ike the ancient and honorable soundex double metaphone favors false positive matches rather than false negative;but double metaphone has better rates on both mostly due to double metaphone double-hash capability,then i tried the metaphone function and the metaphone function worked far better than soundex,edit double metaphone was specifically designed to be better than soundex and work in languages other than english,double metaphone - this algo will give you a better match than soundex at the cost of speed it is really good for spelling correction though,"
"irrlicht","ogre","cleaner overall, is better overall,better overall,","151399,350459,19394943,","irrlicht has a cleaner api lower system requirements and works better across platforms than ogre in my opinion,o i simply can not see a reason why to spent months learning ogre s terrible api - if one has free time i would advice to learn d3d d3d;i can say even more - irrlicht is better than many commercial engines for example - torque absolute lack of documentation forces to start project over existing one etc truevision etc,irrlicht is no way better than ogre 3d or am not trying to prove that,"
"perl","tcl","actually more overall,more verbose overall,worse overall, tk so overall, syntax isn overall,better overall, but broadly overall,more familiarity with actual  overall,","44058365,23933557,393362,4518494,54528926,18208624,1197324,54897792,","while tcl syntax looks more related to shell syntax like bash or tcsh tcl is actually more closely related to perl or php or ruby,tcl is more verbose than perl here,tcl is no better nor worse than perl ruby python or any other scripting language in this regard -- they all do basic file operations with equal aplomb,tkx is a thin layer over tcl tk which gives it access to the new themed widgets in tcl tk so your application would look better;perl tk has a lot more documentation around but the module itself does not seem to be developed anymore,in perl empty elements are undef so i guess the closest translation to perl would be test output;the tcl syntax isn t clear so i went to the source tclsh tcl foreach values in each list are used in order from first to last and each value is used exactly once,i know that perl performs better for some regex but will the tcl performance really be this bad in comparison,it is written in tcl which is a language somewhat simpler than perl but broadly in the same family and not difficult to learn,or if you are not familiar with perl tk have more familiarity with actual tcl tk syntax or familiar with wrappers for languages tkinter for python a better option might be tkx,"
"lan","wan"," can be with network overall,less overall,less stable overall,","37886234,19719152,37886234,","lan is more or less stable wan can be with network issues like,even if all the traffic resides on the same network the client traffic will have been throttled by your wan capacity which is always less than your lan capacity,wan is less stable than lan,"
"fireworks","photoshop","easier overall,more familiar overall,","410176,1870710,","i find fireworks has an easier time dealing with vectors because they re first class objects while in photoshop they re actually a combination of a vector mask and a colour fill,i have fireworks 8 as well as photoshop cs3 on my windows pc but i m more familiar with fireworks,"
"dotnetzip","sharpziplib","more flexible overall,more permissive overall,much easier overall, is a far more overall,","3046580,3860490,10643602,384966,","dotnetzip offers native support and has a quite friendly api and is my opinion more flexible than sharpziplib,i used it because it is small 6 kb compiled with just the extraction making it far smaller than dotnetzip which is more than 150 kb and is more permissive than sharpziplib,dotnetzip is much easier to use than sharpziplib example of zipping all files in folder,i ve found the dotnetzip library to be this easiest way to work with zip files;sharpziplib is a far more powerful and flexible solution,"
"dbcontext","objectcontext","newer api in better newer older, cannot overall,newer overall,api easier in better newer older,worse with  in better newer older, seems much more overall,lighter overall,much simpler in simpler common development,api better in better newer older, api is probably overall, derived class overall,","15875489,6416147,15179448,14727174,7230825,33714850,14903097,12495100,22010602,9264795,10217018,","dbcontext is newer api which should polish developers experience when using most common tasks - simply the api is better designed but you still have to get objectcontext from dbcontext and use the older api if you want to use some more complex features,because of that i think that whole idea of dbcontext api was management failure;at the moment ado.net team must maintain two apis - dbcontext is not mature to replace objectcontext and it actually can t because it is just a wrapper and because of that objectcontext cannot die,i used to use dbcontext for all of my db models until i read ways to optimize entity framework after following the steps i found my self forced to switch to objectcontext instead so there were alot of code changes to be done but i am not sure that i doing the right thing specially after googling the deference i ve noticed that dbcontext is newer and better than objectcontext and also i noticed that i lost alot of things while switching to objectcontext like migrations and find method and much more.,dbcontext api is easier to work with than objectcontext but both approaches use the former,you want to replace data in perfil is a feature it is about objectcontext api but the same is true or worse with dbcontext api,despite the fact that the incident entity had an entitystate of unchanged objectcontext still recognised it as new;dbcontext seems much more state aware,you just need to connect using the entity framework driver and if you develop in any of technologies that uses dbcontext includes code first model first and data base first but is lighter than objectcontext you can use the list below,dbcontext is much simpler to use than objectcontext and will serve the most common development needs,there are many reasons why the dbcontext api is better than the older objectcontext api but in your case it may initially feel like a downgrade,now my answer is mostly - for new users dbcontext api is probably better;dbcontext api is simplified - both in terms of usage and features but you can still get objectcontext from dbcontext and use features available only in objectcontext api,it discovers entity sets based on dbset properties defined on the dbcontext derived class or in general it discovers your model based on your code;objectcontext does not do any discovery and is not convention based,"
"dbcontext","objectcontext","api easier in pleasant api easier,slower overall,much simpler in simpler common development,better overall, is just overall, is a smaller in better newer older,newer in better newer older,slower overall,more pleasant api in pleasant api easier, is not overall,","13000063,10103310,9224507,12689334,13614390,17197973,17285367,10103310,13089533,8297302,","besides the dbcontext api is easier to use than objectcontext,if adding 2000 entities and saving the changes at the end dbcontext is 3 to 5 times slower than objectcontext btw. i know that adding a large amount of entities would be better using sqlbulkcopy but that s not the point,and dbcontext is much simpler to use than objectcontext and will serve the most common development needs,in 6.7 a graph it shown that states that the performance of a objectcontext linq query is better than dbcontext linq query,actually dbcontext is just a layer on top of objectcontext;dbcontext api has much nicer smaller api and is easier to work with,the dbcontext is a smaller api exposing the most commonly used;features of the objectcontext,my understanding is that although dbcontext is newer it s not necessarily better than objectcontext it just provides a different api that might be considered simpler but doesn t that depend on the use case,when it comes to deletion it even gets worse when saving at the end of all entity removals dbcontext is around 18 times slower than objectcontext,dbcontext is a much more pleasant api than objectcontext,a shared dbcontext is not a good idea;since dbcontext objectcontext is not thread safe,"
"panels","scrollable"," size so overall,smaller inner in larger form autoscroll,larger in larger form autoscroll,","55592122,19957747,5682770,","i want full page snapshop of jpanel that have content larger than panels size so content is scrollable but i am unable to get snapshot of non visible area in jpanel,i am trying to create a jpanel that is resizable scrollable and contains x smaller inner panels,the panels itself is larger than the form and is scrollable autoscroll true,"
"filtering","sorting","faster in faster database tier,smaller rather than  overall,general in angular developers logic,more in data second efficient,efficient as  in data second efficient,faster in faster database tier,general in angular developers logic, and is probably more overall, is nothing more overall,more straightforward overall,slow after  overall,","6654141,52451664,53343356,25252630,51706625,45873691,50172439,45328140,51506537,33053106,54576268,","in terms of your speed query i d propose that your pseudomedian filtering is faster because it doesn t involve sorting,filtering and concat since you don t care about the order other than these two grouping and you know the larger array has all the elements from the smaller rather than sorting you could just filtering all critarray items out of array than concatenate critarray and array this assumes _id is a unique identifier,they recommend filtering list items in the component logic the angular team and many experienced angular developers strongly recommend moving filtering and sorting logic into the component itself,sorting is more important to me than filtering so if i have to have both sorting and filtering target the same representation of the data i ll just give up on filtering working,in my point of view your second case is a way more efficient as sorting out or filtering data in simple php will make your application much slower,you d still have to filtering to get a range though it ought to be faster than sorting at least,the angular team and many experienced angular developers strongly recommend moving filtering and sorting logic into the component itself,edit on 01 29 2018 an article about the material data table has been written and can be seen here the material data table includes server pagination filtering and sorting and is probably more up-to-date than my code below since a lot of people are visiting this post i guess there was a real need for this hopefully this will help you all have fun,every sorting paging and filtering is nothing more as reloading the grid which has datatype local with another parameters like page sortname postdata.filters,on the other hand you may want stable sorting in other contexts as well and so if you have the stable_sort function defined this approach would still be more straightforward than filtering out the numbers with each sign and recombining them,also i think that the update of the gui is relatively slow after filtering and sorting,"
"filtering","sorting","less expensive overall, is cheaper in cheaper duplicate field,cheaper in cheaper duplicate field,familiar with   in data second efficient,more overall,general in angular developers logic,longer overall, only dogs more overall, is often faster in faster database tier,general in angular developers logic,cleaner than the  overall,","2911817,47671241,1172381,55533351,38171789,51491867,7274114,46692505,295657,48403229,51955894,","filtering is a lot less expensive than sorting,sorting once before filtering is cheaper than searching for a duplicate in the entire array for every array element o n²,if your author field is selective and sorting is cheaper than filtering,using getelementsbyid allows me to pass by this but objelement then doesn t support .click html button code the end result is to allow the csv to open as a new excel workbook sheet as from there i m very familiar with sorting filtering data,considering sorting is more complicated than summation median filtering will cost longer time,angular doesn t offer such pipes because they perform poorly and prevent aggressive minification ... the angular team and many experienced angular developers strongly recommend moving filtering and sorting logic into the component itself this means you should get rid of your pipe and put the logic directly inside your component,but right now the sorting step is taking way longer than the filtering step so i would like to combine them in some way,irebase realtime database has sorting so i can download the array of neighborhood dogs ordered by name age etc;i can filtering only dogs more than five years old but firebase realtime database can firebase realtime database sort and filtering,the database can filtering faster than any other tier in general because using a where clause on the database will often allow the database to avoid even reading the unnecessary records off the disk which is several orders of magnitude slower than anything you can do with the cpu;sorting is often faster on the db as well because the db can often do the joins in such a way that records are already sorted in the order you want them,the angular team and many experienced angular developers strongly recommend moving filtering and sorting logic into the component itself if you want to sort your items by let s say name here it goes in your html,i know someone will have a good suggestion for something cleaner than the sorting looping and filtering i ve tried,"
"filtering","sorting","general in angular developers logic, it makes easier overall,binary search as  overall, a coping the list overall,such as  overall,slower just in faster database tier, is far easier overall, is more overall,","54183031,57703774,52276163,51138087,54643539,7274258,56168877,47253518,","you shouldn t use pipes for filtering or sorting lists the angular team and many experienced angular developers strongly recommend moving filtering and sorting logic into the component itself,for example with range 0 10 if we filtering list to be within range then we don t have to worry about it and the following list sorting it makes easier to pick next value i want to output list of paths taken to cover the range as follows i tried setting up function that would get list of next possible x y values as follows but it only prints a single path,i would not try to sort first then binary search as filtering is a o n operation and sorting a o nlog n on top of which you have to add a binary search,update this question is marked as a possible duplicate of another question which answers how to sort but i need more than sorting a coping the list b filtering out elements c sort it,you can perform operations on it more readily such as sorting or filtering,if almost all elements fail the filtering then it s considerably slower than just sorting everything since you ll end up selecting thousands of times,prepare some helper to create combined filtering function mix of filtering and searching sorting is far easier,sorting should be supported by the search container;you just need to indicate it in the columns but filtering is more tricky,"
"nsmutablestring","nsstring","more general overall, does not;for example in not for example, is not overall,more efficient overall, is better overall,more things overall, does not in not for example,","12530092,18671780,39264818,14341239,7318764,12529478,16461032,","nsstring is more general as it leaves you the oppertunity to create another subclass of it that could add the altering functionality without using the nsmutablestring s implementation why ever you would want that to do,nsmutablestring nsmutablestring objects provide methods to modify the underlying array of characters they represent while nsstring does not;for example nsmutablestring exposes methods such as appendstring deletecharactersinrange insertstring replaceoccurenceswithstring etc,instead you probably should use the nsmutablestring and append the second string to it;you cannot truly append one nsstring to another because the class nsstring is not mutable,note also that using nsmutablestring is more efficient than creating a new nsstring each time a letter is added,if soapresults is an nsstring either because you declared it so or because you affected an nsstring to it you may instead create a new nsstring by appending string to it then create a mutablecopy from it.;but i don t recommend this as this will generate useless allocations and nsmutablestring is better to avoid this,all-in all the reason is that nsmutablestring is able to do more things than nsstring and can do anything nsstring can hence the direction of inheritance,nsmutablestring nsmutablestring objects provide methods to modify the underlying array of characters they represent while nsstring does not;nsmutablestring exposes methods such as appendstring deletecharactersinrange insertstring replaceoccurenceswithstring etc,"
"libgdx","xna","lower overall,more popular overall,","17584717,19776052,","xna + monogame --- monogame s performance seems only slightly lower than libgdx can build to most platforms,xna is more popular and has more tutorials then libgdx,"
"quadtree","r-tree","faster overall,simpler data overall,more overall,faster in indexes faster nearest,more specifically overall,faster in indexes faster nearest,better perfomance in better perfomance r-tree, is better in better perfomance r-tree,","27930062,20115674,26519960,27930062,12715054,27930062,18145430,17235688,","r-tree are substantially faster than quadtree for window queries like inside contains covers etc,a quadtree is a simpler data structure than the r-tree,i use quadtree more than r-tree and find them super flexible,quadtree indexes are created faster than r-tree,on the other hand the wikipedia article on r-tree seems more specifically targeted towards mapping than the k-d tree or quadtree,r-tree are much faster than quadtree for nearest neighbours queries,you can also try a quadtree it has some better perfomance over a r-tree,if you can use a database you can use the point datatype or you can write your own r-tree;when you have many overlapping pins maybe a quadtree is better,"
"gwt-rpc","requestfactory","more overall,better newer overall,general overall,","20335224,6957715,12179503,","my understanding is the requestfactory is more efficient and recommended over gwt-rpc but it s more of a data entity persistence framework than a request-response framework like rpc,go with gwt-rpc or better newer with requestfactory,more specifically requestfactory can update individual properties fields and can thus increase efficiency by sending the differences in state only;an important architectural difference is that gwt-rpc operates at a more functional level,"
"directx","gdi","slower overall,slower in full acceleration simplification, is a newer interface originally overall,smaller than the  overall, + system.drawing namespace overall, is definitely faster overall,faster in full acceleration simplification,faster in full acceleration simplification,way faster in full acceleration simplification, there isn overall,","35451557,58301,6425529,6727374,1302763,5270194,368466,3991925,5113541,25788405,","using getfrontbufferdata in directx method is slower than gdi itself,speed is usually faster than gdi and slower than directx and depends greatly on how you do things seen something to work 60 times faster after rewriting in a sensible way,gdi is the original graphics interface in windows;directx is a newer interface originally created for games development but now also used by higher level frameworks like wpf,if you are talking about someone else s there is no promise that they will render larger than the screen since would be wise to clip they painting to what is visible and they may be further constrained by other factors such as the size of a directx surface which is smaller than the gdi limit,directx is not something you would use to draw simple shapes rather render complicated 3d stuff also using dx api under c# is a bit trickier although not that hard;read about gdi gdi+ system.drawing namespace for example here,after 100 trials capturing my dual monitor display the gdi implementation averaged 0.65s per screen capture while the directx method averaged 1.72s;so gdi is definitely faster than getfrontbuffer according to my tests,to say directx is faster than gdi is also something of a simplification - wpf and gdi-based rendering technologies just have different performance characteristics,directx is in general much faster than gdi due to the fact that it has full acceleration on most video cards,directx is way faster than gdi,its possible that wpf can get around this since it uses directx to handle its drawing;since winforms uses gdi there isn t a lot of support for floating precision within drawing,"
"xcb","xlib"," s much overall,more direct overall,simpler overall, is not very overall,lower level overall,","27393136,41403168,9039622,5812916,21216728,","n xlib vs xcb - i would recommend xlib over xcb if you re first starting out;xcb is more efficient but xcb s much more low-level and much more verbose think assembly language for the gui,xcb presents a more direct view of the protocol than xlib does so you often have to look at either the protocol specs or xlib source code to find out what the underlying protocol request is to find an equivalent,xcb is simpler to use has a better response to a multithread environment but lacks documentation while xlib is a more dated complex tool better documented and fully implemented,in fact xcb was created precisely because xlib was too high-level masking too much underneath so it was easy to make mistakes and making it difficult or impossible to have full control;also xcb is not very old from an x perspective,xcb is lower level than xlib and allows you to minimise the number of round-trips to the x server leading to lower latency,"
"cglib","javassist","significantly slower overall,way better overall, is not overall, which has a more overall,slower overall,better overall,slower overall,","21202878,9823788,9823788,22038927,9823788,9167436,9823788,","note that javassist is significantly slower then for example cglib because it reads in class files directly instead of using reflective access in order to avoid class loading,the javadoc of javassist is way better than that of cglib,the class engineering api is ok but javassist is not perfect either;in particular the proxyfactory which is the equivalent of the cglib s enhancer suffer from some drawbacks too just to list a few,you can instead register an asm visitor with the cglib enhancer which should be responsible for adding the annotation to the cglib generated class;however you might want to consider creating your classes using javassist which has a more modern api and supports the writing of annotations,this makes for example javassist or proxetta significantly slower than cglib which simply reads the methods via the reflection api and overrides them,here in so i found a lot of questions asking something similar and the answers normally said you can use cglib or asm or javassist is better than cglib or bcel is old and is dying or asm is the best because it gives x and y,also javassist is recognized to be slower than cglib,"
"cpython","ironpython"," is slower so in slower files implementations,compatible with  overall,faster in slower files implementations,probably better overall,better than  overall,worse overall, provides a much more overall,faster in conclusion faster,faster in conclusion faster,more overall,faster in slower files implementations,","472312,57034093,13964362,1168957,4337136,3882058,4637753,620976,4148627,845066,8981895,","after a lot of investigating here are the reasons why i stick to cpython;performance there are some articles out there stating that there are always cases where ironpython is slower so if performance is an issue,however i know numpy is not compatible with ironpython since it s written in cpython,if there s any heavy application logic as opposed to all of the work being networking files database ironpython is much faster than cpython at a few things and much slower at a few others so you probably need to profile and perf-test before you go too far down either path,cpython probably has better ide support at present than ironpython - so this may be a factor in the choosing of one over the other,one of the largest benefits of ironpython is that the largest benefits of ironpython has effectively no gil - meaning that if you are both writing python code and it is multi-threaded - you can often get performance that is better than cpython without having to spawn multiple process and pickle objects across the boundaries,so if you want to use django or something like that it s probably not smart to use ironpython because it isn t really cross-platform doesn t work with some frameworks and it performs worse than cpython,rdinary cpython requires python .net to do the same or similar things;in my experience python .net works most of the time but not always and ironpython provides a much more polished experience in accessing .net from python,i m not sure exactly how you re drawing the conclusion that ironpython is faster than cpython,consequently ironpython is potentially faster than cpython is especially for multithreading scenarios,i am however no cpython guru and can get so much more out of ironpython than cpython as i can easily leverage the .net platform on which i am far more at home,ironpython is faster than c# in certain areas but not faster than cpython however you can link ironpython to any language thus over coming problems but then again you can do the same with cpython,"
"cpython","ironpython"," are backed by heavily in jit-compiled jits kind,slower in slower files implementations,faster in jit-compiled jits kind,slower in slower files implementations,more comfortable overall, was not so overall,","3033387,19013034,30475540,12843154,1441330,57214679,","unladen swallow s possible that jython and ironpython are much faster than cpython as well as jython and ironpython are backed by heavily optimized virtual machines jvm and .net clr,it seems ironpython is just slower than cpython for reading text files,ironpython and jython are also jit-compiled although using the more generic jvm and .net jits so they tend to be faster than cpython for this kind of work as well,also on implementations like jython or ironpython could be a lot slower than with cpython,my understanding was it was written in vanilla python cpython but if you are more comfortable with ironpython it shouldn t be hard to translate,i wanted to verify that cpython was in fact waiting to verify the error handler until a decoding error was encountered and that ironpython was not so i checked the source code for both implementations;cpython below is the code for the pyunicode_decodeutf8stateful function found in the unicodeobject.c file for python 2.6.2,"
"couchbase","couchdb","compatible with  overall,more scalable option overall,more overall,more efficient overall, s very overall,compatible with  overall, is not going anywhere soon overall,","52301409,11960001,9508840,31795832,23889231,54572981,15184612,","as of couchbase lite 2.0 it is no longer compatible with couchdb,i d like to add that couchbase is a faster and more scalable option than couchdb the 2.0 version introduces views at a high level it s a distributed memcached membase server merged with couchdb but of course more sophisticated than just mashing them together,couchbase looks more like a memcache server with a persistence layer powered by couchdb and maybe it does not fit your needs,couchbase btw also uses binary replication mechanism which will be more efficient than couchdb as long as the couchdb protocol is not utilized for bidirectional data exchange and conflict resolution,given the throughput i d expect that couchbase is more suitable for your needs;i ve not used couchbase as much but couchbase s very similar in couchbase s map reduce logic to couchdb,this is a problem for couchbase lit sdk last relaese because it uses sync gateway as its own server no longer compatible with couchdb,meanwhile couchdb is an actively maintained open-source project couchbase server is a completely separate project it is a newer project but it is not a newer version of couchdb - they are not even compatible and since even new tools for creating couchapps still keep being developed eg;see the kanso project then couchdb is not going anywhere soon,"
"https","ssl"," is really too overall, will not overall, doesn t only in file accessible http,more flexible overall, not in file accessible http,familiar with  overall, not in file accessible http,reachable with  in file accessible http,always safer switch in file accessible http,resource slower in file accessible http, which are now overall,","3507128,12651036,15337282,20815358,41773260,52941741,28584475,56346370,22101820,34876725,24006850,","it s not difficult and ssl certificates aren t expensive startcom s free startssl certs might be sufficient for your purposes;if https is really too much work assuming you re using custom session-management code it s not too difficult to return a random mac key on login and sign future requests using the key and a sequence number to prevent replays,data transferred over ssl will not be cached by the visitor s browser instead will be downloaded each time a page is loaded;using ssl https is recommended if a page contains sensitive data personal data or offers interactions like contact forms etc,server authentication for free https ssl doesn t only solve encryption but also server authentication so that the client can verify whether it s actually talking to the right service;guaranteed to work on the internet http https traffic is common on the internet so you won t run into routing problems or firewalls which are hard to traverse,https is more flexible than ssl an application can configure the level of security it needs,the src attribute should point to a valid audio file accessible via an ssl based uri;thus using an https not an http based request,hen i added https at the last second;not being very familiar with ssl i made ssl a much harder problem than i had to so that s my defense,you can recognise when ssl is in use and properly configured because;the url will start with https not http,if your site loads with http but not with https that means your website is not reachable with ssl so check that ssl is enabled,as last resort resolution i don t suggest this for security issues using ssl is always safer switch your registry to use http instead of https,you could always use https that will work both on http and https websites but loading a https resource is slower than loading a http resource because of the encryption and ssl handshakes,io socket ssl does not provide its own proxy hacks like crypt ssleay does and lwp https proxy support as documented using proxy method or env_proxy was broken;with version 6.06 of both lwp useragent and lwp protocol https which are now seperate distributions,"
"https","ssl"," isn t a fluke overall, do not overall,tomcat server with  in load balancer proxy,fine with  in load balancer proxy, certificates are often in load balancer proxy,more problems with the  overall, it s open normally in load balancer proxy,setup with  in load balancer proxy,more with these  in load balancer proxy, is much more in file accessible http,more in load balancer proxy,","4199127,6564252,54109576,53800133,28249053,52365808,56585886,48957071,47786787,6090985,39346295,","some google engineers wrote an article explaining how some previous assumptions about ssl are no longer true;i think the faster performance you re seeing over https isn t a fluke,there are lots of references to this kind of issue on the microsoft support site also internet explorer file downloads over ssl do not work with the cache control headers;everyone who used file downloads on https is likely to have hit this problem with internet explorer,i created certificate for ssl and when i m trying to run apache tomcat server with https it is giving me error like this site can t be reached localhost took too long to respond,i have godaddy ssl certificate and it is properly installed on my domain and my domain is working perfectly fine with https but when i open then it is showing not secure,ssl the s in https does not generally place any restrictions on how your ips work;the ssl certificates are often issued to domains and or hostnames,the most simple solution go to settings change from https to http try it now if it works there is a issue with woocommerce forcing autentication you can simply leave it be and use wp force ssl to keep runing the ssl certification and have no more problems with the ssl certification,i ve tired to configure an nginx server with ssl but the site is not open but with https it s open normally,however google still had our https site indexed and when people clicked those links from the serp page they would get an error because the registrar was not setup with ssl certs for those domains,for this i tried out few things like and few more with these ssl keys. tried to redirect https calls from server to http call using nginx however failed to do so,for additional reading see the questions tagged ssl on security.stackexchange.com;long story short post over https is much more secure than over http,there is a limitation on creating virtual hosts for name based virtual host while using ssl 443 port so if you want to have more than one https port enabled you will need ip based virtual hosting,"
"https","ssl","live with  in load balancer proxy,http than over  in file accessible http, tls;it is highly overall, is no longer overall, doesn overall, connection has higher cost in file accessible http, does not overall, is the better overall, tls is worlds better in file accessible http,url starts with  in file accessible http, to send the request; overall,","50423585,49790359,4515338,53478605,4102615,20395213,5837942,57096366,43311223,52900464,53407221,","in order to resolve this issue you will need to put your node.js server live with ssl certificate or secure https,calls from one web service to another are faster over http than over https with ssl,another edit as pointed out in a comment below remember that ssl tls isn t the only solution to all your site s security needs there is still a lot of other considerations but it does solve a few security issues for the users and solves them well even though there are ways to do a man-in-the-middle even with ssl tls;it is highly recommended these days to run the entire site on tls https that is if possible,see the blog post all new sites on netlify are https by default and this issue on github issue #158 disabling forced ssl even for old sites the option to turn off https or to turn off the redirect to https is no longer available,however the wide availability of ssl tls stacks on various languages platforms os makes it rather convenient for many applications to protect the communication between the client and the server via https;not using https doesn t mean that your system is going to be insecure although it might require a bit of work to protect the data,in many sites you authenticate yourself over ssl https;however supporting ssl connection has higher cost than normal http,added ssl tls uses random numbers in the client_hello and server_hello handshake messages to prevent replay attacks;https does not use public key encryption for transmitting data like http requests it is used for setting up symmetric random keys,if you don t want to send data in clear text through the network then using ssl https is the better option,having some ssl tls is worlds better than not having it at all;after the repeal of fcc privacy requirements for isps all websites on the entire internet need to be https and http should be disallowed entirely,please note - i am using ssl on my website so url starts with https,it should be noted that you must use ssl to send the request;https not http in the following string here is where i put my api key and now my application works like it did before i needed the key,"
"https","ssl","server issue with  in load balancer proxy,more in load balancer proxy, access was fractionally slower overall, so now overall, to secure my xxxx.mydomain.com in file accessible http, support turned on e.g overall, redirect works but later overall, key is more commonly in file accessible http,general overall, means http overall,  nc overall,","50074087,24386103,2212938,47801729,55508381,16540407,56718971,805392,54028593,48014336,52662497,","i thought that ssl handshake errors are mostly server issue with https certificate or its infrastructure,it could be a sni problem if you use any kind of load balancer https proxy or more than 1 ssl certificate on the same ip,i m using ssl on apache which handles access to our subversion repository on a small windows ec2 instance;in testing i found that https access was fractionally slower than http but that s for the obvious reason that encryption decryption is not an instantaneous process as you d expect,after installing ssl and making all pages https by using htaccess i realised that my website actually cannot run with https because it has a lot of embeded code with http protocol which cannot be changed to https or removed but simply removing the rewrite rule which forces https does not work because google has indexed all pages with https so now most part of the google search traffic goes to https version of website and they see mixed content error. i tried to implement examples that i found in forums but when i apply this code the browser always redirects from to here is the full code from .htaccess file,so my xxxx.mydomain.com currently displays xxxx.heroku.com free dyno xxxx.heroku.com is already secure with https but my newly purchased domain is http . i want to use cloudflare s universal ssl or let s encrypt ssl to secure my xxxx.mydomain.com,if https is not printed that is that the libcurl version your are linking to has not been built with ssl support;in such a case you must re-install it with ssl support turned on e.g yum install libcurl-devel should do the trick,the http https redirect works but later this the ssl proxy can t reach the angular app on localhost 4200,if you have a secure ftp sftp server or secure http ssl https server then curl can accept ssh keys although using an ssl key is more commonly accepted on the internet an ssl key can use ssh keys to authenticate and establish a secure connection to the directories which have been made public to your user through the server,anything public facing i would recommend using ssl over https,when your app connects to an webserver or to a domain with localhost or non https means http than post below into your info.plist this let the app make an exception to use urls without ssl https,example.com this iana maintained domain is another good test url printf get http 1.1 r nhost example.com r n r n | nc example.com 80 and compare with https ssl nc does not seem to be able to handle https urls,"
"https","ssl"," connection isn overall, is getting faster overall,","3029122,35880113,","this still leaves the possibility of mitm attacks but performing an mitm attack on an ssl connection isn t entirely trivial so it s probably not an attack vector you need to be too concerned with;you can muck around with client-side hashing but in general posting the credentials over a secure https connection is considered sufficient,gmail measured ssl accounting for just 1 of cpu load and 2 of network overhead when gmail switched everything to https;ssl is getting faster every year reducing already modest costs,"
"dijkstra","floyd-warshall","more overall, is not better in better complexity higher,faster overall,better in better complexity higher, s has a worse in worst performance v|3, s cannot in worst performance v|3,","25250266,49835710,11439561,11778525,1846862,11428258,","short answer floyd-warshall is more efficient in this case than naive application of dijkstra s,and so it is indeed the case that the o n 3 time of floyd-warshall is not better than the o n n + e lgn time of making n calls to dijkstra,no practically floyd-warshall is faster than dijkstra s for all pair shortest path generally,indeed floyd-warshall s algorithm is better than dijkstra s in this case the complexity for dijkstra is o m n 2 and in this problem m is much much higher than n so the o n 3 time complexity of floyd-warshall is better,the floyd-warshall algorithm has a worst case performance of o |v| 3 where as dijkstra s has a worse case performance of o |e| + |v|log |v|,the floyd-warshall algorithm has a worst case performance of o |v|3 where as dijkstra s has a worse case performance of o |e| + |v|log |v|;also dijkstra s cannot be used for negative weights we use bellmann ford for the same,"
"max","median","larger smaller in part equation segment,less overall, value is bigger overall,less overall, and reduce the problem in part equation segment,","28372834,28372834,52921784,21074836,56443598,","so the t 7n 10 is the part of continuing the equation with the max segment of numbers that is larger smaller than the median of medians..,therefore the max number of elements you could have that are greater than or less than the median of median is 3 10 + 2 10 + 2 10 7 10,this is just binary search by median value compare with example code stops iterations when borders collide when we call self.findmedianinlargefile numbers k max result+1 guess right because our guess was too small and median value is bigger than quessed value,so after calculating median of first k elements delete the first element directly from heap min or max according to whether it is greater or less than median using pointers and then use heapify at that position,you can solve it using divide and conquer approach find a random element in between the minimum and maximum check if it s median if the median is lower or higher than the median and reduce the problem to a smaller size only on a subrange of the array,"
"glut","sdl","better overall, are not only overall,more likely overall,much more easier overall,","2842963,4574238,2842963,4115361,","if you re building a simple demo of a rendering mechanism glut is way better than sdl it takes care of a lot of the details that sdl would otherwise require,glut is no longer under active development that s one reason not to use it;another is that other wrappers glfw sdl are not only under active development but generally more powerful,however if you re developing a serious application sdl is more likely to be the tool you need to use as glut abstracts more than a real application would normally want to,glut is much more easier while many of people use sdl or sfml they re more flexible and feature-full than glut,"
"g++","gcc"," but not in command libraries g,better in better, is a c++ in use programs link,simple code with  overall,larger overall, then adding preprocessor overall, does not automatically in command libraries g,looser in significant structs looser,more than just  overall, is more overall,general overall,","603150,8133175,9087899,57221833,8625428,12858484,14842056,128221,37501548,12953739,49082815,","if you only installed c compiler you would have command gcc but not command g++;g++ is c++ compiler not c compiler,so using g++ is better than gcc,use g++ not gcc;gcc is a c compiler whereas g++ is a c++ compiler,i have some issues with compiling very simple code with gcc or g++ main.c below,the g++ version is a little larger than the gcc version but not 100x larger,so maybe the answer is that it s c++ code and the fact they compiled the objects with gcc not g++ made you think it was c code;if they only link with g++ then adding preprocessor checks and extern c is useless that only affects preprocessing and compilation at the linking stage that s already done,so running the gcc command automatically includes the c standard libraries running g++ automatically includes the c++ standard libraries etc;but notice running gcc does not automatically link in the c++ standard library,unfortunately g++ packed the structs significantly looser than gcc which caused significant problems sharing objects between c and c++ code,some distributions chop you might need to more than just gcc package devtools into tons of small packages gcc g++ binutils gdb,since there is only one compiler it is also accurate to call it gcc no matter what the language context;however the term g++ is more useful when the emphasis is on compiling c++ programs,i also need the .cpp to be compiled with g++ and related cxxflags whereas the .c need to be compiled with gcc and simplier cflags,"
"g++","gcc","compiler more in use programs link,better in better,recommend compiling with  overall, means gnu compiler in tool underneath one, is happier and quieter overall, does not overall, obviously doesn overall,  usually overall, 4.3 not in tool underneath one, gets a more overall, not overall,","6371223,8574598,46043074,51018576,42202367,32427702,2062470,19631737,16911586,32286351,154976,","g++ compiler is more strict then gcc compiler,g++ seems to work better than gcc in my experience,the -wall option asks for almost all warnings the -wextra ask for some more of i strongly recommend compiling with gcc -wall -wextra -g for c code or g++ -wall -wextra -g for c++ code also add other useful options such and the -g option is asking for debug information in dwarf format these days,c++ source files should be compiled by g++ not by gcc;remember that gcc means gnu compiler collection and also contains gfortran and gccgo etc.,with this test code gcc produces a diagnostic only for the unsigned member;so although i d also usually prefer unsigned integers for bitfields or for bitwise manipulation g++ is happier and quieter with int here,using g++ to compile multiple .cpp and .h files in sublime 2;the reason why using .cpp in sublime text fails is gcc does not do,the reason for this is gcc essentially says use c rules when driving the tool chain and g++ says use c++ rules when driving the tool chain;g++ knows that to create a working executable it needs to pass -lstdc++ to the linker stage whereas gcc obviously doesn t think this is necessary even though it knew to use the c++ compiler at the compile the source code stage because of the .cc file ending,if gcc g++ are not installed then you need to install them via homebrew;cmake setup in the cmake settings make sure that both the c and c++ cuda host compilers are set to your install of gcc g++ usually in usr local bin,mex is just a tool which calls one underneath and since g++ is the c++ compiler of gcc you should install this one;as far as i know it will still not work because matlab supports only gcc 4.3 not sure about that,a more up-to-date g++ gets a more up-to-date g++ right compiling with with usr libexec gcc i686-redhat-linux 4.8.3 cc1plus via g++ -c tst.cpp we get,also use the correct gcc frontend executable g++ will treat .c files like c++ files unless you explicitly tell it not too;so if you are compiling real c then use gcc not g++,"
"g++","gcc"," to compile c++ in use programs link,use  to compile in significant structs looser, does not in command libraries g, not just overall,","2188765,41530710,30764927,56655942,","use g++ to compile c++ programs it ll link in the standard c++ library;gcc will not,btw you should use g++ not gcc when compiling c++ code;there are significant differences even if gcc is sometimes able to compile c++ or fortran code you ll mostly use gcc to compile c code,also if you use g++ to link the object files it automatically;links in the std c++ libraries gcc does not do this,this is also reproducible with gcc not just g++,"
"stringi","stringr","more overall,faster than  overall,newer overall, runs slightly faster overall, has more overall,","5992152,51086283,40296527,48015406,51861774,","stringr provides more human-readable wrappers around the base r functions though as of dec 2014 the development version has a branch built on top of stringi mentioned below,if speed is your concern base r can be faster than stringr stringi the only consequence is that the na is turned into a literal nana here so the workaround is which makes base r about as fast as stringi and slightly faster than stringr in this case. i m mildly annoyed that paste converts na to na though that s already been addressed here on so .,stringr is newer based on stringi internally and is often even faster,since stringi runs slightly faster and has a larger set of functionalities i prefer to stick with stringi . see stringi vs stringr blog by hrbrmstr i tried to use base r data.frame syntax above where possible to avoid any confusion but if i were doing this for my self i d stick to the full data.table syntax as follows,stringr is based on stringi but stringi has more options,"
"memcpy","strncpy","worse overall, which check each character in sorry lenght function,version not more complex overall,probably faster overall, will not in sorry lenght function, doesn overall,much faster overall, doesn overall, is better in sorry lenght function,slower overall,faster overall,","38255275,13051170,12284783,38255292,35833458,18597709,10513169,6509539,34079163,610260,10513169,","if you compute the length of the string for unrelated reasons or have the length of the string from other resources it s unclear to me whether memcpy is better or worse than strncpy,memcpy does not check for 0 and so is faster;matthew when you already know the length of the string you are copying you don t need strcpy or strncpy which check each character looking for the 0 at the end,the memcpy version is not more complex or more dangerous than the strncpy version,which is more efficient is up for debate but based on cpu bulk instructions which can copy an entire block of memory in one instruction memcpy is probably faster as strncpy would check each copied byte for a nul character,to extract a fragment from a string use memcpy and add a final 0 by hand;incidentally strncpy will not null terminate the destination if the size argument is less than the length of the source string,on the other hand memcpy doesn t care about 0 bytes and just copies the whole thing;to put it another way strncpy finds a 0 byte inside the first integer and stops copying,even if underlying implementation is not so different memcpy is much faster because it does not have to check what it s copying strncpy will stop when it ll copy the end of string character null,ccording to the c standard the real difference between strncpy and memcpy is that if the source string is less then n value then null characters are appended to the remaining n quantity;memcpy is more efficient but less safe since memcpy doesn t check the source to see if the source has n quantity to move to the target buffer,i m sorry i advised you to use memcpy but you have to check the lenght of the string to know how many bytes to copy and is not the correct function to use;using strncpy is better because the copy stops when reach a null byte or,but for the example you give it doesn t matter - if it s going to fail it will be in the initial strlen so strncpy doesn t buy you anything in terms of safety and presumbly strncpy is slower as it has to both check bounds and for nul and any difference between memcpy and strcpy isn t worth changing code for speculatively,memcpy will always be faster than strncpy for any real world situation even in the corner case i spoke before look page_copy_fwd_maybe,"
"pyqt","wxpython","probably easier overall, is better because... overall,general overall, is more overall,less portable overall,","36914683,8028597,28639620,2750804,3787488,","wxpython is probably easier than pyqt or tkinter i don t want to start a framework war,wxpython is better because... or no way;pyqt is better because...,all three of these projects are stable and are currently being developed although wxpython s development is much slower than the others mentioned here;wxpython and pyqt both have widgets that allow you to load web pages,nd now we had to switch into pyqt since qt is integrated in maya 2011;however wxpython is more straight forward and you can easily start working on wxpython and learn wxpython from zero fast .it provides and awesome resources and decumentation,two words of warning against wxpython it is not possible to install it via the popular fink package manager on mac os x currently which makes it far less portable than pyqt and tkinter,"
"crypt","hash"," it is much safer overall,many more algorithms overall,backwards-compatible with  overall,newer overall,","22509386,9108430,51051609,10281373,","to generate a bcrypt hash it is much safer to use the new password_hash function though there exists also a compatibility pack for earlier php versions,the second hashing function is hash which supports many more algorithms and variants than crypt but does not support some algorithms that crypt does,notably backwards compatibility the password functions are essentially a well-written wrapper around crypt and are inherently backwards-compatible with crypt -format hash even if they use obsolete and or insecure hash algorithms,hash is newer and seems to support more hashing alogrithms than crypt,"
"backbone.js","ember.js","larger overall, which is a much overall,package actually more overall, was less trouble overall, doesn overall, provides more overall,simpler overall,more heavy overall,","12999399,14559532,10105437,9403354,11671483,10105437,9755245,12996823,","ember.js is larger than backbone.js but thanks to expires cache-control this only matters on the the first load,from what i understand about ember.js which i haven t used yet it has a concept of adaptors which allow it to integrate with a variety of back-ends one exists for django + tastypie so it does a bit more of the work for you;there s also backbone.js which is a much lighter js application framework which can be made to integrate with tastypie very easily,but on the other hand although new the ember.js package is actually more complete imo than backbone.js,i think it will also be down to inexperience on my part or bad architecture;ember.js i found slightly more moo-ish as an interface though it did not quite click either;frankly backbone.js was less trouble to setup,ember.js is 42k minified+gzip while backbone.js is just 5.6kb;ember.js packs a lot of power in it which the barebones backbone.js doesn t give you,both give you the ability to implement things in a variety of ways which can be confusing but ember.js provides more of the code that you would have to write yourself in backbone.js as standard which for you personally is more important for rapid prototyping than the wealth of backbone.js examples available,ember.js would make things even simpler than backbone.js,i already know that ember.js is a more heavy weight approach in contrast to backbone.js,"
"mouseenter","mousemove","more intrusive overall, code doesn overall,earlier overall,","40365654,10981828,25777344,","mousemove is more intrusive than mouseenter but it tracks your mouse position all the time so that the tooltip moves accordingly,you could even trigger the mouseenter event on mousemove as long as you include some kind of flag state-check to ensure your mouseenter code doesn t get executed repeatedly on mousemove;jquery s mousemove method is a nice solution here as it does get triggered when the mouse is rolled into an element as you scroll even while the mouse has remained static,problably the problem is that mousemove event may fire earlier than mouseenter in some cases,"
"robocopy","xcopy","more powerful overall,better in better,more options overall, d is an easier overall,much faster overall,more powerful tool overall,much more robust overall,command more intelligent overall, probably isn overall,better in better, is more overall,","6912364,23197486,27951518,53449894,24962343,5062597,11546939,41016415,27864983,33527711,49289377,","use robocopy it s much more powerful than xcopy,xcopy will not be better than robocopy at this,you might want look at robocopy as it has many more options than xcopy,as hans passant pointed out in a comment xcopy d is an easier solution. it is- and if no other properties or parameters of robocopy are useful this could be preferred,that would theoretically cut the time in half but it seems that robocopy is much faster than xcopy at least for this use so it took way less time,if your batch file only needs to run on windows vista or later you can use robocopy instead which is an even more powerful tool than xcopy and is now built into the operating system,i have solved it in the past very successfully using robocopy it s much more robust than xcopy,the robocopy command provides a more intelligent exclusion feature switches xd and xf than xcopy does which you could use for your task,if you want to combine it into the robocopy command do something like this;and as magoo points out xcopy probably isn t going to go away any time soon,also robocopy is better than xcopy,i have excellent results copying gigabytes of deep directories during automated builds using xcopy;robocopy is more capable and can control retries much better but is usually not worth the extra trouble of robocopy very extensive and complicated switches,"
"robocopy","xcopy"," has a lot overall,","38962404,","the differences i could see is that robocopy has a lot more options but i didn t find any of t particularly helpful unless i m doing something special;i did some benchmarking of several copy routines and found xcopy and robocopy to be the fastest but to my surprise xcopy consistently edged out robocopy,"
"freebsd","linux","likely as   in likely servers nix-like, mirror runs xshttpd in likely servers nix-like,farmiliar with  overall, is more overall, you cannot overall,more overall,much better overall,relaxed license than  overall, and windows probably more overall,","20971479,1893638,51565351,30887647,9693000,30887647,530086,119991,53147856,","no guarantee but it s rather likely in your case it s even more likely as linux freebsd and os x are all posix-compatible,all servers are nix-like freebsd os x linux but not all run apache;my freebsd mirror runs xshttpd,so i m relatively farmiliar with linux been using it for a few years now and recently i ve been toying around with freebsd and dragonfly bsd,this is in part why bash is predominant on linux;the default shell on freebsd is more usable out of the box,freebsd has an emulation afaik but i don t know if you can access it natively;if you want to target other platforms than linux you cannot use futex,be aware that the bourne shell in freebsd is more capable than on linux,just a tip but from personal experience i ve found that freebsd works much better on low-end hardware than a modern linux distro does,the freebsd kernel has a much more relaxed license than linux i think you might find the freebsd kernel worthwhile,works on linux freebsd and windows probably more operating systems as well the github,"
"padrino","ruby-on-rails"," and supply the developer in faster ruby-on-rails they, has much more overall,less complex in faster ruby-on-rails they,better in faster ruby-on-rails they,","12686116,8093058,14831065,12682214,","sinatra and padrino are not automatically faster than ruby-on-rails;they are just smaller than ruby-on-rails and supply the developer with a smaller more focussed toolkit,this is because sinatra and padrino promote take what you need and gradual complexity far better than the more take it all at once ruby-on-rails approach but on the other hand ruby-on-rails has much more documentation blogs support etc,as wuliwong said sinatra and padrino are way less complex than ruby-on-rails,but i don t know anything about sinatra and padrino is the footprint and speed really that better than ruby-on-rails,"
"elementtree","lxml"," is much more overall,better overall, and not overall,more overall, doesn overall, also supports xpath overall,faster more effective in application speed effective, library doesn overall, is another option overall, it has much better overall, but offers more control overall,","260148,1563301,22312105,37943680,26494754,49614554,7949706,44271825,51312330,53507662,13993979,","t provides a nice easy-to-use binding for libxml2 and libxslt and it also implements the elementtree;libxml2 libxslt also come with their own much lower-level python bindings but lxml is much more straightforward and pythonic and it seems to have great performance as well,lxml -- 100x better than elementtree,lxml is a much better library but if external dependencies are not allowed this will have to do;if you must use elementtree and not lxml,note please do tell me if lxml is more easy or this elementtree,update based on the new information that lxml.html is the parsing library lxml doesn t use pure xpath in the same way pure xml libraries might;instead it s a marriage of xpath and the elementtree elementtree that s common to a lot of python xml html parsing libraries salted with a handful of its own homespun approaches,i wouldn t say that lxml is faster than et across the board as both modules offer tons of functionality;to provide a little context elementtree also supports xpath but particularly et has a unique and useful function called iterparse that remakes the xml document as an iterable,but in general you will find that lxml is faster more effective and has an api which adheres closely to a python standard the elementtree which comes with the python standard library,the namespace map passed to element and subelement another lxml extension maps that uri to a prefix which is used for the output;python s standard elementtree library doesn t support cdata sections so you ll need to make sure you re using lxml,python 3.6 output i prefer lxml over elementtree because of better xpath support but elementtree is another option same output as above .,updated to be well-formed python output if you re able to use lxml it has much better xpath support than elementtree,the lxml library uses the same elementtree but offers more control over the output,"
"elementtree","lxml","faster in application speed effective,","20621248,","lxml is faster than elementtree but i ve never found an application where the speed boost paid for the hassles of distribution,"
"qqmlapplicationengine","qquickview","more powerful in headline newer powerful,newer in headline newer powerful,","40271518,46342425,","headline qqmlapplicationengine is newer and more powerful than qquickview,since qqmlapplicationengine is newer than qquickview one would naturally assume that it is more powerful,"
"cpu","hyperthreading","less than your  overall,more overall, is better etc overall,more overall,more consistently overall,","55331936,149212,57622151,19170153,36958661,","recommanded that the number is equals or less than your cpu count or hyperthreading count,almost any new cpu has more than one core or if it has just one it might support hyperthreading and thus pretending it has more than one,i ve run some benchmarks using the code above in linq pad both on my i7 machine and the amd threadripper and these are the results test on i7 quad-core 3 67 ghz windows 10 pro x64 sync version 15 sec 100 cpu async-await version 20 sec 93 cpu test on amd 32 cores 3 00 ghz windows server 2019 x64 sync version 16 sec 50 cpu async-await version 140 sec 14 cpu i understand there are hardware differences maybe the intel hyperthreading is better etc but this question is not about the hardware performance,vcpu reg are virtual cpu registers that exist on many processors that have things like multiple cores hyperthreading or other features that enable higher layers to believe that there is more than one cpu present when there isn t -- such as today s x86s,i tried using schedule static num_threads 4 and noticed that my programme always completes in 11.5s to 11.7s always below 12s at about 320 cpu runs more consistently and uses less resources even if the best run is half a second slower than the rare outlier with hyperthreading,"
"cloudera","mapr"," sandbox has a lot overall, has gone more overall,familiar enough with  overall,clearly cheaper tco overall,","54190274,17965429,51531036,46320326,","mapr has a faster version called mapr streams which implements kafka;i was not wanting to use that for what i was wanting to do but mapr sandbox has a lot of up-to-date items straight out of the box -certainly compared to cloudera,being the first one cloudera definitely has an extra edge in terms of experience and a solid customer base;but mapr has gone more innovative in terms of significant changes to the mapreduce and hdfs components to improve performance,i am not familiar enough with cloudera or mapr to know where to look at,mapr has clearly cheaper tco and technologically superior platform cloudera is a bit more polished hortonworks for pure open source don t read this as free or cheap as most companies end up spending more money on hw to get any value,"
"magento","zen-cart","simpler overall,","2225598,","alternatively zen-cart looks simpler than magento but without some of the flexibility,"
"appium","selenium","better luck in testing apps winappdriver,e2e test with  overall,more mobile overall, is a better in testing apps winappdriver,experienced with  overall, for testing web apps in testing apps winappdriver,","29148367,56595495,41502422,41502422,56327875,54463047,","if you really want to use selenium inside an android app you might have better luck with appium,i run mobile e2e test with appium against selenium grid,you can use appium more mobile oriented than selenium and you can use c# to write your tests,but selenium is more web oriented;why appium is a better choice,i m new to appium but quite experienced with selenium,we recommend using selenium for testing web apps and appium with winappdriver for testing desktop and uwp apps,"
"printf","strlen"," gives length overall, str is larger overall,faster overall, doesn overall, only works on null overall,","40737264,7928739,31654631,39167523,41870235,","this might be causing the printf to not print the string;please do remember that strlen gives length without the 0,you should use printf with the real length of buffer or at least handle the size limit yourself inside the loop;in case strlen str is larger than buffer s size 2,strlen is fast alloca is fast copying the string up to the first n is fast puts is faster than printf but is is most likely far slower than all three operations mentioned before together,also though printf interprets its argument as a format string - what it ends up printing can be a very different length from the string it was given if it inserts values into placeholders;as bub said part of the difference is that strlen doesn t print the string just returns its length,note strlen only works on null terminated strings and will cause undefined behaviour if the string is not null terminated;printf also needs more error checking which can be found on the man page,"
"bison","jison","easier overall,","44829697,","indeed it would be relatively easy to preprocess a bison grammar file but it is easier with jison because you can compute the grammar programmatically and pass it to jison as a json object,"
"h.264","hevc","even better overall,","29082672,","edit h.265 hevc is even better at compression 50 of h.264 size in some cases but support is not yet widespread so stick with h.264 for now,"
"colorbox","fancybox","much faster overall,","9065359,","i ve had to replace lyebox with colorbox which is much faster and seemingly has less problems conflicting with other scripts lightbox fancybox and others wouldn t work for me,"
"dealloc","retaincount","greater overall,","10116389,","if your array s retaincount is greater than 1 at the start of dealloc some other object is retaining it at least temporarily,"
"uibezierpath","uiimage","class easier overall,","9974036,","the uibezierpath class seems easier to use so how could i get that drawing in a uiimage,"
"liblinear","scikit-learn","better overall,","18379716,","also linearsvc class from scikit-learn performs even better than liblinear whch is also surprising considering that it s a wrapper of liblinear,"
"uinavigationcontroller","uitabbarcontroller"," not overall, must be the root overall,more overall, should have a rootviewcontroller overall,","4592648,6902604,1117074,31331192,","uiviewcontroller or uinavigationcontroller which implement their user interface in their own viewdidload;you should create your view controllers in viewdidload of the uitabbarcontroller not in the viewdidappear,embedding a uitabbarcontroller inside a uinavigationcontroller is not supported;apple has a careful hierarchy of container view controllers and a uitabbarcontroller must be the root of its view controller hierarchy,i am writing an app based on uitabbarcontroller which has more than 10 viewcontrollers with corresponding uinavigationcontroller,as you have correctly done the uitabbarcontroller should not push a uiviewcontroller but a uinavigationcontroller;that uinavigationcontroller should have a rootviewcontroller,"
"pyfftw","scipy","better overall,","15165621,","edit more recent scipy does a better job of not always padding to powers of 2 length so is closer in output to the pyfftw case,"
"gulp-ruby-sass","gulp-sass","slower in rich features of,slower in rich features of, which is slower in rich features of,","25985795,29794151,27214893,","i ended up using gulp-ruby-sass that while is a bit slower than gulp-sass is rich with features such as loadpath,but the plugin gulp-ruby-sass slower than gulp-sass,gulp-ruby-sass instead of;gulp-sass which is slower but more feature-rich,"
"stargazer","texreg","more efficient overall,","24193452,","yet from my point of view texreg is more efficient that stargazer and easier to customize than xtable,"
"multicast","packets"," is not overall,inefficient as  in form discovery broadcast, that aren overall, is a lot overall,better in form discovery broadcast, not overall, size is more overall,","33684307,56759796,12773876,28857961,22075284,12347246,48102278,","multicast packets do not usually get past routers;this post s answer explains why multicast is not a good choice,or vice versa but multicast is a better approach because broadcasting is inefficient as packets are sent to all nodes in the network irrespective of whether they are interested in receiving the communication or not,broadcast packets are intended for all hosts on the ethernet to see and multicast packets are intended for all hosts in the address s multicast group to see;normally an ethernet adapter would ignore packets that aren t sent to its ethernet address to the broadcast ethernet address or to a multicast address for which it s configured to receive packets,at the network interface level only those hosts that have joined the multicast group will receive packets associated with the rendezvous traffic;similarly at the network switch level multicast is a lot less resource-intensive,a third possibility would be to send out some form of discovery packets either by broadcast or better by multicast udp,the traditional solution to this is to create an igmp packets yourself which would allow multicast to work via a switch but not on the local machine;unfortunately this needs access to send raw ip packets not tcp or udp and chrome.socket doesn t provide that,i am suspecting that the current nic s mtu speed has to do with something since when i type like this below i get this ifconfig| grep mtu up broadcast running multicast mtu 1500 metric 1 up broadcast multicast mtu 1500 metric 1 up broadcast multicast mtu 1500 metric 1 up loopback running mtu 65536 metric 1 here is the result graph of latency as you can see the f-stack performance decreases when the packets size is more than 1500,"
"median","percentile"," is not yet directly overall,more than 90  overall,more than 75  overall, which implies a greater overall, does not overall,more productive overall,","44072300,51945134,52074105,51980147,23879930,3877986,","for the median there is an mdx function available;percentile is not yet directly supported looks as a good idea to add but we can user the vector mdx+ function applying the percentile object function,important edit i have a suspicion of the reason for the error here data item return code action 1 11202 6 137 0 2 11202 3 137 0 3 11202 5 137 0 4 11202 6 137 0 5 11202 4 137 0 6 11202 10 137 0 7 11202 1 137 0 8 11202 19 137 0 9 11202 16 137 0 10 11202 6 137 0 11 11202 11 137 0 12 11202 20 137 0 13 11202 19 137 0 14 11202 13 137 0 15 11202 14 137 0 16 11202 13 137 0 17 11202 21 137 0 18 11202 10 137 0 19 11202 16 137 0 20 11202 8 137 0 21 11202 15 137 0 22 11202 8 137 0 23 11202 25 137 0 24 11202 17 137 0 25 11202 21 137 0 26 11202 14 137 0 27 11202 15 137 0 28 11202 6 137 0 29 11202 5 137 0 30 11202 11 137 0 31 11202 11 137 0 32 11202 8 137 0 33 11202 12 137 1 38 11202 17 137 1 39 11202 9 137 0 40 11202 7 137 0 41 11202 4 137 0 as you can see before one category of action we have 14 zeros but after one category we have only 4 zeros in this case we calculate 14 zeros before first one then 4 zeros after one. it can be this situation edit three item return code action mask output 11683 77 40 0 na 77 11683 165 40 0 na 68 11683 100 40 0 na 100 11683 84 40 0 na 84 11683 80 40 0 na 80 11683 52 40 0 na 52 11683 1 40 0 na 1 11683 106 40 0 na 106 11683 70 40 0 na 70 11683 88 40 0 na 88 11683 49 40 0 na 49 11683 107 40 0 na 107 11683 25 40 0 na 25 11683 18 40 0 na 18 11683 77 40 0 na 77 11683 70 40 0 na 70 11683 54 40 0 na 54 11683 74 40 0 na 74 11683 115 40 0 na 68 11683 45 40 0 na 45 11683 22 40 0 na 22 11683 95 40 0 na 95 11683 73 40 0 na 73 11683 69 40 0 na 69 11683 70 40 0 1 70 11683 71 40 0 1 71 11683 37 40 0 1 37 11683 20 40 0 1 20 11683 49 40 0 1 49 11683 102 40 0 1 102 11683 113 40 0 1 68 11683 110 40 0 1 110 11683 117 40 0 1 68 11683 42 40 0 1 42 11683 7 40 1 na 7 11683 117 40 1 na 117 11683 117 40 1 na 117 11683 132 40 1 na 132 11683 108 40 1 na 108 11683 68 40 1 na 68 11683 51 40 1 na 51 11683 8 40 1 na 8 11683 63 40 1 na 63 11683 88 40 1 na 88 11683 90 40 1 na 90 11683 92 40 1 na 92 11683 80 40 1 na 80 11683 54 40 1 na 54 11683 5 40 1 na 5 11683 139 40 1 na 139 11683 122 40 1 na 122 11683 68 40 1 na 68 11683 43 40 1 na 43 11683 29 40 1 na 29 11683 21 40 1 na 21 11683 12 40 1 na 12 11683 0 40 1 na 0 11683 43 40 0 1 43 11683 33 40 0 1 33 11683 53 40 0 1 53 11683 101 40 0 1 101 11683 61 40 0 1 61 11683 13 40 0 1 13 11683 51 40 0 1 51 11683 83 40 0 na 83 11683 30 40 0 na 30 11683 59 40 0 na 59 11683 37 40 0 na 37 11683 20 40 0 na 20 11683 9 40 0 na 9 11683 125 40 0 na 68 11683 33 40 0 na 33 median and percentile calculate by these data 10 before 7 after 70 71 37 20 49 102 113 110 117 42 43 33 53 101 61 13 51 median is 53 but all that more than 90 percentile replaced on 68 cause 68 is median for all obs,by group variable action- 0 1 for zero category i want find 75 percentile by return variable and if value is more than 75 percentile it must be replaced on median by zero category,replace the output value if it is greater or equal than the 90 quantile by the median in all zero action rows before the first row in each group;note that the op is talking about to find values that more 90 percentile which implies a greater relation but the expected result had also values replaced where return equals the 90 quantile,notice the median does not appear because we kept its closest neighbor instead weight 3 180;also percentile 75 has two associated mpg values,as one good programmer can be more productive than two average programmers i vaguely remember an old ibm study concluding someone in the top percentile was 27x more productive than median it s useful to see the same programmers doing it both ways,"
"ng-include","ng-view","more processor overall,","29088603,","i could add the dropzone in a ng-include above the area and have it hidden when the template is looking elsewhere but have heard that ng-include uses more processor than ng-view so would prefer to keep it all together,"
"ion","robospice","more overall,","23863829,","i ve used both robospice is more robust but ion is easier on the eyes,"
"myisam","tokudb","slower in slower disk usage, which is an older overall,faster in slower disk usage,","21863823,39187518,8615317,","at the beginning the insert performance of innodb is almost 50 times slower than myisam and tokudb is 40 times slower than myisam,most of other options listed in your screenshot are either special-purpose storage engines which are not suitable for general use memory example federated archive csv and blackhole or are not available in standard builds of mysql most of the rest inifinidb brighthouse and tokudb;the lone exception is myisam which is an older mysql storage engine which does not support transactions is more prone to corruption and generally tends to perform worse than innodb,from my personal use i experienced about 5 - 10 times less disk usage due to tokudb s compression and it s much much faster than myisam or innodb,"
"spsite","spweb","longer overall, objects not overall,","2000182,36038377,","since spsite takes a longer url you may also be able to open the right spweb as well using site.openweb,any subsites within a site collection are represented by spweb objects not spsite objects;the spsite object refers to a site collection,"
"i386","powerpc","faster overall,","21257633,","on i386 the dyld stub is much faster than the powerpc equivalent so we didnâ t bother doing extra work to bypass it,"
"centroid","threshold","less overall,","33594868,","precision parameter if centroid amount of change is less than a threshold delta stop the algorithm,"
"hdfs","hive"," was creating less number overall, does not in table table_test hive-ql,smaller than the  in partition overhead costly, defines much overall,slower than against plain  in partition overhead costly, is better then overall, does not overall, will not in table table_test hive-ql,bigger than one  in partition overhead costly,general overall, is faster in faster application latency,","42825152,19820402,48062215,13128258,9167763,10252104,57739718,37192041,50859078,52522853,54531999,","write output of spark to hdfs and used hive to write to s3;performance was much better as hive was creating less number of part files,if you are not into writing mapreduce jobs you can probably rig something combining hive ctas create table as select and hdfs operations;the underlying file system hdfs does not support updating or even appending files,then actually query against the partition rather than scanning all the rows columns because hive does partition pruning;also hdfs doesn t like files smaller than the hdfs block size 128 mb,another way is to tune number of reducers - sometimes hive defines much less reducers then needed - so you can set it manually to better utilize your cluster;if you have number of queries to run to do your transformation - you can define low replication factor for this temporary data in hdfs,plus the overhead of doing it is extremely costly- hive queries against hbase are on my cluster at least an order of magnitude slower than against plain hdfs files,specifically to your question i can see a few main scenarious when hive is better then rdms;a data is already in the hdfs and we have some other usage of a data there like mr jobs,you can see below pages for more info my answer about msck repair language ddl - apache hive hope the above answer helps you;when you add partitions manually in hdfs and not through hive query statements hive does not capture these directory details automatically and we need to make the hms to be aware of the newly added hdfs directory by running msck repair,as an example if you create an external table called table_test in hive using hive-ql and link the table to file file then deleting table_test from hive will not delete file from hdfs;external table files are accessible to anyone who has access to hdfs file structure and therefore security needs to be managed at the hdfs,assuming that your hive data was split on 3 partitions and each hdfs partition contains a single file that is not bigger than one hdfs block a select with a where on a partition value should trigger a single yarn mapper,you could use an hadoop platform to build a data warehouse by using mapreduce to process data and load it into orc files that will be stored in hdfs and that can be queried by hive,i found hive over hdfs is faster than hive over hbase,"
"hdfs","hive"," spark has bigger latency in faster application latency,faster than in  in faster application latency,general overall, will not in table table_test hive-ql, does not in partition overhead costly, requires a schema overall,slower than reading  in faster application latency,","36875756,55564196,53153596,50739795,27573467,57373441,50606520,","choice is not easy sometimes we use rule of thumb impala is good for queries without joins if all your data stored in hdfs hive spark has bigger latency but joins are reliable all your data stored in hdfs hive spark has bigger latency supports more data sources and has rich non-sql processing capabilities like mllib and graphx,for instance in a streaming application saving data in hdfs or hbase will be much faster than in hive,i understand that it can be done in hive but need to how it can be done in hdfs,as an example if you create an external table called table_test in hive using hive-ql and link the table to file file then deleting table_test from hive will not delete file from hdfs;external table files are accessible to anyone who has access to hdfs file structure and therefore security needs to be managed at the hdfs file folder level,however hadoop based data warehouse hive does not support updates and does not support all the sql statements;hence it is better to read those files from hdfs apply filters transformation and load the result to traditional data warehouse appliance such as netezza to write your queries for cubes,hive cannot extract the schema from the data;in order to read the data from the avro files in hdfs using a hive query hive requires a schema,as spark-sql uses hive serdes to read the data from hdfs it is much slower than reading hdfs directly,"
"bitarray","bitset","more overall,","5005191,","1 might not yet be sparse enough to make this better compared to just a plain bitarray assuming longs storing 64 bits each it doesn t take more than 2 longs to have more than one bitset on average but if the sparsity increases beyond that the space and time savings will show,"
"git-merge","git-rebase","tidier history overall,","37939252,","git-rebase creates a tidier history while git-merge back and forth may create a complex commit graph in the end,"
"jhat","jvisualvm","much more overall,","39790794,","another way to do the same thing is to use jvisualvm which is much more user friendly but also more memory consuming than jhat,"
"abstract-class","subclass"," doesn overall, cannot overall, is not overall, makes more sense overall, cannot overall, will not overall,more overall, are not overall,","19595607,12198850,17961429,13079329,43153159,4819094,190913,5297935,","have the abstract-class use a generic and each subclass defines the generic;an aside your abstract-class doesn t compile,you can make it not abstract or create a subclass that invokes super on your abstract-class;although abstract-class cannot be used to instantiate objects they can be used to create object references because java s approach to run-time polymorphism is implements through the use of superclass references,make sure your concrete subclass is marked as component stereotype and being component scanning by spring;component on the abstract-class is not needed as it cannot be instantiated,however consider complex as being a subclass of number if number were not abstract the add method found in number assuming it s a real number addition algorithm would make no sense for complex numbers;here an abstract-class makes more sense,any subclass must define a body for each abstract method otherwise it too must be declared abstract;because abstract-class cannot be instantiated they must be extended by at least one subclass in order to be utilized,here my abstract-class is baseparameter and i am replacing the stringvalue property with a different value from the subclass not shown;note that although you can rebind to the correct type the form values associated only with the subclass will not be automatically roundtripped because the modelbinder only see the properties on the base class,making abstract-class before i had more than one subclass in mind for them,you can use a subclass or implementing class;interfaces and abstract-class are not concrete classes,"
"moles","moq","much better overall, is not overall,","8000104,9486406,","personally i think moles is much better suited for mocking the unmockable and i prefer a combination of moq for my own interfaces and classes and moq for framework and externalities like file i o gui db connections etc,in other words moq is not going to be any help to you here;for mocking statics i use a tool called moles which is free,"
"flotr","highcharts","better overall,","11477001,","you should be able to do this in flotr which has a better license than highcharts by just throwing what they showed in this blog post on its side,"
"hotswap","jrebel","much more powerful overall,","15466805,","it is worth mentioning that there is a commercial product named jrebel that is much more powerful than jvm hotswap,"
"persistent","transient"," and has no effect overall,less overhead overall,so more overall, column doesn overall, and cannot overall, entity cannot overall,","17826484,8792019,2349847,38064859,53401399,29263171,","second a pojo can be either persistent your hibernate session is aware of the object and will save changes to the database when your transaction gets fflush or closed or transient hibernate doesn t know anything about this object and won t save changes to the database;calling save makes a transient object persistent and has no effect on a pojo that s already persistent,my intuition is that transient properties would incurr less overhead than persistent properties because they do not need to be persistent and read from the database so converting them to persistent properties will likely only make things slightly worse,memcache data lives in memory and isn t persistent so is for more transient data,the persistent column shows the number of objects that currently exist while the transient column shows the number of objects that have existed but have been deallocated;so if you expect an object to be deallocated but the number in the transient column doesn t change then you know the object hasn t been deallocated and you have a problem,the second case the operation is persistent not merge therefore not cascaded;the persistent context sees contacttype as transient and cannot go further with the persistent because one of the dependencies is in transient state,if the persistent context participates in a transaction the in-memory state of the managed entities will get synchronized to the database;a detached transient entity cannot be used with any entitymanager operation that requires a managed instance,"
"oncreate","setcontentview"," alone is faster overall, to inflate your layout overall,nothing more overall,","7104307,36106376,11704785,","if you use startactivity in the new activity s oncreate you will also set the contentview;so with respect to time setcontentview alone is faster since it doesn t start a new activity,activities are run in order of the android lifecycle so oncreate is not called first;it is just the first opportunity you have to call setcontentview to inflate your layout,i can make this happen with the minimal project where oncreate does nothing more than call super and setcontentview,"
"nsoperation","nsprogress","more overall,","19188476,","but i ve had difficulty implementing this it seems as though nsprogress is meant more for long operations that execute all their code on one background thread but have separate sections that make it easy to determine when progress has been made if this is the case then the use of the term suboperation is a bit misleading as it brings to mind the use of nested nsoperation,"
"aptana","dreamweaver","better overall, is more for developers rather overall,","8664117,6187291,","i had previously used aptana 3 for a couple days and may have come to the conclusion that it is better than dreamweaver at actual html css coding,id go with dreamweaver because is has the design view as well as the code view;aptana is more for developers rather than designers and developers,"
"rational-numbers","zero"," is greater than  then in possible positive equal,greater in possible positive equal,","53254932,1780489,","i m trying to prove a that if y a rational-numbers is greater than zero then y is not equal to zero,is there any way in haskell to get the constant that is the largest and smallest possible positive rational-numbers greater than zero that can be represented by doubles,"
"phpstorm","webstorm"," cannot open overall,less functionality overall,","49438130,46015895,","webstorm phpstorm project consists of a single module only web_module type;webstorm cannot open more than one project in single frame,however i do not know if webstorm has a comparable option like that as it has less functionality than phpstorm or even intellij,"
"fastcgi","scgi","simpler overall,","15033764,","you can also use scgi it is a lot simpler than fastcgi,"
"icollection","ilist","more specialized overall,","26339314,","ilist is a more specialized interface that extend icollection,"
"moq","nmock","older overall,","12625646,","this is another reason why i d prefer rhinomocks and moq over nmock nmock uses the older expectation style whereas rhinomocks and moq both support the arrange act assert approach where you specify you expected interactions as assertions at the end of the test like this,"
"amd","umd"," is even better overall, is more overall,","44340865,55669839,","umd is even better supported since umd is compatible with commonjs and amd so that would be a safer bet,the main diferrence between the two is that umd is more abstract and will also work in node.js and amd rather than just browsers,"
"bltoolkit","nhibernate","better solution overall,","3345095,","update for our purposes bltoolkit ended up being a better solution than nhibernate,"
"tlistbox","tlistview"," s itemheight property overall,better choice overall,","5570244,28203811,","while the tlistview does not expose an itemheight property the tcustommultiselectlistcontrol is also the ancestor for the tlistbox which does expose this property;the tlistbox s itemheight property is implemented in tcustomlistbox which descends from tcustommultiselectlistcontrol,depending on your actual needs which you did not explain a multi-column tlistview in vsreport mode may be a better choice than a tlistbox,"
"behat","phpunit","more overall,","14831333,","for general testing phpunit is a more well-established product while behat is focussed on tests written using bdd methodology behaviour driven design,"
"rtp","rtsp","more overall,","41291886,","rtsp is more of a handshake done with the server while rtp is the actual stream coming in once the handshake is done and you start streaming,"
"backgrounding","multitasking","really more overall, application is more overall,","3543267,7343978,","btw multitasking is really more fast switching than backgrounding in ios.,how the device handles backgrounding application is more a function of the ios than the device;even 2nd generation phones running ios 4.0 have multitasking,"
"passenger","thin","less memory overall,","21745505,","memory optimizations - phusion passenger uses less memory than thin and unicorn,"
"grafana","kibana","more overall,more overall,","42785707,42781010,","kibana focuses more on logs and adhoc search while grafana focuses more on creating dashboards for visualizing time series data,grafana is more of visualization only while kibana is for searching the logs is that right,"
"maven-shade-plugin","pom.xml","better overall,","42719511,","this is a lot more pom.xml code but i like it better than the maven-shade-plugin because it does just what i want and nothing more,"
"heidisql","phpmyadmin"," for working with mysql overall,also larger overall, not overall,","1302592,46255006,24653125,","since you re on windows i can highly recommend heidisql over phpmyadmin for working with mysql,phpmyadmin also shows the larger number however i have switched to heidisql since i find it s gui is superior for my local development,but if phpmyadmin hangs i d try to execute your query my mysqlc or another mysql client like heidisql;i use heidisql not phpmyadmin and your query works fine here,"
"fleet","orchestration","no longer actively overall,","45852536,","fleet docs say fleet is no longer actively developed or maintained by coreos and link to container orchestration moving from fleet to kubernetes,"
"fgets","printf","earlier then overall, is outputting a newline overall,","11575102,28444802,","i am getting a problem using printf and fgets as in my code printf is written earlier then fget but it does not run it runs after fgets runs,the reason printf is outputting a newline is that you have one in your string;fgets is not adding a newline --- it is simply reading it from the input as well,"
"iodbc","unixodbc","better luck overall, is more widely overall,","17328798,7552533,","the iodbc included with os x has some things that are listed as deprecated and i ve had better luck with unixodbc in the past,iodbc is lgpl bsd;in practice there is not a lot of difference but i think you ll find unixodbc is more widely used,"
"qtableview","qtablewidget","slightly easier in model beginner qtablemodel, is much more overall, instead is much easier in model beginner qtablemodel,","38782197,9166163,13118389,","the qtablewidget is slightly easier to implement than the qtableview which also needs a qtablemodel as backend but it has less capabilities,the following example is actually using a qtableview + qstandarditemmodel;the reason is because qtablewidget is much more limited since you can only use methods from the widget,qtableview is model based if you don t know what model is then i suggest you read here;use qtablewidget instead is much easier for a beginner and you can add a row just like this,"
"bullet","sfml.net","more overall,","30130528,","i try to make shooter game on c# with sfml.net but i can t imagine how to make an ability to shoot more than 1 bullet because now i have just one null-object of bullet-class and when player presses space key this object gets link to the new bullet,"
"bcrypt-ruby","bundler","newer then overall,","10783218,","if the bundler is newer then version 1.1.2 it will remove bcrypt-ruby 3.0.1-x86-mingw32 automaticaly,"
"keras","lasagne","better overall,worse overall,","39871218,39871218,","if everything is correct why keras perform so much better than lasagne,as a result of this comparison i see that lasagne is performing so much worse than keras that i m starting to doubt about my code,"
"cprofile","timeit","more accurate overall,","24627415,","you should always use the timeit module for time trials it is far more accurate than cprofile here,"
"bcolz","pandas","more overall,","29968370,","i haven t played around with it recently but i think bcolz is more flexible here in terms of mixing dtypes but doesn t give you all the pandas dataframe conveniences obviously,"
"maven-jaxb2-plugin","xjc","newer overall,","25810873,","however it may be that maven-jaxb2-plugin uses a newer version of xjc than you re using with enum,"
"sublimetext","textmate","more overall,much thicker overall,","13445838,24598390,","but i ve found that sublimetext 2 has some features i like more than textmate and also tm v2 which is in beta,macvim sublimetext is much thicker whereas textmate is slicker and sophisticated,"
"fresco","imageview","customizable than  overall,","35290262,","the fresco docs explain how to use drawees and you can see right from the fresco docs xml definition that the fresco docs are much more customizable than imageview,"
"icmp","mtu","greater overall,smaller overall,","17880950,26364809,","what happen when icmp is disabled in an router and when packet size greater than mtu how the router fragments that packet,any device in the path of communication between the sender and receiver whose mtu is smaller than the packet will drop such packets and reply the sender with icmp destination unreachable datagram too big message containing the device s mtu,"
"cargo","maven","more general overall,","10936609,","you could also use apache cargo which is more general than the tomcat maven plugin,"
"dateadd","strtotime","better overall,","3750603,","that said the date class is much better in php5.3 -- it even has dateadd subtract functions which would also solve your problem much better than strtotime .,"
".htaccess","httpd.conf"," to establish basic authentication overall, is slower harder overall,faster overall, that is better overall,better results overall, and set allowoverride overall,","24071044,40903015,9329982,5848081,21095965,10264503,","first off using .htaccess is a bit slower and requires that you have set the allowoverride directive accordingly;it is recommended that you instead use httpd.conf to establish basic authentication,.htaccess is slower harder to manage and potentially less secure;if you have access to the httpd.conf then placing rules there can be easier to manage in one place faster allowoverrides none means that the server does not look in the current directory and any parent directories for an override file to parse and follow and since .htaccess files are not present in the website directory they cannot be edited and if created will be ignored,using apaches httpd.conf is faster since accessing the .htaccess adds a small overheadâ apache checks every directoryâ and parent directoryâ for the file and it will be loaded for every request,this goes in your root .htaccess but if you have access to;httpd.conf that is better,instead of mucking with httpd.conf you might get better results with .htaccess,i believe the issue is that allowoverride is set to none for the web directory you are using which means the .htaccess will not be processed;you will have to edit httpd.conf and set allowoverride all,"
"asihttprequest","cfnetwork","better overall,","6836365,","you can use asihttprequest which is quite better than cfnetwork,"
"apache-fop","xhtml","stronger syntax overall,","29554491,","xhtml has a stronger syntax than html and is easier to convert to apache-fop this way,"
"crf++","crfsuite","faster overall,","17912109,","crfsuite is faster than crf++ and it can deal with a huge training data,"
"mathjax","mathml","matters further overall,faster in webkit content jsmath, and takes better advantage in webkit content jsmath,configure  to use in webkit content jsmath,","41676041,29350954,3497719,13435113,","in addition the need for polyfills for mathml complicates matters further as they may handle the interaction differently as can be seen in the sample below when using the button to load mathjax,if you are targeting android you can use firefox for android by developing a webapp that has native mathml support and because of it will be much faster than mathjax or other javascript solution,it follows pretty much the same principles as jsmath but adds support for mathml and takes better advantage of modern browsers webfonts and javascript technology;while davide will answer questions about jsmath no future development will likely occur on no future development and no future development users are strongly encouraged to move to mathjax,additionally mathjax has better mathml support than webkit and you can always configure mathjax to use the native mathml support if you want -- say when you know your content should render fine in webkits native support,"
"arraylist","treeset"," does not overall, wouldn overall, would be a better in arraylist later mind,more effective overall, is better overall, doesn overall, is better in arraylist later mind,","39432088,23036819,3607684,38555784,24371582,24024067,7919037,","it would be easiest to use a sortedset arraylist does not have sorting built in by default;treeset is the concrete implementation and then pass a custom comparator which just delegates to the date object,then we could use treeset to sort an array faster than the method made specifically to sort it the saving gotten from not having to insert into the arraylist is fairly small;if this were consistently true java developers would simply replace that method with treeset wouldn t they,so if you re mainly retrieving and don t sort often arraylist is the better choice;if you sort often but dont retrieve that much treeset would be a better choice,so arraylist and sort is more effective than treeset,if you only sort once then the arraylist is an obvious winner;the treeset is better if you add or remove items often as sorting a list again and again would be slow,if you don t add an comparator the treeset implicitly expects the arraylist to realize the comparable-interface which it does not do;the treeset doesn t know how to sort the arraylists because there is no natural ordering for lists,now let s say you ve implemented your list with an arraylist;later you change your mind deciding that a treeset is better,"
"bower","jspm","much larger overall,","26267036,","well jspm is much larger and ambitious project than bower,"
"ember.js","sproutcore"," is focused on more overall,much better overall,","9433251,10828935,","from the technical standpoint ember.js is focused on more modularized code and so called semantic-templates for views;sproutcore is more monolithic,i have seen a lot of post where they are claiming that ember.js is much better than sproutcore but looking at the differences i have seen that the widget support is not there in ember.js,"
"bufferedreader","fileinputstream","faster overall,","23964531,","see the referred post for an example that reads a single file in parallel with fileinputstream which should be significantly faster than using bufferedreader according to these benchmarks,"
"blockui","jqmodal","higher overall,","6444837,","one item to note be aware the default z-index for jqmodal is below default for blockui so using them together requires you set blockui basez option higher than jqmodal s default 3000.,"
"gedit","kate","better overall,better overall,","25925631,43830296,","if vim seems too much by the moment i think that kate is better than gedit,you can customize kate and i think you will like it better than gedit,"
"loops","while-loop"," is better overall, is more overall, works almost overall,more overall,","50509871,10058020,20953857,32678931,","also a user-prompt loops is better implemented with a while-loop to avoid adding a frame to your call-stack everytime the user enters a wrong input,you use a for-loop for an unknown number of loops iterations;this is not wrong but a while-loop is more appropriate,if the condition in the while-loop is not met loops never executes;a do-while loops works almost the same way except the condition is evaluated after execution of loops hence loops always executes at least once,if you re just going to loops through point use a for-loop as while-loop is more prone to dropping into an endless loops if you forgot to increment counter or doing wrongly or do it in multiple places,"
"cobertura","jacoco","recommend  as  overall,easier than  overall, and are very overall, generated reports more overall,","33934128,44594037,42279496,37294608,","so you would have to configure your big build to generate a coverage report with the engine you want to use i would recommend jacoco as jacoco is supported out of the box by the sonar java plugin if you want to use another engine clover cobertura etc.. you would have to install a dedicated plugin,similarly you might find using jacoco easier than cobertura,after that and more issues with cobertura our moved to jacoco and are very happy with the result,it provides additional features compared to cobertura;i tried jacoco and other tools and found jacoco generated reports more comprehensible and useful compared to other,"
"ng-controller","ng-view","more overall,","25096138,","unlike the ng-view it s possible that there are more than one ng-controller in the same page,"
"jquery","scriptaculous","more configurable overall, is no more overall, and putting an img overall, is better; overall,better overall,","5327986,28526040,1394648,669924,2762721,","i ended up using scriptaculous for most of my animations simply because it provides smoother animations and more configurable than what jquery ui provides,so you have to use jquery in noconflict mode;scriptaculous is no more being updated,by the way the page is using lightview wich works with prototype and scriptaculous but not jquery;you could try the jquery ui dialog plugin to stick with jquery and putting an img inside the content to show,jquery is better;jquery has noconflict method that allows you to easy migrate having both prototype scriptaculous and jquery library included,i also kind of get the impression that scriptaculous drag and drop is better than jquery ui drag and drop based on the online demos,"
"heroku","openshift","powerful than the  overall, s was easier overall,","15770527,30419422,","having used both i can tell you that an alternative s a much more polished platform the servers are about 4 faster you can run as many apps as you want and the heroku toolbelt is much more powerful than the openshift s client tools,oth heroku and openshift are within amazon s space but both heroku and openshift customer interface is different;i thought heroku s was easier for beginners to get started but as i mentioned there s no free lunch on the database side of things,"
"cryptlib","openssl"," is more overall,","28573509,","openssl is more similar to peter gutmann s cryptlib and gnu s gnutls,"
"vao","vbo","more overall,more overall, but not overall,","36241764,15438605,26559063,","i have issue with drawing big vbo with vao because the vbo has more than 65536 vertices and my vao is just uint that has maximum 65536,is it possible to use the same vbo with more than one vao like this,in this case the vbo is automatically unbound from the currently bound vao but not from other vaos that are not currently bound;if other vaos have references to the vbo the vbo will stay alive until all those bindings are broken or the vaos deleted,"
"memcpy","memset"," routine is faster overall, code is more overall,slower in slower operations understandable,faster in slower operations understandable,","33712440,51333872,43168595,21631430,","some compilers including gcc are able to optimize quite well with gcc -o2 at least calls to standard functions memcpy and to memset which as my former colleague pascal cuoq commented may be inlined to efficient assignment machine code;sometimes gcc is even able to optimize some assignment to some structures as calls to memcpy sometimes calling an efficient memcpy routine is faster for large enough struct including for structure assignment,also since zero initialization doesn t work on heap-allocated structs there s something to be said for consistency of always using memset;similar goes for situations when you might want to copy a few adjacent structs part of an array - again if you always use memcpy code is more consistent,the problem is that memcpy is only slighly slower than memset when i expect it to be about two times slower since it operations on twice the memory,it is understandable that memset is faster than memcpy,"
"md5","murmurhash","way faster overall,faster overall,","17963853,30806419,","but as that reference points out murmurhash is way faster than md5 and sha functions although it doesn t do a direct comparison to the object.gethashcode method i mentioned above,murmurhash has 64 and 128-bit versions so you can experiment between the two and it s faster than md5 largely owing to md5 being a cryptographic hash function whereas murmur doesn t have the added expense complexity of being cryptographically secure i m assuming that you re not concerned about anybody attempting to intentionally generate hash collisions or anything like that,"
"tinyxml","xerces","heavier overall,","3172803,","for more complex xml reading writing you better check xerces which is heavier than tinyxml,"
"beautifulsoup","elementtree","more in forgiving well-formed things, does not in forgiving well-formed things, is probably better overall,","22599300,30682089,4586678,","beautifulsoup is more forgiving in terms of well-formed xml structure see i had to edit the xml a bit to make things work for elementtree and it s actually much easier to work with,for xml documents it may be that the elementtree offered by lxml is more productive;it supports xpath queries for example while beautifulsoup does not,we do something sort of like this with rss feed using python -- we use elementtree since rss is usually guaranteed to be well-formed;beautifulsoup is probably better suited for parsing html,"
"biicode","conan","more overall,","36101735,","conan uses a more direct and easier approach to library dependencies management than biicode supporting both binary packages as building from source,"
"keras","theano","slower overall,general overall, not mxnet vs overall,","42162709,34812631,36295821,","should i be surprised that the keras theano backend is about 18x - 19x slower than the keras tensorflow backend,i would recommend using theano binary lstm link or keras tutorial for this because their are fairly simple to understand and are well documented,theano has low-level primitives to build machine learning models and on itself does not define any layers or optimizers and you would usually use it with some deep-learning library such as lasagne or keras while mxnet is higher level;so fare comparison would be mxnet vs keras not mxnet vs theano,"
"fullcalendar","jquery-week-calendar","more overall,","4660742,","in some ways the jquery-week-calendar has more features at the moment but fullcalendar has a bigger following and is closing the gap on features,"
"rcharts","shiny","sankey diagram with  overall,better overall,","48642011,24273444,","further reading here sankey diagram with rcharts into shiny application,possibly part of the reason of my problems is that the original code from ramnath uses rmaps while i m using rcharts also developed by ramnath as it is more developed better integrated with shiny and of course includes leaflet,"
"hash","whirlpool","more secure overall,","26016244,","if you want longer hash lets call more secure you should use something like sha512 whirlpool etc,"
"touchesbegan","touchesmoved","select it with  overall,greater overall,","47901650,41318401,","gameviewcontroller gamescene as you can see with touchesbegan we can detect the touched world and select it with touchesmoved we are able to move the moveable node to the correct position and to detect the nearest visible world and with touchesended we can create the smooth scrolling to automatic move to the current nearest visible voice after a touch,for some reason when the duration of touchesbegan is greater than the duration of touchesmoved my player will complete the touchesmoved actions first even though touchesbegan had to come first and then jump to a different x-position and then move with duration 4 to complete the touchesbegan,"
"nhibernate","queryover","possible with  overall,much more robust overall,","52728648,16040119,","all orders sorted by customer s spending also possible with queryover sample from nhibernate docs,right now queryover is much more robust in the nhibernate community mainly because it is based on criteria queries which have been around for quite some time,"
"fgets","strcpy","more overall,","46337879,","fgets of course does not process escape sequences any more than strcpy would,"
"skip-lists","tree"," but has much overall, was not anymore overall, is less overall,better in balanced search better,better concurrent overall, but is conceptually more in balanced search better, is far more localized only overall,","234967,34004017,23906397,576296,7989690,7697557,260277,","nd i used a skip-lists to implement my std map;the reason i went with my std map is because it is a simple algorithm which is very close to the performance of a balanced tree but has much simpler iteration capabilities,in 1989 or 1990 as a student i implemented both it was not a good implementation of the skip-lists i must admit i was a novice in that time;however the avl tree was not anymore difficult to implement,another idea would be a skip-lists where the skips are annotated with the number of elements they skip;but this implementation is not trivial since you have to update the skip length of each skip above an element that is inserted or deleted so adjusting a binary search tree is less hassle imho,so as you can see the random binary search tree was rather a lot better than the skip-lists,in a single thread world is different you can use a sorted set a binary tree or your custom data structure that would perform better than concurrent skip-lists,i do not know of a .net implementation but a data structure that might work for you is an indexible skip-lists;a .net implementation but a data structure that might work for you has similar o lg n performance like a balanced binary tree but is conceptually more like a linked list,the rebalance operation can affect large portions of the tree which would require a mutex lock on many of the tree nodes;inserting a node into a skip-lists is far more localized only nodes directly linked to the affected node need to be locked,"
"aptana","webstorm","better overall,","1627531,","if you re focussing on pure client code then i find webstorm to be an excellent javascript html ide - even better than aptana,"
"fifo","queue","more overall,compatible with   in amazon sqs sns,compatible with   in amazon sqs sns,compatible with   overall, grows larger overall, cannot overall, see your service overall,compatible with   in amazon sqs sns, not so overall,","33907669,51507110,48957574,53085632,45905690,55103774,51505256,55811268,5841194,","i have to develop a better queue that works more efficiently than the fifo queue,according to the aws article tutorial subscribing an amazon sqs queue to an amazon sns topic see the following note amazon sns isn t currently compatible with fifo queue,note amazon sns isn t currently compatible with fifo queue,as seen on the screenshot your query is fifo and the documentation says amazon sns isn t currently compatible with fifo queue,it sets the udp socket to non-blocking mode then loops around select to recvfrom any incoming udp packets and append in a thread-safe manner both the incoming packet s data and its source-address port info to a fifo queue for other less time-sensitive threads to take out and work on later;that way even if a packet or series or packets takes a relatively long time to process the result won t be that packets get dropped although it might temporarily increase ram usage as the fifo grows larger,my mistake was to think that the queue proxy is not iterable because it is the proxy;however the proxy works fine in place of the actual queue if put get are used also a fifo queue cannot be closed and does not have a natural end which i found annoying because it means one has to send around special end of queue entries but not too many of them to not unintentionally block the queue,the following features of aws services aren t currently compatible with fifo queue auto scaling lifecycle hooks aws iot rule actions aws lambda dead letter queue for information about compatibility of other services with fifo queue see your service documentation,important amazon sns isn t currently compatible with fifo queue,note that lifo queue are a good fit for singly linked lists;fifo fifo not so,"
"ctrlp","fuzzyfinder","more powerful overall,","36865589,","if you re looking for a ctrlp equivalent webstorm has a fuzzyfinder that s actually much more powerful than ctrlp,"
"libsvm","scikit-learn","much more overall,","39161839,","the libsvm results seems much more stable but scikit-learn results have some drastic fluctuation,"
"do.call","tapply","slower overall,","33806404,","or is the do.call with by just typically much slower than tapply,"
"ntfs","xfs","faster overall,","23875193,","extfs4 or xfs are between 25 and 40 faster than ntfs or refs depending on the optimization,"
"relaxng","schematron","more overall,","35346589,","other options to check are relaxng which is more flexible and powerful than xml schema or schematron which allows for exactly this sort of validation that needs to go deeper than structure and simple type-checks,"
"phpthumb","wideimage","more modern overall,","65079,","wideimage is the more modern php5 approach while phpthumb has much more features,"
"strdup","strlen"," actually copy the string overall,cleaner overall, would be better here overall,more convenient overall,","8118370,43080950,33667116,39599359,","in this case strdup is better;and you don strlen actually copy the string returned by strtok you copy the pointer,as i noted in comments however if you re willing to rely on posix s strdup then that s cleaner than strlen + malloc + and has the same semantics you take responsibility for freeing the memory allocated for the copy,c strings are supposed to be nul-terminated and strlen doesn t account for this;perhaps using strdup would be better here,additionally if you have strdup then it is much more convenient than strlen + malloc + strcpy with identical result including the same obligation to free the allocated storage when you no longer need it,"
"touchesbegan","touchesended"," but is more overall,more overall,","51488551,29729778,","you have to replace the touchesbegan with a tapgesture or replace the uilongpressgesturerecognizer with touchesended but is more precise the first option,if the value of touchesended is more than 3 seconds beyond the current time and the time of touchesbegan is still before touchesended then it s time to show your hint,"
"bash","shellcheck","more explicit overall,","29157663,","shellcheck tends to be more explicit than bash,"
"gprof","valgrind","more information overall,","11952551,","another important difference is that valgrind can report a lot more information than gprof does but that s not specifically related to using it,"
"irrlicht","ogre3d","more mature overall,","4617930,","ogre3d seems to be more mature than irrlicht judging by the amount of applications and games that is,"
"fuzzy","stemming"," is done when indexing overall,","15522125,","stemming gives better performance than fuzzy search because stemming is done when indexing and the actual search can be performed as exact match,"
"nstextstorage","nstextview","process much simpler overall,","29032101,","the nstextview process is much simpler as you only need to attach the nstextstorage object,"
"magento","opencart","larger extensions overall,","22501933,","magento is way ahead and has larger extensions than opencart,"
"clutter","gtk"," which is much more overall, which is much more overall,","10371167,10371167,","use clutter which is much more suited to animation and integrates with gtk,here s an example of two non-intensive proof-of-concept games written with clutter with links to use clutter which is much more suited to animation and integrates with gtk source code,"
"cublas","magma","higher performance overall,","14543887,","magma routine magma_gemm has higher performance than cublas in some cases,"
"font-size","margin","more overall, gets larger overall,","42619643,1003562,","if you don t want to give padding or margin at all then set the line-height property of headings more than their font-size,if that still gives you trouble remove the margin and try a position relative on the child with a top 10px;finally try adding a overflow hidden to your parent element to force your parent element to not budge when the font-size gets larger,"
"ganglia","metrics","worse overall,","17813524,","however i wonder if this is the best way of doing this if you want to observe specific behaviour you might be better off isolating a specific input file and debugging against a local pseudo-cluster and if you want system metrics you could do worse than give ganglia a lookm as it is pretty mauch already built into hadoop,"
"bit-shift","xor","operators more overall,","22549562,","the official tutorial on bitwise and bit-shift operators has more information about other related operators and xor left shift right shift,"
"nlopt","quadprog","more powerful solver overall,","26924683,","for quadprog ipopt would be a much more powerful solver than nlopt,"
"expression-blend","silverlight","much more overall,","3743941,","expression-blend has much more advanced wpf silverlight visual designer than visual studio including wysiwyg animation editor etc,"
"ireport","subreports","better overall,","1936127,","if your report is complicated and have many subreports and subdatasets its better to go this approach since ireport will make it quicker and easier for you to design and maintain later,"
"webchromeclient","webviewclient","easier overall,lesser time overall, doesn overall,","9592792,12154530,40937708,","to sum up the webchromeclient is easier to use but the webviewclient allows you more configuration,my observation is that you get the webpage title using webchromeclient in lesser time than using webviewclient,instead of that you can use webchromeclient and set progressbar as below;webviewclient doesn t have onprogresschanged method,"
"md5","pbkdf2","more computation than  overall,","16044003,","so we can use pbkdf2 with sha512 to gain a very significant advantage over the phpass algorithm mainly because sha512 is a harder algorithm with more computation than md5,"
"cdr","cons","simpler overall, is part overall, to make a list overall,","20574442,25058826,36499752,","as a practical matter a cons is simpler than a list so you can get the value with a straight cdr rather than the conceptually more complex cadr the car of the cdr,in this case cdr would not be part of tree structure;note that while the car 1b component of each such cons is part of the,thus a chain of pairs where the last cdr is not matches this;append is a procedure that uses cons to make a list with all the elements of the argument lists left to right,"
"justcode","resharper","more overall,","5837140,","together resharper was finding more than justcode,"
"ocp","principles","easier overall,","18428363,","i think adhering to the dip principles makes it easier to comply with the ocp principles,"
"dotnetnuke","umbraco"," is a better overall,better overall,more lean overall,","14779513,7608578,4841806,","if your a die hard programmer needing to integrate custom code - umbraco might be a better fit;if you want to leverage a larger community and add code to the base dotnetnuke is a better fit,consider umbraco its architecture is definitely better than dotnetnuke s,i also had good result with umbraco cms it s a bit more lean than dotnetnuke but not as user friendly and definitely not as popular,"
"thunk","trampolines","more overall,","42914976,","trampolines never runs more than one thunk at a time so if you break,"
"distance","pdist"," is much faster then overall, does support the option overall, is greater overall,","36651979,22082136,53654660,","also note that in my code above manually calculating the euclidean distance is much faster then calling pdist,as you can read in the docs you have some options but haverside distance is not within the list of supported metrics;matlab pdist does support the option though see here,i have an array i have calculated its cosine distance with pdist and got the distance array now i want to get only those pairs which s cosine distance is greater than 0.4 and less than 0.49,"
"kurtosis","variance","more in measure infrequent image,more in measure infrequent image,","14691778,29967130,","for this measure higher kurtosis means more of the variance is the result of infrequent extreme deviations as opposed to frequent modestly sized deviations,higher kurtosis means more of the variance in the image is the result of infrequent extreme deviations as opposed to frequent modestly sized deviations,"
"osx","yosemite","more overall,","30706316,","it s true that the yosemite compatibility exists but i ve found that earlier mac machines does not have the propper hardware and you experience some lacks in memory and graphics when upgrading more than one osx,"
"hibernate","jdbi"," is much lower overall,simpler overall,","6258793,4630832,","jdbi is much lower level than things like hibernate or jpa,other useful pieces would be jdbi for database access much simpler than hibernate or other full orms async-httpclient for doing calls to other web services,"
"buildout","plone","older overall,","12998588,","i ve even tried to pin archetypes and atcontentypes so that the buildout uses older versions example those used in plone 4.2 by adding the following to my pinnedversions.cfg file which gets called after everything else,"
"java.util.calendar","java.util.date","slower overall, is much worse overall, which is more overall, is not overall,","4044107,50522974,8687084,35556459,","construct java.util.calendar is comparative slower than java.util.date,next you add the typeconverters annotation to the appdatabase class so that room can use the converter that you ve defined for each entity and dao in that appdatabase appdatabase.java a side note java.util.date is considered to be badly designed and java.util.calendar is much worse,additionally using java.util.date is not good practice at all since most of java.util.date method are deprecated so i suggest you to use java.util.calendar which is more flexible,java.sql.preparedstatement.setdate int java.util.date java.util.calendar is not applicable actual and formal argument lists differ in length;java.util.date cannot be converted to java.util.date by method invocation conversion,"
"json-rpc","xml-rpc","more overall,","14355078,","json-rpc is more pythonic than xml-rpc or shudder soap,"
"moq","typemock","better overall,","19092194,","or would a tool like typemock be better suited than moq in this case,"
"avfoundation","qtkit","more overall,","36523854,","since apple has decided that qtkit is no more and avfoundation appears to only support prores 4444 and 422 flavors how are developers supposed to write modern software that can handle 4444xq 422lt and 422 proxy,"
"cfstring","nsstring","better functionality overall,","5868039,","cfstring has some slightly better functionality than nsstring but is a bit harder to use,"
"alloca","malloc","smaller resource than the  overall,worse than  overall,present with  overall,recommend  over  overall,","7950970,56393506,53981950,10019281,","while alloca gives you automatic de-allocation on function exit the stack is usually a smaller resource than the malloc heap and if you exhaust the heap the stack gives you back null,alloca is worse than malloc because alloca causes unpredictable stack usage and can lead to a stack overflow which won t be detected at all or which will only be detected by a program crash,i don t think that anybody has mentioned this but alloca also has some serious security issues not necessarily present with malloc though these issues also arise with any stack based arrays dynamic or not,i would recommend alloca over malloc if you would like the data allocated on the stack and if the stack s available but malloc will work in most situations,"
"nsobject","nsstring","higher level overall,","30000679,","nsstring has a higher level it also inherited form nsobject const on it should have no effect in fact not the same meaning about the no effect on immutable object,"
"pusher","websocket","technology bidirectional more info overall,","42486232,","the underlying websocket technology is bidirectional more info here but pusher s pub sub model is unidirectional,"
"silex","slim","uglier code overall,bigger community overall,","35993770,33462609,","i think silex produces a bit uglier code than slim but i m afraid to choose slim because it s changing constantly and i will be stuck with the current version if it breaks backward compatibility,silex seems to have a bigger community than slim maybe it s just my point of view,"
"ppm","tga","more common overall,","21354215,","you could use the tga format which is more common than ppm and allows true grayscale images,"
"puremvc","robotlegs","faster overall,","20356474,","puremvc is generally much faster than robotlegs in terms of performance,"
"text-align","width","less than the  overall,less overall, does not overall,wider overall,","14027386,9755894,16871754,30417308,","when the total width of the inline-level boxes on a line is less than the width of the line box containing inline-level boxes that are not inline boxes such as replaced inline-level elements inline-block elements and inline-table elements inline-level boxes that are not inline boxes such as replaced inline-level elements inline-block elements and inline-table elements horizontal distribution within the line box is determined by the text-align property,the has text-align center so the will be centred if the size of the is less than the width of the,your width will have no effect as you can not set the width of an a tag without changing it s display properities;this is why text-align does not appear to be working,but i don t need a perfect width even if the width is wider than the longest word it is ok as i will text-align center to enhance visibility,"
"lift","scalatra","more powerful overall,","7510119,","it fills the gap between scalatra and lift more powerful than scalatra and easier to use than lift,"
"fadein","hide","effect much softer in fadeing better softer, isn overall,better than show  in fadeing better softer, s still overall,","43613881,8404631,56920246,25227387,","note the show hide effect is much softer because of the commented fadein fadeout declarations that were triggering console errors probably because jsfiddle uses jquery slim now,jquery adds fadein hide and other effects to it s effect s queue;so it s calling .load instantly just after it sends the hide to the effect s queue and the hide isn t completed,but for fadeing effect using fadein fadeout maybe looks better than show hide,he jquery fadein won t work because of the visibility;so for the latest bootstrap hide is no longer is use but the latest bootstrap hide s still in the min.css file,"
"algol","apl","weird than  overall,","3725905,","as far as the syntax goes i guess the syntax goes s a bit different from the more classic syntax of the algol family languages but the syntax goes s no more and possibly less weird than apl,"
"cube","skybox","more polygons overall,","30060980,","i recently switched from unity to unreal and the first thing i wanted to do is create a skybox but the only thing it seems to support is a skydome which has far more polygons than a cube and the generated .dds cubemap files are far too huge to be used in webgl projects,"
"calloc","free","harder overall, and get this automatically overall, is more overall,","17617309,14389869,50380578,","malloc free is harder because thereâ s also calloc and realloc,no need to initialise to null before malloc but any variable pointers that you later free should be set to null and only free if not null;also initialise pointer values in your structs to null easier to use calloc and get this automatically so your guards work properly,sometimes calloc is more preferable call in c as it protects on some platforms against overflow and remember to free,"
"oncreate","ondestroy","more overall, here is a diagram overall,general overall,","13288579,51027893,53578538,","otherwise if oncreate is being called more than ondestroy receiver is registered multiple time and the app mis-behaves,let me help you with a simple manner - lets assume we are going to use whatsapp app when you click on the app the methods that will be called are oncreate onstart onready lets now assume you pressed home button and the app got minimized the methods that will be called are onresume onpause onstop and now lets assume now you are going back to the minized whatsapp app methods that will be called are onrestart onstart now lets assume you just closed the app and removed it from task manager the methods that will be called are onstop ondestroy here is a diagram to get the sequence,since we have multiple schemas for various data objects i want to make my code cleaner by using something like this activities and fragments can inherit from realmowner and implement the mandatory realmcontainer using lifecyclerealmcontainer and passing their lifecycle into it then - the initrealm would be called with oncreate and closerealm would be called with ondestroy,"
"notepad++","vim","more overall,","26526628,","lately i have used notepad++ a bit more than vim,"
"elgg","wordpress","more complex overall,","31861085,","it is working only for simple .php files but wordpress is more complex and after long period of loading it show elgg nothing found site loaded,"
"jruby","rubinius","slower overall, also has better support overall,","31906922,22591043,","again we see the pattern of it getting faster during the first two runs after which it settles somewhere in between slightly faster than yarv and the other jruby and slightly slower than rubinius,rubinius is preferred if you plan to work on a multi-threaded architecture like under puma;jruby also has better support for threads and also has the rich library portfolio of the java community,"
"contourf","imshow","more sense overall,","20146989,","as an example of the second option i ll use imshow here because it makes more sense than contourf for random data but contourf would have identical usage other than the interpolation option.,"
"memmove","strcpy","faster overall,efficient than  overall,","12429831,56585945,","marcus yes memmove is faster than strcpy and faster than python but why,it is usually more efficient than strcpy which must scan the data it copies or memmove which must take precautions to handle overlapping inputs,"
"jlabel","jtable","more overall,","22259109,","why don t you do a simple test that tries to read and display an image in a jlabel first since using a jtable is more complicated than using a jlabel,"
"visualvm","yourkit","general overall,easier than  overall,","3421762,36976937,","but by all means if you have the budget i d recommend getting yourkit as the budget provides more sophisticated features and better metrics that visualvm don t have,i prefer yourkit as i find yourkit easier than visualvm,"
"tabcontrol","tabitem"," is not actually overall,more than 1  overall,more overall, which corrects this issue overall,","4386251,52113045,17307459,13276564,","you can download snoop or other tool that shows you visual tree and see there that content of tabitem is not actually the visual child of this tabitem but the visual child of tabcontrol;so tabitem is the logical child of tabitem s content and tabcontrol is the visual child of tabitem s content,the tabcontrol contains more than 1 tabitem,edit i can only reproduce this error when i have a tabcontrol with more than one tabitem,this case had to be fixed in the tabcontrol itself because tests showed when the fake visility is set for the first time the tabitem does not have access to it s tabcontrol yet;because of that i also used a attached property for the tabcontrol which corrects this issue,"
"jmeter","neoload","better overall,","26055441,","for load testing jmeter is better option but you have quite enough funding then go for loadrunner neoload rational performance tester and for cloud try blazemeter,"
"icarousel","uicollectionview","better overall,","14078238,","my only problem is that is seems that using uicollectionview is a better approach and i have read some seemingly off-hand comments about icarousel being a poor approach,"
"cobertura","maven","better overall,","1207519,","i find cobertura and emma to be largely interchangeable in terms of coverage checking just that cobertura has a better maven plugin and emma has a better eclipse plugin in my opinion,"
"celery","kombu"," is not overall,lower level overall,","57354683,16328341,","to communicate with rabbitmq kombu uses either py-amqp or if installed librabbitmq;celery is not made to be arbitrary mq consumer,i did some reading on kombu and it seems to be able to do what i m thinking of although at a much lower level than celery,"
"avalonedit","scintilla","better overall,","8711597,","personally i felt that scintilla performs better than avalonedit,"
"tlistview","tstringgrid","more limited overall,","8731943,","i have just spent a lot of time working with tlistview in view style mode vsreport and i find it is even more limited than tstringgrid for example it provides no in-place edit support,"
"django","turbogears","easier overall,more overall,","3144382,1442201,","compare the very active django tag on stack overflow with that of pylons or turbogears though and i d argue getting started is simply easier with django irrespective of anything to do with code,web2py may be young but the mailing list has 2000 messages month which is similar to django and far more than turbogears,"
"dojo","dwr","better choice overall, was much cleaner and more overall,","4623300,4623300,","they merged with dojo which in my opinion is a better choice than dwr,dwr sounded good but in practice it was buggy and difficult to use;dojo was much cleaner and more modular,"
"jar","manifest.mf","way better overall, are not overall, which contains org.slf4j.logger class in slf4j org.slf4j.logger project, file is little more in slf4j org.slf4j.logger project, file uses a space overall,","47364708,44547476,47298988,28758271,26736598,","i think that s why i can t use to launch my project this way works but jar way is better because everything including the main_class is described in manifest.mf,your module.xml which contains spring jar is named as com.sits.grails.hence your manifest.mf should not include org.springframework as its not defined as a separate module;the module names specified in manifest.mf are not package names but module names as define in the module.xml in jboss,your slf4j jar which contains org.slf4j.logger class is missing in the manifest.mf file of the project which is trying to load org.slf4j.logger class,a jar file is little more than just a zip of all classes and resource files;you need to add details into meta-inf manifest.mf file so that the class containing the main method to be executed is know,yes gagravarr i discovered that the jar command doesn t use the -classpath argument although you don t get an error if you include it in the command line;more importantly the class-path section of the manifest.mf file uses a space to separate the items instead of the semi-colon,"
"mallet","weka","smarter overall, has more documentation overall,","28362098,8350591,","i have tried weka too but mallet is smarter than weka on this aspect,the problem with mallet is that the training uses gb of memory and it can take hours if you have large training sets;weka has more documentation but most of it makes no sense,"
"markerclusterer","markermanager","more involved overall,","36861965,","the markermanager has a more involved setup than the markerclusterer but it does allow for more customization of what and where it displays,"
"jlayeredpane","jscrollpane","smaller overall,","4901460,","the problem is that with the code below the jlayeredpane always expands to fit the size of the jscrollpane and in the event that the jscrollpane is smaller than the jlayeredpane it does not provide the scrolling ability,"
"actioncontroller","helpers"," is not directly accessible directly overall,more often overall, is not overall,","3844173,16974509,7431650,","one thing to note since the helpers uses the predispatch hook i believe it will get called on every action even an internal forward;the actioncontroller is not directly accessible directly from a front-controller plugin,actionlink helpers is more often used to generate url properly based on the actioncontroller route values etc and anchor tag based on htmlattributes etc,i think the issue might be that the url_for method that you re used to calling in your views and defined on actionview as a helpers is not the same url_for method that gets called when you re in a controller;actioncontroller base has its own similar but not the same method called url_for method,"
"adb","monkeyrunner"," this is even easier overall,more reliable overall,","32380012,12630645,","for example using a jython script using monkeyrunner you can input touch events like this slightly modified from their documentation;with adb this is even easier adb shell input tap x y,adb is more reliable since you get the feedback immediately unlike monkeyrunner which does not provide reliable exceptions in case of any failure in triggering the event,"
"compiled-language","dynamic-languages","better overall,","3548478,","because you can determine what code in a compiled-language is likely to do better than a dynamic-languages it has been determined more error free to use a compiled perspective for more things so as you can see with the availability of offset-management and lookup compiled behavior given to you in core perl there is no reason to mess with the symbol table if you don t have to,"
"branch","squash","general in merge merging works,issue much simpler in merge merging works,","49200905,13082096,","while squash merge is simpler merging done right provides a healthier history and more information for git to work from;you ll get more out of git if you understand how branch and merging works,if you will have only yours commits on your branch issue is much simpler you can do squash those git squash,"
"browserstack","saucelabs","better overall,","22010163,","edit i just came across saucelabs which seems to be even better than browserstack for manual testing and they also cover many web browsers tablet mobile devices,"
"lapack","openblas","slower than  in linux repo likely, often is faster in linux repo likely,","21647126,48410284,","if you are on linux and have chosen blas and lapack from a repo it is very likely that you are on linux and have chosen blas and lapack from a repo are much slower than openblas,as debian maintainer for r i take advantage of the fact that we have several blas lapack builds;base can be ok openblas often is faster but be careful when you then launch multiple cores from r via the different mechanisms and there is also atlas,"
"mod-alias","mod-rewrite","faster overall,","2096899,","evidently though mod-alias can be somewhat faster than mod-rewrite all other things being equal of course,"
"popover","tooltip","common with a  overall,more than one  overall,","10643923,53090690,","a tooltip has more in common with a popover but common are generally regarded as a popup historically,after hours of searching and trials i came up with the most reliable solution for this problem and it even solves the problem of opening more than one tooltip or popover at the same time and the problem of opening automatically after losing focus etc,"
"freshmvvm","mvvmcross","better overall,","44111651,","it s very heavyweight and not fully featured for forms so prism or freshmvvm works better as these are designed for forms from the ground up but if you do use mvvmcross for forms its easy to go native if you need,"
"division","subtract","result larger overall,general overall,","26920930,14637712,","if division result is larger than 1 push the current transformer to the results array and subtract the current wattage from the total wattage,adding numbers together is relatively easy so is subtract;doing multiplication and division is a little harder,"
"ncurses","pdcurses"," is not overall,more overall,","30693775,9114289,","noting comments pdcurses is not a port but a separate program;ncurses has a workable port to windows for what it s worth,i ve found pdcurses which is more like the ncurses but it lacks a lot of ncurses libraries,"
"force.com","salesforce","easier integration overall,","17879015,","consider using force.com canvas now ga which is an added-value iframe with more security and easier integration with salesforce apis than raw iframes - see,"
"heapsort","in-place","slower standard overall,general overall,","8311230,55134891,","but heapsort is assumed to be on average somewhat slower than standard in-place quicksort,sort the data by some unique identifier i recommend using heapsort as it s in-place generic in that you can provide a comparison function and it will work with that and you could probably look up an iterative implementation in python,"
"expressionengine","wordpress","better choice overall,","10113369,","but in my opinion for most sites expressionengine is a better choice than wordpress,"
"roboguice","robolectric","better overall,","17476067,","you cannot use mocks in conjunction with roboguice it s better to let robolectric provide the context you need here,"
"snap.svg","svg.js","raphaels younger overall, code is more overall,","34997173,21917370,","if you definitely need svg and want to only use existing elements you may want to look at snap.svg which is raphaels younger sister and shares a lot of the same codebase or another like svg.js,the snap.svg syntax is a bit more concise the svg.js code is more readable,"
"datepart","getdate","shorter overall,","6687049,","i prefer using these short forms because to me year getdate is shorter to type and better to read than datepart yyyy getdate,"
"beyondcompare","winmerge","more comfortable overall,","39738554,","my friend used beyondcompare however i am more comfortable with winmerge,"
"principles","srp","more overall, is harder overall, is far more overall, is more overall,","10609215,12476575,18422435,54577445,","there is another principles which i like even more than srp - dry,code the violates the srp is harder to understand maintain and more difficult to test,solid is interesting because single responsibility principles is often at odds with don t repeat yourself and you have to come up with a balance;in most cases dry loses because srp is far more important,cohesion can be seen a software quality metric while srp is more of a subjective software quality principles,"
"qtkit","quicktime","better overall,","7271789,","you can do it with qtkit but works better in 32bit quicktime framework,"
"flac","mp3","file newer overall,","2483418,","if a flac file is newer than the corresponding mp3 file or the corresponding mp3 file doesn t exist then i want to run a bunch of commands to convert the flac file to an mp3 file and copy the tags across,"
"emacs","sublimetext","better overall,","37955314,","this is the one thing i think sublimetext does better than emacs,"
"mediacodec","mediarecorder","more convenient overall, but still overall, but requires more work overall,faster overall,","16446982,51336685,52348492,13418093,","android 5.0 api 21 allows surface input to mediarecorder which is often much more convenient than mediacodec,i guess there is no solution so the answer mediarecorder android is buggy or mobile companies didn t care of all android features while developing their devices update mediacodec is also buggy with canvas it works on much more devices with mediacodec but still some devices may fail to record video correctly using this method so final answer don t ever use lockcanvas or lockhardwarecanvas when working with mediacodec or mediarecorder it s buggy,if you are stuck with the old api consider one of the samples that record video using mediacodec and mediamuxer;it is more powerful than mediarecorder but requires more work,i d like to use mediacodec to encode the data coming from the camera reason it s more low-level so hopefully faster than using mediarecorder,"
"debian","opensuse","newer overall,","13459028,","note that centos software versions are older than debian s but versions of opensuse software are newer than debian s versions,"
"terminate","timeout","smaller than default  overall,more overall, 8000 milliseconds info org.apache.flink.runtime.taskmanager.taskmanager overall, which also overall,","56576659,31255118,49399834,52418512,","you have no errors regarding sqs because timeout of lambda is smaller than default timeout of http.client inside svc.sendmessage sendmessage is just a post request to aws api and lambda is terminate before it gets any response from sqs,but the problem is that i need to terminate a process if it lasts more than a timeout 3 seconds,when i start the cluster the log in taskmanager like this info org.apache.flink.runtime.taskmanager.taskmanager - trying to register at jobmanager akka.tcp flink master1 6123 user jobmanager attempt 1 timeout 500 milliseconds info org.apache.flink.runtime.taskmanager.taskmanager - trying to register at jobmanager akka.tcp flink master1 6123 user jobmanager attempt 2 timeout 1000 milliseconds info org.apache.flink.runtime.taskmanager.taskmanager - trying to register at jobmanager akka.tcp flink master1 6123 user jobmanager attempt 3 timeout 2000 milliseconds info org.apache.flink.runtime.taskmanager.taskmanager - trying to register at jobmanager akka.tcp flink master1 6123 user jobmanager attempt 4 timeout 8000 milliseconds info org.apache.flink.runtime.taskmanager.taskmanager - trying to register at jobmanager akka.tcp flink master1 6123 user jobmanager attempt 5 timeout 16000 milliseconds info org.apache.flink.runtime.taskmanager.taskmanager - trying to register at jobmanager akka.tcp flink master1 6123 user jobmanager attempt 6 timeout 30000 milliseconds info org.apache.flink.runtime.taskmanager.taskmanager - trying to register at jobmanager akka.tcp flink master1 6123 user jobmanager attempt 7 timeout 30000 milliseconds info org.apache.flink.runtime.taskmanager.taskmanager - trying to register at jobmanager akka.tcp flink master1 6123 user jobmanager attempt 8 timeout 30000 milliseconds info org.apache.flink.runtime.taskmanager.taskmanager - trying to register at jobmanager akka.tcp flink master1 6123 user jobmanager attempt 9 timeout 30000 milliseconds info org.apache.flink.runtime.taskmanager.taskmanager - trying to register at jobmanager akka.tcp flink master1 6123 user jobmanager attempt 10 timeout 30000 milliseconds it seems can not get connction with jobmanger bu the web ui can see the taskmanager. when i submit a job to the cluster jobmanager get some error log caused by akka.pattern.asktimeoutexception recipient actor akka flink deadletters had already been terminate i guess akka got some error but i do not know how to solve it. many thanks for helping me,windows jdk7 should set -djava.net.preferipv4stack true see java bug 7179799 info|8728 8|service yajsw|18-09-12 14 59 36| at java.io.fileinputstream.readbytes native method info|8728 8|service yajsw|18-09-12 14 59 36|wrapper manager received stop command info|8728 8|service yajsw|18-09-12 14 59 36| at java.io.fileinputstream.read unknown source info|8728 8|service yajsw|18-09-12 14 59 36| at java.io.bufferedinputstream.fill unknown source info|8728 8|service yajsw|18-09-12 14 59 36| at java.io.bufferedinputstream.read unknown source info|8728 8|service yajsw|18-09-12 14 59 36| at org.rzo.yajsw.io.teeinputstream source.run teeinputstream.java 221 info|8728 8|service yajsw|18-09-12 14 59 36| at java.util.concurrent.threadpoolexecutor.runworker unknown source info|8728 8|service yajsw|18-09-12 14 59 36| at java.util.concurrent.threadpoolexecutor worker.run unknown source info|8728 8|service yajsw|18-09-12 14 59 36| at java.lang.thread.run unknown source info|8728 8|service yajsw|18-09-12 14 59 36| info standardfilesystemmanager - using c windows temp vfs_cache as temporary files store. info|8728 8|service yajsw|18-09-12 14 59 36|unable to classify garbagecollectormxbean g1 young generation info|8728 8|service yajsw|18-09-12 14 59 36|unable to classify garbagecollectormxbean g1 old generation info|8728 8|service yajsw|18-09-12 14 59 36|2018-09-12 14 59 35.518 info oejs.server jetty-7.x.y-snapshot info|wrapper|service yajsw|18-09-12 14 59 36|restart process due to default exit code rule info|wrapper|service yajsw|18-09-12 14 59 36|shutdown wrapper due to exit code rule info|wrapper|service yajsw|18-09-12 14 59 36|process not in state running - delaying stop info|wrapper|service yajsw|18-09-12 14 59 36|shutdown wrapper due to exit code rule info|wrapper|service yajsw|18-09-12 14 59 36|calling onstop info|wrapper|service yajsw|18-09-12 14 59 36|win service stop - timeout 30000 info|wrapper|service yajsw|18-09-12 14 59 36|win service wrapper.control - stopping application info|wrapper|service yajsw|18-09-12 14 59 36|process not in state running - delaying stop info|wrapper|service yajsw|18-09-12 14 59 36|win service stop - after shutdown info|wrapper|service yajsw|18-09-12 14 59 36|win service stop - before notify info|wrapper|service yajsw|18-09-12 14 59 36|win service terminate i have read many articles and many of them has suggested to keep hibernate.c3p0.idle_test_period less than c3p0 timeout which also makes sense,"
"aquamacs","emacs","more overall, has a more overall, acts a little more overall,","6205509,1517,26263784,","on unix like environments emacs is more native and you can use slime which is usually available as a os package or as a separate download for aquamacs,someone syncs carbon emacs with the upstream tree quarterly i think;aquamacs has a more irregular schedule but it s seen some pretty major updates over the last year,or the current version there are a few options to choose from of which the most popular seem to be emacs for os x and aquamacs;the main difference is that aquamacs acts a little more like a mac app in aquamacs handling of keyboard shortcuts and things like that,"
"inline-functions","inlining","shorter overall, macros is faster sometimes overall, is not overall,","31355050,23899128,36212467,","using options such as -os for the compiler to tell it to make the code small in other words don t inline-functions unless the code is shorter by inlining than by calling the function and don t unroll loops etc etc - but do inline-functions called only once as that does make the code shorter,to trigger inlining enable optimization in your compiler settings gcc -o2 and put your functions to the .h file as static inline;please note that sometimes inlining macros is faster sometimes a real function call is faster depending on the code and the compiler,an inline-functions should be as fast as a macro if it is indeed inline by the compiler;note that the inline keyword is not binding but just a hint to the compiler which may ignore it if inlining is not possible,"
"bluej","greenfoot"," is way more graphical still overall,better overall,","259080,259080,","greenfoot is developed by the same people that made bluej but greenfoot is way more graphical still allowing full java programming of the applications,i think greenfoot is better than bluej for 8-years old,"
"textctrl","wxpython","more overall,higher than the  overall,","7097874,56199023,","i have been working on a simple textctrl project to get more acquainted with wxpython and i have hit a small road block,summary i m trying to align multiple wxpython widgets horizontally at the bottom of a boxsizer but they are off by just a pixel as seen in the following screenshot as you can see the bitmaptogglebutton is positioned one pixel higher than the textctrl widget,"
"bugzilla","mantis","simpler overall, and found  better overall,more complete overall,","12054692,405231,1188910,","research showed me that mantis is simpler to use over bugzilla so i decided to use mantis but have no idea how to install it.can anyone please tell me what are the steps you need to take in order to install mantis bug tracking system in a small company to track website mobile portal bugs, ve used bugzilla and mantis and found bugzilla better from a technical point of view but mantis wins if some of your bug reporters are not programmers not programmer minded;mantis interface is less threatening for a novice bugtracker user,since mantis is a bit more complete than bugzilla when it comes to features you could define separate issues with custom field for your tasks,"
"thymeleaf","velocity","faster overall, team not overall,","28026223,57591885,","i m working in a spring mvc project and i want to use a template engine i originally choose apache velocity because a slideshare presentation that says it was two times faster than thymeleaf page 41 and since i m working with big tables i need to use a fast template engine does apache velocity supports html5 if not is there other template engine that supports html5 that is not thymeleaf,like nathan hugues said you can still use velocity it s just that spring dropped a support package for it;as mentionned in the blog post you quoted thymeleaf has spring support it is just provided by the thymeleaf team not the spring team,"
"cclabelttf","ccmenuitem","more overall,","16316610,","if i create the same cclabelttf once then i can t add it to more than one ccmenuitem because it will give runtime error about label already added,"
"dns","spf"," system was first overall, records not overall,more than 10  in process lookups record,record more in process lookups record,","37035773,53566134,56943963,34619533,","finally the majority of dns services all offer easy configuration of dkim and spf ssl isn t something that is provided at the dns level it is merely part of the lookup to validate it;when the dns system was first conceived there were two addressing mechanisms that were used,with content distribution networks load balancing the ehlo name and ip reverse dns often don t match;you could check the ip s validity against spf records not every domain has spf records but it is quite common,this happens when you have more than 10 dns lookups in your spf record,your spf record requires more than 10 dns lookups to process,"
"atoi","strtod","better overall,","31328588,","also strtod is a better alternative to atoi,"
"ng-show","ng-switch","faster overall,","19748236,","note that some people feel that ng-show is a little faster than ng-switch and ng-if for file-based templates,"
"coldfusion","railo","generally stricter overall,","24370779,","there are many other small differences railo is generally stricter about syntax and semantics than adobe coldfusion and often those decisions are driven by performance concerns in that compatibility with adobe coldfusion would make railo slower,"
"graphlab","pandas","more such overall,","44887738,","i m using the module graphlab in my python script and many more such as pandas and numpy etc graphlab module only works on python2 not for python3,"
"memmove","strncpy","faster overall,  instead overall,","2884915,25113969,","if you know the lengths of the strings memmove is a sensible choice - and nominally faster than strncpy because it does not have to check for nulls as it goes,you can use strncpy instead of memmove but strncpy instead of memmove has a more complex loop condition than memmove strncpy instead of memmove has to check for a null byte as well as the count whereas memmove only has to check the count,"
"emacs","textmate","better overall,better overall,","48015,967805,","textmate is a better emacs for macs though that won t help you with solaris,i feel like emacs is so much better than textmate,"
"gnome","kde","larger overall,quicker overall,","8838690,11794298,","the motivation is that font size 12 under kde seems to be much larger than under gnome,on kde aptana ran perfectly quicker than on gnome at the best of times,"
"chaco","matplotlib","faster overall,","20483834,","enthought-dev is chaco faster than matplotlib,"
"delphi","oxygene"," .net ever overall,compatible with the  overall,","15781281,15781281,","right now oxygene is much better as right now oxygene supports a broader set of language and platform features than delphi .net ever did,oxygene is less compatible with the delphi language because the platforms the delphi language supports warranted for some language deviations that in practice are very useful,"
"speex","wma","better overall,","2526831,","speex is designed for speech and should perform better than wma mp3 or other audio codecs that are designed to handle music if you are just encoding speech which i assume since you are grabbing from the mic,"
"javascriptmvc","sproutcore","heavyweight than  overall,","509703,","it s probably overkill for what you need but sproutcore is an mvc framework and it doesn t look any more heavyweight than javascriptmvc or trimpath s junction,"
"extjs","gxt","more time overall,","1034539,","i have worked pretty heavily with both flavors of the gwt extjs libraries though i ve spent more time with gxt recently,"
"janusgraph","titan","more up-to-date overall, is no longer overall, requires a bit overall,","44815800,44389736,45348128,","janusgraph is a more up-to-date version of titan despite the low version number,titan is no longer maintained but has been forked by the janusgraph project which has a lot of active development on it,if you are just getting started and are just interested in learning about graph databases then i likely wouldn t recommend starting with titan janusgraph as titan janusgraph requires a bit of configuration to get started schemas backend selection etc,"
"jpgraph","phpgraphlib","far more overall,","14926288,","jpgraph has far more capabilities than phpgraphlib,"
"coffeescript","livescript","more compatible overall,","12585426,","livescript is coco but much more compatible with coffeescript more functional and more feature rich,"
"jms","jta","more overall,","5528145,","you ll have to use jta if you need transactional access to more than one jms session,"
"magnolia","opencms","newer than  overall,","15018912,","however if i d built a new site with a cms on java i d probably go with magnolia cms because the documentation and community is more mature despite that d s newer than opencms,"
"agda","haskell"," enforces a phase overall, which includes correctness proofs overall,more overall,","11243999,27695354,3771477,","haskell unlike agda does not have dependent types so there is no way to do exactly what you want;types cannot be parameterized by value since haskell enforces a phase distinction between runtime and compile time,now i believe this approach is a fair bit more practical than indexing with monoids because haskell doesn t have kind classes or first-class type level functions that would make the monoid approach palatable;it would be nice to have a verifiedmonoid class like in idris or agda which includes correctness proofs besides the usual methods,coq is probably the most prominent language of the style but agda has a more haskell-y feel as well as being written in haskell itself,"
"slimdx","xna"," isn overall, doesn overall,better overall, is not; in gpu least vertex, is a more overall, only supports directx in gpu least vertex, is not in gpu least vertex, does not overall, is lacking a number overall,","76040,3825328,11730008,2872864,6741606,2872864,2872864,689320,4173072,","xna isn t the only alternative though;there is also slimdx which is under constant development as a means of providing a lean wrapper of directx in a similar fashion as managed directx which was i believe discontinued by microsoft in favor of xna,based off of your concern of slimdx documentation and the fact that you want to use multitouch which i can t determine if that is accessible from slimdx - refer to documentation comment i d suggest going with xna;xna doesn t have immediate support for ogg but there are several nice converters out there,considering the lack of windows 8 love that xna is getting support being dropped apps won t be sellable on the app store -- i d go with slimdx -- it s better than xna anyway -- the only down side is that you can t make xbox apps with it,xna is a cross platform between windows xbox 360 zune and windows phone 7 while slimdx is not;xna has a strong community creators.xna.com with tons of tutorials and help materials,microsoft used to offer managed directx which has been deprecated in favor of xna;slimdx is a more modern wrapper for the directx libraries exposing most of their functionality but does have some performance cost,xna requires a gpu with a least pixel vertex shaders 1.1 while i think slimdx does not;slimdx supports directx10 and 11 while xna only supports directx 9,slimdx supports directx10 and 11 while xna only supports directx 9;xna is a cross platform between windows xbox 360 zune and windows phone 7 while slimdx is not,unfortunately microsoft no longer supports managed directx and it s successor xna does not support 64bit either;slimdx is an open source alternative to managed directx and it supports 64bit,personally i d recommend using slimdx as xna is lacking a number of features that the newer directx versions provide,"
"ng-app","ng-controller","more overall,","36743175,","a ng-app can have more than 1 ng-controller,"
"hidapi","libusb"," doesn overall,better fit overall,","55638494,32592171,","that hidapi doesn t give you the same level of control over the packets you send s higher level than libusb and some of the parameters that that hidapi doesn t give you the same level of control over the packets you send uses for requests are fixed brequest value,also if this controller is an hid the hidapi library might be a better fit than libusb,"
"fftw","gsl","slower overall,","36222272,","you can have a look at this speed performance benchmark from fftw which suggests that gsl is about 3-4 times slower than fftw 3,"
"icalendar","rrule","shorter overall,","27554974,","it s possible for icalendar software to make this a lot shorter with rrule but these calculations are hard and not everybody implements it,"
"qcodo","qcubed","community much stronger overall,","843733,","qcubed community is much stronger at the moment so you might want to check in qcodo forum for your answers on basic problems but post on the qcubed forum,"
"clgeocoder","mkreversegeocoder","lower overall,","12599418,","in ios 5.0 and later you can use clgeocoder of core location framework as for ios lower than 5.0 mkreversegeocoder of map kit framework,"
"ethernet","wireless"," is easier overall,more than ten  overall,more overall,","20831875,50427795,10007514,","you can use bluetooth or usb or wifi perhaps wifi ethernet to communicate to your ipad app;there is a camera connection kit to give you a standard usb plug on the ipad but wireless is easier,i have more than ten ethernet ports no wireless etc,if you are working on physical machine that has cable and wireless connectivity then you will have more than one ethernet adaptor choise,"
"oledbcommand","sqlcommand","less same overall,","15882849,","as shown in sample below for sqlcommand which is more or less same as oledbcommand,"
"cassandra","scylla","more overall,efficient than  overall,different between  overall,recommend  over  overall, doesn overall,","47127723,47158538,51010282,57332156,54136723,","i wrote a detailed description of the sstable format on scylla s site scylla is a more efficient c++ re-implementation of cassandra to which i contribute,scylla a cassandra clone which is generally more efficient than cassandra also has similar issues with huge partitions as in cassandra moderately large partitions are fine but similar issues are actively being worked on including re-designing the file format so eventually scylla a cassandra clone which is generally more efficient than cassandra should support arbitrary-sized partitions,the internal implementation of the memtables might be slightly different between scylla and cassandra but for the sake of simplicity let s assume it is the same,details here given your use case if you envision storing multiple tb of data i would recommend scylla over cassandra,scylla considered such an implementation but hasn t implemented it either see;there is a reason why cassandra doesn t support the feature you describe - it s because it can be inefficient first the secondary-index must indeed list as you noted all the matching row keys and not just distinct matching partition keys,"
"qtreeview","qtreewidget","more overall, only not overall,","25554005,26289863,","if your exchange data between 2 widget i suggest to use qtreewidget more than qtreeview because data in qtreewidget can edit dynamic data row and value,as mumush mentions andrea s answer applies to the qtreewidget only not a qtreeview;qtreeview has no selecteditems method so you have to use selectedindexes which will return you a list of qmodelindex objects,"
"md5","whirlpool"," are not overall,slower overall,","19547446,5082524,","namely that you can use the smaller and cheap checksum to see if doing the more expensive md5 sha whirlpool is worth considering in the event of changed files;adler-32 and md5 are not comparable in this way,at best it is a computationally expensive hash function like whirlpool that for example is five times slower than md5 and thus allows only a fifth of the number of hash operations in opposite to md5,"
"median","range"," which contains only smaller overall,earlier than the  overall, is much smaller overall,smaller overall,","48178603,53220265,11229402,19693299,","let s say we want to find the median of the range starting at the second 2 inclusive and ending at the 1 exclusive . these are 7 elements thus the median has rank 4 fourth-smallest element in that range. now using a rank0 1 call in the root bitvector at the beginning and end of this range we find the corresponding range in the children of the root as you can see the left range which contains only smaller elements has only 3 elements thus the element with rank 4 must be contained in the right child of the root,the block timestamp is within the valid range in bitcoin the timestamp cannot be greater than 2 hours from the current network time and cannot be earlier than the median time of the past 11 blocks,o if you are calculating rolling median over a set of integers which vary from 1..65536 then you only need 128kb to store 128kb and can insert delete query using o ln n where n the size of the range 2 16 operations;this is a big win if the data range is much smaller than your rolling window,you should also adjust your conditionals to check for a low high range as it should quickly get smaller as your median value approaches the real value,"
"viewdidappear","viewdidload"," will not overall, method is better in better tasks long, doesn overall, not overall,sooner than  overall,earlier in earlier difference method, is not called again overall, is more in time vc self, is called the map overall, is called every time in time vc self, and not overall,","13043056,9894925,14543387,25420905,56558989,25648503,16182317,27442513,51320932,56265741,42196524,","is not allocating and returning the controller returning an already created controller viewdidload will not be called;in viewdidappear it is safe to adjust the views related to your controller,if you have anything tasks that may take long to execute the viewdidload method is better to do things like buttons labels in the viewdidappear as the view has already been loaded,it is because viewdidload is implemented before segue;using viewdidappear doesn t help here either as it would load the text after secondviewcontroller is presented to the screen,you should use viewdidappear not viewdidload;viewdidload means view was loaded to memory but not drawn,viewdidload will always be called much sooner than viewdidappear,you could try the block of code in the viewdidload method which is called earlier than viewdidappear,this would mean that viewdidload is not called again and the _groups variable is therefore not updated;i would try putting your _groups code in viewdidappear followed by a reloaddata call on the tableview,also don t present modal vc from self at viewdidload at the time of this method execution the vc is often not yet presented the vc viewdidappear is more fitting for such tests,you need to set the region of the map in viewdidappear not viewdidload;when viewdidload is called the map view has just been loaded - the map hasn t been rendered yet so you can t set its region,the sample code for that library has you call fpc.addpanel in your view controller s viewdidload not in viewdidappear;the viewdidload function is only called once in the lifetime of a view controller but viewdidappear is called every time a view controller get s re-shown like when it is redisplayed after being covered by a modal and then uncovered again. the two are not interchangeable for that reason,the reason why is because viewdidload only gets called once and if you are hitting the back button from the navigation controller and its not setting the color as intended that would be the first step to see if it works like you need;without really seeing any code i would assume you need to set the background color of cell in viewdidappear and not viewdidload,"
"viewdidappear","viewdidload","earlier in earlier difference method, will be called so overall,such as  overall, is a better in better tasks long,","14063610,10527259,53689521,22499256,","so viewdidload is called slightly earlier than viewdidappear the only difference is that when viewdidappear the view have been already drawn instead in viewdidload the view has still to be drawn,first of all viewdidload will not be called so you can remove it from here;both viewwillappear and viewdidappear will be called so you only need it one of these 2 methods,to test that theory move all those calls out of viewdidload and into something later such as viewdidappear,i agree with the other answer that this animation shouldn t be in viewdidload - you ve got no guarantee the view has been added to your window at that point viewdidappear is a better place,"
"admob","inmobi","higher overall,","37393576,","i set ecpm for inmobi and mobfox higher than for admob,"
"qlist","qmap","larger overall,","16594128,","this relates to another question i asked a while back at size of qt containers is qmap much larger than qlist,"
"armadillo","openblas","faster overall,","46097040,","i wish program with armadillo and openblas is faster than with only armadillo,"
"boolean","tinyint","better overall, takes less space overall, not overall,more space overall,","21227175,4528318,30755932,1731950,","for all thing with boolean is better tinyint 1 or enum y n, bit column generally can t be indexed i can t necessarily speak for all databases on that statement hence generally so something like a tinyint would make more sense if indexing is required;keep in mind that depending on the use and on the system using the system while a boolean takes less space because the system s just a single bit depending on the implementation an int is the native word size of the hardware,tinyint 1 value inside tinyint is necessary;for mysql use the type boolean not null type,does tinyint in mysql take up more space than boolean,"
"notepad","textpad","more advanced overall,better overall,","1860622,7377521,","as it happens my choice is textpad but just about anything which is more advanced than notepad ought to be able to do this,on windows machine i would probably go with textpad personally i now use programmers notepad 2 however it does not support a spell checker and i don t really thinks it s better than textpad by any measure,"
"colormap","matplotlib"," has many more overall,more in endpoints data single,more in endpoints data single,","52498777,43811598,43811382,","matplotlib has many more color maps but it is not straightforward to apply these colormap to given opencv images,to allow you to scale down the data away from endpoints 0 and 1 i had to do this when combining colormap in matplotlib single pcolormesh with more than one colormap using matplotlib so you can likely see how the code works but basically say you have values -5 1 10 in a sample but want to normalize based on a range of -7 to 7 so anything above 7 our 10 is treated as a 7 effectively with a midpoint of 2 but shrink it to fit a 256 rgb colormap,to allow you to scale down the data away from 0 and 1 i had to do this when combining colormap in matplotlib single pcolormesh with more than one colormap using matplotlib so you can likely see how the code works but basically say you have values -5 1 10 in a sample but want to normalize based on a range of -7 to 7 so anything above 7 our 10 is treated as a 7 effectively with a midpoint of 2 but shrink it to fit a 256 rgb colormap,"
"boofcv","opencv","faster overall,","32721644,","i have made an app similar to yours for android we use boofcv whose surf is much faster than opencv,"
"fuzzy","lemmatization","broader scope overall,","33300859,","lemmatization implies a broader scope of fuzzy word matching that is,"
"emgucv","opencv","slower overall,blurrier overall,","28241977,8214222,","unless you re doing very heavy processing working with a single frame is probably faster than transferring it to the server as far as i know emgucv in c# isn t considerably slower than opencv in c c++,the results end up very close but the emgucv image comes out a little bit blurrier than the opencv image,"
"sudo","umask","more restrictive overall,","2406721,","if you forget to use this and your default umask is more restrictive or less restrictive then you will need to re-run the non- sudo,"
"fusioncharts","highcharts"," flash has more overall,","10647688,","customization and variety of animations highcharts has more customizable animations when compared to fusioncharts javascript charts;nevertheless fusioncharts flash has more advanced animations and that are really easy to setup,"
"bash","tcsh"," isn overall,better overall, is the default overall, alias not overall, does not overall,","4929975,30290413,28399600,29923885,54204324,","here s one way you can get around that using bash to interpret the escape sequences although it s a little ugly;the problem is that tcsh isn t interpreting the escape sequence in your variable name so the environment variable ends up with a literal e in it when you try to set it with tcsh,i want to do this in tcsh i know bash is better to use but i have to use tcsh,unlike in most linuxes bash is not the default shell in freebsd;rather tcsh is the default,i m not sure you can get an exact duplicate of that alias in bash since i don t think bash will do history expansion on an alias expansion so the closest you can get is a function that takes files as arguments;that example is a tcsh alias not a bash alias,requisites qdbus klipper xargs bash create a bash executable foo.sh;note this needs to be bash as tcsh does not support multi-line arguments,"
"reboot","uptime"," which makes more sense overall,more 3-y overall,","52581670,37192473,","1800 is the number of seconds for 30 minutes change it to whatever delay you need on the other hand you may be writing a script that will reboot your server and you may want to keep it from working if it is run before 30 minutes of uptime which makes more sense,yesterday i needed to reboot production server on debian 6 with more than 3-y uptime and 3rd party configured billing system on it,"
"atoi","fgets"," is not overall, solution is a lot overall,","22819227,31624015,","so here fgets is reading more than one integer at a time from file so atoi is not getting the int too,uggested solution use fgets to read i into a buffer then atoi or strtol to convert a buffer to an integer;or you could do an extra fgets after scanf or getchar in a loop until you ve read a newline but seriously the fgets solution is a lot more robust and worth learning now.,"
"caliburn","viewmodel","more general way overall,even easier overall,","30369414,11127247,","how can i connect a method and not a command with a binding from the viewmodel to an attached event to ask in a more general way with caliburn micro,with caliburn it is even easier since you just need to bind a property on your viewmodel to selecteditem,"
"ftp","smb","faster overall,","482412,","push log files to a central location ftp is faster than smb the windows ftp command can be automated with -s scriptfile,"
"pbkdf2","scrypt","better option bcrypt overall,stronger overall, one is clearly better; overall,","13424062,30308723,30854551,","pbkdf2 is arguably a better option than bcrypt scrypt having been much more thoroughly studied and tested,bcrypt is weaker than scrypt although still three orders of magnitude stronger than pbkdf2 because it only requires 4 kb of memory,the scrypt one is clearly better;pbkdf2 is using 20 000 iterations,"
"erlang","ocaml","more expressive overall, pattern is more overall, is more overall,","34956771,34956160,2882325,","while erlang is more expressive ocaml pattern matching is simpler which means a simpler language definition compiler etc. and you still can do what you need at the cost of writing more code,the erlang pattern is more powerful because the erlang pattern can match against something determined at run time;the ocaml patterns match against things fixed at compile time,erlang is more practical but not quite as amenable to metaprogramming;ocaml is another possible choice but suffers a bit on the practicality front as well,"
"palindrome","square-root","less overall,","29933222,","the mathematical reason for this is that once the solution 906609 is found it will no longer be possible to find a larger palindrome where the larger factor is less than the square-root of 906609 which is 952.160.,"
"qgridlayout","qwidget","gauge1 always bigger overall,","29731804,","however the qwidget gauge1 is always bigger than the other one.i want to use a qgridlayout because the application has to work on different sizes of a screen,"
"jedi","python-mode","slower overall,","29195460,","if it s too slow i would ditch the autocompletion part of python-mode because it uses rope which is slower than jedi,"
"constexpr","inline"," leaving just  then overall,better than the  overall, member variables no longer overall, or not overall,","57407675,24602957,57059723,14391320,","constexpr does not imply inline for variables c++17 inline variables while constexpr does imply inline for functions it does not have that effect for variables considering c++17 inline variables;and remove the inline leaving just constexpr then the variable gets multiple addresses which is the main thing inline variables avoid,you may be interested in looking at the constexpr keyword which i think is better than the inline keyword when you can make use of the inline keyword,with the introduction of inline variables static constexpr member variables no longer need to be defined outside of the class,yes dcl.constexpr 7.1.5 2 in the c++11 standard constexpr functions and constexpr constructors are implicitly inline 7.1.2 .;note however that the inline specifier really has very little if any effect upon whether a compiler is likely to expand a function inline or not,"
"countif","named","more overall,","27692551,","the roundabout method i ve employed is to create additional columns with a regular countif and counta formulae - in other words i count the number of countries a named has visited more than x times countif and then use the counta to sum the number of named in that column who have visited x times,"
"bada","marmalade","more check overall,","7025166,","cocos2d-x already works with marmalade so if you want to take your cocos2d game to ios android symbian webos bada rim qnx and more check out marmalade,"
"cvs","perforce","less commonly overall, gui is flashier overall,","4332537,249589,","since perforce is less commonly used compared to cvs svn amongst bamboo users we generally get less feedback about it and hear less about existing issues,the company i worked for at the time switched from the free cvs it was using to perforce gui is flashier,"
"atof","strtod"," it is much better in input part strtof, is simpler overall, to stop parsing in basile starynkevitch job, isn overall,better in basile starynkevitch job, to recognize locale-specific number overall, is a better in input part strtof, is better overall, to check doubles;atoi in input part strtof,","48476029,41995275,11895699,39813559,8165120,11895699,48436934,5678975,78627,","or even better alk suggested about the input part you can do this though instead of using atof it is much better to use strtof or strtod functions,it would be better to use strtod for this purpose as it allows for error-checking but atof is simpler to use and so is used here,you can actually switch to strtod always a much better idea than atof and ask strtod to give you the character position that caused strtod to stop parsing,atof is essentially obsolete now because it is not robust enough to handle malformed input;however strtod isn t flexible enough to accept particular rules for well-formed input,edit basile starynkevitch mentions that strtod is better than atof for this job as it gives the ending character,meanwhile atof is not locale-aware;the language specification does not require atof strtod to recognize locale-specific number formats for locales other than c locale,for double you can use atof function but you have to check firstly if the string contains a dot;edit as user3386109 mentioned strtod is a better solution for double,i just remembered the problem with atof -- it doesn t tell you where the number ended so reading several numbers in sequence is difficult;strtod is better in that regard,use strtof strtod not atof to check doubles;atoi and atof convert the initial part of the string but don t tell you whether or not they used all of the string,"
"kdtree","octree","less memory overall,","29076540,","kdtree needs less memory than octree and sometimes is even faster,"
"gridbaglayout","grouplayout","harder overall,","20558525,","i find grouplayout to be fine to code by hand certainly no harder than gridbaglayout though i can certainly see why it would be a favorite for tools to use,"
"bfg-repo-cleaner","git-filter-branch","faster solution overall,","35399505,","but bfg-repo-cleaner is a much faster solution than git-filter-branch,"
"flyway","liquibase","better off by using  overall,","32159287,","the downside is that you are losing the big part of the flyway s appeal in the flyway s simplicity and technically could be better off by using liquibase s dsl,"
"wadl","wsdl","lightweight easier overall, is not necessary;http exposes already overall, is much overall,","17340057,6830694,27564131,","like the rest of rest wadl is lightweight easier to understand and easier to write than wsdl,i think wsdl is not appropriate for rest and wadl is not necessary;http exposes already what wadl could describe in a separate file,wadl is much simpler;so the difference is remote execution ws wsdl vs resource rest wadl,"
"schematron","xquery","more overall,","21823929,","but xquery will be more than schematron and i use xquery heavily to validate xml values and i should be able to produce html reports well,"
"ico","png"," is not overall,larger overall,","55368092,35544336,","your icon file should be icon type not png may be because png is not supported by cx_freeze;in your setup.py change to please note the icon file must be in ico format don t act smart and just change the extension,however ico files if stored properly are not significantly larger than png files because since windows vista ico files can store png,"
"carrierwave","fog"," aws 0.3125 mib overall,more example with gem  overall,much faster overall,","48146812,22438908,9817544,","running bundle exec derailed bundle mem gives me this top 92.3008 mib carrierwave-aws 16.7461 mib carrierwave storage aws 16.6094 mib aws-sdk-core s3 7.9414 mib aws-sdk-resources 2.5703 mib aws-sdk-core 2.5117 mib jmespath 1.8867 mib jmespath nodes 1.1406 mib jmespath nodes function 0.3867 mib seahorse client base 0.9414 mib seahorse client plugins net_http 0.5273 mib aws-sdk-core xml parser engines rexml 0.8633 mib rexml document 0.8516 mib rexml element 0.5508 mib rails all 13.6992 mib rails 8.9453 mib also required by active_record railtie active_model railtie and 6 others rails application 7.2188 mib rails engine 6.8672 mib also required by coffee rails engine rails railtie 6.5898 mib also required by sprockets railtie jbuilder railtie rails configuration 6.3398 mib also required by rails railtie configuration active_support core_ext object 6.0586 mib active_support core_ext object conversions 4.7227 mib active_support core_ext hash conversions 4.0039 mib also required by active_support core_ext hash active_record serializers xml_serializer active_support time 3.8008 mib also required by active_record base active_support core_ext time 3.0938 mib active_support core_ext time calculations 2.9922 mib also required by active_support core_ext numeric time active_support core_ext string conversions active_support core_ext time conversions 2.2148 mib also required by active_support core_ext time active_support core_ext date_time conversions active_support values time_zone 2.1602 mib also required by active_support time_with_zone active_support core_ext date_time conversions tzinfo 1.8984 mib active_support core_ext array conversions 0.6211 mib also required by active_support duration action_dispatch http request and 2 others active_support xml_mini 0.5508 mib also required by active_support core_ext hash conversions active_support core_ext object json 1.0352 mib also required by active_support json encoding json 0.7227 mib also required by active_support json decoding pg text_encoder and 19 others json common 0.3906 mib also required by json ext active_support 0.7109 mib also required by active_support time active_support railtie and 5 others active_support dependencies autoload 0.332 mib also required by rails active_support rails active_support inflector methods 0.332 mib also required by active_support core_ext string inflections active_support core_ext time conversions and 5 others active_support inflections 0.3164 mib also required by active_support inflector active_support logger 0.3281 mib action_dispatch railtie 0.668 mib also required by action_controller railtie action_dispatch 0.6289 mib also required by action_controller active_support rails 0.4414 mib also required by active_record active_model and 4 others active_support deprecation 0.418 mib also required by sass rails helpers formtastic deprecation active_record railtie 2.8555 mib active_record 1.9766 mib also required by friendly_id active_record connection_adapters abstract_adapter 0.8516 mib arel 0.8086 mib also required by active_record base arel visitors 0.3438 mib action_controller railtie 0.875 mib also required by rails all sprockets railtie action_controller 0.6797 mib also required by heroku_rails_deflate serve_zipped_assets action_controller metal live 0.332 mib sprockets railtie 1.8516 mib also required by sass rails railtie sprockets rails helper 0.918 mib action_view helpers 0.8945 mib also required by action_view base action_view helpers form_helper 0.3828 mib also required by action_view helpers form_options_helper sprockets 0.9102 mib also required by sprockets rails helper sprockets directive_processor 0.5586 mib yaml 0.5078 mib also required by active_support ordered_hash hirb and 7 others psych 0.5078 mib rails3-jquery-autocomplete 9.9102 mib action_controller base 8.4922 mib action_controller metal 1.8281 mib abstract_controller base 1.0898 mib erubis 0.8477 mib also required by action_view template handlers erb heroics erubis engine 0.4063 mib also required by erubis engine eruby action_dispatch middleware stack 0.6367 mib active_support dependencies 0.5703 mib also required by abstract_controller helpers active_record base abstract_controller rendering 1.5625 mib action_view view_paths 1.4531 mib also required by action_view rendering action_view base 1.3477 mib action_view lookup_context 0.5156 mib action_view template 0.3789 mib also required by action_view template resolver action_controller metal redirecting 1.0586 mib action_controller metal rack_delegation 0.9297 mib action_dispatch http request 0.8555 mib action_controller metal request_forgery_protection 0.5859 mib action_controller metal params_wrapper 0.5 mib action_dispatch http mime_type 0.375 mib also required by abstract_controller collector jbuilder jbuilder_template action_controller metal url_for 0.3008 mib rails3-jquery-autocomplete formtastic 1.2578 mib formtastic inputs base 0.8125 mib fog 8.6523 mib fog vcloud_director 1.0156 mib fog vcloud_director compute 0.9961 mib fog vcloud_director query 0.4453 mib pp 0.418 mib also required by scout_apm fog openstack 0.8398 mib fog cloudstack 0.5859 mib fog cloudstack compute 0.5859 mib fog internet_archive 0.543 mib fog internet_archive storage 0.5352 mib fog internet_archive core 0.3125 mib fog rackspace 0.3867 mib fog cloudsigma 0.3594 mib fog cloudsigma compute 0.3047 mib fog vcloud 0.3203 mib fog vcloud compute 0.3125 mib fog aws 0.3125 mib twitter 8.3711 mib addressable uri 4.2422 mib also required by twitter base twitter rest request and 8 others addressable idna 2.7891 mib addressable idna pure 2.7461 mib public_suffix 0.3242 mib twitter streaming client 1.7266 mib twitter streaming response 1.4453 mib http 1.418 mib http response 1.2109 mib http cookie_jar 0.9219 mib http cookie 0.9102 mib domain_name 0.8203 mib domain_name etld_data 0.7813 mib twitter configuration 0.8477 mib also required by twitter rest help twitter base 0.6016 mib also required by twitter entity twitter identity and 12 others twitter null_object 0.4688 mib also required by twitter trend_results naught 0.4063 mib twitter cursor 0.8164 mib also required by twitter rest utils twitter rest friends_and_followers and 2 others twitter rest request 0.7852 mib also required by twitter rest utils twitter rest friends_and_followers and 8 others faraday 0.6406 mib also required by twitter rest client twitter rest response parse_json and 6 others twitter rest client 0.418 mib twitter rest api 0.3828 mib asset_sync 4.9609 mib asset_sync multi_mime 4.5039 mib mime types 4.4883 mib also required by mime types columnar mime types registry 4.1914 mib asset_sync storage 0.4375 mib fog core 0.4063 mib also required by fog fog xml and 48 others platform-api 4.5273 mib platform-api client 2.8359 mib newrelic_rpm 3.875 mib new_relic control 3.875 mib new_relic agent 3.4883 mib new_relic agent agent 1.7422 mib new_relic agent configuration manager 0.4961 mib also required by new_relic agent configuration new_relic agent configuration default_source 0.3633 mib new_relic agent transaction_sampler 0.3516 mib carrierwave 2.8281 mib also required by carrierwave-aws carrierwave uploader 1.7656 mib friendly_id 2.5273 mib friendly_id object_utils 2.4453 mib active_record base 2.4258 mib active_record querying 0.3906 mib compass 2.1328 mib also required by compass-rails compass sass_extensions 1.2461 mib compass sass_extensions sprites 0.6875 mib compass sass_extensions sprites engines 0.4609 mib compass sass_extensions sprites engines chunky_png_engine 0.4492 mib chunky_png 0.4492 mib compass sass_extensions functions 0.5078 mib compass configuration 0.6172 mib compass configuration data 0.6133 mib pg 2.1055 mib pg_ext 1.9102 mib feedjira 2.0781 mib loofah 0.625 mib sax-machine 0.4414 mib sass-rails 1.8047 mib sass rails 1.8008 mib sass rails helpers 1.7578 mib sprockets sass_functions 1.7383 mib sass 1.7383 mib also required by sass rails importer sprockets sass_importer sass engine 1.4492 mib sass script 0.5586 mib also required by sass script css_parser sass scss 0.3281 mib sass scss parser 0.3047 mib scout_apm 1.418 mib webrick 0.5664 mib jbuilder 1.1016 mib also required by jbuilder jbuilder_template active_support cache 0.3906 mib rmagick 1.0234 mib rmagick_internal.rb 1.0195 mib rmagick2.so 0.6875 mib lazyload-rails 0.8555 mib nokogiri 0.8555 mib also required by top sax-machine handlers sax_nokogiri_handler and 5 others sitemap_generator 0.5195 mib dalli 0.4648 mib unicorn 0.457 mib hirb 0.3984 mib i imagine 92 mb is quite a bit in terms of gems but i do use them all in the app,one more example with gem fog and heroku configuration for carrierwave with amazon s3 hosted on heroku,i ve read that carrierwave is much faster without using fog,"
"nsorderedset","nsset","faster overall,","36971254,","is nsorderedset faster than nsset,"
"qabstractitemmodel","qstandarditemmodel","more useful overall,","14528353,","do need implementation of qabstractitemmodel that can be more useful than qstandarditemmodel,"
"atan2","sqrt","faster overall,better overall,","9318108,9325404,","with typical libraries on common modern hardware sqrt is faster than atan2,indeed sqrt is better than atan2 and 1 sqrt is better than sqrt,"
"posix","vfork","less strict overall,","28040596,","both old and some modern systems implement a special vfork call which has somewhat strict limitations although less strict than the posix requireemnts for vfork but avoid this copy for performance reasons,"
"swing","windowbuilder"," is quite overall,easier overall, that makes it more overall,","6533387,36899666,24689371,","jigloo swt swing gui builder - home page;the windowbuilder is quite better tool,in my opinion swing is easier to start because there are tools like the eclipse windowbuilder which enables you to create your application in a graphical interface but javafx is more likely to be used in the future because it has some great improvements over swing like css skins etc.,in swing like anywhere else what matters is how you reason through your usage of polymorphism where it s reasonable theres nothing inherent about swing that makes it more difficult to implement than it is anywhere else;i don t know anything about windowbuilder but if it s keeping you from reasoning about your usage of polymorphism or otherwise limiting or influencing your thought about other architectural decisions then the tool is probably not as great as you think the tool is and is almost certainly not worth that cost,"
"automake","autotools","slightly more complicated overall, is not overall, is newer overall,","1035179,41360302,48566774,","in the case of autotools it is slightly more complicated because not everybody who compiles the software would need automake and autoconf installed only those that need to change the build system adding new files counts as changing the build system,your second option adding a bootstrap script that calls autoconf and automake to generate the configure scripts is also a bad idea;this defeats the entire purpose of autotools which is to make your source portable across systems including those for which autotools is not available,or what is so broken with autotools that they feel the need to break my build because my version of automake is newer than the package was configured with,"
"gson","moshi","better overall, upcoming kotlin support overall, implementation brings your question overall,","44781960,54242017,49795383,","if you re using retrofit and okhttp to perform the network calls i suggest you use moshi as it s also from square and claimed to work faster and better than gson,here s ten small reasons to prefer moshi over gson upcoming kotlin support,in fact gson does not provide runtimetypeadapterfactory per se in its standard bundle;quick googling for a moshi implementation brings your question at the top of the search results at least for me,"
"fractions","rational-numbers","more overall,","11600191,","and most importantly all decimals stored on a computer are terminating because in a computer a decimal fractions is not much more than a rational-numbers m n with n being a power of 2,"
"ansible","puppet","earlier stage chef overall,","22212327,","ansible is in an earlier stage than chef puppet and other tools out there when it comes to community support,"
"onclick","onsubmit"," is better in mplungjan submit button, is clearly better in mplungjan submit button,more reliable overall,","19759644,6899938,11028384,","it s just a supposition but try recreating the form widget onsuccess or even hide the form onsubmit actually onclick is better and show another form to the user,mplungjan onclick of submit just falls out of that being a button;form onsubmit is clearly better,you can indeed execute scripts on almost any page using content scripts that can manipulate the dom allowing you to add an onsubmit event listener to a form more reliable than onclick on a button as it is fired however the form is submitted - hitting enter,"
"styledtext","swt","package harder first overall,","23455654,","but adding a class to swt package seems harder first because there is different jars depending on the operating system used org.eclipse.swt.cocoa org.eclipse.swt.gtk .... and also because copy pasting the code of styledtext is not that easy since there is call to external methods waiting for a strict styledtext,"
"uipageviewcontroller","uitableview","more detailpageviewcontroller overall,","21461847,","in my app i have a rootpageviewcontroller which contains the uipageviewcontroller and one or more detailpageviewcontroller with a uitableview as a childview,"
"fabricjs","snap.svg","faster overall,","21791388,","my speed test claims that svg is significantly faster than canvas at least snap.svg seems to be significantly faster than fabricjs,"
"grepl","substr","operation slightly better overall,","41553334,","finally i also made a benchmark test which shows that substr operation is slightly better than sapply grepl and significantly better than vectorised grepl alone,"
"ostream","std"," objects cannot in standard output int, ; works with more overall, s wouldn overall, not in standard output int,more overall, it s also easier overall, is not overall,possible with  in standard output int, not overall,general overall, had a copy overall,","18764485,55964813,35039172,35833071,33477368,49139622,2707839,53574519,29744764,48068716,55770006,","since std cout is an ostream object its type is derived from std ostream it cannot be copied;std istream and std ostream objects cannot be copied,just use std string and std ostream;ostream works with more of the stl and with strings you don t have to care about its memory usage and potential memory-leaks,or simply std ostream s wouldn t work mdash;because std ostream has no accessible default constructor,std cout is a global object of type std ostream not std ofstream;std ofstream is a derivative of std ostream,so if using std ostream is more limiting than std basic_ostream,the actual thing that does the writing in the iostreams api is std ostream it s also easier to remember to use and write rather than deal with the maze of cryptically named functinos that is the streambuf api,std streambuf has a virtual function overflow which you can override and use to alter the bytes before passing them to the wrapped output class;std ostream is not the best place to implement filtering,the downside is that the only place it prints its output is standard output you may send an std ostream object and use it instead of std cout converting an int to an std string is very conveniently possible with std to_string,it s complaining that str is missing in aka std ostream not std ostringstream;std ostringstream s still returns it inherits these operators from std ostream which has no str member,that expression does have type std ostream,you cannot use since std ostream does not have a copy constructor;even if std ostream had a copy constructor using the above interface would cause the following problems,"
"flot","highcharts","more dynamic overall, does a better overall,","6178094,17604670,","highcharts is svg and as such it is much more dynamic than flot you can restyle graphs with css attach events perform animations etc..,also flot does a better job to synchronise common axis scale with many series;highcharts does a really good job here mixing bubble chart with other kind of series but isn t free for us government context,"
"glassfish","resin"," 4.0.2 not overall,slower overall,","2700609,12723426,","glassfish 3.0 very low level and sometimes complex glassfish 3.1 has new refactored websocket support which is more developer friendly v 3.1.2 supports rfc6455;caucho resin 4.0.2 not yet tried v 4.0.25 supports rfc6455,glassfish seems to be slower than resin,"
"vba","vbscript"," do not overall, features not overall, not overall,interchangeable with  overall,quicker than using  overall,better overall, to ; doesn t overall,more sense overall,","3621561,13802548,26275528,55537353,49971908,962399,25928417,1001855,","vbscript is also the language used to create custom outlook forms outlook-forms;although vbscript has much common syntax with vba do not tag vbscript questions as vba unless you are specifically asking about both,for exact details on the differences between the two see info visual basic for applications features not in vbscript and info vbscript features not in visual basic for applications;vba has a built-in ide and debugger which you don t have when running code under wsh but you can use visual sudio to debug the script file,vbscript not essential but preferred by me as controling script;vba to move the mouse,vbscript is not interchangeable with vba,i am trying to find the author and last modified by details of every file in a directory and i am using vbscript to loop through since i think that that would be quicker than using vba,vb6 vba though a little better than vbscript in general still has many similar issues where for their domain they require much more boiler plate to do simple tasks than what i would like and have seen in other scripting languages,see here for more information about translating vba to vbscript;vbscript doesn t provide implicit parent objects like the vba runtime environment does so you need to make everything explicit,i believe that the reports are using something more along the lines of vbscript edit oregonghost says vba which actually makes more sense than vbscript than vb.net and very limited at that,"
"gedit","sublimetext","better overall,","8951744,","i have tried with sublimetext 2 and it work very very good better than gedit p,"
"hql","jpql","more overall,","16886935,","because hql is more or less superset of jpql it of course works also in hql,"
"quicksort","shellsort","slower overall, has a better overall,","128055,4299612,","if you use shellsort no extra memory is needed at all though shellsort will be much slower than quicksort,if worst case is critical just don t use quicksort;shellsort has a better upperbound,"
"comparable","comparator","easier overall,class better in econd judge implement, you couldn overall, not in objects new useful, which is more overall, approach is more overall, is much better in sort clear natural, is better in econd judge implement, which is a bit in objects new useful, which is simpler too overall, is actually overall,","47012236,13545953,2632426,11609264,30487079,5687741,13442781,40219523,420255,12642808,9136110,","using just the keyfn return a comparable value that matches your requirements is much easier than implementing comparator,a comparator class is better since use of comparable would mean using,if the type would have to be comparable you couldn t create a treeset with a non-comparable type and a comparator which you can as it is now;one way to fix this while still being type-safe would have been to have two classes one with a comparable type parameter and one with a non-comparable type parameter and no default constructor only the constructor that takes a comparator but i suppose the java devs didn t want to introduce two classes that basically did the same thing though one could easily be implemented as a wrapper around the other,you save memory by not creating new comparator objects and comparator is useful if you are comparing objects of different types which is not your case;your listdata object should implements comparable not comparator interface,magine a comparator as a pair of scales a comparator does not measure a comparator but two different objects weights;for this reference you have to implement another interface comparable which is more similar to the built-in object equals,he comparable approach mandates that the keys are always sorted in the same way and would be used for keys which have a canonical ordering like integers where you want to use this ordering;the comparator approach is more flexible since you can use the same key objects for different maps where they are differently ordered and you can use the comparator approach for keys over which you have no control or who don t have a canonical ordering like list trees graphs etc,that said using a standard sort with either comparable rows or a comparator is much better than mixing up the sort logic and the comparison logic;comparator is the more flexible way,but if you implement comparable interface you will have to change code of all the model bean classes to override comparator method;so for loose coupling comparator is better,comparable which has comparator requires the objects to be compared in order to use a treemap or to sort a list to implement that interface;then you have to implement a comparator which is a bit less convenient to use,you could also achieve your objective by implements a comparable in the class to sort but you can t as long as string is final so you cannot extends;therefore there is no other way than to implement a comparator which is simpler too,if you can using a comparator is actually much simpler;in fact a comparator gives you a couple of additional options that you cannot have with comparable such as defining alternative sort orders,"
"comparable","comparator"," this is a better in econd judge implement, is more loosely overall, not overall, was not in objects new useful, is more in objects new useful, is a better in sort clear natural, is simpler overall,","16053421,23134352,36374630,44448438,2892969,16135267,36872655,","econd you can implement a comparator to judge which is bigger;if you don t want the object to be comparable which means you don t want to change the existing code to let the existing code implement comparable this is a better way,if you later want to be able to sort by something else too then that would not be possible using comparable;so using a separate comparator is more loosely coupled and thus offers more flexibility,so first it checks whether comparator is provided if yes treemap uses compare method to compare keys otherwise it uses comparator method of comparable for comparison;treemap uses comparator method of comparable not equals method from object when it tries to put an element into map,2 a priorityqueue can sort its elements either by a comparable s object comparator method or using a comparator for objects that are not necessarily comparable;the siftdowncomparable method is only invoked if a comparator was not provided when the priorityqueue was created,if the ordering of an object is an implementation detail of the object the comparable is more appropriate;if the ordering of the objects is controlled by the caller then comparator is more appropriate,n object should implement comparable if that is the clear natural way to sort the class and anyone would need to sort the class would generally want to do the class that way;if however the sorting was an unusual use case of the class or there can be multiple sort orders then a comparator is a better option,but if the actual value comparator is simpler it may improve the code;there s even a parameter-less variant map.entry.comparingbyvalue for the most trivial case that the map values are comparable,"
"scite","sublimetext","much more time overall,","44034227,","sublimetext 3 needs much more time several seconds more then scite before the opened file shows up and i suppose it is because sublimetext does some pre-evaluation of the file content like detecting areas suitable for folding there are fold triangles available depending on indentation of non-white characters in the file,"
"phong","shading","more nuanced overall,","15802920,","phong is a more nuanced shading model albeit a more hacky one which says that light is composed of ambient + diffuse + specular components,"
"code128","code39","better overall,","1243627,","data density in code128 is better than code39 - since it gets mentioned that often in other replies,"
"break","control-structure","more special other overall,","6114085,","e.g. break with labels as in java or even a more generic goto. continue does not seem more special than other control-structure mechanisms except that it is present in more languages,"
"pathogen","vundle","simpler than  overall,better overall, which doesn overall,","8254162,17790897,35142148,","vundle s simpler than pathogen in my experience and certainly simpler than installing the plugin manually,also i tried installing pathogen and that worked for some bundles but would not work for perlomni.vim because it was an ftplugin and pathogen was not appending the bundles correctly and after research i saw most people saying vundle was better than pathogen,but anyway vundle is not a good choice if you don t have access to the internet;instead you can use pathogen which doesn t try to download plugins and only loads them,"
"onload","onreadystatechange","function more overall,","14729732,","it looks like the onload function is a more modern convenience method and the old way of checking the result is using onreadystatechange instead,"
"xdebug","xhprof","lighter overall,","1283195,","thanks mikushi for the comment another possibility that i haven t used much is the the xhprof extension it also helps with profiling can generate callgraphs -- but is lighter than xdebug which mean you should be able to install it on a production server,"
"cagradientlayer","calayer","smoother results overall,","24724320,","however i wish to use a custom calayer that uses an internal cggradient for drawing instead as this should produce smoother results than cagradientlayer see here,"
"mouseleave","mousemove","better in look better combinations,helper plane between two  in look better combinations,","6937090,50366526,","take a look at mouseenter and mouseleave events they are better than mousemove and mouseenter combinations,the reason this happens is because the mouseleave the helper plane between two mousemove events,"
"catransition","uiswipegesturerecognizer","better overall,","10407713,","if you want to use a continuous gesture that tracks the user s finger you can use uipangesturerecognizer rather than uiswipegesturerecognizer and i think animatewithduration is better than catransition in that case,"
"associativity","logical-or","operator higher precedence overall,","15163782,","i looked it up and the logical-or operator has a higher precedence than the conditional operator and the conditional operator has right-to-left associativity,"
"perf","profiler","more detailed overall,","410465,","a profiler gives you more detailed information which can help to diagnose and fix perf problems,"
"express","hapijs","more overall,","37680583,","we started experimenting with hapijs on smaller services and kind of like it more than express,"
"expressionengine","magento"," and use  just overall,","9080288,","thinking you can format magento blocks any better in a cms such as expressionengine and use magento just for the backend will put you on a hiding to nothing to end up with a fairly useless setup that isn t going to go anywhere,"
"coderush","dxcore","newer name overall,","6608244,","dxcore is the newer name for coderush dxcore and coderush are a refactoring and productivity tool by devexpress that extends the functionality of microsoft visual studio 2002 2003 2005 2008 and 2010,"
"jspm","npm","more suitable overall, is more overall,","37119794,36085375,","assuming that you are building a webapp jspm is more suitable for managing your frontend dependencies than npm,in my experience this is weird with jspm as jspm is more geared towards deps for your app that are actually in production in the browser;what i do in my projects is install modules i need to run with scripts in dev gulp karma express etc using npm then you can run scripts in dev gulp karma with node,"
"mouseenter","mouseleave"," which is more work overall,div thicker overall, event is triggered so overall, not overall, won t overall, events is better so overall,general overall, doesn overall,","3826469,6888665,17886312,24901814,48218835,54612870,52410600,29297484,","you can simulate an anchor using css cursor pointer and events like mouseenter and mouseleave which is more work but does not break the expected behavior of an anchor tag,mouseleave div becomes thicker which will cause an almost automatic mouseenter,when the hoverdiv is inside the other div mouseleave will not be triggered when you hover the hoverdiv;this is happening because when the hoverdiv is shown your mouse is on it thus the mouseleave event is triggered so the hoverdiv disappears and then your mouse is on the first div again so the mouseenter event is triggered so hoverdiv appears again..,maybe when the mouseenter event trigger the ddsmoothmenu do something and it doesnot finish before the mouseleave event trigger;finally i found the reason the event to collapse all menu should be mouseleave not hover,so if mouseenter occurs in less time that the delay setted the things to do on mouseleave won t be done,as a jquery solution the .on method with the mouseenter and mouseleave events is better so that you can handle each event separately,i would recommend using mouseenter and mouseleave instead,if you use mouseover opposite to it is mouseout not mouselave mouseleave is opposite to the mouseenter event;the main difference between these two is that mouseenter mouseleave doesn t bubble but mouseover mouseout do,"
"d3.js","leaflet","better overall,","38617263,","i think d3.js looks so much better than leaflet but for quick zooming panning functionality my first question is is this better off in leaflet or can d3.js handle slick zoom pan even though it s not tile-based,"
"scala","typesafe","less reliable overall, it makes more sense overall, library is more overall, coding style encourage overall,","17532715,18050184,48500009,32727933,","scala does not provide an alternative and any alternative that was provided would likely be less reliable as typesafe does not have the resources that sun oracle ibm etc,typesafe is working toward removing the 22 limit from tuples and functions and doing this is necessary for scala and once this is done slick will no longer have problem with 22+ column tables;since the issue must be fixed in scala it makes more sense to do this than create a new orm and work with the community to adopt it,spray json library as it is typical for a scala library is more typesafe than jackson,scala provides more expressive power for describing types statically without runtime conversions;and scala coding style encourage using heavy type constructions for keeping code typesafe,"
"glsl","hlsl"," shaders opengl is kind overall,better overall,","50440569,3075629,","either way finding hlsl shaders is harder than finding glsl shaders opengl is kind of a universal thing.. which can then be ported to hlsl,opengl is better for playing around and experimenting because you can easily draw triangle without messing with hardware buffers available on larger selection of platforms plus glsl is better than hlsl because it doesn t compile into assembly,"
"ilist","iqueryable","better overall,","6841928,","generally ienumerable and iqueryable are even better than ilist as they re more generic unless you want to limit this with good reason of course,"
"gunicorn","uwsgi"," does not overall,better overall,better performance overall,","56353153,27571641,34023826,","you can use gunicorn uwsgi to serve django;but gunicorn uwsgi does not serve static files hence using reverse proxy server to serve static files,i ve been using uwsgi in production on heroku for over a year and it seems to handle everything a lot better than gunicorn,according to many benchmarks uwsgi seems to provides better performance than gunicorn and if the performance doesn t change significantly you ll be able to focus your investigation on nginx or ec2 configurations,"
"adodbapi","pyodbc","better overall,","11666564,","i am not sure but by reading the documentation i think you need to download the pyodbc it seems to be better supported than adodbapi,"
"angle","concave"," is bigger overall,less overall,bigger overall,","52748441,26031127,22340300,","a concave edge is an edge which inner angle is bigger than 180,if the total of the angle is less than 180â the triangle must be â œhyperbolicâ on a concave surface which might be the shape of our universe,edit2 okay i just see that if the angle is bigger than 180â it s concave so i have to change the shape for be simple than a triangulation,"
"lisp","prolog","easier in cool easy data,easier than  in cool easy data,","406426,53715172,","i also checked prolog and it seems a pretty cool language easy to do relations between data and easier than lisp but i d like to hear what you think,this is also the reason why prolog s transformation is so much smaller and easier than lisp s,"
"atof","istream","indeed better overall,","32496181,","atof is indeed better in reading floating point values than istream,"
"sqlcmd","ssms","more overall,faster overall,","24651261,24651261,","in ssms it takes more than 1 minute to complete while in sqlcmd it takes no longer than one second,is sqlcmd always that much faster than ssms,"
"dialog","popupwindow","better overall, is more appropriate then overall,","13891924,57042085,","i give up using popupwindow and use dialog instead after some tests i found dialog is far better than popupwindow there are quite a few problems with popupwindow,given that the dialog is complex i feel like popupwindow is more appropriate then popupmenu,"
"google-api-python-client","oauth2client","older version overall,","43080972,","there s a known bug that when you install the googleads library using pip it will install a newer version of the oauth2client library which will break my google analytics api scripts because the google-api-python-client uses an older version of oauth2client,"
"hazelcast","terracotta","generic than  overall,bigger overall,","53468509,5271450,","in terms of implementation terracotta ehcache may be based on top of jgroups services and provides a specific set of apis specific of a caching system and so less generic than hazelcast,we started with ehcache terracotta server array cause it s well-known backed by terracotta and has bigger community support than hazelcast,"
"tabs","toolbar","more overall,standard appbar with  overall,general overall, not overall, is just overall, is more space overall,","32543099,51951241,49121455,48199571,30185086,2100414,","search to each tabs in more than one toolbar,it s a simple drawer with standard actionbar classic detail with image going under the translucent status bar supposed to use collapsingtoolbarlayout to turn into a standard actionbar when scrolling up in this case it is non-standard actionbar i d call it a floating toolbar cause it doesn t expand to the full with of the screen and contains an already expanded searchview edittext fairly standard appbar with tabs list of issues that arise from leaving the single activity can t share viewmodel s between activities complex navigations which re-use parts already defined in another activity navigation graph have to be duplicated moved into a dedicated activity back navigation re-construction doesn t work between activities those are issues i want to avoid if possible but how do you guys manage these kind of situation on a single-activity with navigation component,detect if at max width you ll probably want to check if the document s width is lesser than the screen width you can t do this with height however as the browser has reserved vertical space for tabs and toolbar and there is no way of getting the height of those,tabs are not visible images for you reference my toolbar is half visible expected actual broken toolbar toolbar is properly visible with fla icon tabs are not visible expected tabs are visible clearly learning and quiz actual tabs not visible in this picture,in fact the tabs are not in the toolbar;the toolbar is just hidden,constantly changing tabs would make a ribbon too inefficient;if so using a menu bar and a single toolbar is more space efficient than a ribbon,"
"des","rsa","larger overall,faster than the  overall,","864659,10094969,","des code is 8 times larger than rsa,by comparison des see section 3.2 and other block ciphers are much faster than the rsa algorithm,"
"itextsharp","razorpdf","older version overall,","29825482,","razorpdf uses an older version of itextsharp which i believe was the latest free itextsharp version,"
"onclick","onmouseup"," does not work there; overall,more appropriate overall,","6335921,9850188,","because onclick does not work there;onmouseup do the job like not exactly the same click it works for me,note also that onclick would be much more appropriate than onmouseup for this,"
"gtk2hs","wxhaskell"," and is released more overall,better overall, project doesn overall, is a better overall,","601996,2896854,568356,601996,","most of what i say below is a personal take on the differences and why i choose to work on and with wxhaskell myself;gtk2hs has a larger team working on gtk2hs and is released more regularly,in practice i ve never had resource management issues with wxhaskell although i agree that it s possible and is an area handled better by gtk2hs which uses reference counting in the underlying library,when wxhaskell was first announced at the haskell workshop the development of the native haskell api looked extremely promising but was still incomplete;it looks as if the haskellized api for wxhaskell is still being worked on whereas the gtk2hs project doesn t mention this issue at all,i think that if linux was my main development and delivery platform i d probably use gtk2hs;it isn t however i deliver mainly to windows with occasional osx and i think wxhaskell is a better match to these platforms although both options support all three platforms,"
"font-size","fontfamily","lesser overall,","6409986,","the actual visible size is determined by the font-size and to a lesser extend by the fontfamily,"
"lapack","magma","always slower overall,","9742864,","b magma runs always slower than lapack sequential around 10 times slower,"
"uitabbarcontroller","uitabbaritem","more overall,","2895389,","i have a uitabbarcontroller with more than 5 uitabbaritem so the morenavigationcontroller is available,"
"pycharm","spyder","better than  overall,slower overall,","53439205,43038122,","i used anaconda built in ide named spyder . spyder is better than pycharm,i like the auto-complete feature of pycharm but from my experience it is slower than spyder,"
"double-quotes","single-quotes","tidier overall,general overall, and not overall,more work overall,","37335836,5360831,32431725,17127591,","another side note single-quotes inside xpath string looks a bit tidier than escaped double-quotes imo,makes first i d recommend using single-quotes around the variable itself easier to preserve double-quotes in the actual html code,just a quick follow-up on this -- on windows you need to use double-quotes and not single-quotes throughout the command line;single-quotes are just considered part of the value so the above would send client_id instead of client_id,first you can use double-quotes or q it just takes more work than single-quotes or q,"
"each","prop","more overall,","8743173,","needless to say each product has more than one prop which is kept in the producthas table,"
"spring-data-commons","spring-data-jpa","older then overall,","46858551,","you may encounter noclassdeffounderror if the version of spring-data-commons is older then required by spring-data-jpa,"
"dropbox","icloud","reliable than  overall,","22235828,","instead of dropbox i could use key value store as rest web service which i want to do later but have no time for this for now in my opinion my solution is more reliable than icloud and which is very important i have full control on how my solution s working mainly because my solution s my own code,"
"centos","solaris","more overall,","47192219,","i would give you more info except a ip aliasing on centos 7 is more involved than i like much easier on solaris and b i m not familiar at all with configuring nginx very easy on apache,"
"monobjc","monomac","less mature overall,","15046560,","when i evaluated monomac it was less mature than monobjc,"
"bellman-ford","floyd-warshall","faster overall,","7265454,","because bellman-ford runs in time o mn the overall asymptotic runtime is still o mn + n 2 log n so if m o n 2 note that this is little-o of n this approach is asymptotically faster than using floyd-warshall,"
"matlabpool","parfor","less worker overall,","10800861,","if you want to batch and parfor at the same time open one less worker with matlabpool than you otherwise would,"
"uibutton","uitapgesturerecognizer","more flexible overall,","28340726,","the first thought is that you could build a uiview showing image and text then attach a uitapgesturerecognizer to it which would be more flexible than uibutton,"
"arcmap","mapguide","more overall,","303277,","i mostly work with esri software though i ve had a little bit of work with mapguide mostly for clients whose engineering departments are using cad more than arcmap,"
"alfresco","openkm","more well overall,","12050599,","i think the preview function in openkm is more well than alfresco,"
"pygments","rouge"," has lesser language overall,","30630740,","note that at time of writing rouge has lesser language support compared to pygments,"
"np-complete","subset-sum","problem hard more precisely overall,","17125649,","the subset-sum problem is hard more precisely it s np-complete which means that your variant is hard too it s not np-complete because it s not a decision problem but it is np-hard,"
"aptana","textmate"," it s quite overall,more overall,","256398,4725870,","if you want something more minimalistic there is e text editor which supports textmate bundles not free though;i ve been using aptana it s quite good with lots of features even in the free version you probably don t need pro,in studio 3 much of this sort of functionality is still coming as aptana is going more of a textmate bundle route for a great deal of their features which i think is really cool,"
"boxlayout","grouplayout","better such in components concrete example,example better in components concrete example,","28289928,22849876,","some don t take too well to added components for example grouplayout while others do better such as boxlayout,as a concrete example this boxlayout example is better done using grouplayout illustrated here and here,"
"opencv","vips","better job overall,","33195055,","this is done on an extremely large image and i feel vips might do a better job than opencv on this,"
"nspersistentdocument","uimanageddocument","less hassle overall,","21869216,","unfortunately the sample apps are not based on uimanageddocument or nspersistentdocument but if you look at the way they handle backup files you could probably use that approach to create a document based app with less hassle than using uimanageddocument,"
"bitarray","bitvector","more efficient overall,better overall,","903548,21745124,","why is the bitvector 32 structure more efficient than bitarray,you will probably find that bitvector performs a good deal better than bitarray,"
"hmac","pbkdf2","also more overall,","44505548,","pbkdf2 also uses a more complex construction in particular hmac over direct digest to make recovering the input password from an output value more difficult,"
"mysql-python","oursql","nicer overall,more proficient overall,","2542930,15832451,","i hear oursql is nicer the mysql-python,at least for .executemany oursql seems to be more proficient than mysql-python as the sql statement is only prepared once for all submitted values,"
"runtimeexception","unchecked-exception"," extends exception overall, that s its purpose overall,simpler overall, are a subset in possible synonym here, are not in possible synonym here,","26853168,47696354,19805646,14435909,2700131,","subclasses of exception other than runtimeexception will not;exception has subclasses that are unchecked-exception runtimeexception extends exception,runtimeexception doesn t have to be declared or caught;it s an unchecked-exception that s its purpose,using throw new runtimeexception e is simpler to comprehend than throwables.propagate in the scenario where you want to throw an unchecked-exception wrapping a checked exception,runtimeexception are a subset of unchecked-exception for exceptions from which recovery is possible;unchecked-exception are not checked at compile-time which means that the compiler doesn t require methods to catch or to specify with a throws them,to summarize runtimeexception are a subset of unchecked-exception for exceptions from which recovery is possible but unchecked-exception is not a synonym of runtimeexception as many are answering here;as stated by their name unchecked-exception are not checked at compile-time which means that the compiler doesn t require methods to catch or to specify with a throws them,"
"cakephp","zend-framework","lighter overall,","6018469,","since cakephp seems to be much lighter than zend-framework i would suggest that you take a look at cakephp,"
"notepad++","sublimetext","more broken overall, but not overall, is also in manually sublimetext customizable,more customizable in manually sublimetext customizable,","10467472,5253775,12780476,12729127,","notepad++ which is often recommended are even more broken than sublimetext 2 which i find to be pretty good actually,on windows notepad++ does a pretty decent job another good option is aptana especially for javascript although it can be a bit heavy weight depending on the job;i ve also used sublimetext but not in any great depth and then there s microsoft expression web which is very similar to dreamweaver i thought they had a free express edition but apparently not,notepad++ is also not great at detecting it but when you know the encoding you can set it manually;sublimetext is as far as i know best at detecting the encoding also in large files,i prefer sublimetext because it s a little more customizable than notepad++ but they re both great options and lighter than dreamweaver,"
"quickcheck","scalacheck","richer overall,","9351767,","scalacheck s api is also a bit richer than quickcheck s api,"
"primes","square-root","less overall,","29443876,","iterate over the primes already found which are less than the square-root of p,"
"libjpeg","libtiff","narrower functionality overall,","3932742,","the situation is critical if on some platform libtiff provides a narrower functionality and does not link to libjpeg which will not be available on that platform at all so the above command for linking will fail due to unsatisfied library dependency,"
"scipy","sympy"," is not overall, does not overall, is not overall, do not overall,better option overall,","48044072,49416206,32906273,32790754,16105354,","if this equation was not a polynomial i d recommend using scipy s solvers like fsolve instead of sympy;sympy is not the right tool to find the numeric solution of an equation full of floating point coefficients,scipy does not know that;sympy is a library for symbolic math operations such as derivatives,mystic depends on numpy and will use scipy if it is installed however scipy is not required;mystic utilizes sympy to handle symbolic constraints but it s also not required for optimization in general,if you are talking about symbolic differentiation then as far as i know numpy and scipy do not provide this you already noticed how to calculate derivative at the point;so it looks like sympy is your only option,first of all if it was not for the relative complexity of the expressions here scipy would have been definitely the better option over sympy,"
"pandas","pytables","more overall, approach so far overall, and not overall, might not overall,","42074431,21009442,46870870,38881503,","pytables seems more flexible but i am unclear about what the most direct way of using it to save a full pandas dataframe with multiindex and all,since functionality provided by pandas is not needed and the processing is much slower see notebook below the best approach is using pytables or h5py directly;i ve tried only the pytables approach so far,the complication of course is that it looks like you are using pytables via pandas and not raw hdf5;pytables is hdf5 but adds a layer of structure and semantics on top of hdf5 s groups and datasets,i don t have enough karma to reply to luke h s answer but reading it into pandas might not be a good idea;pandas hdf5 uses pytables which is an opinionated way of using hdf5,"
"affinity","processors"," and spread the load overall,","27416478,","so the difference between the two is that if you assign affinity at the thread level you can assign the two to more than one processors and spread the load more than with all threads assigned to one processors,"
"apache-poi","docx4j","more convenient overall, offers more support overall, is better overall,","35200946,16486045,25399399,","if you are dealing with docx document docx4j is more convenient than apache-poi,i have used both apache-poi and docx4j extensively and having said that docx4j is more robust as docx4j offers more support out of the box for not only the document itself but for tables and images,i used apache-poi for excel sheet and i know that apache-poi has a good api support for word doc too but i heard that docx4j is better in handling word documents,"
"glassfish","wildfly","better overall,","41608974,","a time ago i change glassfish to wildfly it works better than glassfish but i m having some problems with this exception,"
"caf","wav","better overall,","1618687,","2 caf seems a little better than wav but not much,"
"cairo","drawingarea","larger overall,","28038754,","i found another approach using the cairo context passed to the handler of draw events but it resulted in capturing a region of the parent window that was larger than the drawingarea,"
"jmeter","soapui"," takes more overall, cannot however overall,more reliable overall, has much better overall, which consumes less overall,","54457269,45209091,18339600,38103429,43063554,","because soapui takes more than a second to respond the jmeter requests are held open for this time,another option is using taurus tool to convert soapui xml project into jmeter .jmx test script see soapui support and how to convert soapui xml to jmeter jmx articles for more details;itself jmeter cannot however you can record soapui execution through jmeter s http s test script recorder,i have found jmeter to be more reliable than soapui or loadui,jmeter is a multi-protocol load testing tool which for sure can be used for apis testing as jmeter evidenced by testing soap rest web services using jmeter article besides jmeter has much better reporting capabilities than soapui does,better is to run the soapui load test from the command line;for more performant systems i prefer using jmeter which consumes less resources at the client side,"
"stringtemplate","xtend","nicer overall,","43201794,","please consider also to use xtend for code generation that s much nicer than stringtemplate and integrates smoothly with xtext infrastructure,"
"boost","eigen","faster overall,","16991255,","inspecting the assembly shows that in the sequential access case eigen is faster because the sum becomes vectorized while it does not when using raw boost multi_array,"
"ffmpeg","handbrake","simpler encoder overall,","15694380,","you may also use handbrake which is a simpler encoder than ffmpeg,"
"gruntjs","package.json"," is not installed locally overall, is not overall,gruntfile.js less overall, file is here overall, does not overall,","15698397,23135113,38026255,25938890,38619200,","it s likely that gruntjs is not installed locally in your project folder which is different than grunt-cli;you have it in your package.json so try doing npm install or alternately npm install gruntjs,run npm install grunt-cli -g if gruntjs is not found message gets displayed;your package.json is not a valid json,dist fonts gruntjs gruntfile.js js less license package.json readme.md,i think you are confusing gruntjs with npm because the gruntjs cannot handle dependencies it s simply a task runner;the package.json file is here because you need the devdependencies for the gruntjs installation and all its plugins like grunt-contrib-concat,peerinvalid the package gruntjs does not satisfy its siblings peerdependencies requirements;indicates a library dependency for the project as specified in the package.json is incompatible with the version of another,"
"alfresco","nuxeo","definitely more overall,","35253152,","nuxeo is definitely more advanced in this because with its web interface you can customise almost everything but alfresco has other advantages,"
"lz77","lzw"," has better compression overall,more overall,","54622231,39759195,","according to some articles lzw has better compression ratio and according to others leader is lz77,if so what is the nature of the output of lz77 that makes it more suitable for huffman compression than lzw or some other method entirely,"
"lasagne","theano","way simpler overall,more deeply overall,","37088930,31073251,","and using keras or lasagne is way simpler to develop nns then pure theano which was just side research project in montreal to support development of pylearn,it is similar in spirit to keras which is built as a high-level api on top of theano tensorflow cntk although the main difference is that lasagne implements only one backend allowing it to integrate more deeply with theano features,"
"rtcp","rtp"," to allow monitoring overall,sender overall,","5464228,34574076,","rtp does not address resource reservation and does not guarantee quality-of-service for real-time services;the data transport is augmented by a control protocol rtcp to allow monitoring of the data delivery in a manner scalable to large multicast networks and to provide minimal control and identification functionality,the strange thing is that when streaming is done via rtsp unicast rtcp generates both sender reports and receiver reports but when streaming is done via rtp multicast only sender reports are generated,"
"minix","netbsd","better overall,","27707650,","i want test unix-like operating systems on historicial computers like commodore amiga atari st and i386 cpu and run posix compatible apps like pkgsrc x window system and blackbox wm for 40mhz higher versions minix 1.5 is best os for those systems better than netbsd but it is not posix-compatible,"
"background-color","transparency","more overall, using inherit overall, it s better overall,","19879489,56799215,48417391,","you just set the div with border-radius so that it is a circle and background-color is the more transparency color in the circle,background-color is not inherited so it will use its initial value which is transparency that s why you see no color;now if we consider the fallback cases 3 using initial means the initial value of background color so transparency using inherit means inherit the color but there is nothing to inherit so transparency again using unset we will have a mix of inherit and initial the unset css keyword resets a properties to its inherited value if it inherits from its parent and to its initial value if not,your code is redundant with background transparency and with background-color transparency it s better to use however you can act directly to your in your css adding,"
"bugzilla","redmine","more overall,","323542,","i think you ll find that your team will like either trac or redmine more than bugzilla or mantis,"
"blazemeter","jmeter"," services provides a better overall, that is more overall,better overall,general overall,","44964610,41900279,40932322,55341338,","blazemeter uses the jmeter services to record the script just the jmeter services provides a better visual interface,as a load testing tool it s better to use jmeter but in some cases especially when you write tests for ajax-rich applications or when you need to imitate real users scenarios it s necessary to combine advantages of jmeter that is more suitable than selenium for generating load and collecting performance metrics and selenium that is more suitable for interacting with a web page;and of course it s recommended to use blazemeter as a cloud based solution for running tests,why blazemeter is better than jmeter,i don t think you can run the recording using jmeter as blazemeter chrome extension exports recorded scripts in yaml format suitable for taurus tool so you can run it using taurus tool as bzt test-combined-jmeter-and-selenium.yaml if you want to convert it to vanilla jmeter - execute the following command bzt test-combined-jmeter-and-selenium.yaml -gui jmeter gui will open where you will be able to modify and save generated jmeter script as .jmx file which can be executed using jmeter,"
"equals","icomparable","less than or  in object testing greater,less overall,greater in object testing greater,less than  overall,","53718729,1421320,36867352,56462780,","icomparable is for testing for greater than less than or equals to,icomparable is an interface that defines that two instances of the implementing class can be seen as greater than less than or equals to one another,no - icomparable is for seeing if one object is greater than equals to or less than another,in a class named icomparable declare a function compareto public function compareto byval other as object as long must return -1 0 or +1 if current object is less than equals to or greater than obj. must be empty here. end function now you can declare classes that implement this interface,"
"mef","prism"," is not overall, and not overall, is not longer overall,easier overall, create the view overall, uses  unity overall,","7544425,4074938,56226884,16455913,28695389,10909575,","so mef is not the same thing as prism;prism is a ui composition framework which can be used with wpf silverlight wp7 surface and probably with c# based metro style apps for win8,well keep in mind that mef won t be officially part of prism until version 4.0 is released it is in ctp right now but they can definitely be used together;on the other hand mvvm is not strictly part of prism it is possible to use prism and mvvm but it is possible to use prism and not use mvvm and vice-versa,i am converting a prim 6 using mef for ioc to prism 7 unity since mef is not longer supported,mef seemed a bit easier than prism and i started to do a hello world mef app with this tutorial,in this case mef does not participate in the object s life cycle;let prism create the view instance for you so the object can be handled by the ioc container including the part composition and dependency injections,prism uses mef unity or any other inversion of control library to implement composition and dependency injection;mef is newer and easier to work with than unity but it all depends on the templates you decide to use,"
"hebrew","persian","slower overall,","4333757,","i am not sure it can satisfy you but hebrew test is 4 times slower than persian,"
"cairngorm","puremvc"," just doesn overall, isn overall, is the visualbasic overall,","109038,109038,109038,","puremvc is anti-flex cairngorm just doesn t use many of the good parts of flex;by this i mean that puremvc reinvents many things that flex already have because it wants to be platform agnostic and because of its architecture specifically the mediators it makes it harder to use bindings to their full power,puremvc is more invasive than cairngorm meaning that your code is heavily dependent on the framework you have to subclass implement the framework classes interfaces but that doesn t mean that cairngorm isn t,in short cairngorm is the visualbasic of flex it works but will teach you a lot of bad habits;puremvc isn t so bad it just isn t a very good fit for writing flex applications,"
"paxos","raft","slower than  overall,more than standard  overall,better overall, has stronger overall,","54320786,41121070,27558708,53765626,","this behavior is why tcp exists because each individual packet can go through separate routes between servers and there is a high chance of out-of-order messages and most applications prefer the ease-of-mind of a strict ordering. other protocols such as plain old paxos can work with out-of-order messages but are typically much slower than raft,one thing that can be improved from standard paxos also raft zab .. is that there is a distinguished leader which ends up doing more than standard paxos fair share of the work and may therefore end up being a bit of a bottleneck,i couldn t understand why paxos is better for split-brain scenarios or other network failures than traditional bully algorithm because i can easily find out when quorum of nodes leave from the cluster without using raft,dr paxos is optimal but raft has stronger practical guarantees of liveness,"
"interpolation","splines"," here not overall, presents less overall,better overall, but requires more computation overall, is less worse overall,better served by a  overall,crazier than a  overall,","842610,55639793,29103996,25106574,43430497,52410905,52185740,","finding a local maximum of an interpolation splines is an easy enough thing;note that you should generally use a true splines here not a pchip interpolant,i think you re using few values for the interpolation by changing to i get the following and changint the splines degree to i get the following i think a good starting point for the interpolation could be n 2 and as the interpolation presents less data deformation,with a lower order splines that works better but then you lose the advantage of cubic interpolation,i did the math for interpolation with trig functions rather than polynomials as mdurant suggested;it turns out to be very similar to the cubic splines but requires more computation and produces worse results so i won t be doing that,using a zero-padded ifft to upsample produces a similar high quality interpolation except for circular boundary effects;if you have to use 1 of your 3 available methods splines is less worse than pchip as the latter may clip a bit and both of those are less worse than linear,that is you could use to give but for the more interesting case of images like you mention at the end of your question then the axes probably won t be integer multiples and you ll be better served by a splines or other type interpolation,i am quite free in terms of the type of the interpolation but it shouldn t be too fancy nothing crazier than a splines or a low order polynomial,"
"curvycorners","jquery","bigger overall,","7823163,","i know another plugin named jquery corner is available however it s file size being massively bigger than curvycorners i decided to with curvycorners,"
"interstitial","startapp","better overall,","26170889,","imo startapp is better than admob especially when using their interstitial ads,"
"bitblt","stretchblt","x75 faster overall,","23450377,","from my tests bitblt is x75 faster than stretchblt,"
"getline","istream"," extracts one less overall,safer overall,","55995192,46683458,","also your example has name with 20 symbols at 8 line but istream getline extracts one less than the specified,note the use of std getline is safer and more convenient than std istream getline,"
"fossil","git"," is a database overall,easier than  overall, doesn overall,","2120804,4435582,21141068,","from the article the article sounds like fossil isn t a database any more than git is a database,fossil is another one that s easy to use i would say easier than git to learn but it uses an sqlite file to store your code and i m not sure if it scales to really big projects,and another lesser-visible system using real database fossil doesn t have stellar performance pdf either;git started as a minimal set of tools implementing a versioned file system and its author linus torvalds originally envisioned that a full-blown vcs will be a tool based on git,"
"struts","wicket"," style just overall,","17072165,","you could argue that the struts style is simpler and you can do the struts style in wicket too the struts style just isn t optimal but keeping the state only in the server has many advantages,"
"avassetexportsession","avassetwriter"," is probably overall,much easier overall,","6381886,24191989,","you could use avassetreader and avassetwriter to merge the two files but that s probably further along the control power curve than you need;an avassetexportsession is probably a better fit as you can merge audio and video tracks in a few lines of code,this will work directly with avassetexportsession which is much easier than playing with avassetwriter unless you have a very specific requirement typically you need to manipulate the actual video frames,"
"agda","ats","less elegant overall,","10800833,","ats is a dependently-typed language that is designed for low-level programming though it s somewhat less elegant than agda,"
"puma","webrick","also better concurrency overall,","13532865,","puma also has better concurrency but that threadsafe confgi make webrick multi-thread for me,"
"caanimation","uiview"," animation is generally easier overall, is more overall,","23037815,23417599,","uiview animation is generally easier and more intuitive to use than caanimation,if you use caanimation instead of uiview animation you can also create your own custom timing function using the control points of a bezier curve that describes the desired timing function;however caanimation is more complex to use and less well documented,"
"cairo","gdk","more overall,","3221088,","to draw you can use either plain gdk which is more basic or use cairo which is more flexible drawing library nb,"
"datejs","momentjs"," does not currently overall,better overall,","31711854,17263103,","momentjs does not currently have the functionality to interpret a string like that;datejs hasn t been updated in 8 years,you should try momentjs i think its much better than datejs,"
"apache","hiawatha","faster overall,","29657343,","i had the same problem configuring apache until i gave up and change to hiawatha much more easy to configure and according some benchmark is faster than apache,"
"tolower","uppercase","comparison more overall,","2256479,","further uppercase comparison is more optimized than tolower if that tiny degree of performance matters,"
"printf","strcpy","careful with   overall, is more overall,general overall, is clearer overall,faster overall,","52379563,2606557,48020802,54278989,28663679,","be also very careful with printf strcpy strcat .,printf is more robust if you want to format your string;if you only want to concatenate use strncpy don t use strcpy since it s more efficient,i also recommend using printf rather than sequences of strcpy and strcat printf msg user s r n user,the latter might look like this you can use strcpy and strcat instead if you prefer but in this particular case i think printf is clearer and cleaner,at first glance it must be significantly faster because strcpy must be significantly faster than printf,"
"mongojs","mongoose","slower overall,","17690970,","in either case mongoose is going to add a tiny amount of overhead so it s likely to be technically slower than mongojs but not by any meaningful amount like it takes 5.1 seconds to insert 20k records vs 5.0 seconds,"
"permalinks","slug","more overall,","43915592,","i want my permalinks to be accessable via more than one slug,"
"bootcamp","vmware","faster overall,","9422814,","i ve found that bootcamp was slightly faster than vmware non bootcamp image but i still use vmware the majority of the time because i like using the host os for things like mail chat browsing,"
"appfuse","spring","better overall,bring  in to lighten overall,","4370193,3098357,","if you actually need dao s this is something that comes out-of-the-box in appfuse and put partly in spring roo but there is also a great addon for spring roo hades addon that does a great job at this maybe even better than appfuse,once you have that in hand then bring appfuse in to lighten your load;spring does not require that it be in the root directory to work,"
"jinternalframe","jlabel","smaller overall,","8088885,","my problem is this i don t want the jlabel or the jscrollpane to stretch to the size of the jinternalframe if the jlabel is smaller than the jinternalframe,"
"imagemagick","magick++","more correct overall,","7678511,","now i m trying to do the same thing with imagemagick to be more correct with magick++,"
"calloc","realloc"," may not overall, call asks for less overall,easier overall,","10690755,56830622,7762023,","do not free ptr if realloc is successful;some folks may point out that in some esoteric systems calloc may not do what you expect specifically for pointer and floating point types,this is evidently what happened in your example since your realloc call asks for less memory than was allocated by calloc and we can see that it didn t do that here since the address of ptr when free is called is the same as the block allocated by calloc,the various alternatives calloc realloc work roughly the same way calloc is easier to use when dealing with arrays and zero-fills the data while realloc is useful when you need to resize a block of memory,"
"flac","ogg","less overall,","3169917,","i suspect the computational demands of flac decoding are probably considerably less than ogg and if dsp gets you excited taking the time to figure all this stuff out is 100 worth it even if the flash route possibly leads to disappointment,"
"scrollbar","tooltip","text more overall,","20739296,","tooltip text is more scrollbar is displayed to scroll through to,"
"xsl-fo","xslt"," is not necessarily overall, not overall,more overall,","2748392,51487568,512569,","generating xsl-fo and xhtml from xslt is not necessarily an either-or choice;xsl-fo is generally used to generate pdf,you might generate xsl-fo via xslt 1.0 or 2.0 but the table you cited lists functions that you might include in the xsl-fo not functions that you can include in your xslt;note that your xslt is generating html not xsl-fo,all in all t4 to me is more of a inside vs code-generation tool to quickly create c# or other files while xslt is more of a data-processing data-transformation tool to turn data from xml into another format typically html or pdf using xsl-fo,"
"httpclient","jersey-client","easier overall,","5704086,","you can also use jersey-client it is gae compatible and much easier than httpclient,"
"qtwebengine","qtwebkit"," is much slower overall,much worse overall, is significantly faster and more overall,","40436995,41974756,28758928,","regarding rendering of pygal charts qtwebkit does not support all dynamic features while qtwebengine does;the drawback of qtwebengine is much slower startup,unfortunately qtwebengine has much worse and poorer integration with the rest of qt facilities than qtwebkit,i ve also noticed that qtwebengine is significantly faster and more stable that the old qtwebkit based widgets,"
"fadein","removeclass","better overall,faster overall, works a lot overall,","3206128,13281297,51857592,","for some reason fadein works better than show or simply removing a hidden class via removeclass if you take that approach,that s because removeclass is executed faster than fadein method you can remove the class when animation is complete,the reason for this is because the fade effect is not good for fast changes between fadein and fadeout;addclass and removeclass works a lot better for this,"
"colorbox","thickbox","better overall,","6462838,","colorbox is implemented as a jquery plugin which is a bit different and better than thickbox s sort-of-self-contained widget model,"
"charts","pie-chart","larger than your  overall,more overall,","40060306,28714739,","if h is larger than your pie-chart radius you are placing your label outside the charts you should consider a constant r for that with a nice looking line,if you want to do a split slice operation on a pie charts first you need to create a pie charts with slices.here from what i understand you tried to give the option split charts first which actually is to make differrent pie charts in the same row or column which needs more than one pie-chart,"
"onpause","onresume"," is called again in method interact visible, is no longer in method interact visible,more overall,","3287735,10594881,9647275,","if your activity s onpause method is called then you should assume that your activity s onpause method is no longer active and the user cannot interact with your activity s onpause method is called anymore until onresume is called again,in your activity onresume will be called when your activity onresume becomes visible and onpause will be called when your activity onresume is no longer visible,and you should not access vehicledataprovider before onresume in the life cylcle and no more after onpause,"
"cactus","junit","older overall,","7558548,","junit is older and have more extensions dbunit cactus etc,"
"formencode","wtforms","much easier overall,","4610027,","i also recommend wtforms it s much easier to use than formencode and have builtin sqlalchemy extensions,"
"bigloo","ecl","more overall,","9873521,","this one is more for bigloo - is it more functional than ecl,"
"qlabel","qlineedit","better overall,","26849876,","qlabel has no any other unnecessary things so it is better than qlineedit or qprogressbar,"
"jchart2d","jfreechart","way better overall,","24244037,","as a note jchart2d is way better than other libraries as jfreechart for real time applications,"
"svn","tortoisegit","more overall,better overall,","2096222,384102,","the compare dialog of tortoisegit is more about file and with git you have to compare two master branches not just two commits of the same branches both pointing to the same svn central repo,nobody has ever accused git of being easy-to-use no tortoisegit for example although i hear it s coming so you might be better off with svn beanstalk,"
"colt","jama","slower with  overall,","879201,","colt does this already but i have found it is more complicated and more powerful than jama which may explain why simple functions are slower with colt,"
"opentype","woff","better choice than the  in license glober woff2,better compression in license glober woff2,","55821089,29221367,","if you purchase or already have a package with a web font license for glober they should include woff and woff2 files as well which will be a better choice than the opentype font,truetype opentype font that provides better compression than woff 1.0,"
"documentfilter","jtextcomponent","documentlistener overall,","9430683,","for listening changes into jtextcomponent is there documentlistener if you have to need control over inputed char sings whitespace chars or word s you have to implements documentfilter,"
"express","node.js","compatible with  overall,general overall, fiddler shows no more overall, framework now overall,better .the overall,new beginner with  in sorry beginner new,new with  overall,new with  in sorry beginner new,","50274228,57547911,54396522,49635168,19130458,56981150,52249744,57342376,","i have read that express 4.x is not compatible with node.js native http2 from 8.4+ and i was hoping for more progess on express 5.x than it has. but as i started thinking that express5.x will probably be released to late for my next node.js project - i came over nest.js,there are many different ways to do this but i would recommend using node.js + express for middleware and mongodb for your database,now want to display some dynamic message after receiving upload so i tried hbs not express-handlbars or express-hbs template prompts for file ok but always got app structure app.js index.js index.hbs already checked other so including cannot post error using express and cannot post form node.js - express fiddler shows no more than status 404 and cannot post,the express framework is unique among http frameworks in that the express framework gets smaller each release;one of the parts that was removed from the core is body parsing so the express framework now handles request bodies as base node.js does unless you add middleware,first its not a bad idea to use node.js to satisfy your requirement the uniqueness of node and added advantage compared to conventional technologies like apache-php is clearly explained in this following video why node.js is better .the notion of node being just used for advanced or complex applications is wrong you can use node for building applications which are very simple to anything of huge proportion.it would be a not so comfortable task for a node beginner to code without using any frameworks but by using this web application framework called express you need not code from scratch and its very easy to setup html pages using its simplified html templating language called jade and route requests also you can even use your html pages if you are not willing to use jade for rendering the views of your application .here are the series of tutorials which would help you tutorials on beginning with node.js basics of expressjs,i am an extremely new beginner with node.js and express,i m pretty new with node.js and i m trying to implement simple user registration and login form using node.js express bcrypt express-session and mongoose,sorry i m pretty new with node.js and express,"
"qfile","qtextstream","more conveniently overall,","10093960,","a qfile may be used by itself or more conveniently with a qtextstream or qdatastream,"
"cobertura","eclemma","better overall,","11801652,","also forgot to mention junit testing framework and a great eclipse plugin eclemma for code coverage much better than cobertura ecobertura and easier to use and of course you can try maven as the other post mentionned or try ant as build tool,"
"audiorecord","mediarecorder","lower level overall, object then overall,","4059119,56855744,","this class leverages audiorecord which is a lower level approach to using mediarecorder,audiorecord does take longer to understand and implement though if you want to stick with a mediarecorder object then i recommend using aac_adts as the audio encoder and allowing the outputformat to be three_gpp,"
"facebooker2","koala","better in discussion facebook graph,better in discussion facebook graph,","5201993,5453213,","is fb_graph or koala gem better than facebooker2,relevant discussion here as well is fb_graph or koala ruby gem better than facebooker2 using the facebook graph,"
"info.plist","ipa","more overall,","32953147,","we are having an issue with uploading ipa files to our mdm solution when there is more than one info.plist present in the ipa file,"
"selectedindex","selecteditem"," is not explicitly overall, doesn overall,more overall, aren overall, shouldn overall,","48050657,12059546,27923683,14269667,6421602,","when the user selects something the selectedindex is set directly invalidating it and changing it which causes the selecteditem to become invalid;however the selecteditem is not explicitly changed until it is recomputed,after debugging when the field is added back to the list of fields it still retains the previous selecteditem value - and then the combobox s selectedindex is immediately set to -1;preventing this in the setter for formfield.selectedoperator and by trying selecteditem selectedindex doesn t help,i ve found that setting the selecteditem is more reliable than setting the selectedindex,the symptoms of the bug are that selectionchanged doesn t fire selectedindex selecteditem aren t reliable and that back navigation into a page with panorama resets the panorama selected item;for example the following code sample will never fire the messagebox and selectedindex selecteditem won t indicate the correct expected values,things like selectedindex are specific to the way the data is shown in the ui if you change the sort order of a datagrid then the selectedindex can change even though the selecteditem is still the same;in this particular case the selecteditem can be bound to the viewmodel but the selectedindex shouldn t,"
"daemons","systemd"," or not overall,safer overall, is much easier overall, itself cannot overall,better solution than writing  overall, does not in script fastcgi init, time ever overall, allow open file overall,less than the  overall, should generate the status overall, is better overall,","23478172,2277474,31220135,38831955,54905061,13183747,53509483,2088188,27788311,55755136,40925645,","and whether the app is a daemons or not is also irrelevant;what s relevant is whether the operating systemd considers your device to be a keyboard or not,use daemons mode and then simply touching the wsgi script file when an atomic set of changes have been completed isn t that hard and certainly safer than a systemd which restarts arbitrarily when it detects any single change,as a new newbie programmer i find that making daemons over systemd is much easier than over init.d,if that computer needs to be encrypted for security reasons then you re not going to be able to have your daemons load immediately;that computer s systemd volume is encrypted with filevault 2 which means that the operating systemd itself cannot start until one of the users supplies their password which is needed to derive the volume encryption key,systemd if there is better solution than writing daemons please let me know,maybe the shell used by the daemons does not recognize the construct;perhaps when run from a deamon the systemd command is using a different shell than the one used when you are running as yourself,current systemd uptime is available and i d expect the software watchdog -- the daemons -- to be able to handle changes with the wall-clock time without reboot the systemd;first off the watchdog daemons shouldn t be updating the systemd time ever and that would include prior to causing the systemd to reboot,alternatively you could further raise the technological stakes by having the daemons return not the password but rather the open socket ready to be wrapped in a db-api compliant wrapper;some unix systemd allow open file descriptors to be sent between unrelated processes a prereq for this approach -- and of course you d have to substantially rework the mysql-based db api to allow opening a connection around an already-open socket rather than a freshly made one,if you want the systemd to still be responsive just configure slurm to let the systemd believe the systemd has 1 core and 2gb of ram less than the systemd actually has to leave some room for the os and the slurm daemons,in general systemd doesn t send a message to your daemons;your daemons should generate the status notifications every time the internal status changes,restart docker daemons;on unbuntu 16.04 check also if configuring docker through systemd is better,"
"daemons","systemd"," does not overall, will spawn another process in script fastcgi init, which provides a better overall, doesn overall,","46682927,8886805,44866334,4169523,","change it to simple if your daemons does not go this way;for systemd assume the child or child of child of the start command is the daemons process,but pay attention in case you use that init script to start up the fastcgi daemons do not use any init tools like service monoserve start rhel centos or rcmonoserve start;for me this will not work i suspect the init systemd will spawn another process with different environment variables,also ensure you are using daemons mode of mod_wsgi for running the django application as that is a better architecture and provides lots more options for setting timeouts so you can have your application recover from various situations which might otherwise cause the server to lock up when overloaded;for a systemd which provides a better out of the box configuration and experience than using apache mod_wsgi directly and configuring it yourself look at using mod_wsgi-express,in my case i have the daemons doing more than 1 thing so it s very helpful to have systemd keep track of all the timers for me so that i can effectively poll stream_select for the xmpp connection;i m actually work on something very close to what you described but in my case the daemons doesn t poll for event it get s them asynchronously via xmpp but that s besides the point,"
"ivar","redeclaration","cleaner overall,","13670789,","this redeclaration allows a cleaner way to access and mutate the property internally without resorting to fragile ivar synthesis which is becoming an antiquity now that the compiler does it for you,"
"lisp","rebol","more overall,","42854953,","aside does this mean rebol is more homoiconic than lisp,"
"uiview","uiwindow","more overall, has and more overall, is not overall,","26706709,7545483,44150025,","uiview is more generic than uiwindow and should be preferred,a uiwindow has everything a uiview has and more,uiwindow is not regular view and has its own implementations of methods from uiview s interface;each regular uiview including the main window s root view should have a parent view,"
"jmockit","mockito","older library overall, uses cglib overall,better overall,","7717332,14244931,5418052,","mockito s a much older library than jmockit so you could expect that it would have many more features,that mockito cannot bypass means mockito isn t currently able to overcome final classes or final methods in order to mock them because at moment mockito uses cglib to generate subclasses of the type to mock;but other frameworks like powermock or jmockit may be able to do it as they have other strategies to overcome this,comparison between mockito vs jmockit - why is mockito voted better than jmockit,"
"viewdidappear","viewwillappear"," is better overall, is not overall, is an even better overall,general overall,more overall,","22212079,30351776,8311518,52101495,4297436,","use viewdidupdatelayout or viewdidlayoutsubviews if you need to perform layout adjustments in your controller but viewwillappear doesn t work for a weird situation such as a timing issue use viewdidappear is better done in a uiview subclass in my opinion,you should either use viewdidappear or add a slight delay before displaying the alert controller;viewwillappear is not a good location to present another view controller since the current one being presented is not yet in the window hierarchy and is also in a transition animation,if you put your code in viewwillappear animated this should work;edit as per saphrosit s comment viewdidappear is an even better place to do this,edited after some development and many bugs i finished with this elegant solution that i am sure apple would not be happy to approve but i do not know that works from ios 10 to ios 12 when the searchbar is in the tableheaderview the above code can be called in viewwillappear but if it is in the navigationitem on ios 11 and above it should be called in viewdidappear,in this case viewdidappear behaves more like viewwillappear ..,"
"codeception","phpunit","compatible with  overall,more elegant overall,","49059726,25675213,","update 3 i can now confirm that phpstorm v2017.3.4 is not compatible with codeception 2.4 because the later after v2.4.0 moved to phpunit v7.x which phpstorm v2017.3.4 seems to not be compatible yet,1 split those tests to unit and acceptance and use a tool like codeception to help you do acceptance which is way more elegant than phpunit for this kind of test,"
"libc","tcmalloc","better overall,","31078698,","the tcmalloc library for example can be easily inserted into an application to evaluate performance gains in heavily threaded applications where tcmalloc tends to perform a lot better than libc s malloc implementation,"
"csv","tsv"," is not just overall,more overall,better than  in insert statements way, is better way. in insert statements way, is much better in insert statements way,","33728487,44288514,55022685,55811455,52870473,","tsv is not just csv with tab delimiters it s all good if in your case it works;just keep in mind trying to read a tsv using a csv parser is a bad idea as characters such as n or t may be escaped as actual sequences of and n,also your csv looks more like a tsv tab separated instead of comma separated,going from there through the excel 95-2003 format option might work better than csv or text tsv,then choose export to sql insert statements format but imho csv tsv is better way.,csv tsv is much better in that respect as the headers only appear once at the top of the file,"
"mysql-workbench","phpmyadmin","more advanced overall,","4739146,","i think you should install phpmyadmin on your server this will allow you to access your database from work school cafe etc mysql-workbench is more advanced and gives you more features so you can deal with changing the structure and editing any rows columns relations and much more look at phpmyadmin s features it has most if not all,"
"arc4random","srand","much better in seeds worse properties,worse properties than  in seeds worse properties,","4253674,30765702,","from what i can gather arc4random generates much better random numbers than rand does however i haven t seen a way to seed it and i would like to just like using srand,first srand seeds the rand function which is a different pseudo-random number generator with somewhat worse properties than arc4random,"
"cefglue","cefsharp","more difficult overall,","28732691,","however integrating cefglue into your winforms or wpf app may be more difficult than cefsharp,"
"caliburn","prism","more overall, is especially overall,","12522366,5825333,","prism is more of a modularity framework and caliburn is more of a mvvm framework i know both have overlapping features but it would be great to take best features from two frameworks when they are combined,for smaller apps caliburn micro provides a simpler framework;to me prism is especially useful when you need to define regions in your application,"
"datatables","tablesorter","simpler overall,","12522533,","one thing you could look into is using tablesorter - which is much simpler than datatables - with a drag and drop plugin like sortable for jquery ui or this,"
"richtextbox","textblock"," not overall,simpler overall,","40093737,1287034,","update i think you originally were talking about a textblock not a richtextbox;if the solution absolutely requires a richtextbox you ll need to look into finding a usable rtf parser somewhere,good advice but in the end i decided a bindable textblock was more useful and simpler than a richtextbox,"
"thin","webrick","slower overall,","1267852,","from my experience webrick is slower than mongrel is slower than thin,"
"accessor","ivar"," directly is faster overall, is faster overall,faster overall,","14113098,6791026,16664597,","technically accessing the ivar directly is faster than using accessors but there are very few situations in which it will make a significant performance difference and would probably be a case of premature optimization,that is one possible reason for the sample code to eschew accessor in favor of direct access but precisely why it is done in any given case is not necessarily easy to know;another possible reason is that accessing the ivar is faster and on ios in particular cycles are hard-won,nonatomic properties don t use locks but direct ivar is faster because it skips the accessor call,"
"avr","microcontroller"," is much easier and cheaper overall, will not overall,cheaper overall,","12380215,51637376,4156810,","i don t know the criteria you chose the target processor on but consider using an avr 8-bit microcontroller if the project don t need the arm m3 32-bit processing power;avr is much easier and cheaper to start with especially for beginners,if the avr fuse bits that specify its clock source get corrupted and the avr expects you to connect an external clock or crystal but you do not have such a clock or crystal in your circuit then the avr will not have a clock signal and you be unable to program it;luckily there is actually a way to revive such avrs you can get another microcontroller to generate a pwm signal and apply it to the xtal2 or xtal1 pin of your avr as a low-speed clock signal 100 khz,i see that the microcontroller used in commercial devices are much cheaper than the avr or pic,"
"mbprogresshud","uialertview","more features overall,","9816661,","mbprogresshud has more features than uialertview so it might be better suited and more easily adapted for your purposes,"
"crypt","md5"," doesn overall, is no longer overall, value is much shorter overall,better solution overall,","8331291,13102496,54695615,19040830,","unix md5 crypt doesn t use plain md5;that would be insecure because plain md5 is fast and password hash should be slow,note that you may also want to verify what encryption algorithm is actually used when you call crypt;md5 is no longer secure and it can be broken with a good gpu no typo you can use your graphics card to break a hash,then i filled it with a md5 encrypt model- id the md5 value is much shorter and because it also uses laravel s crypt it can t be guessed,i should also mention that crypt has this built-in and may be a better solution than md5,"
"antivirus","norton","better overall,","215539,","many other antivirus software that works just as well if not better than norton like avg avast and others do allow you to uninstall from the add remove,"
"xlsxwriter","xlwings","way faster overall,","43527906,","i know that xlsxwriter is way faster than xlwings and have functionality to add border but it is just a writer it can t update already existing spreadsheet,"
"avassetexportsession","avassetreader","slightly more performant overall,","36726335,","in my experience avassetexportsession is slightly more performant than using avassetreader and avassetwriter for a straight forward format a - format b type conversion however that said it s probably not by enough to be too concerned about,"
"bufferedwriter","fileoutputstream","even worse overall, isn t the problem overall,","6976914,23674340,","for a few bytes passed in just one call probably the bufferedwriter is even worse because it problably later calls fileoutputstream,again fileoutputstream has a constructor allowing you to append instead of overwriting;the bufferedwriter isn t the problem - it s the way you re constructing the filewriter,"
"xlsb","xlsm","faster overall,","39221744,","but i heard that xlsb loads way faster than xlsm so i would not like to change the fileformat,"
"imagemagick","pillow","better performance than  in comparisons simd fork, pil doesn overall,better coverage than  overall,better performance in comparisons simd fork,","37254177,37857677,56408567,37254177,","maybe worth checking out how a simd fork of pillow which claims to have much better performance than imagemagick or plain pillow compare,it would be interesting to know if imagemagick is using this library under the hood or has its own implementation of the standard;unfortunately pillow pil doesn t understand the smpte digital picture exchange format yet,imagemagick might be another good choice to try because it probably has better coverage than pillow does,there exists a simd fork of pillow which claims to have much better performance than imagemagick or plain pillow but there are no comparisons to opencv,"
"argumentexception","invalidoperationexception","more appropriate overall,","16434842,","i think the argumentexception is more appropriate because the invalidoperationexception documentation says that the object itself on what the method is called has to be in an invalid state right,"
"autoboxing","casting","cheaper overall,","10019000,","is that casting from long to int plus sparsearray optimizations are going to be cheaper than autoboxing long to long for my hashmap operations,"
"factories","presenter","lighter overall,better overall,","14440549,32147921,","which is basically the same thing as the factories then depend on sub-factories but at least they are lighter than presenter views services and they don t need to load the sub-factories until they are needed,a presenter is better to accept some kind of a view factories or a supplier in general as we can already hold an existing view especially in a gwt case and let the presenter decide when instantiate or accept the view,"
"jsr168","jsr286","impossible with  overall,newer overall,","23373326,9036984,","as the previous poster points out this would be easier with jsr286 portlets wef and non-wef as the eventing is standard but jsr286 portlets wef and non-wef as the eventing is standard s not impossible with jsr168 if you re limited to that version in the short term,the posts you see seem to be logical as jsr286 is a newer spec and there are some features that make jsr268 portlet not a jsr168 portlet,"
"chown","umask","better overall,","2561800,","but changing the chmod maybe even working with umask is better than dealing with root-access and using chown,"
"libc++","libstdc++","more complete overall,compatible with  in different libraries binary, is a newer overall,more overall, which are completely in different libraries binary,simple program with  overall, doesn overall, is more overall,","17901868,48079189,49353557,12574479,53445795,51137052,46766548,16788372,","the above links to libc++ which is the preferred c++ standard library for mac os x with clang and a lot more complete than libstdc++ even when considering the newest gcc,here libc++ is not fully binary compatible with libstdc++,that s a bit confusing with clang on the mac because there are two of them libc++ and libstdc++;recent versions default to the former libc++ is a newer implementation built by the llvm clang team to work better with c++11 and later,libstdc++ does this a lot more than libc++ and the former is the default library for llvm-g++ gcc llvm the latter for clang apple llvm which means a lot of errors will seem to go away when you switch to gcc llvm or just stick with apple llvm and switch your library but your code is still wrong,they are very definitely not compatible between libc++ and libstdc++ which are completely different libraries with completely different implementations,i have a very simple program with libc++ i get the following last line good 1 fail 0 bad 0 eof 0 expected or with libstdc++ good 0 fail 1 bad 1 eof 0 i have tested on osx with xcode 9.4.1 or on linux but always the same,gnu libstdc++ and llvm libc++ are two different implementations of the standard c++ library;apparently your libstdc++ doesn t yet support the coroutines ts so you have to stick to libc++,libc++ is not 100 complete on gnu linux and there s no real advantage to using the native library when libstdc++ is more complete,"
"matlab","s-function","faster in computing-intensive solution embedded, function is faster less in computing-intensive solution embedded, is newer overall,","20815265,41376419,33988085,","my c s-function is faster than my embedded matlab function block in matlab environment but when i use it in rt-lab the embedded is faster,however the matlab function is faster less computing intensive when the same time step is employed;maybe the least computing-intensive solution is a c s-function,it is possible that your version of matlab is newer;than the version of matlab used to produce the s-function,"
"tapestry","wicket","more powerful overall,recommend checking out  overall,","2556364,844519,","in my opinion this is a more elegant solution than jsf and i think tapestry s ioc container makes it more powerful than wicket,beyond ones already suggested i would recommend checking out tapestry and wicket many developers like tapestry,"
"inorder","postorder","trickier overall, walk using the iterator overall,","38729809,338815,","postorder is trickier because the stack has to store nodes to visit and nodes to process and they aren t always simply related like they are in the inorder case,however the other posters are right java doesn t expose any of the tree mechanics so a preorder or postorder isn t possible at this view;you can at least do the inorder walk using the iterator and a for each loop,"
"fancybox","thickbox","smaller overall,","3458145,","i m using facebox and fancybox both smaller than thickbox,"
"gridview","radgrid","heavier overall,","7382971,","each tab contains multiple radgrids as radgrid is heavier than asp.net gridview,"
"tidekit","tidesdk","better overall,","24966264,","here is the blog post about the comparison how tidekit would be better than tidesdk,"
"adventureworks","northwind","better overall,","597847,","talk about sql server 2005 adventureworks is better than northwind,"
"delimiter","linefeed","longer string overall,","8644931,","i ve taken the example of two linefeed here but the same is true for any character string i could subvert the problem by maybe taking a longer string as the delimiter but that would have two undersirable effects,"
"modulo","multiplication","lower precedence in operator higher lower,more than  overall,slower overall, p is gp p overall, is division overall, not overall, has a higher in operator higher lower,","45400129,56929261,45747034,31223414,40869328,51802000,22178187,","it appears that you consider modulo to have lower precedence than multiplication and division when in fact it does not,i have also heard that the complexity of a modulo is usually slightly more than multiplication,note that is equivalent to i 10 but much faster since modulo is around 10 times slower than multiplication,if you want to mimic the original ideal of multiplying by a prime the obvious generalization is to do arithmetic in the galois field gf 2 8 - see and note that you can essentially use log and antilog tables of size 256 to replace multiplication with not much more than table lookup -;arithmetic over a finite field of any sort will have many of the nice properties of arithmetic modulo a prime - arithmetic modulo p is gp p or gf p 1 if you prefer,before this let s examine the myth of sqrt being expensive but a large bunch of multiplication is not;first of all let us note that sqrt is not that expensive anymore on older cpu-es x86 32b it used to be twice as expensive as a division and a modulo is division on newer architectures the cpu costs are equal,here multiplication done by summation;if modder 1e12 then your modulo not work,he line works because of integer division the line works because of modulo;the line works because of operator precedence multiplication has a higher precedence than addition + so it will be done first,"
"haskell","idris","probably more overall, uses the broader overall, with greater overall,","41109958,48817691,32538122,","in dependently-typed languages like idris it s probably more useful than in haskell,if a is false then nothing is said about b which seems right for case expressions and may mean the same as or it may have the meaning for functions given below which is means the function f maps the set x into the set y so my guess is that idris just uses for the narrow second meaning for mapping one type to another in type signatures whereas haskell uses the broader interpretation where it means the same as,unfortunately haskell doesn t yet have proper support for dependent typing and existing workaround solutions involve some boilerplate and complications;idris is a language with haskell-like syntax and full dependent types so i can illustrate the idea in idris with greater clarity,"
"domcontentloaded","onload"," handler has a better overall,event longer in preferable longer body, is less in preferable longer body,","9021564,9078062,24677242,","the onload handler is not exactely equal to the domcontentloaded handler but the onload handler has a better support and may be preferred here,sometimes it s preferable because onload event takes longer to fire then domcontentloaded,remove the onload from your body element using domcontentloaded event instead the onload is less intrusive,"
"codeblocks","codelite","better overall,","3067921,","we sort of settled on it but later found codelite and liked it better than codeblocks,"
"leaflet","openlayers","general overall, which is more overall, both do the job overall,more experience with  overall,more lightweight overall,","11744855,50151182,19711483,49575972,13807524,","here are several javascript libraries that you can use to draw maps in a widget on your webpage but the most common are openlayers and leaflet;each of these libraries has a basic tutorial showing how to use each of these libraries to draw a map on a page though leaflet s is nicer and friendlier,i m trying to use openlayers with angular 5. i m testing different ways to implement maps i ve already tested leaflet and openlayers in simple html files and i ve choosed to use openlayers which is more efficient in my case,openlayers and leaflet both do the job well;leaflet though is a bit easier to grasp if your new to the topic,so if someone has more experience with openlayers and leaflet i hope could help me to choose the best option,while leaflet aims to be more lightweight than openlayers openlayers is by far the more mature proj,"
"cat","tee","then more data overall, command has a higher overall,","25335034,39426715,","tee then writes more data t and tries to read from the pipe then goes to sleep until cat writes more data,redirection to the same file will create truncate the file before cat command is invoked as cat command has a higher precedence;you could avoid the same by using intermediate file and then from intermediate to actual file or you could use tee like,"
"openpgp","x509","simpler overall,","2282055,","openpgp is simpler than x509 but more limited especially if you wish to have a strong legal meaning behind the certificates,"
"cas","shibboleth"," makes more sense overall,more similar overall,","6150664,38311403,","the first shibboleth makes more sense to compare central authentication service cas with security assertion markup language saml which is the protocol used by shibboleth,conceptually it seems like a type of central authentication system cas but is more similar to shibboleth also a federated identity management system,"
"chameleon","jinja2","slightly faster overall,","5324691,","template engines are rarely the cause of performance problems even if chameleon is slightly faster than jinja2 i doubt the effort of learning a new template language etc,"
"gridbaglayout","springlayout","robust than  overall, is more overall,","10837003,2889354,","not many layout problems can t be conquered using it and it easy to understand as opposed to springlayout groupedlayout etc and far less code and more robust than gridbaglayout,the springlayout has an example that does exaclty this;the gridbaglayout is more difficult to use but also support row column type layout,"
"dotcover","ncover","less expensive overall,","4006956,","as of now i think dotcover and resharper combined are less expensive than ncover,"
"jpype","py4j","bigger in discussion performance solutions, has bigger overhead in discussion performance solutions,","18484879,47956518,","in terms of performance py4j has a bigger overhead than both of the previous solutions jython and jpype because it relies on sockets but if performance is critical to your application accessing java objects from python programs might not be the best idea,you can see the discussion here py4j has bigger overhead than jython and jpype,"
"qlistwidget","qtreewidget","more involved overall,","23836692,","posting here an example showing how to implement same approach but now applied to qtreewidget which a bit more involved than qlistwidget,"
"wsdl2java","wsimport","better overall,","12248473,","wsimport is better don t use wsdl2java,"
"achartengine","jfreechart","more features overall,","13732689,","android plot and other free chart solutions mentioned here doesn t support annotations the only one is afreechart which is a port of jfreechart for android i am currently using it and it is awesome and has much more features than achartengine,"
"xts","zoo","faster than  overall,","38492601,","however there are many others ts xts which is generally faster than zoo .,"
"cakephp","phalcon"," is more overall,","32407950,","phalcon is more powerful verstile but to get started with it i feel you have to be a better php developer than you do to get started with something like cakephp,"
"celementtree","lxml","slower than  overall,such as  overall,faster overall,","3605831,2360509,22295513,","as a side-effect of implementing proper parent traversal lxml is a bit slower than celementtree for parsing,object models such as lxml s objectify or amara might be more natural for python developers when speed is not a consideration;celementtree is faster when only parsing is required,instead doing it with lxml which i found to be the fastest somehow even faster than celementtree,"
"cudnn","tensorflow","more over  overall, encountered error overall,older overall, isn overall, not overall, does not currently overall,","55214879,51006842,43243162,36870968,48566863,42680925,","because newer versions have some conflicts and more over tensorflow requires the exact version of cuda and the cudnn library for it to work properly,this will cause a build error in future versions error error loading package tensorflow encountered error while reading extension file cuda build_defs.bzl no such package local_config_cuda cuda traceback most recent call last file tf-persist third_party gpus cuda_configure.bzl line 1166 _create_local_cuda_repository repository_ctx file tf-persist third_party gpus cuda_configure.bzl line 1014 in _create_local_cuda_repository _find_libs repository_ctx cuda_config file tf-persist third_party gpus cuda_configure.bzl line 667 in _find_libs _find_cuda_lib cudnn repository_ctx cpu_value cu... ... file tf-persist third_party gpus cuda_configure.bzl line 587 in _find_cuda_lib auto_configure_fail cannot find cuda library s ... file tf-persist third_party gpus cuda_configure.bzl line 210 in auto_configure_fail fail n scuda configuration error ... cuda configuration error cannot find cuda library libcudnn.so.7 if it s not clear i m not very familiar with bazel,currently tensorflow supports the older cudnn v.5.1 while there is a newer cudnn 6.0 available on nvidia site,therefore tensorflow won t attempt to call cudnn when training this model;i m not sure that tensorflow will automatically fallback to a slower convolution algorithm when cudnn isn t available,according to the tensorflow install documentation for version 1.5 cudnn must be installed for gpu support even if you build it from source;there are still a lot of fallbacks in the tensorflow code for the case of cudnn not being available -- as far as i can tell it used to be optional in prior versions,tensorflow does not currently use nnpack to accelerate its math kernels;instead it uses a combination of eigen tensor cudnn cublas xla-generated and hand-written kernels,"
"pyenv","virtualenv","global than  overall, is more robost and easier overall,general overall, doesn overall, makes switching between they really overall, and installed python2-pygame overall, --no-site-packages isn overall,wider scale overall,","46344026,54361347,54969970,56124867,56523368,37521232,23129099,46344026,","python versions you install using pyenv go into it directory by default pyenv root is .pyenv so are more global than virtualenv,pyenv is for installing and managing different versions of python so you might not need it;this way your virtualenv is more robost and easier to get up and running if you move it around or share it with others,and by the way python doc recommends using virtualenv because pyenv was deprecated,or you can just install python from source to somewhere under your home dir -- but pyenv makes it easy to switch to that installation and back.;a virtualenv doesn t have a full python installation,i would highly recommend using pyenv to manage multiple verions of python and pyenv makes switching between they really easy both locally and globally and use pyenv in combination with a virtualenv per project you are working on,i have created a 2.7.11 virtualenv using pyenv and installed python2-pygame with pacman;with my virtualenv activated i couldn t import pygame from python,you can remove pyenv and install latest version of virtualenv and ask virtualenv to create the env for you by this command;if you re using the latest version of virtualenv --no-site-packages isn t necessary anymore,pyenv operates on a wider scale than virtualenv - it holds a register of python installations and can be used to install new ones and allows you to configure which version of python to run when you use the python command,"
"opera","safari"," window is closed don in ctrl_q alt_f4 ctrl-q, window is closed don in ctrl_q alt_f4 ctrl-q, is even worse overall, to test right now overall, do support mp3 however overall, is not yet implemented successfully overall, but not overall, is lower overall,slower overall,smaller overall, doesn overall,","14552933,14870798,1947902,13079078,2233311,14853818,25489717,3563703,8993541,13137862,894539,","or opera safari will not pass those events to the dom at all;so if you bind ctrl_q or alt_f4 and your safari opera window is closed don t be surprised,or opera safari will not pass those events to the dom at all;so if you bind ctrl-q or alt-f4 and your safari opera window is closed don t be surprised.,i found that on google-chrome safari and opera many special keys don t get passed through to ajaxterm including backspace the arrow keys ctrl+c ctrl+h home end etc;opera is even worse when you press shift the keypress gets translated into p so you can t type capitals,edit it does come with enough information to know it is an ipad i myself have a project that matches ipad in the user-agent string and works for google-chrome safari and i don t have opera to test right now but i guess it works too anecdotal on no complaints;safari is not the only browser in the apple devices,well firefox and opera do not support non-free codecs such as mp3 as with the opera 10.5 alpha firefox 3.5 and later supports only pcm wav and ogg vorbis for audio;i believe google-chrome and safari do support mp3 however,so i end up forgetting selenium test for safari and just go with others google-chrome firefox ie and opera has limited support too you cannot maximize it for now;i seems as though selenium support for safari is not yet implemented successfully and is still experimental,i ran into this issue and found that the problem was google-chrome opera and firefox safari have different tolerances for creating a new javascript date object;this works in google-chrome and opera but not firefox and safari,opera is lower at 30 cookies;safari 3 has no apparent cookie limit,incidentally the scrolling on the windows version of safari is fine albeit a little slower than ie opera and firefox,works in google-chrome firefox a bit weirdish blur effect on hover opera ends look smaller safari ends look smaller,google-chrome and opera have this feature;ie doesn t apple safari doesn t either,"
"opera","safari"," works perfectly; overall,familiar with  overall, 11.10 has partial support overall,","7162038,47824203,768287,","macosx 10.5 firefox v5 and v0.8 opera works perfectly;safari 5.0.6 doesn t work at all,i m not familiar with safari or opera but users of those browsers are strongly advised to check whether or not similar support issues apply when using pointer events,opera 11.10 has partial support for fileapi;safari - i couldn t find a good official source for this but this site suggests partial support from 5.1 full support for 6.0,"
"combinators","fold","more complex overall,","7533837,","i m also looking for more examples and explanations of complex combinators more complex than fold in common programming languages,"
"octree","quadtree","better overall,","2856593,","quadtree is better for big open spaces and octree is better for in-door spaces with many levels,"
"many-to-many","many-to-one","much more complex overall, side is much more overall,","24982430,25251777,","where instead of expected many-to-one is much more complex and partially expressed many-to-many,t makes no sense for many-to-one or many-to-many since you don t want to delete a parent whenever the child is removed;inverse is for bidirectional associations and most often the child s on the same side with cascade but that s because the many-to-one side is much more efficient to control the association than the one-to-many one,"
"box2d","scene2d","more overall,","33025970,","so basically when box2d is more about how objects will behave themselves during application running the scene2d is more about how you write your code before application running,"
"m2crypto","openssl","no longer maintained overall,","18808106,","m2crypto is no longer maintained and doesn t work with openssl 1.0 and newer,"
"asp.net-mvc","html.actionlink","more overall,","3056869,","i am using asp.net-mvc 2 and i see more than one html.actionlink method that takes an idictionary as a parameter for the html attributes,"
"qt-designer","wxpython","more overall,","515325,","as far as i can tell qt-designer is more powerful than any wxpython counterpart like boa constructor and pyglade,"
"ejml","jama","faster overall,","10110317,","ejml looks really good it works almost 2x faster than jama on my data.,"
"hash-collision","key"," or not overall,space vastly larger overall, being hashed aren t random overall,","30227242,3114315,56261680,","the trick is to use objects instead of strings as key and to override the hash comparison see how hash-collision are resolved in python dictionaries;the comments below debate the technical aspects of whether these are duplicate key or not but this is a side issue,but if your key space is vastly larger than the number of targets you ll have a sizable number of hash-collision where you ll have to check if the target stored there is really the key you re looking,exactly what ratio depends on how hash-collision will be handled - namely either alternative buckets will be found open addressing aka closed hashing with a good hash function 20-50 more buckets than key is a generally sane range each bucket contains some chain of elements that hashed there separate chaining with a good hash function it doesn t matter as much so you could have half as many buckets as key or twice as many and things will rock along without any great drama that said when the hash function isn t good and the key being hashed aren t random enough to help the hash function perform adequately it helps to have a tablecapacity that reduces collisions try any prime number around the value derived from the number-of-keys-being-hashed and the ratios listed above,"
"caffe","mxnet","use larger overall,","39544441,","i like caffe but the amount of gpu memory caffe use is larger than mxnet i test in resnet-50 with mxnet-memonger,"
"qgraphicsview","qwidget","sometimes faster overall,","33046694,","i ve read that painting to a qwidget is sometimes faster than qgraphicsview but it would by a lot of extra work for the mouse handling i think,"
"stringbuilder","stringwriter","more versatile overall, not overall,","31559640,20522095,","building on the previous good answers stringwriter is actually much more versatile than stringbuilder providing lots of overloads,usually the choice is between string and stringbuilder not stringwriter;stringwriter is a character output stream and i suppose is not what you need,"
"presentationml","spreadsheetml","more similiar overall,","2481595,","the one thing to keep in mind is that spreadsheetml is more similiar to presentationml than it is to wordprocessingml in file structure inside the package for every sheet there is a seperate file,"
"lxml.html","pyquery","much faster overall,","16679703,","there is a project pyquery that is much faster as it uses lxml.html library speed comparasion can be found here,"
"findviewbyid","oncreate"," will not overall,earlier overall,","6417334,15314477,","oncreate is the right place to put them;in the constructor your object does not yet have a context so findviewbyid will not work,you can t findviewbyid earlier than oncreate,"
"onchange","onclick","more reliable overall, is a better overall, is more overall, which isn overall,more meaningful overall,","6817388,2267285,5575414,53538520,43392549,","here s something that should get you started - this is all based on the assumption that there are only 3 checkboxes on your whole page and that you re interested in all of them - you ll want to make a method like this respond to your checkbox elements onclick events - i ve found that to be more reliable than onchange,when settting the checked state programatically the onchange handler never fires in those browsers;i concur with pointy that onclick is a better way to handle checkboxes if you want to avoid surprises,internet explorer only fires the onchange event when the checkbox loses the focus onblur;so onclick is more of a cross browser solution,you need to assign the click event to card like check this answer for more details applying classname onclick onchange not working on custom react component in general if you need to pass on all the props passed from parents you can use the following syntax destructuring all the props would ensure that the props are passed and this you won t need to do it manually for each prop;onclick needs a function the way you are using it you are assigining the returned value of imageselected to onclick which isn t a function and thus on every render it is executed,checkbox onchange should be more meaningful than onclick,"
"m3u","mp3","file nothing more overall,","3107988,","an m3u file is nothing more than a text file listing mp3 and or other format digital audio files to be interpreted by player software as a series of audio files to be played in succession,"
"crypt","salt"," is always overall, is much safer overall, manual contains more overall, are not actually overall, is not specified then overall, md5 is a better overall,much easier overall,","16743563,4196925,8110300,15727563,19065618,317990,37308056,","heck-out crypt as well;using a salt is always good because a salt makes a salt harder to find the original password reverse engineer,in modern applications website password aren t stored using crypt;using the hash value with a salt is much safer if someone breaks into your db no password is there,to call crypt with the blowfish algorithm use a salt that begins with followed by a number the cost parameter between 04 and 31 followed by a and then 22 digits from the alphabet . 0-9a-za-z;the php crypt manual contains more details on how to use crypt,the actual salt is 128 bit but the encoded salt in the crypt format is 22 character 8 bit character 3 4 132 bit;so 4 bits of the encoded salt are not actually used,the php crypt function generates a unique hash;if the salt is not specified then a random salt will be generated,by computing a hash from a visitor-supplied string plus some salt of course i can tell whether the user provided the same password twice without the security risk of allowing my application to be able to decrypt the provided password possibly maliciously;my sense is that encode and decode are probably good solutions when you want the data to be recoverable but that unrecoverable hash using crypt md5 is a better approach for stored passwords,although you could tell dancer2 plugin passphrase to use a 4-byte salt it s much easier to just use crypt saltedhash everywhere,"
"ogg","wav"," files takes a lot more overall,way bigger overall,","21193738,20539275,","this is not really the answer i sought since the wav files takes a lot more space but at least it works;i also tried ogg format which makes the same problem of mp3 .,it is even slower since i could only upload wav which is way bigger in size compared with ogg and opus,"
"charles","fiddler","better ui overall, which has easier configuration overall,","17037632,6583063,","charles has a much better ui than fiddler,so you configure your app to call localhost 8888 and fiddler forwards to remote 80;another option is charles which has easier configuration for this setup,"
"mousedown","mouseup","position more overall, event doesn overall, event is better in value character events, is less in value character events,better overall, is far better overall,better with  in value character events, is probably better in value character events,","15815658,10796652,41566072,53531380,26230436,14720991,56540196,42690762,","something like if mousedown position is more than 10px away from mouseup position,the problem is that you click on the text your current target is the text now it means mousedown event is already fired inside the text area and it s waiting for mouseup event to be fired in order to complete a click event;however you drag cursor out of the text and release mouse outside the text area and your current target now is the button not the text anymore mouseup event doesn t fire inside text area but inside button area so click event won t happen,the mouseup event is better to attach the mouseup mousedown events to the root element and keep track of which element you are dragging in a variable,the zoom is ignored if the elapse time between mousedown and mouseup is less than the double click time,is there any reason mousedown should be inherently faster better than mouseup, grant you that there may be times where you want to take action on a mousedown such as in dragging but it is the exception and when done properly the mouseup will be seen - except in some versions of ie when the mouseup will be lost if you drag the cursor off the page - to the top left or right;if you want to move things around and be able to see the mouseup the mouseup is far better to use divisions or such than things like links,jquery doesn t detect changes on val so that function is the way to go but it s being called out of scope and the event would be better with mouseup like this this will set the value one character delayed for it to work correctly i would suggest reading the virtualkeyboard doc to change it s event from click to mousedown,based on this answer here s how you can simulate a double click using mousedown mouseup mouseup is probably better since click is fired after the user lifts their mouse and not when their first press it,"
"attr","src"," function is better in display function attribute,attribute better in display function attribute,","35958453,29883611,","display attr function is better to get src attribute,if you just want change the src attribute is better you use attr,"
"db4o","eloquera","www.eloquera.com better overall, is better overall,","5302408,5302408,","i think that eloquera www.eloquera.com is better than db4o and additionally has better licensing,for instance eloquera is free for commercial use and db4o not db4o has the same licensing options than mysql free for free but paid for commercial use;for this and technical things i think eloquera is better,"
"fputs","getc","faster overall,","35480396,","using fgets and fputs is faster than multiple calls to getc and putc all you need is a buffer a little buffer in this case to store the current line,"
"libpng","zlib","worse overall,","10644164,","several things i found in the cocos2d forums lead me to believe it s an issue with the zlib implementation on ios but i ve tried using both stb_image and lodepng to avoid zlib and those libraries actually performed even worse than libpng,"
"nsstring","uilabel","less overall, has more overall,","23756258,10586541,","but the problem i am facing if text is doted inside the label it calculates nsstring size less than uilabel size in the below code,as for the uilabel i personally use core text since the uilabel has more font support but those views may also be using something simpler like nsstring s drawatpoint withfont method,"
"jmenuitem","jpopupmenu","more just overall,","2581314,","the jpopupmenu has this behavior but i need more than just jmenuitem,"
"scribe","twitter4j","easier overall,","15810279,","i have an app that uses the public part of the twitter api the on who not requires to login but with the update the login is required so i need to implement oauth i ve seen there are libraries like twitter4j who makes this easier but my app has a lot of code and i don t wanna rewrite it not now so i ve think to use scribe or oauth-signpost but i dont realy know how to,"
"cifs","smb","newer name overall,","28497298,","now if you re intending to use a remote filesystem with ofs which is the primary use-case you have to first install the relevant remote filesystem packages on the os you re using then use for example if we ve got cifs which is the newer name for smb samba,"
"cruisecontrol","teamcity","far easier overall,","226593,","for a one man shop teamcity is far easier to setup and configure than cruisecontrol,"
"smartgwt","vaadin","more overall, is really faster and richer overall,","7583971,7583971,","thus i think vaadin is more suitable for intranet and takes less time to develop your app while smartgwt is better for more complex gui or unstable connections where additional roundtrips matter,comparing the same need evaluating smartgwt and vaadin for ria application hosted on gae so far i think smartgwt is really faster and richer because all gui logic is compiled to javascript and the same need evaluating smartgwt and vaadin for ria application hosted on gae request server-side very lazily only if client needs new data which were not loaded to browser before +some flexible caching stuff and so on,"
"dplyr","purrr","general overall,better regular overall,more compact overall, reduce transformed  full_join overall,slower than  overall,familiar with  overall,","57039893,39176792,46294040,48602626,52591801,57381437,","we recommend using dplyr for data frames or purrr for lists instead,note that while you could use purrr for the job it s not particularly better suited than regular dplyr,with library purrr you can have a prettier more compact form see soto s answer for an even more compact one with dplyr,with purrr reduce and dplyr full_join the result can be obtained purrr reduce transformed dplyr full_join # type one two # 1 1 a a # 2 2 a a # 3 3 b b,we found for example that purrr map is much slower than dplyr do which is deprecated we also found that functions like pull are very slow,i am more familiar with dplyr and purrr and used them to implement it below,"
"mousedown","mouseenter","slower overall,","34848460,","this would explain why in your case mousedown or click are slower than mouseenter which in a touch device happens as soon as you touch the element being monitored,"
"datagram","recvfrom","larger overall,smaller than the  overall,","27689332,6243852,","the datagram is larger than your buffer so it gets trucated you get an error return from recvfrom and getlasterror returns 10040 wsaemsgsize,socket.error errno 10040 a message sent on a datagram socket was larger than the internal message buffer or some other network limit or the buffer used to receive a datagram into was smaller than the datagram itself;so i am not sure why you would hang on recvfrom if a 6-byte message was originally sent,"
"qbs","qmake","more powerful overall,","41993264,","please keep in mind that qmake and qbs are two very different build systems with fundamentally different designs and capabilities and qbs verbosity is in part due to its fundamental architectural differences which make it far more powerful than qmake will ever be capable of,"
"cvsnt","svn","simply much better overall,","559225,","just a wild guess here but if you re using cvsnt on the server this observation might be based on the fact that cvsnt simply is much better at merging than svn because it has the notion of mergepoints,"
"filechannel","fileoutputstream","faster overall,","3162604,","i don t see any reason why filechannel could be any faster than fileoutputstream in this case,"
"slurm","torque","better overall,","8485138,","i performed survey on torque slurm loadleveler slurm is better than torque in handling large nodes but in a single cluster,"
"qobject","qthread","more than one  overall,","34159328,","finally consider that you can have more than one qthread as well as more than one qobject running on a qthread,"
"for-loop","if-statement","worse overall, but is more overall,","12656655,49744983,","this is little strange to me why if-statement is in this aspect treated worse than for-loop,just some minor modifications to clean up your code can simply become and within your for-loop there is no need to check the complete range of i only its upper bound can become this will have the same result as your if-statement but is more elegant since i will never be less than 0,"
"free","strdup"," is not overall,worse overall,","38744798,12993581,","that requires the caller to take the responsibility of free ing the reversed string;strdup is not a great function because it needs to find the length of the string that is being duplicated which means you end up traversing one extra time,in c memory most other things are managed by the programmer so strdup is no worse than forgetting to free malloc ed memory failing to null terminate a string using incorrect format string in scanf and invoking undefined behaviour accessing dangling pointer etc,"
"fread","textscan","more powerful overall,","1457281,","then for reading i find textscan to be more powerful than fread fscanf the differences between them all are summarized here,"
"uibutton","uilabel","larger overall, does not longer overall, is much more overall,more overall,","26728652,33112706,31451558,27420107,","if the uilabel is larger it will then wrap around the uibutton to the next line,to define a range for the attributes you have to create a uilabel set an attributed string to its attributedtext property and then create a uibutton with a custom view;when you want to use this approach you have to know you have to set the target and the action to the custom view because the uibutton does not longer handle it,as time passes i found out that creating a single sprite + uilabel is much more flexible and useful brings a lot more possibilities;also if you for example have to have different sprites for uibutton s states disabled highlighted etc it would be a big pain to create n 4 sprites,secondviewcontroller which is a subclass of firstviewcontroller also has a uilabel but has one more uibutton,"
"kif","xctest","less complex overall,","46401096,","kif makes ui testing really easy and useful with lots of great apis that are less complex than xctest,"
"onclick","onfocus"," is still overall,more appropriate overall,","48172775,23768046,","even if the onblur and onclick do not get along very well but obviously onfocus and yes onblur;since even after the menu is closed the onfocus is still valid for the element clicked inside,also the onfocus event would be more appropriate than onclick because it handles other scenarios such as pressing the tab key to change the active input,"
"ostringstream","std"," is a way overall, is probably better overall, is no more so incredibly overall, sregex_iterator instead overall,general overall, has certainly better overall, also provides easier access overall,slower overall, manages all memory automatically so overall, is probably faster overall,","36909299,57554447,49267798,57420759,45795899,22669198,22244099,4340396,10849815,19845080,","in c++ you could associate the output stream with a null device too and test the number of charactes printed with std ostream tellp;however using ostringstream is a way better solution see the answers by devsolar or angew,olding a lock while logging isn t great for performance although t s better than your existing log method which should use std lock_guard for exception safety;building and discarding a temporary ostringstream is probably better but if you care about performance you ll need to benchmark and may well end up requiring something more elaborate per-thread circular buffers mmapped files or something,oss.clear here is the code here are the results now using std string is still faster and the append is still the fastest way of concatenation but ostringstream is no more so incredibly terrible like it was before,however i don t know if use a std ostringstream is better than std string append so as you can see i used std sregex_iterator instead of std sregex_token_iterator,edited since you chose the second approach i would recommend using std ostringstream than sprintf and using cclog instead of outputdebugstring because you just print std ostringstream out and independent os no need extra arguments,std ostringstream has certainly better properties regarding memory safety as snprintf has better properties over sprintf .,by returning a std basic_stringbuf it s easy to see that std ostringstream uses a stringbuf internally and std ostringstream also provides easier access to the str functions of std basic_stringbuf functions which doesn t exist in std basic_streambuf,on ideone the ostringstream is about 3 times slower than std copy + back_inserter + std vector and about 15 times slower than memcpy into a raw buffer,std ostringstream s more convenient and safer than std ostrstream because std ostringstream manages all memory automatically so you don t need to call freeze false to ensure the memory gets freed when you re finished with the memory,however std ostringstream is probably faster for appending if you absolutely have no idea about the size you have to reserve,"
"binary","octal"," you get meaningless mush overall, notation is less overall,more useful overall,","1733555,3763864,12205716,","hex and octal are close enough to binary;most data on a hdd is binary data not meant to represent printable characters so if you use cat a command meant to output text not binary you get meaningless mush,some languages use a 0 prefix to represent octal notation 0777;binary notation is less common often hexadecimal is used instead as it makes the notation a bit more compact and ends up using the right padding for octets,you ll need to convert binary to another base here i use decimal when writing this code because c doesn t have binary constants which would be ten times more useful than octal constants,"
"mutators","setter","better accessor overall, and not overall,","27413746,17529470,","these methods which give us access to private members are known as getter setter or better knows as accessor mutators,you can use the fact that mutators is a monad;using untainted from the settable type class in control.lens.internal.setter it is possible to combine two setter but the result will also only be a setter and not a getter,"
"bitwise-and","modulo","slower overall,","21889838,","i read in couple of blogs that in java modulo reminder operator is slower than bitwise-and,"
"fileinputstream","fileoutputstream","smaller files overall,","17099870,","at first i tried reading the original pdf with a fileinputstream and finding the signature hex strings to split it into smaller files with a fileoutputstream as i have done with jpgs,"
"fasttext","word2vec","better overall,","45565389,","fasttext is not better than word2vec as for as semantic similarity is concerned,"
"es2015","tree-shaking","more out  overall,module more efficient overall,","50712225,38842577,","i have been happily transpiling a typescript library of mine to cjs for some time but decided i wanted to transpile to es2015 to get more out tree-shaking,es2015 module is more efficient than the other formats and can facilitate the creation of smaller bundle size through tree-shaking technique importing just the bits you need instead of importing the whole thing,"
"parsec","uu-parsinglib","more declarative overall,more familiar overall,","27967512,26206529,","it seems that the idea of uu-parsinglib it to be more declarative than parsec so you just have pure,looking briefly at the list-like combinators for uu-parsinglib i m more familiar with parsec i think you can solve this by folding over the result of the psome combinator,"
"uigesturerecognizer","uitapgesturerecognizer","more overall,","10949107,","i added the check for uitapgesturerecognizer in case your class handles more than 1 uigesturerecognizer s and you only want to remove that,"
"c3p0","hikaricp","better overall,","36564484,","i would also recommend you to check hikaricp as its way much better than c3p0,"
"do.call","mapply","more efficient overall,","33077700,","if for some strange reason you can do the obvious do.call would be more efficient than mapply,"
"lift","wicket","less verbose overall,","3241030,","lift s ajax support is far less verbose than wicket s,"
"filewriter","ioexception","more overall,","22332678,","for example public static void copy reader r writer w throws ioexception is more useful reusable than public static void copy filereader r filewriter w throws ioexception,"
"collabnet","svn","protocol much faster overall, depends more heavily overall,","2352784,853150,","however svn protocol is much faster so i suggest using collabnet svn for performance reasons,i think now with merge tracking in svn the process would be quite a bit simpler assuming you follow the best practices outlined by collabnet;but bear in mind that tfs is a big product with really great gui tools for managing your source while svn depends more heavily on the command-line so this complicates things if you re used to working with the gui,"
"jax-ws","saaj","lower level overall,","2205637,","one could use saaj soap with attachments api for java which runs at a lower level than jax-ws,"
"stringbuilder","stringtokenizer","faster overall,","41343728,","stringtokenizer is faster than stringbuilder,"
"aurora","marathon","better capabilities overall,","29511619,","even though aurora has better capabilities i prefer marathon due to auroras complexity overhead and lack of ui for control api,"
"background-image","background-size","larger overall, isn overall,","38863728,25376828,","however for some reason ulkit is making a background-size larger than my background-image,background-image isn t supposed to be animatable in the first place;on the other hand background-size can be animated but it s very clear from your css that you don t want to transition background-size,"
"xom","xstream","better overall,","4492941,","but if convenience is more important than efficiency yes you probably should consider a tree model not dom necessarily xom is better for example or data binding jaxb or xstream,"
"uialertview","uiwindow","much more complicated overall,","20342799,","the internal view hierarchy of uialertview is much more complicated than it appears and starting from ios 7 it is not even added to any uiwindow so it s no big surprise that it doesn t participate to the responder chain as you expect,"
"gridview","objectdatasource","nicer overall,","9131691,","from what i ve read the gridview control plays a lot nicer with the objectdatasource,"
"stripes","struts","more lightweight overall,better overall,","2085070,2051851,","i ve found stripes to be really effective and surprisingly lightweight....it aims to be more lightweight than struts,stripes the actionbean approach doesn t look much better than struts,"
"addition","subtract","greater overall, is always better overall, it is easier overall,integer division with  overall, is the inversion overall,","11563070,39589499,14819855,48318668,48609227,","if you continue to use your method of performing this task then you need to manually check if the addition of the two lower order decimal digits has caused a carry by checking if the result of the addition is greater than 10 and if it is subtract 10 from the number and then add 1 to the higher order digit calculation,if a ends in 0 then we could have done this in 5 steps add divide three times subtract if a ends in a 1 then even in 4;so addition is always better,think of long division - you do a series of subtract - shift operations and you don t know what you need to do next until you have completed the previous part of the operation;for addition it is easier to see how you could achieve a complete operation in one cycle,similar to sam westrick s definition number of times you can subtract n2 from n1 without going negative you could also do integer division with addition and greater-than using the definition number of times you can add n2 to integer division before integer division is greater than n1 .,i had a thought that could addition be faster than subtraction so i suggested above but just realized that the only difference then between adding and subtract is the inversion that takes place,"
"canopy","ptvs","more complete integration overall,","19575436,","the upcoming canopy 1.2 release in a couple of weeks will include more complete integration with ptvs and visual studio that will automate this and remove the need for canopy to be the default python environment,"
"glkbaseeffect","opengl-es","shader functionality overall,","9821580,","among other things the apple template code will include creation of a glkbaseeffect which provides some shader functionality that seems to be required in order to be able to draw with opengl-es 2.0,"
"autoresetevent","manualresetevent"," gives you more overall,longer overall,","9463889,32123986,","you can also use the thread.join which andrewb suggests but i prefer a manualresetevent or autoresetevent since a manualresetevent or autoresetevent gives you more control over how and when a manualresetevent or autoresetevent s called,edit this is why autoresetevent does not have a slim version - basically it s because the wait times of autoresetevent are typically longer than manualresetevent so it isn t appropriate to use busy spinning,"
"median","rank","greater overall,","23921595,","we use ave to see which rank are greater than the median for each group and we select where those are true,"
"bluecloth","rdiscount","more reliable overall,","277564,","rdiscount seems to be much faster and more reliable than bluecloth,"
"qnetworkaccessmanager","qtcpsocket","higher level overall, is way more overall,","5990009,42233691,","qnetworkaccessmanager is higher level than qhttp and qtcpsocket so they have abstracted away the sockets connection state,this is the blocking way of waiting for a connection to be established instead of connecting to the qtcpsocket connected signal;like yuriy pointed out qnetworkaccessmanager is way more convenient to handle http requests as a client,"
"ios","tvos","more similar in runtimes older present,older than  in runtimes older present,","34006933,55189091,","tvos is more similar to ios than it is to watchos although they all have some similarities,xcode 10 s release notes weak linking against frameworks that are not present in simulator runtimes older than ios 12.0 tvos 12.0 and watchos 5.0 yet are present in macos mojave can cause an app to crash on launch when running in those older simulator runtimes,"
"odoo","qweb","nicer templating overall,","31691085,","qweb is nicer templating tool and it was created by odoo and its designed to work cross language originally which can be forked from git - qweb,"
"datalist","gridview","faster than   overall,much more bells overall,","921765,756327,","for displaying a list of things repeaters are typically faster than gridview datalist and things repeaters other counterparts,gridview has much more bells and whistles than datalist,"
"altera","xilinx"," did a better overall,better overall,","36487966,3697715,","altera on the other hand rather sees qor as a more or less fixed metric which depends on the context;when comparing both definitions i think xilinx did a better job because they also gave insight into the methodology they use to determine and maintain qor throughout the evolution of their eda tools,xilinx tools suck but their raw silicon is better than altera,"
"couchdb","rethinkdb","more overall,","44327492,","rethinkdb is more similar to pouchdb couchdb,"
"charat","indexof"," approach mentioned much slower overall,faster overall,","25352980,18848063,","also fwiw the indexof approach mentioned below also works but the indexof approach mentioned much slower the charat in this particular chase,i ve found that using a simple for-loop iterating over all elements in the string and comparing using charat performs faster than indexof or regex,"
"gulp-sass","node-sass"," is faster overall,more just overall, have not overall,","41435066,36294249,51886455,","gulp-sass is faster than the sass compilation with ruby because is using libsass a port of sass in c++;first you ll need to install node-sass with npm or yarn and call it in your gulpfile,the cons of gulp-sass is that you get more than just node-sass - you also get gulp-utils and other things - which is a problem if you are not using gulp in the first place,we use gulp-sass to compile our css;from what i understand gulp-sass which uses node-sass have not provided any way to stop charset utf-8,"
"ucma","ucwa"," is more overall, doesn overall, is not in available skype,somewhat easier overall,beneficial than  overall, is not in available skype,","26273263,51469306,37221078,33989895,53810840,37221110,","i have done this in the past it has just been a case of including reference to ucma 4 and changing the .net version;ucwa is more of a client end sdk and depending on what your current,ucwa is a web sdk version of ucma kind-of;the ucwa is a lot more limited than the ucma sdk but ucwa works for skype for business online whereas the ucma doesn t directly work with skype for business online,as such ucma is not available for skype for business online office 365;ucwa is a restful web api that acts on behalf of a single skype for business user,old thread but in my experience writing server-side code with ucma is somewhat easier than trying to use ucwa - and all that ucwa really is is a ucma application sitting on your lync s4b server with a rest wrapper,i was told that ucwa is more beneficial than ucma,ucma is not available for skype for business online for the reasons explained in this question and answer;ucwa is now available for skype for business online,"
"boo","nemerle"," is not nearly enough overall,more powerful overall,","4576122,4575558,","boo primarily integrates with sharpdevelop while nemerle has visual studio 2008 integration;adding caas to a language like c# or even macros to boo is not nearly enough to reach nemerle s level of metaprogramming for which the language has been designed from the start,i read somewhere that nemerle s macro service is more powerful than boo s,"
"glsurfaceview","opengl-es","more flexible overall, context running concurrently overall,","17374578,18291714,","textureview will display opengl-es rendering but is much more flexible than glsurfaceview and will follow the normal layout hierarchy in android which allows views to be moved on the display,if you want more than one opengl-es context running concurrently that is much more complicated but it has been discussed recently on the khronos.org opengl-es forum;glsurfaceview is not designed to do this but you can do it by using textureviews instead,"
"nssplitview","nstableview","shorter than the  overall,","241621,","a pattern i also use is to make a simple nstableview slightly shorter than the nssplitview and have buttons at the bottom add delete etc,"
"itext","pdfsharp","library more overall,","8123876,","as noted in the comment by quandary the pdfsharp library offers a more relaxed license mit compared to the commercial or agpl license offered by itext,"
"mapply","melt","more succinctly overall,","31927100,","could i use mapply to do this more succinctly than running melt and dcast separately for each of employed girls guys,"
"autocompletetextview","spinner","more overall, not overall,","15828256,26559967,","also from your requirements autocompletetextview is more suitable than spinner,note as i experienced the last point is correct in spinner not in autocompletetextview;because autocompletetextview is an edittext itself,"
"vlcj","xuggle","lower level overall,","6963013,","apart from that you might want to give xuggle a try - it works on a much lower level than vlcj but this should give you much more control over what s happening and unlike jmf it s a good actively maintained project.,"
"avro","thrift","required  schema to be overall, are very overall,far more overall,","47291781,5535764,10465052,","unlike thrift avro doesn t save any meta information about the avro schema in the data;avro required avro schema to be present at both write and read time,apache avro doesn t do that tagging and will save you a tiny bit more space because of that;like others have said google protocol buffers or thrift are very popular binary serialization tools,but avro offers far more than just doing away with the need for your own writables although that is in my view a considerable plus it offers fairly efficient serialization the choice between serializing against generated entity classes like thrift requires or using a so-called genericrecord structure instead and not having to have tagged data,"
"strlen","strncpy"," not overall,greater in source first bytes, does not overall,more than the  overall,source greater in source first bytes,","27987355,10388582,7897716,54727927,21974397,","your loop is safe because it is based on strlen not the number of substr you computed;strncpy does not null terminate strings,this is the case here because strlen hello is greater than 3 your last strncpy argument,for example strlen returns the number of characters excluding the terminating null so buffers need to be one byte larger than what it returns to hold things;also strncpy does not guarantee null termination while snprintf does,instead pass strncpy the size of the buffer or one more than the strlen str,however if strlen source is greater than n then strncpy will simply copy the first n bytes and will not terminate the string dest with a null byte because there is no space for it,"
"cocos2d-x","libgdx","more ram overall, api; is much more overall,","27213729,11630356,","performance i did some test with simple game similar to swing copters and found that game created with libgdx consumes 15-20 more ram on ios device that the same game created with cocos2d-x but then i used apportable to run cocos2d-x game on my android device and found it almost not playable,personally i dislike the cocos2d-x api;libgdx is much more low-level and allows you to completely decouple the rendering logic from your game logic,"
"eigen","ublas"," is slightly more overall,faster overall, is more overall,","52738776,7380818,52738776,","eigen has additions for 3d geometry such as rotations and quaternions not ublas;ublas is slightly more complete on the most basic operations,from my own experience mtl4 is much faster than ublas and it is also faster than eigen,eigen lacks some things such as projection indexing a matrix using another matrix while ublas has it;for features that both have eigen is more terse resulting in expressions that are easier to read,"
"tabcontrol","treeview","height smaller overall,","18518693,","i can t find any way to make the treeview s height smaller or the tabcontrol s larger,"
"layoutparams","viewgroup","more overall, s generate method. overall,","23710232,7630695,","unless you know exactly what you are doing do not use the same layoutparams for more than one viewgroup,the takeaway from this is that layoutparams are not parsed by the view s constructor they re parsed by another step that your code above isn t doing;the layoutinflater involves the parent viewgroup s generatelayoutparams method.,"
"pyomo","scipy","less overall,faster than in  overall,","46609929,47821346,","pyomo runs in less than one minute and scipy takes 4 hours,modeling in pyomo is much faster than in scipy but pyomo documentation does not seem to say explicitly if this is feasible,"
"gpx","kmz","quicker overall,","12825810,","kmz loads much quicker than gpx and can be integrated with the google maps api as a kml layer,"
"icmp","udp","slower overall, will be filtered often overall, request is more overall, is also overall,less limitations overall,","2290931,49936734,55279694,33704573,30832868,","an icmp packet has a header that is 20 bytes and is probably going to be slightly slower than udp,ping sends icmp packets not tcp ip;so the network over tcp or udp can work great but icmp will be filtered often called something like stealth mode or invisible,a udp request is more or less as fast as an icmp request,udp nmap also facilitates the udp scan where icmp based port unreachable packet shall be returned in case the udp packet arrives on a closed udp port this also depends on the stack in the os;unlike tcp the udp is not a connection-based protocol and icmp is also connection-less so you might need to send some significant number of udp packets for a short interval and evaluate based on the responses or some similar logic,in fact icmp especially the ttl excceeding message has even less limitations than udp on nats for the error message can be sent from every corner of the internet and routers with nat cannot be aware that these messages are not the true ones,"
"lwjgl","slick2d","newer overall,way easier overall,","44324111,16659010,","slick2d is made to work with lwjgl 2 lwjgl 3 however is way newer than slick2d and therefore slick2d does not support using lwjgl 3,you could use slick2d it is way easier to use and works with lwjgl,"
"crossfilter","reductio","easier raw overall,","40770472,","i think reductio will make this easier but with raw crossfilter this would look something like,"
"gmail","hotmail","closer overall,","43749225,","in outlook.com hotmail it looks closer to the results shown with gmail,"
"jgraph","jung"," is easier overall,","11692277,","i ve created an application with this library instead of jgraph because i thought that jung is easier to learn than jgraph for newbies even if it provide less features,"
"erlang","scheme","older overall,","29173287,","just for the record the tipc addressing scheme is several years older than distributed erlang,"
"odp.net","system.data.oracleclient","better overall,","7935618,","agreed that using odp.net is better than deprecated system.data.oracleclient,"
"cron","jobs"," are not overall,general overall, may not overall,common approach with  overall, takes more overall,more sophisticated overall, do not automatically overall,general overall, can have multiple triggers; overall,general in certain amount starts,general overall,","56889523,56967348,31211925,55441008,41484860,19941712,9440133,52742285,29079638,4717381,20755676,","the overlapping jobs are not queued and are skipped;the behaviour of fixedrate and cron is different,so you can use cloud scheduler to deploy scheduled cron jobs,note also according to man 5 crontab you can set a mailto variable in your crontab file which will direct output errors from your jobs to an email address;beware that the path used by cron may not include usr local bin where python is installed,a more common approach with cron for this is by simply adding multiple cron jobs and imposing an increasing delay on each of them,if the jobs takes more than one hour because of high cpu utilization for example then there will be another instance of the job and likely you will get the duplicated results in the csv file;so you should prevent the cron jobs from being executed if there is already a running one,i would suggest scheduling your jobs using something more sophisticated than cron,cron jobs are only installed onto the application master instance;when your application master fails and a failover occurs the cron jobs do not automatically transfer to the promoted instance,i have been using the article here as a guide to what i need to add into my current deploy.rb to setup the cron jobs on the server the article suggests there are some booby traps here 1 we need to set the identifier to a unique name generally we choose our app name . this is important as the default behavior of whenever is to use the current path which in the case of mina deployment is unique every time we deployed. therefore if you did not set the identifier each time you invoke update_cron_jobs there will be duplicated cron jobs created in the server because whenever cannot identify its previously generated cron jobs,it s either a simple schedule as above or multiple cron triggers quartz jobs can have multiple triggers;cron is not meant to solve such problems,then run a cron job that kills pending jobs that have been running for a certain amount of time or try to reprocess a certain amount of time,secondly if you want to schedule a re-boot of the machine i would recommend using cron jobs for a re-boot of the machine,"
"cron","jobs"," interval is shorter overall, run sudo in script root sudo,familiar with   overall,better grip over   in better grip laravel,more overall, should be setup overall, scheduler works better in better grip laravel,less than three  in script root sudo, do not only or particularly overall, were not including up file overall, does not overall,","44885781,28473349,32231586,40588110,11041968,25927588,51174053,54555545,52209618,54591440,45514219,","and the sem solution will prevent overlapping cron jobs if the cron interval is shorter than the job duration,root cron jobs do not need to specify sudo in the script path to run with administrative privilege root has the top-level privilege;to add a cron job to list of root s cron jobs run sudo crontab -e,sources for cron jobs as i am less familiar with cron jobs than with sed,please read trough it for a better grip over cron jobs in laravel,check out the quartz scheduler it does what cron does and more is easily integrated in a web app and allows you to call jobs written in java without any complicated plumbing,now if you can configure the nix cron like igor parra answered the background and offset cron jobs will not run any more where as the time set in the control panel or crontab will be in effect;it seems that every one has just answered the way a nix cron should be setup,i suggest you download and install then you can use unix style cronjobs to run your jobs;you can also look into using the windows task scheduler but using the cron scheduler works better in my opinion because the windows task scheduler only allows once per 5 minute triggers,the ugly the simplest way to do it for the schedule is to put the following in your jenkinsfile in blueocean every branch and every pull request counts as its own job -- so if you follow standard git workflow the above code will create no less than three cron jobs the feature branch you push even though it hasn t been merged the pull request branch whether or not you merge it the master branch if when you merge the feature branch there is no way to undo this damage unless you push more commits to the feature branch and keep the pull request open under the same number -- otherwise you will be configuring a different job than the one with the cron script,cron jobs do not only or particularly need absoute paths;the main differences compared to interactive shells is that cron jobs run with a different and more spare environment where the shell s path various library paths environment variables etc can be different or missing altogether,now all cron jobs are working fine;i have found issue for this it was path issue cron jobs were not including up file above directory include .. config.php,configured correctly one node could be brought down for patching while the jobs running on quartz continue to process;there are also features of quartz that cron does not provide,"
"cron","jobs"," user s path overall, cannot overall, aren overall, job doesn overall, doesn overall, isn overall, periodically meaning the job in certain amount starts, will not overall,","4943179,55479202,11744417,15972754,16691111,44586469,19515492,8320343,","cron jobs do not run under your user and environment;the path to wget may not be in the cron user s path,i was able to get cron jobs working by touch home ubuntu crontest.log to create a log file sudo vim etc crontab which edits the system-wide crontab add your own cron job on the second to last line using the root user such as which dumps stdout and stderr into the logfile you created in step 1 verify it is working by waiting 1 minute and then cat home ubuntu crontest.log to see the output of the cron job;on aws ubuntu ec2 machines cron jobs cannot be edited or made to run by using crontab -e or even sudo crontab -e for whatever reason,and when you do as dorien says be sure that your cron jobs are setup properly;tagclouds can get out of sync if you re cron jobs aren t firing often enough,it also allows you to run a cron job immediately in your browser great for debugging cron jobs or schedule the job for immediate execution;if your cron job doesn t appear in aoe scheduler s list view there s an issue with the configuration of the cron job,wordpress is also using the same method when cron jobs doesn t available or not been configured;you can schedule the jobs using a mysql database or a file as described here,cron jobs are great for repetitive tasks that are scheduled at the same time everyday;however for tasks that are created on the fly and have a countdown element then cron jobs isn t the right tool,cron just runs jobs periodically meaning the job starts and stops and no more often than once minute;cron doesn t handle dependencies,one approach would be to make sure that if those two jobs are separate cron jobs - there is enough time inbetween to surely cover the run of job 1;another approach is locking as others here suggested but then note that cron will not re-run your job just because it completed unsuccessfully because of a lock,"
"ng-controller","ng-include","lower overall,","24390251,","the priority level of ng-controller is lower than that of ng-include,"
"qcheckbox","qtreeview","more column overall,","20013717,","i need show files from qfilesystemmodel in qtreeview and customize that tree to show one more column with qcheckbox so user can pick 0..n files from that qtreeview,"
"couchdb","ravendb","more overall,","4163673,","ravendb more than couchdb but that s neither here nor there,"
"kivy","pyqt","better choice overall,","39360654,","if you don t plan to move to mobiles later tkinter or pyqt may be a better choice than kivy,"
"gedit","textmate","more than  overall,","1365849,","i tried a few ides on ubuntu and it just didn t seem right - gedit is lightweight and i actually enjoy using right - gedit more than textmate on os x,"
"arp","icmp","slower overall,","12085301,","in addition seeing arp being slower than icmp doesn t necessarily mean icmp isn t deprioritized---it might mean bandwidth is insufficient to hit the limiting threshold,"
"floor","sqrt","larger overall,","19306324,","the other suggestion keep repeating until x stops changing does not work either because for non-perfect squares x will alternate between the floor and the ceiling of the root â because of integer mathematics the term n x will alternate when x is slightly smaller or slightly larger than sqrt n,"
"mergesort","timsort","more complex algorithm overall, that is much more overall,","10179057,41758937,","timsort is used by python and java for their sort methods and is rather supernaturally fast but it s a more complex algorithm than mergesort which matters if you re working in something like c - so if you just need something that works pretty well and is simple go with mergesort,rior java 8 that was not a big issue the sorting implementation mergesort would not throw exception;java8 changed default sorting implementation to timsort that is much more sensitive to comparators with invalid contract hence invalid contract might throw an exception,"
"checkboxlist","gridview","control better overall,","19729088,","since you re presenting multiple columns a gridview control is a better alternative to checkboxlist,"
"capybara","rspec","more overall,","9906488,","i use rspec and capybara together but the language syntax of capybara makes more sense is semantic as rspec when testing for request expectations,"
"palindrome","primes"," s length overall,more iteration overall,","23849144,26344108,","if the array is longer than 5 characters objects it will copy the first one in the fifth position and the second in the third position the palindrome is just a particular case;and of course if primes s length is less than 5 it will fails because there would be no,calculating primes takes more iteration than checking for a palindrome,"
"irc","xmpp","protocol more simplistic overall,","9795702,","regardless the irc protocol is more simplistic in nature can handle orders of magnitude more client connections than xmpp for the same memory utilization uses less bandwidth on the wire doesn t require authentication although you can add this feature etc,"
"laravel","silverstripe","better overall,","36183018,","the silverstripe cms is a great php framework better than laravel the most popular at the moment but its documenation lacking,"
"btrace","profiler","overhead far less overall,","13726773,","in my experience btrace overhead is far less noticeable than any profiler depending on activity of course - if you want to trace execution of all methods this will be expensive no matter what,"
"boilerpipe","goose","more than  overall,faster overall,","27058474,14164350,","goose allows more fields published date author main image in article and a few more than boilerpipe title content,2 readability library content is passable slower on average than goose but faster than boilerpipe,"
"coalesce","sum","less overall, is less than not less overall, is a better overall,","960774,47976129,26038416,","i m pretty sure the linker will coalesce equivalent strings so the total will be less than the sum of the parts for these sections but i guess typically not by much,join the table to itself on lesser ids because that s what you re ordering on summing all the rows from the join and keeping those rows whose sum is less than not less than or equal the threshold;the coalesce call is added to cater for the lowest id which we want to keep always having no rows to join with,you probably intend sum amount is null;as mentioned in another answer coalesce is a better solution,"
"passenger","puma","more features overall, doesn overall, is a lot overall,","15435871,49436505,16395461,","phusion passenger 4 enterprise bring many advantages over has more features than puma and rainbows for example it has out of band garbage collection can dynamically adjust the number of processes based on traffic completely automates rolling restarts so you don t need scripting etc,the other issues memory limits and concurrency limits are relatively less important but they can t be mitigated without using language extensions the iodine server is written in c and passenger is written in c++;since puma doesn t currently require any language extensions except for it s integrated http parsers in c and java these issue remain,but i would recommend phusion passenger over puma;phusion passenger is a lot easier to setup especially when you hit production,"
"asterisk","ivr"," doesn overall,better not overall,","39482466,5966362,","i found that when user hangs up the call asterisk doesn t send sighup to my agi script;it does work when ivr is called from internal network,microsoft has the microsoft speech api sapi however if you want simple ivr it is better not to reinvent the wheel and customize an asterisk implementation which i guess falls under the great ivr library for c++ category it s c not c++ but if you know c++ you should be able to understand the c . using asterisknow you may not even need to write any custom code it may do what you want already,"
"opencv","vlfeat","smaller overall,","15091737,","have a look at sift and surf and at vlfeat which has a good sift implementation and also implements mser and hog and is much smaller than opencv,"
"odoo","openerp","more overall,","31589303,","its just a name they choosed it because odoo provides more than openerp for example web site modules,"
"flv","wmv","easier overall,","2562,","i found the production of wmv much better and easier than flv because all windows flv encoders i tried are not really good and stable whereas pretty much every tool can natively output wmv,"
"tfs","vsts"," because is 100 online in online newer features, we use the login overall,more overall,smaller than in  overall, 2010 doesn overall, on-premise not overall,available with  overall,better than  in online newer features,","50217852,48636678,1961714,57319608,39826412,36620489,48393089,51517231,","vsts is better than tfs because is 100 online,it seems you are using vsts not on-premises tfs;usually login option can be used on on-premises tfs we use the login option to specify the team foundation server user account to run a command,unless something has changed a lot with 2010 the basic vsts install is more or less the same as the pro version of vs - and then you install the team explorer on top to gain the tfs specific client-side extensions,but this customizable range in vsts is smaller than in tfs,vsts is free for 5 users you can start it for free;as giulio vian mentioned tfs 2010 doesn t have the capability of building from a bitbucket repository,collectionuri for vsts is always other format can t be recognized;the tutorials are all for tfs on-premise not vsts,release notifications are only available with tfs 2018 later and vsts,secondly the vsts is newer than tfs so some features is better than tfs,"
"monodevelop","sharpdevelop","better overall, has more in box xamarin replacement,more in box xamarin replacement,","1640244,32759274,17615657,","i like the monodevelop ide even better than sharpdevelop but i m not sold on installing and using mono s compiler just yet nor do i know if it can even handle the latest vb.net c# language features,out of the box monodevelop xamarin studio only has a few t4 templates available so you would need to either write your own or find some from elsewhere;sharpdevelop has more t4 templates or mvc which could probably be used but may require some modification,monodevelop is more of a replacement of an ide for mac and linux in windows the recommended thing is to use visual studio or sharpdevelop,"
"ratingbar","textview","smaller overall,smaller overall,","19805576,19805199,","unfortunately i dont understand what you mean by stating ratingbar is smaller than textview,the thing is like ratingbar is smaller than the textview i don t get a correct aligment,"
"entity-relationship","erd"," model isn overall, modeling doesn overall,modeling more overall,","47122832,47834465,12446158,","normalization is normally done after a conceptual erd is translated to a logical relational model not directly on an erd or in the process of creating one;understanding how entity sets attributes and relationships translate to domains relations and dependencies may invite one to try normalizing directly in the erd but a problem with this approach is that the entity-relationship model isn t a complete and consistent logical model and so some relations may not be representable in an erd,entity-relationship modelers can picture this in an erd using a figure for generalization and specialization;but entity-relationship modeling doesn t tell you how to build the tables,first entity-relationship modeling is more than just an erd,"
"cos","sqrt","slower overall, is harder overall,","8248588,1882390,","in term of speed square rooting is easy a few arithmetical operations for some newton-like method but it is not clear what asin does probably quite costly cos is likely to be one order of magnitude slower than sqrt and thus one square root is likely to be quickier than those two transcendental function calls,i ve used a look up table for circular functions sin cos tan successfully many times in high performance real time systesm;the sqrt is harder this way but if your input range is restricted to say screen pixels it s hard to beat for speed and you can tune the space accuracy trade off exactly,"
"dataframe","multi-index"," to show just overall, did not overall,more overall,","57396394,46354298,37569881,","i have two dataframe one bigger with names and family names defined as a multi-index family and name dataframe age weight family name marge simpson bart lisa homer harry potter lilian lisa james and the another dataframe is smaller containing just some of the names of the first dataframe family name simpson lisa simpson bart potter lisa i want to filter the first dataframe to show just the names that exists in the second dataframe,building an intermediate series of active days from iterating through each row in dataframe and adding each date in the active range to the series and then grouping by date was much slower;running the code in the question on a dataframe with a multi-index did not change the performance,the real dataframe has more columns in the multi-index,"
"operands","scalar"," type has greater rank overall,higher alignment overall,","57108484,42140790,","double and float4 vector of 4 float values 4.0 256.0 s x2-1 y2-2 + s x2+1 y2-2 + s x2-2 y2-1 + s x2+2 y2-1 + 2 88 29 error scalar operands type has greater rank than the type of the vector element,vector instructions may use array operands that require a higher alignment than any scalar,"
"radrails","rubymine","more overall,","6521764,","webstorm + pycharm + rubymine + phpstorm though to be fair rubymine has more features than radrails atm,"
"qabstractitemmodel","qabstracttablemodel","more specialized interface overall,","20298589,","well obviously you created an implementation qabstracttablemodel which has a little bit more specialized interface than qabstractitemmodel,"
"explode","substr","faster overall,","38780119,","just wanted to add that using strrpos + substr is slightly faster than explode + end,"
"qitemdelegate","qstyleditemdelegate","newer overall,","28871121,","qstyleditemdelegate is newer and the qt documentation about qitemdelegate states that,"
"cmmotionmanager","uiaccelerometer","much larger overall,","17659352,","basically it seems like cmmotionmanager is much larger and slower than uiaccelerometer is,"
"flex","osmf","older overall,","4487496,","flex uses an older less capable version of the osmf platform for its videoplayer,"
"sin","sqrt","faster overall, is harder overall,","39808091,1882390,","it s like asking whether sin is faster than sqrt,i ve used a look up table for circular functions sin cos tan successfully many times in high performance real time systesm;the sqrt is harder this way but if your input range is restricted to say screen pixels it s hard to beat for speed and you can tune the space accuracy trade off exactly,"
"mixture","poisson","more overall,","47198637,","so intutively it appears that to explain the variance we should go for mixture of more than two poisson distribution,"
"add-in","vsto"," is no more overall, that does much more overall, that launches wpf window overall, development much overall, ribbon cannot overall,more familar overall, is not overall,more in legacy available users, loader is just overall, has more in legacy available users,","37345111,56232763,50785870,6380342,54635868,28491633,26790153,40342201,6254431,53725033,","as a former vsto add-in dev you have to rethink the way you design office add-in;a web add-in is no more than a web app using a library office.js to fill the bridge between your logic and the office host,i would prefer to disable it completely on the desktop since there s a vsto add-in that does much more then the js api is even capable of but since i have not found a way to do that so my only option now seems to be make the js version work globally,i m making outlook vsto add-in that launches wpf window,unfortunately vsto is only a managed wrapper around the office interop libraries so vsto has no more hooks than using the interop libraries;it just makes add-in development much easier with helpers and giving us designer support,note that a vsto ribbon cannot be called by vba code.;and the ribbon is not stored in any document. with ribbon xml in vba and com add-in it s possible to share ribbon controls by assigning them a q id,the comaddin base class makes it a bit easier to create an office add-in and reduce infrastructure code to be more familar with vsto - thats the whole idea,vsto is not designed to support udfs if you really need to have udfs for excel written in c# or .net generally you have some solutions to choose from includes but not limited to;automation com add-in here is a good article about this,however since i know that the add-in is more or less working as is for all users some of whom use excel 2010 and some of whom use excel 2013 and some of whom may explicitly have vsto installed and some of whom may not i d like to be sure that i m not setting myself up for a bruising later on when some features of the add-in stop working for no apparent reason due to changing the setting,1 the reg entries pointing to it or to the manifest in the case of vsto either aren t there or aren t right;2 the add-in has some prerequisite or dependency that you ve missed and since it s not there the add-in loader is just failing to load the add-in,the legacy vsto add-in has more features but it s only available on the outlook windows desktop client,"
"hasownproperty","indexof","faster overall,","41878143,","however i ve seen that object s hasownproperty is much faster than indexof,"
"latex","stargazer","more general overall,","22337804,","stargazer is at least in my experience more general than latex from hmisc,"
"matplotlib","seaborn"," does not overall,higher overall, that offers a slightly overall, is more overall, is often easier overall, plot is a bit overall, which gives shorter code overall,better than  overall, here considering that dash in functionality it complimentary,higher together with  overall, and not overall,","30675687,42659940,42640285,51964508,49101278,52628745,53748185,52308749,56053248,53546300,46798827,","seaborn does not change the behavior of matplotlib functions in the way you describe;matplotlib exposes a number of options for customization that take effect by changing the default values of various plot parameters,seaborn is a higher level library above matplotlib,consider using seaborn;it is a wrapper of matplotlib that offers a slightly more streamlined api,our simplified case is more appropriate for matplotlib s plot function than anything from seaborn;seaborn is more for making the plots more readable with less direct intervention in the script and generally gets the most mileage when dealing with pandas dataframes for example yields as to how to set the styles the way you want for the variables you re trying to show that i m not sure how to handle,using seaborn is often fine for standard plots but when some customized requirements come into play falling back to matplotlib is often easier,when using pure matplotlib the result is as expected;to replicate the exact look of the seaborn plot is a bit more work then,here a solution using seaborn which gives shorter code but gives up some flexibility compared to using matplotlib directly and here the result,i am trying out seaborn to make my plot visually better than matplotlib,i would recommend using plotly in place of matplotlib and seaborn here considering that dash is built on top of plotly,so i would recommend either using matplotlib 1.5 together with seaborn 0.7 or matplotlib 2.2.3 or higher together with seaborn 0.9,as you found out yourself you need to call seaborn.set to get the default style of seaborn or use any other of the seaborn styles;you can also use seaborn styles with matplotlib and not using seaborn at all,"
"matplotlib","seaborn","general overall,happy with  overall, and not in functionality it complimentary, which allows more customization overall,","57601901,56203457,44717062,56723020,","also by using statsmodel s as_pandas true your code becomes a bit shorter anyways if you want to plot the month as hue i recommend using seaborn over matplotlib this gives,another option if you re not happy with seaborn but don t want to go too deep in matplotlib api is to create the errorbars with seaborn then tweak them at your convenience as i said they are just matplotlib objects if you provide a more concrete example i might be able to help more,seaborn is built on top of matplotlib and merely extends its functionality;it should be thought of as complimentary to matplotlib and not a replacement for it,however i propose an alternative method here using seaborn which allows more customization of the plot while not going into the basic level of matplotlib,"
"quanteda","tidytext","package faster overall,","46318346,","the quanteda package is faster and more straightforward than tm and works nicely with tidytext as well,"
"sdl","xlib","faster overall,","23998096,","in my experience xlib via mit-shm extension was significantly faster than sdl surfaces not sure i used sdl in the most optimal way though,"
"testdriven.net","xunit.net","easier overall,","4103837,","one option is to install testdriven.net which makes it easier to run unit tests on any of the major unit testing .net frameworks nunit xunit.net vs tools etc,"
"add-on","xpcom","way better approach overall,","12964786,","you could try using c-types with ff which is a regular dll being called by javascript in your add-on this is way better approach that using xpcom because if the interfaces you use in there can change in each ff version indeed you will have to do multiple dlls each for your add-on supported ff versions,"
"qregexp","qregularexpression","general overall,more familiar overall,","28907897,29902956,","qregularexpression is similar but has some advantages over qregexp;qregexp is a little older and may have more examples of it,qregularexpression and i m much more familiar with qregexp,"
"clion","qt-creator","nicer overall,","43932756,","another alternative that people might mention is clion which is from jetbrains but its closed-source not well supported and whilst it might look nicer than qt-creator it has more or less the same features but is much more resource-heavy and laggy so i d pass on that,"
"mouseenter","mouseover","better choice in time better use, is somewhat more overall, not in time better use, is more in time better use,","2157980,51993188,39161368,43469434,","the mouseenter event is usually a better choice than mouseover,in this particular case it makes little difference but in other situations mouseover can lead to unexpected outcomes whereas mouseenter is somewhat more predictable,use mouseenter not mouseover;unlike the mouseenter event the mouseover event triggers if a mouse pointer enters any child elements as well as the selected element,also note that the mouseover event will be triggered every time the user move the mouse over the button;mouseenter is more apropriate in this case,"
"geany","vim","slower overall,","37644682,","it looks like vim is slower than geany,"
"idictionary","iequalitycomparer","better overall,","25990934,","using iequalitycomparer which is better because works without altering initial idictionary signature,"
"cpu","irq","more overall,more overall,","18322925,28687599,","cannot sleep run atomically in soft irq context and are guaranteed to never run on more than one cpu of a given processors for a given tasklet,in some cases irq constitutes more than 8 of cpu for a process,"
"afnetworking","sdwebimage"," is much faster overall,better choice overall,","27188421,37085338,","it was taking a hell lot of time in afnetworking and the code is also too long;sdwebimage is much faster and short,so i think afnetworking would a better choice than sdwebimage,"
"sbt","scala-ide","tighter integration overall,","11480225,","it d be really nice if scala-ide supported tighter integration with sbt,"
"min","rank","greater value overall,less overall,less than  overall,","21209545,13146750,51520168,","a simple way to get the indices of a rank 1 array arr for elements greater than value min is,thus if the rank of an nxm matrix is less than min n m then the matrix is singular,actually it is better to use it than the standard formula as you will get a result even if rank of x is less than min n k whereas standard formula will fail or produce numerically unstable result if is nearly singular,"
"punctuation","words","longer than 7  overall, and take everything in shorter sentence marks,compare  with  overall, is more overall,longer overall, characters so overall, it becomes simpler overall,more whitespace overall,more than one  in shorter sentence marks, p a more overall,shorter than 25  in shorter sentence marks,","56227031,1018007,49866661,37040638,36800519,50162907,843116,45464100,55999130,55759019,50418038,","it adds words s to a buffer until your conditions are met words contains punctuation and the buffer is longer than 7 words,usually contains seperate words but not sentences ending containing commas and ending in period or equivalent punctuation;you could look for the first and last element containing sentences with punctuation and take everything in between,i m all good up until the decoder function needs to compare words with punctuation in them,since users can just use left right arrow keys to read individual letters it wouldn t take much for regular screen reader users to read the words slowly;i would say the punctuation is more dangerous if their verbosity settings are set high,now i have it so that the line is split into an array of strings using the .split method however it splits on white space and punctuation meaning that cities with names longer than one words won t work,this unfortunately counted punctuation as words characters so hello world,then you process one sentence at a time after removing all punctuation commas semi-colons colons and so on;then when you re left with an array of words it becomes simpler,note that in case your words are separated with more than whitespace punctuation for example use w+,in a program the regex on gets text after that initial phrase then split returns the list of strings between commas or other punctuation and grep filters out strings that have more than one words dagger,you can check if it is inside the words the important line is this we have 3 rules for 3 rules to count the puntuation p is in the words 2 the words w does not start by the punctuation p the words w does not ends by the punctuation p a more pythonic way of doing such would be to use the str available methods endswith and startswith .. if p in w and not w.startswith p and not w.endswith p ...,the problem i m having with this code is that it will mark sentences shorter than 25 words if you include commas in the sentence - this might also happen with other punctuation marks but so far i ve experienced it with commas specifically,"
"punctuation","words"," from causing a split overall, but only overall,","1648101,55114233,","it looks for your punctuation set not preceded by a string starting with http and ending with a whitespace character;the trailing b prevents a hyphenated words from causing a split,for example if the tested sentence was this is a sentence that is much longer than eight words but only the first eight words will be outputted. the output would be this is a sentence that is much longer if i added punctuation that the function looks for to change the input to this is a sentence,"
"phpquery","querypath"," also implements some pseudo overall,often faster overall,","3659729,3659729,","generally querypath is better suited for manipulation of documents;while phpquery also implements some pseudo ajax methods just http requests to more closely resemble jquery,it is said that phpquery is often faster than querypath because of fewer overall features,"
"uinavigationbar","uinavigationcontroller","more overall,","6305055,","the ardalahmet solution is not the right one if you need different backgrounds for each uinavigationbar when you have more than one uinavigationcontroller such as in a uitabbar because it using uinavigationbar categories,"
"jqvmap","jvectormap","more overall,","11182812,","jvectormap comes with more pre-generated maps than jqvmap,"
"canvasjs","d3.js","more overall, is more overall,","41302433,41302433,","canvasjs is more of creating charts using canvas element of html5 and d3.js uses svg,if you want quick response time and not much calculation filters on data then you can go with canvasjs;d3.js is more of filtering charts based on filters which can be used in your case,"
"command-pattern","facade","easier overall,","32532729,","i guess the point that the article is trying to make here is that a facade is easier to implement when you already have this command-pattern in place,"
"coldspring","model-glue","easier overall,","7291086,","fortunately model-glue has tight integration with coldspring a popular bean container for cfml and model-glue 3 makes it easier than ever to use coldspring beans in your controllers,"
"eclemma","ecobertura","plugin nicer overall, does not measure coverage correctly overall,","5265916,12913535,","the eclemma plugin is nicer and more mature then ecobertura,no idea whether ecobertura has the same issue though;one factor may be that eclemma does not measure coverage correctly for exception handling code - it may tell you that code is not covered when in fact it is executed if you step through with the debugger the code is indeed executed,"
"strsplit","substr"," is even more overall,more overall, there s an easier in multiple characters the, is probably overall, may not in multiple characters the,","53415103,17960609,1798628,32647879,40391802,","you don t need any of costly operations like split and substr substr creates a new string object and calls system.arraycopy under the hood strsplit is even more costly,joe duffy s blog implies using substr is more efficient than strsplit,usually you use strsplit to break a string in parts but with substr there s an easier solution for your problem,if you really want to use strsplit you can do what you want by limiting the number of results by doing;i don t know your exact issue but as has already been said replace or substr is probably a better option,in some cases strsplit will split a string on substr having multiple characters;the offset into the string of the beginning of each substr may not be sufficient,"
"masonry","packery","better overall, is a more up overall,","29288180,25514677,","for my project packery is better then masonry because packery is filling up all gaps masonry is just looking for the first possible gap and doesn t fill empty gaps,to expand on dan s answer having just had this problem myself it seems that packery is a more up to date;much more maintained version of masonry - from the same author,"
"http.sys","httplistener","more customizable overall,","1374448,","and also don t forget - http.sys much more customizable than httplistener,"
"diagonal","minimum","shorter overall,lesser global overall,","36036177,42716989,","even with 32 levels of recusions it will never explode in a rectangular draw area whose diagonal is shorter than 2 32 pixels the limit would be reached only if you are splitting a virtual bezier in a virtually infinite space with floating-point coordinates but you don t intend to draw it directly because you won t have the 1 2 pixel limitation in such space and only if you have set an extreme value for the flatness such that your minimum square sine constant parameter is 1 2 32 or lower,one way would be to fill the diagonal elements with something lesser than global minimum and then use argsort -,"
"nsnumberformatter","nsscanner","easier overall,","4092589,","i was originally using nsscanner because it was easier than nsnumberformatter to use but i ran into the same problem it doesn t parse the entire string just the first number in the string,"
"do-while","goto","better overall,","45495264,","but goto is rarely used in modern coding it is not likely to perform any better than a do-while loop after compiler optimizations are applied and it has limitations on how it can be used,"
"ucs","unicode","more overall,","1674236,","according to wikipedia the bmp of the ucs has 65536 characters the latest version of unicode contains more than 107000 characters and the ucs has more than one million code points,"
"gedit","komodo","quicker than  overall,","2066036,","when i m in a hurry i use gedit purely because gedit is quicker than komodo edit,"
"tabcontrol","user-controls","larger overall, is scrolviewer overall,","10881361,54575381,","some user-controls s are larger than the tabcontrol and got clipped so i modified its template by wrapping the contentpresenter in a scrollviewer with horizontalscrollbarvisibility and verticalscrollbarvisibility set to auto,hello i have problem with my wpf page loading it took more than 2 seconds my page contains 2 listview one listview contains 70 items and second one contains 355 items of course everyone would say use virtualization and you done yes it used but not on listview but on higher level since my app use one scrollviever one page method i highly think due to this method virtualization does not work as expected since if it would work loading times be much better one row took between 15-25ms to render this is my problematic code this which may raise questions buttons buttons is user-controls inside it is border and inside border is textblock helpbutton helpbutton is user-controls inside it is border inside it is canvas with textblock and image after some time image is disabled and on first time only image is shown only headerremover sets gridviewcolumnheader visibility to collapsed listviewitemwithgridstandard is style which modify control.template adding inside border and inside it adds gridviewrowpresenter as you can see sectionatabs is tabcontrol tabitem child but tabcontrol is in page between page and tabcontrol is scrolviewer,"
"passenger","unicorn"," is more overall,less memory overall, is a bit more overall,general overall,general overall, 3 is a bit overall,","18398991,33593995,3480284,54419034,4113570,8298866,","passenger is not as stable as most people think;a nginx + unicorn is more stable imho,unicorn does not use less memory than passenger,if you are looking at a new setup think about passenger or unicorn;both are great unicorn is a bit more complicated and i would not recommend it to a beginner,there are more than just puma unicorn passenger etc,unicorn also has a similar feature but phusion passenger s version is more flexible because,unicorn scales properly and is a grown up unix citizen - it s what i use for production personally;edit - have just noticed that passenger 3 is a bit more rvm-friendly as given in the documentation above - but you still have to run against a wrapper,"
"netty","vert.x","better benchmarks overall, implementation is a little overall,","45214442,14325529,","also vert.x has little bit better benchmarks than netty,the client side is generally written in javascript but they have a number of servers written including two in java vert.x and netty;it looks like the vert.x implementation is a little more mature at this point in time,"
"pygtk","wxpython","lighter overall,better overall,","2502963,3165474,","also pygtk is lighter than wxpython but i ended up bitting the bullet and using wxpython for the same purpose recently it is heavy but it didn t have any affect on the script performance,thanks to opencv i managed to rewrite everything with wxpython which i know better than pygtk,"
"allegro","sdl","better overall, but seems simpler overall,easier overall,","8957972,8195090,35510183,","i was also looking at sdl allegro sfml i didnt see much tutorials help on this but heard its better than sdl,you could try allegro;it is somehow similar to sdl but seems simpler to use,i d strongly recommend to go with allegro 5.1.x it s a little bit more difficult than sfml but very functional and easier than sdl it s got nice support and it s compatible with plenty of os,"
"soappy","suds"," has support overall,better overall,","557079,5788942,","soappy has support for attachments on its todo list;suds does not mention the word attachments anywhere,finally if soappy just isn t working for you try suds it s better documented than soappy,"
"fakeiteasy","moq","nicer syntax overall,less powerful overall,","11473992,8445110,","fakeiteasy seems to have an overall nicer syntax than moq like the strongly-typed way the former deals with passing parameters to a constructor of a faked class,btw why do you want to use fakeiteasy it looks to me less powerful than moq,"
"notepad++","scite","more lightweight overall,","1838114,","by the way if you like minimalistic approach there is also scite editor which is my personal preference for doing small bits of code it s even more lightweight than notepad++,"
"caliburn","reactiveui","easier overall,","17108323,","personally i really like reactiveui which is fairly unusual but does make getting things right an awful lot easier than caliburn,"
"autotest","guard","better overall,more overall,","7222776,7222776,","i recently moved from autotest to guard for a reason it works better than autotest and it has a lot of available guard,in other words guard is more a generic framework for every process which needs to do something triggered by a file change autotest is restricted to running tests,"
"istanbul","lcov","more complete overall,","21890213,","istanbul has more complete coverage stats not just lines - branches etc and can export to lcov for tools like code climate,"
"maven-antrun-plugin","maven-wagon-plugin","better overall,","10839694,","instead of maven-antrun-plugin it is better to use upload goal of maven-wagon-plugin,"
"ora-06502","procedure"," has more parameters then overall,","20841260,","your problem is that your procedure has more parameters then you pass to your procedure on mybatis call so at any point after the missing parameter you should have your ora-06502 pl sql numeric or value error error since the following paramters doesn t have the same type on the order you are passing the order,"
"decoder","encoder"," randomly generated pixel overall, is running faster overall,more overall, by tagging in-memory string overall, is not overall, does not support recovery then overall, is a little more overall, not just overall,","11581186,48288961,46753030,1557382,34945129,46820408,57708249,41376294,","for most use cases h.264 will have much better compression than mjpeg but the encode decoder process is a lot more complex which is why things without much computing power webcams spit out mjpeg;google would have to be a rather pathological case for an good h.264 encoder to perform as badly as a good mjpeg encoder randomly generated pixel values,new java 8 encoder decoder is running faster than any other implementation but you need java 8...,in the end you have a scheme whose encoder is more complex but whose decoder couldn t be simpler,an amf3 decoder should not use the traits from a partially-constructed object -- such input should be flagged as erroneous;the strings-reference table is implemented at the encoder by tagging in-memory string objects as they are serialized,maybe the header of the file required to initialize the video decoder is not at the start of the file but at the end and the browser has to download the whole file until it reaches it;this is uncommon nowadays but old video encoders used to place the mp4 header at the end of the file instead of at the beginning because it simplified the encoder s implementation,how it works when encoder encodes number n+1 packet it add there some information about packet n that is useful for decoder if it supports recovery;if decoder does not support recovery then this information is useless and wasting bandwidth,f you re concerned about performance you could write something like this that decoder the singleton object the singleton though and in our desired representation our have a wrapper;we can accommodate that with a single extra line at the end this will fit right in with a generically derived instance for our updated item class import io.circe.generic.auto._ io.circe.jawn.decode case class item id long name localizedstring and then the customized encoder is a little more straightforward and then this approach will work for any number of dynamic fields like this you can transform the input into either a or jsonobject and work with the key-value pairs directly,base64 encoding is not meant to work piece by piece it s meant to encode a complete block of data as one single unit so when the decoder sees the symbol which appears at the end of each line it thinks it s reached the end and there is nothing left;the proper way to solve this is to go back to your original code and pass the entire string to a base64 encoder not just one line at a time,"
"greenplum","postgresql","better overall,gather overall,","1792370,12586673,","greenplum and other similar solutions should work a bit better than postgresql depending on your data sets and use cases,according to the postgresql which i gather greenplum is based on documentation for sequence manipulation functions it should return the value most recently returned by nextval in the current session.,"
"uialertcontroller","uialertview","whitier overall,better experience overall, also has same limitation;i overall,easy with  overall,","43092664,26431886,26777166,49035616,","but in your case alertview is displayed above alertcontroller ie why it feels like uialertview is whitier than uialertcontroller,however take some time to consider how you re currently using uialertview and whether you are able to give ios 8 users a better experience by supporting uialertcontroller,uialertview also has same limitation;i will suggest to use uipopovercontroller and implement your own dismiss button since uialertcontroller s purpose is more to display alert messages to the user short message per apple documentation,a little tricky with uialertview before ios 9 but pretty easy with uialertcontroller for ios 9+. thanks for any thoughts,"
"jax-rs","jax-ws","much younger overall, does not overall, has more overall,","29928306,28404975,12543091,","jax-rs is much younger than jax-ws jax-ws had a final draft in 2006 jax-rs came out in 2008-9,jax-ws is meant for xml based web services such as soap;jax-rs does not have the same restriction,people typically find jax-rs easier to start with and the more modern approach although jax-ws has more build-in type-safety features which is also exactly where most of jax-ws has more build-in type-safety features which is also complexity comes from,"
"spritebatch","xna","more overall,","941685,","in xna when is it appropriate to render your game content using more than one spritebatch,"
"getchar","getline","probably better overall,","2472862,","getline is probably better than getchar in most cases,"
"csc","vbc","smarter overall,","9209810,","so yes at least in this respect csc is smarter than vbc,"
"ocaml","racket","faster overall,","4305402,","ocaml is faster than racket for most of the benchmarks on languages benchmark game,"
"qtableview","qtreeview","slower overall,","17464100,","qtreeview is known for being slower than a qtableview and consume a lot of memory and you are using a plain table model anyways so try with a qtableview,"
"fastclick.js","hammer.js"," is a more overall,","16737168,","hammer.js is a more full-featured touch library has many swipe commands than fastclick.js most upvoted answer,"
"innerhtml","nodevalue"," is safer overall,faster overall,","22453669,21311670,","morever to get the content of a td element you cannot use nodevalue which is defined for text nodes only;you could use innertext but due to issues in browser support the good old innerhtml is safer,nodevalue is a little more confusing to use but faster than innerhtml,"
"grooveshark","soundcloud","better overall,","29315632,","i used soundcloud api and sdk and it is better than the grooveshark api which till now i don t know how to embed songs and use my own player anyways soundcloud is good just try it goodluck,"
"jks","keystore","differ overall, and try the default overall,called  explorer to convert overall,format better overall, truststores correctly more overall,","12621128,9055653,34962619,28050018,57516660,","java has build-in support for work with pkcs#12 keystore work with this containers doesn t much differ than standart jks keystore,instead you can support more than the jks and try the default keystore type and if the default keystore type fails try the #pkcs12,since jks is not supported on android i made a copy of the keystore and renamed the extension to .bks on the new one;then i used a program called keystore explorer to convert the keystore.bks to bks-v1 format so it works on android,in this default implementation the jks format is better suited for a single keystore that is to handle both trusted entries and key entries in the same container,it could possibly be that we didn t build our jks keystore truststores correctly more on how we built these files below,"
"adplus","procdump","simpler sysinternals overall,","2731984,","if you have a customer who is willing to work with you a bit it might shed some light on the situation to get a crash dump with adplus or maybe simpler with sysinternals procdump when the error message is showing,"
"matlab","simulink","faster overall, without doing any programming overall,","29978481,1738309,","for that purpose i want to transform the simulink model into a c version and launch it from a matlab script so that the process would be much faster than opening simulink environment,generally matlab has a better programming environment better documentation better debuggers better object browser and is easier to use you can use matlab without doing any programming if you want;simulink allows you to visually program by connecting blocks in graphs,"
"fileoutputstream","filewriter","better overall, does not silently overall, is more overall,","27571929,2738559,10369204,","you need to create a fileoutputstream and decorates it with printstream or better filewriter with printwriter,usually better than filewriter already suggested is to use fileoutputstream which also like filewriter has an append parameter in one of filewriter constructors and which unlike filewriter does not silently assume the default charset encoding slightly bad practice imo,if i remember correctly fileoutputstream is more general purpose - it can be used for binary data or text data;filewriter is used for text only,"
"catransition","uiview","more limited overall,","630770,","the uiview class methods are convenience methods for common animations but are more limited than catransition,"
"lz4","snappy","probably better overall,","21870564,","lz4 is probably better though snappy was developed by google with protobufs in mind so you might want to test both on your data set.,"
"grouplayout","springlayout","more overall,","7128912,","in it developers seem to agree that grouplayout is a more effective replacement of springlayout,"
"cmder","cygwin","better overall,","28890026,","imho cmder is better in windows than cygwin to work with unix commands,"
"glfw","lwjgl"," but also overall,better way overall,","36266049,36265609,","ostensibly this is what java s awt api is supposed to be but again awt and swing have performance problems glfw does not;lwjgl is a wrapper for glfw but also a wrapper for the broader opengl api including functions you d normally have to manually load in c c++ typically through what s called glew or the opengl extension wrangler,so glfw is just a library for creating windows in a better way than lwjgl did before,"
"printf","strtod","slower overall, not overall,","5830907,3791562,","a fairer comparison would be comparing stringstream to the printf sscanf line of functions which would be slower than strtod but still faster than stringstream,keep in mind thatr when i said reconstruct the original value i mean the actual value which was held in the variable expression passed to printf not the original decimal you wrote in the source file which got rounded to the best binary representation by the compiler;since you asked about scanf one thing you should note is that posix requires printf and a subsequent scanf or strtod to reconstruct the original value exactly as long as sufficiently significant digits at least decimal_dig i believe were printed,"
"lan","vpn","slower overall,","14573814,","the reason why the vpn is slow is well because your vpn is likely 50 to 100 times slower than your lan local area network,"
"jsf","wicket","easier than  overall, was faster overall,","2852664,5381240,","i would advise you to try wicket wicket is very easy to learn much easier than jsf and it let s you re-use many existing components as well,i m in the hate it part so anything i said is biased plus in our test prototypes developing in wicket was faster than jsf,"
"mdpi","tablet","smaller android overall,","29828523,","owing to the fact that the screens density on the tablet is smaller android therefore uses the images in the mdpi folder for the tablet and the slightly bigger ones in the hdpi folder for the phone,"
"gitkraken","smartgit","better overall,","44673730,","i think you should use gitkraken it will be better than smartgit on ubuntu,"
"pdfbox","pdflib","better overall,","22675690,","edit 31 march 2014 for what it s worth i have found that pdfbox is much better at text extraction than itextsharp notwithstanding a bespoke strategy implementation and pdflib tet is slightly better than pdfbox but it s quite expensive,"
"objectdatasource","sqldatasource"," is a perfectly overall,also more overall,","15327337,1423595,","i d say sqldatasource is not very appropiated for n-layered applications except n 1 since it talks directly to sql server;on the other hand objectdatasource is a perfectly acceptable option since allows you to call methods from classes on any of the layers the web layer has access to,objectdatasource also allows for more efficient paging than a simple sqldatasource i m assuming that s what you re using,"
"glfw","glut"," gives you better overall, is a better in posters opengl menus, is much more overall,better in posters opengl menus,much closer overall, is more overall,","10154782,14482729,14664193,4497793,2254966,2151314,","you could stop using glut;glfw gives you better control over the loop so that it s easier to do other processing,edit explained as i m guessing you are somehow forced to use glut but i agree with other posters glfw is a better option if you can choose,i would recomend moving away from glut and freeglut;glfw is much more modern and easier to understand,or if you re really focused on only doing opengl i may suggest using glfw which is the better glut but you ll have to draw the menus yourself using opengl,between these glfw is much closer to glut in character -- a small toolkit for abstracting away most of the os-dependent parts so you can produce opengl programs with relatively little hassle,also much of your requirements is already handled by the various opengl utilities - namely glut and glfw;if you intend to stick with opengl both will do the job glfw is more advanced and provides more control,"
"bugzilla","trac"," is better overall, has a script overall,better overall,","4710605,167681,471259,","if you really want something with php then i guess bugzilla but imho trac is better,of course trac doesn t have support for blocking blockedby tickets out of the box so if you want to import this data too you ll have to use the masterticketsplugin and then modify the script yourself which is what we did when we migrated;for bugzilla trac has a script bugzilla2trac.py that will automate the process of importing bugzilla bugs to trac tickets for you,trac s issue tracking is little better than bugzilla s...i know a lot of folks love trac but i find it very inflexible,"
"androidviewclient","monkeyrunner"," that is easier overall,slower overall, bugs see this example; overall,","27562701,30491862,13259359,","i recomend to use dtmilano tool androidviewclient that is easier to use than monkeyrunner,i need to perform several device.touch events as fast as possible however androidviewclient seems to achieve those significantly slower than monkeyrunner,there are also cases where androidviewclient provides workarounds for know monkeyrunner bugs see this example;androidviewclient while not faster because it relies on the same protocol as hierarchyviewer may provide a simpler alternative to write tests,"
"igraph","networkx"," package easily overall, has a lot overall,much easier overall, scales way better overall,","52409239,53689247,22098160,50926801,","the igraph package offers significantly more functionality for community detection including an implementation of weighted fastgreedy;you can save your graph file in networkx as .gml which would make the igraph package easily transferable to igraph,the networkx graph will only connect to the 4 nearest neighbors;so igraph has a lot more connections,networkx is much easier to deal with and usually performance is good enough but for large brute force algorithms like this igraph will probably be at least an order of magnitude faster,for people who ended up here having the same problem but have too many nodes here are few simple improvements on hooked s answer although i am sure there are better solutions out there as hooked mentioned in comments this is just a quick copy-paste fix for people who ended up here with the same reason as i did and had scaling problems 1 igraph scales way better than networkx 2 we can only take a neighborhood of a node to eliminate most of the unnecessary combinations for example if we are looking for a motif in larger network both igraph objects,"
"fread","fseek","faster overall,use  to read overall,faster overall,","10118483,41682407,10118975,","so i guess fseek should be much faster than fread,fseek does not read from the file it just moves the file pointer around for your next read;you need to use fread to read from the file,i probably feel fseek might be bit faster than fread as fseek changes the pointer position to the new address space that you have mentioned and there is no date read is happening,"
"placeholder","required"," is nothing more overall,more overall, which takes up less overall,","573582,17626712,8278035,","that is our quite simple bind object would not fit into the small buffer and would required operator new to be stored;if the bind object above would use a compressed_pair a compressed_pair can actually reduce a compressed_pair size to 8 bytes or 4 bytes for non-member function pointer often because the placeholder is nothing more than an empty object,in such a case passing by value may no required actually passing anything at all -- the value that s passed is basically little or nothing more than a placeholder for this is the object,as for ability to remove right side - it works in the original version but to remove left side there is required a placeholder which takes up less space because all the elements are floated and page would collapse otherwise,"
"openweathermap","wunderground","higher tiers overall,","26804596,","i see that the paid version of wunderground has some higher tiers with more features but openweathermap s free tier allows a huge number of uses,"
"opencover","partcover","better overall, has memory overall,","6988219,6798177,","you may also find that opencover will work better than partcover as it has 32 and 64 bit support as well as supporting .net2 and .net4 - and copes much better with target processes that spin off more processes to do the actual testing,opencover is newer and is also on github opencover on github opencover has 64 and 32 bit support and overcomes some of the limitations that partcover has memory and results delivery,"
"svg","vml","slower overall, was slower then overall,","7892772,10856576,","ie s vml is slower than other browser s svg,we tried vml on ie as backup for canvas but it was much slower than flash;svg was slower then all the rest,"
"diagram","erd"," using chen s overall,lower overall, is probably more overall,","6235395,23322086,9847548,","the inclusion of field names muddies this a bit - an erd shouldn t really express this information afaik;yes this is an entity relationship diagram using chen s notation,the circle in erd denotes the lower multiplicity bound of 0 see diagram bellow,an erd is probably more useful to a database designer to make sure that the database can support the relationships correctly;a context diagram from what i understand would probably be more useful to a web developer or interface designer so that they can pull the correct data into certain views or forms,"
"normals","vbo","model worse overall,","9655339,","when i use vertex array model looks perfect but when i switch to vbo model looks worse because of vertex normals,"
"strcpy","strdup","nicer function in memory callee nicer, doesn overall, is not overall, or create the array overall, is much cleaner overall, is more overall,portable than  in memory callee nicer,","1969127,31801392,36075711,3901979,40423473,44729847,52960483,","in general it is nicer in c to have the caller allocate memory not the callee - hence why strcpy is a nicer function in my opinion than strdup,strcpy doesn t allocate space for your copy - it assumes you ve already done that;strdup allocates memory for you,and strcpy is not safe you d better use strncpy instead;you d better use strdup and include string.h,strcpy does not allocate new memory area for the string it only copies data from one buffer to another;you need to allocate new buffers using strdup or create the array pre-allocated,or indeed if your system has strdup or you re willing to write an implementation then strdup is much cleaner than malloc + strcpy,strcpy str newstr should be strcpy newstr str;strdup is more convenient,there is also a strdupa function but it is less portable than strdup as described in this question strdupa in c - dangers and duplicates if you have already reserved memory locations for the result-strings you can use strcpy,"
"free","memmove","more efficient overall,","388146,","the compiler is free to choose a method that is more efficient than memmove,"
"nserror","nsexception","much more overall,","1117426,","aside from obejctive-c convention and best practices nserror is much more robust and flexibly than nsexception and allows the caller to effectively ignore the problem if they want to,"
"nsis","uninstaller","more overall, is much easier overall, process is getting more overall,","10593657,1073776,55704850,","can a nsis section create more than 1 uninstaller,you might be better off using the popular free nsis installer platform rather than using batch scripts;it s possible to do all the same things you re doing with it and building an uninstaller is much easier,currently using nsis on windows and packagemaker on osx but have issues improving them as my install uninstaller process is getting more complex with time,"
"django-userena","pinax","better so overall,","9572536,","i was thinking of using the accounts app but django-userena seems better so i created a project zero with pinax and tried to integrate django-userena with it,"
"suffix-array","suffix-tree","faster overall,","11956604,","i am interested in this because the factor oracle is easy to construct with 30 lines of c++ suffix-array needs about 60 and suffix-tree needs 150 and it runs faster than suffix-array and suffix-tree,"
"datalist","repeater","more light overall, does not overall,","24339151,17931535,","repeater is more light than datalist as datalist creates the view by creating a table whereas repeater doesn t,you can either use a datalist which does have such property repeatdirection or use css to make sure that elements render horizontally first until they fill the available width and then continue to the next row;out of the box the repeater does not allow you to set the repeatdirection,"
"itextsharp","pdfbox","much better overall,","22675690,","edit 31 march 2014 for what it s worth i have found that pdfbox is much better at text extraction than itextsharp notwithstanding a bespoke strategy implementation and pdflib tet is slightly better than pdfbox but it s quite expensive,"
"adc","uart","higher priority overall,","24189309,","the only problem is that when the mcu is transmitting data uart transmission interrupt has higher priority than the adc reading interrupt the adc is not sampling data hence there will be data loss sample rate is around 500 samples sec,"
"joomla","typo3","more professional overall,","683028,","i didn t work with these applications yet but afaik typo3 and ezpublish both php are considered much more professional than joomla,"
"jscrollbar","jslider","more overall,","24593045,","aside you should probably look into using a jslider which seems more natural than using a jscrollbar for value adjustment,"
"asp.net-mvc","openrasta","much closer overall,","585100,","i d suggest having a look at openrasta which is much closer to http semantics than asp.net asp.net-mvc is,"
"cashapelayer","uiview","easier overall,","30911368,","you can use a bezier path either in a custom uiview w drawrect or easier with a cashapelayer whose curvature can be controlled via its control points,"
"button","right-align","easier overall,","7810180,","instead of figuring out the individual positions for each image button it is easier to simply right-align them by modifying the anchorpoint to 1.0f 0.5f and then position the images button at exactly the screen width and variable screen height position screen width 100,"
"strcpy","strlen","more in byte returns use, is not overall,shorter overall, which is more in byte returns use,","5941669,41184633,33238443,15659693,","if strcpy is anything like strcpy it will write one byte more than strlen returns to zero terminate the string,with memset you have also to call strlen for get the length and add one;with strcpy is not required,strcpy incoming connected will overwrite dataa and maybe datab if your first token pointed to by header is shorter than strlen connected,since is a string use strlen;by the way you could also use strcpy which is more suitable for strings,"
"justmock","typemock","less expensive overall, which is good;the better overall,","12626118,10901876,","justmock less expensive than typemock but still expensive,it can mock almost anything like typemock and is also cheaper than typemock which is good;the better part is they have a trimmed version of they product - justmock lite which is free,"
"gedit","vim","better overall,better overall,","16252339,18436657,","if you are on linux eg ubuntu you may try gedit for ease of use with some plug-ins but to handle very large files you re better off with vim,if you re just going to use vim the same way you use gedit don t bother - gedit is better than vim at being gedit,"
"gson","xstream","more overall,","2788981,","gson is more about json-based persistence in the vein of xstream which can do json too and sounds a bit too heavyweight for just an applet,"
"freemarker","rythm","faster overall,","13855484,","probably you can take a look at rythm template engine which is much faster than freemarker and velocity also much easier to use,"
"cut","dash","symbol better results overall,","40914062,","the smaller the dash symbol is the better results you will get the existing api code doesn t seem to cut off the symbol at the final vertex of a polyline segment and doesn t seem to account for the section of the symbol that went past the polyline after the vertex,"
"ceil","parseint","faster overall,","17551105,","the math.floor ceil method being marginally faster than parseint and mod,"
"hdfs","namenode"," is not overall, cluster; is not currently overall, keeps the information overall,more overall, solved my issue overall, requires a more overall,","30914642,19924023,30873728,47349397,51010641,28138669,","in this case you may check if the namenode is started correctly on the master by checking logs at your yourhadoopfolder logs hadoop- hadoop-user -namenode-master.log;it is often caused by the hdfs is not formatted before,the namenode is a single point of failure for the hdfs cluster;hdfs is not currently a high availability system,namenode keeps the information about files and their associated blocks;hdfs will not move blocks from old datanodes to new datanodes to balance the cluster if a datanode added in hadoop cluster.to do this you need to run the balancer,hdfs high availabilty is only available when you have more than one namenode configured,command for formatting hdfs hdfs namenode -format when namenode was not working after formatting hdfs;i was facing the same issue formatting hdfs solved my issue,since the namenode is a single-point-of-failure in hdfs the namenode requires a more reliable hardware setup,"
"tcp","tftp","probably better overall,","32126306,","you are right that using tcp is probably better to use for something like this or even an existing protocol like tftp,"
"mongoid","mongomapper"," has better support overall,more overall,","3312450,14064146,","mongomapper has better support for relational associations non-embedded and has greater third-party support;mongoid has better query support much better documentation mm has close to none though a website is supposedly in the works rail 3 support and thus devise support and a slightly more active community on google groups,over the last year it looks like mongoid has been more regularly maintained and updated than mongomapper,"
"crossover","mutation"," but not overall, does not occur then overall, operator is less overall,typically easier overall, operator is a better overall,","29262873,43360716,13950475,9824732,37733425,","a common mistake is to encode everything as a bitvector and then in crossover to split the bitvector at random places splitting up the good thing the bitvector represented and thereby destroying the thing that made the individual float to the top as a good candidate;a vector of limited integers is likely to be a better choice where integers can be replaced by mutation but not by crossover,mutation does not occur then the only way to change genes is by applying the crossover operator;regardless of the way crossover is performed its only outcome is an exchange of genes of parents at certain positions in the chromosome,add a new operator alongside mutation and crossover that performs your local search;you might find that the mutation operator is less useful in the hybrid scheme so just replacing that could be viable.,mutation is typically easier to do this with than crossover,some form of mutation is another possibility;probably if the characteristics of cx aren t satisfying a different crossover operator is a better choice staying with simple operators one of the most successful is order crossover 2,"
"tcpreplay","wireshark","different less overall,","43753735,","i use tcpreplay to replay it on an interface but the problem is that the number of attempted packets in tcpreplay is different less with number of packets showing in wireshark,"
"compareto","icomparable","more overall,general overall, interface is offcourse better overall,","11193158,1734747,13502632,","implementing icomparable is a more fine-grain way of comparison as it provides the compareto method which is a greater-than less-than comparison as opposed to equals which is simply a is-equal-or-not comparison,the icomparable defines order less than greater than;the method defined by the icomparable defines is compareto with which you can determine the order between two elements,the icomparable interface is offcourse better to have a strong typed compareto method,"
"median","minimum","greater current overall, value is closer overall, is lower in hi m+1 smaller, is considerably lower overall,smaller in hi m+1 smaller,higher than the  overall,","21073127,53613333,56443598,50996743,41420080,26663593,","maintain 2 heaps maximum heap for numbers less than current median and minimum heap for numbers greater than current median,that is because we have the same amount of elements at both side but the median value is closer to the minimum thus there should be elements with smaller difference between both side,you can solve it using divide and conquer approach find a random element in between the minimum and maximum check if it s median if the median is lower or higher than it and reduce the problem to a smaller size only on a subrange of the array,the following example used a matrix with 500 500 elements resulting in those timings only for 2 runs unit seconds expr min lq mean median uq max neval cld parapply 191.5861 191.5861 192.6157 192.6157 193.6453 193.6453 2 a dt_parap 135.0570 135.0570 163.4055 163.4055 191.7541 191.7541 2 a the minimum is considerably lower although the maximum is almost the same which is also nicely illustrated in that boxplot full code,there are at least n m+1 2 elements no larger than the maximum hi of these median and at least n m+1 2 no smaller than the minimum lo,if the number is higher than the median the median is the new minimum,"
"jinternalframe","jscrollpane","bigger overall,","30339173,","specifically jscrollpane have height bigger than jinternalframe,"
"gridster","highcharts","bigger overall,","32656402,","if i resize the window and a widget with a highcharts is bigger than the window i want to resize it to make it smaller and gridster should recalculate the positions of the others,"
"if-statement","loops"," is faster overall,more overall,","9143497,33905400,","in vba it is more optimal to use 2 if-statement in these cases that way you aren t checking for florida if you don t find miami;the other advice i have is that a for-each loops is faster than a for-loop,nevertheless i need a dynamic list for my loops with nested loops which is processed more than 500 times and multiple if-statement therefore the arraylist,"
"buttonclick","clicklistener","more overall,","28341548,","if you need more than a simple method call you could just as well create a more elaborate clicklistener implementation that has more than the buttonclick method,"
"lisp","smalltalk"," that had a more overall, if really overall,more fine-grained overall,lesser extent overall,","52429893,8673820,3057686,3819159,","as it is commonly implemented smalltalk is focused on messages being sent to methods making decisions at runtime and common lisp is more focused on macros calling non-generic functions making many but not all decisions at compile-time. these are fundamental design and implementation decisions but there s no reason you couldn t create a lisp that had a more smalltalky feel by utilizing clos and deferring more decisions to runtime,n programming languages of the lisp family scheme comes to mind if is an expression meaning that it returns a value and is implemented as a special form;on the other hand in pure object-oriented languages such as smalltalk if really is a method more precisely a message typically implemented on the boolean class or one of a method subclasses,smalltalk methods tend to be more fine-grained than lisp functions so that may be a good place to begin,common lisp is an image base language although usually to a lesser extent than smalltalk,"
"buildr","maven","faster overall, is much more standard here overall,faster overall, does not overall,","145433,7056519,7776944,19492850,","and even though everybody says ruby is slow buildr was 2-6x faster than maven,also using gradle buildr means that to build the product one needs to install gradle buildr;maven is much more standard here,i read in a blog article that buildr a ruby based build tool was two times faster than maven for a simple build,the coolest approach is imo taken by lockjar which builds a frankstein of bundler and maven and provides a nice abstraction to avoid the famous maven uncertainty principle;buildr does not resolve dependencies by design,"
"bag","set"," gets much more overall,more overall,","24707870,28813068,","if your elements are drawn from a small finite set can only be compared for equality then your bag gets much more expensive and i m pretty sure you end up with something like o nk instead,take into account that lazy mappings will be disabled so it may be useful to evaluate what will be need to be set to fetchtype.eager with fetchmode.subselect if more than 1 bag is needed to be initialized with it,"
"fsunit","xunit.net","more natural oo overall,","1468941,","i d rather use fsunit or fstest to write tests in f# it feels more natural than oo xunit.net style tests,"
"printf","strdup","faster overall, could overflow the buffer overall,","33003723,14681551,","doing a single printf and strdup is faster and simpler than doing 2-3 printf calls,f you want to return that string strdup it then you ll need to free the result of strdup later;ps printf is more dangerous than printf because ps printf could overflow the buffer,"
"fgetc","fgets"," would not overall, is likely faster overall,simpler overall,faster multiple overall,","50897289,33370537,13322357,5186505,","indeed fscanf can set the end of file condition of a stream if it reaches the end of file which it does for s if the file does not contain a trailing newline whereas fgets would not set this condition if the file ends with a newline;fgetc sets the condition only when it returns eof,fgets is likely faster but it limits line length to some constant;fgetc gets 1 unsigned char at a time or eof,fgetc is a function to read a single char simpler than using fgets,if not multiple fgets calls will still be faster than multiple fgetc calls because the overhead of the latter will be greater,"
"ciimage","uiimage"," doesn overall, is not overall, does not overall,slightly larger overall, is not something overall,same as  overall,convert  to ;let overall,","54396537,40820337,22795556,11301560,43082057,52713704,54436501,","you need to use the ciimage created on top from the current image as below;you are using uiimage instead of ciimage in combinedparameters and uiimage doesn t have extent method so you are getting the crash,the heart of the matter is that passing through ciimage is not the way to crop a uiimage;for one thing coming back from ciimage to uiimage is a complicated business,it merely provides a reference to the ciimage that is already the basis of this uiimage;but this uiimage does not have a ciimage basis,it seems like if the uiimage is slightly larger the ciimage is double the size whereas if the uiimage is slightly smaller this isn t the case,there is no scale and no screen a ciimage is not something that is drawn so there are no pixels;a uiimage backed by a cgimage is the basis for drawing and in addition to the cgimage it has a scale,i am merging two images using ciimage filter cisourceovercompositing issue is it is losing alpha opacity of watermark foreground image if its applied opacity is less than one. i applied alpha value like this i think issue is i need to change alpha from ciimage through some calculation as ciimage working space is not same as uiimage,update in case if you are fatalerror means uiimage doesn t exists with ciimage so you need to convert uiimage to ciimage;let s say your scaledimage is an uiimage so you can directly get an ciimage without creating a new one,"
"jmock","junit"," which doesn overall,things easier overall, flushes the instance overall,","9194355,10572775,8984779,","jmock depends upon hamcrest and junit uses and includes some hamcrest classes in it s jar so this is probably where the problem comes from;the easiest fix is to use the version of junit which doesn t have the hamcrest libraries included junit-dep,the later version of jmock makes things easier by integrating with the junit lifecycle as a runner,one more thing you don t need to set the fields in the test class to null junit flushes the instance for every test;jmock isn t thread safe,"
"imagemagick","libjpeg","slower overall, is definitely better overall,","965313,50770228,","basically imagemagick was only slightly slower than libjpeg,if you are going to install imagemagick into not standard path then you will need to specify it s path like below cimg imagemagick_path c imagemagick-6.9.2-q16 convert.exe;if you don t want to install imagemagick then you should link your code with libjpeg library and enable native jpg file support in cimg putting this before including cimg.h #define cimg_use_jpeg #include cimg.h using cimg with libjpeg is definitely better,"
"jung","prefuse","better layout overall,","4670846,","somehow the layout algorithms in prefuse seem to display a better layout than in jung rendering is also better i think though most of the layout algorithms in prefuse are based on jung implementation,"
"sip","voip"," and never overall,more overall,","54443632,5309949,","i am not familiar with sip voip and never written voip application,almost all sip voip providers allow you to forward calls from an attached did telephone number to more than one sip device so your request could be satisfied by a basic account on your friendly neighbourhood voip provider,"
"concurrenthashmap","readwritelock","more concurrency overall,","35215283,","the other question why not use a concurrenthashmap here since it will provide some concurrent writes to different mapentries and provide more concurrency than readwritelock,"
"libxml2","lxml"," is much more overall, the more overall, that has a bug overall,more overall, is more overall, library so overall, docs goes into more overall,","260148,50882842,36004048,10715774,12891191,3819023,4055983,","libxml2 libxslt also come with their own much lower-level python bindings but lxml is much more straightforward and pythonic and it seems to have great performance as well,it s installed with lxml in fact you probably want to use lxml instead of libxml2 because lxml is based on libxml2 is more pythonic sudo pip install lxml libxml2 page says note that some of the python purist dislike the default set of python bindings rather than complaining i suggest some of the python purist have a look at lxml the more pythonic bindings for libxml2 and libxslt and check the mailing-list,you re not using lxml wrong and it s not that lxml doesn t support preserving whitespace in this scenario as so many other so entries might have you think;it s just that you were unwittingly using a version of libxml2 that has a bug that s since been fixed,the chapter starts with short course to xml general talk but with the atom syndication feed example then it continues with the standard xml.etree.elementtree and continues with third party lxml that implements more with the same interface full xpath 1.0 based on libxml2,between lxml and beautifulsoup lxml is more equivalent to nokogiri;because it is based on libxml2 and it has xpath css support,lxml is much easier to use than the xml libraries included in the standard python library;it s a binding for the c libxml2 library so i m assuming it s also faster,the parsers page in the lxml docs goes into more detail about setting up a parser and iterating over the contents,"
"stderr","stdout"," to dev null use in use streams popen, not in use streams popen, is fflush then in fflush line much,better in better errors couple, which makes it easier overall, not overall, is used more often overall, is more overall, on not overall, isn overall,more control overall,","9149751,36308646,35359182,3113442,30315018,23151227,14686887,25755038,53821167,55785822,47591405,","you re probably still receiving emails because you re only redirecting stdout but not stderr;to redirect stderr to dev null use,use to redirect stderr 2 to stdout 1 or fork the process and manually deal with all three streams;popen catches stdout not stderr,with stdio 3 output streams that refer to terminal devices are line buffered like stdout while stderr is not buffered;the program above exits immediately after fprintf so stdout is fflush then,stderr is better than stdout for errors for a couple of reasons,this also allows you to redirect the stderr into the stdout which makes it easier to read,so you need to redirect stderr of strace to stdout before piping to grep;this is because strace writes all its output to stderr not stdout,standard error or stderr;of the two stdout is used more often both by internal commands like copy and by console utilities or external commands like find and others as well as by third-party console programs,to save subprocess s stdout and stderr is more complex because you should consume both streams concurrently to avoid a deadlock,a better solution would be using this batch file the command for runs in a separate command process with cmd.exe c more precisely compspec c in background the command line dir outputs to handle stdout of started command process in bare format because of option b the file names with extension and with full path because of option s of all only non-hidden files because of option a-d-h attribute not directory and not hidden with file name index.js found in specified directory or any subdirectory because of option s;the error message output by dir to handle stderr on not finding anything matching these criteria is suppressed by redirecting to device nul,see this running in an online interpreter at advice re best-practice usage prompts are conventionally written to stderr on unix so for prompting-related purposes retaining the original stdout isn t generally called for;prompting on stderr is what read -p does,in other words instead of mis-using stderr like this - look into ways to gain more control over stdout,"
"stderr","stdout"," to prevent the process in output reader present,zero difference between  overall, is not overall, not overall, is not overall, is a little more overall, is not overall, get s fflush in fflush line much, is much less overall, devices works better in better errors couple, is not overall,","9791321,48769111,45263795,39717571,45317939,4201996,2096717,54614957,23465339,43257596,4569743,","finally simply having a reader present on the stdout is not going to be enough;you need to actively be reading from it and from stderr to prevent the process from hanging if it writes too much output,the fix is to redirect stderr or redirect both stdout and stderr background when processes are initially created they generally always have three initial file descriptors already opened for them 0 stdin standard input a read-only stream 1 stdout standard output a write-only stream 2 stderr standard error a write-only stream there is precisely zero difference between stdout and stderr other than convention and the file descriptor number,that is any thread that writes anything to stdout followed by a n that line will be printed on the terminal intact not interleaved;in contrast stderr is not buffered - it s output immediately which means that two threads writing to stderr simulatanouesly will cause the output to be interleaved,if you still want to expose stdout to see the start-up message for example use;you need to redirect stderr not stdout,consequently anything printed to stderr appears immediately while anything printed to stdout will be held until the buffer fills up on linux normally 8kb;note that when ls detects that stdout is not a terminal it sets the -1 flag by default one filename per line,in general you shouldn t be writing to stdout at all from your library - even in a console application that could corrupt the output that the application is producing;stderr is a little more forgivable but you still shouldn t really use that unless the application requests it,stdout is line-buffered by default so that is relatively safe;stderr is not,i guess that is because stdout is buffered while stderr is not or not so much;so stderr get s fflush after the line agent no process killed is streamed while stdout is fflush after the script . somescript.sh exists,most posix-centric tools prescribe a very careful specification for their stdout stream so the tool s output can be piped elsewhere;stderr is much less prescribed and used liberally for logging and errors,if you use java.io.bufferedreader stdout stderr devices works better,like stdout stderr is usually directed to the output device of the standard console generally the screen;that means stderr is not redirected to stdout but they share a common file descriptor,"
"stderr","stdout"," makes it easier overall, it is more in output reader present,","1068025,54837065,","always stdout makes it easier to pipe to less grep it etc;if you are showing the help text because there was a problem with parsing the command line arguments then you might use stderr,ffmpeg being built as a command line tool that can take it s input from stdin and output to stdout stderr it is more simple to use these capabilities than to try to have ffmpeg handle the http reading writing,"
"mono","xamarin.android"," managed httpclient handler overall,faster overall,","48475612,22214966,","by default xamarin.android uses the older mono managed httpclient handler that does not support tls 1.2,i m using xamarin.android to write c# but the native java rsa key generator is much faster than the mono one,"
"content-length","content-type"," was not overall, and not overall, is greater overall,","55782360,12098951,25000107,","charset utf-8 host graph.microsoft.com content-length 111 expect 100-continue connection keep-alive odata.type microsoft.graph.opentypeextension extensionname com.test.nickname date openextension;content-type was not specified and it was going as text,the most common quirks with ie9 that often do not bother web browsers are mismatches in content-length well this did bother safari last time i looked possibly content-type this acts in reverse - ie9 sometimes correctly gleans html mimetype even if the content-type is wrong connection close;so yes it could be a problem with http pipelining specifically if you pipeline a request with invalid content-length and not even chunked-transfer-encoding ie might wait for the request to finish,...web api relies on system.net.http library for getting request headers and system.net.http library gives a null for httprequestmessage s content.headers.contenttype in this case and web api sees that content-length is greater than 0 but no content-type header and hence returns a 415 unsupported media type,"
"vb6","vbscript","better than  overall,much more convenient overall,","962399,1124848,","vb6 vba though a little better than vbscript in general still has many similar issues where for they domain they require much more boiler plate to do simple tasks than what i would like and have seen in other scripting languages,vbscript is much more convenient than creating an activex on vb6 or c# vb.net,"
"knitr","latex","familiar with  overall, may not overall,clunkier pure overall,easier overall,richer interactions with  overall,new with  overall,","47736500,8910048,28529833,27659793,57308117,51468239,","it is common to write something like this which works fine. for those not familiar with knitr or sweave this echos the code and output in a latex verbatim environment and also adds the completed plot as a figure in the latex document. but now i would like to write more detailed line-by-line commentary like the problem is that there are now two knitr code chunks for the same plot,if you do have spaces knitr will be able to produce the output with the long lines wrapped with so will sweave i think if you set;this is a difficult and extreme case because you do not have spaces among those a s so latex may not be able to wrap the words,even when latex editors support knitr their support makes working with .rnw files much clunkier than pure latex .tex files,you might want to use r markdown and knitr which is easier than using latex and r as also zhaoy suggested,3 if your papers and written in latex you should be able to use the latex class provided by your journal and not be worried by the final appearance of your tables. as far as i know i m not really a rmarkdown or knitr user and not at all and rstudio user org-mode for emacs allows more control on the final output and richer interactions with latex,i am quite new with knitr and i am trying to write my first document importing some newcommand from my latex files,"
"insertion-sort","quicksort","less commonly overall,","18309423,","in cs insertion-sort is less commonly used because we have much better algorithms quicksort and merge-sort come to mind,"
"ostream","ostringstream","more overall, is a way overall,","12054458,36909299,","ostream is more general subclasses support writing to different places ostringstream is a specific one writing to a string,in c++ you could associate the output stream with a null device too and test the number of charactes printed with std ostream tellp;however using ostringstream is a way better solution see the answers by devsolar or angew,"
"ng-class","ng-show"," aren overall,better option overall,","45929983,25702788,","basically what i did was remove things like ng-hide ng-show from the elements that were supposed to have animation and instead i ve put ng-class to manipulate the visibility while maintaning the animation;the reason that ng-hide ng-show aren t the best ideas for animation is that they apply display block property to your element thus any transition applied to them will not work,edit as mentioned by chandresh using the ng-show directive with a true false value would be a better option than using ng-class,"
"malloc","memset","greater overall,general overall, to clear it manually overall,","4868335,4309323,14805339,","it requests memory from the os kernel but the request is not satisfied until the memory is written to with memset . this allows for greater efficiency in the system s memory management but it can result in misleading malloc behaviour,it s like plus a memset but feels cleaner to me;you also shouldn t have a cast on malloc calloc etc generally speaking it can obscure useful warnings and you need to allocate six slots like i said so you can have the null-terminator which is the zero-valued character so we don t need to set it explicitly,you can use calloc to automatically clear the memory or memset to clear it manually;malloc doesn t clear the memory so you get garbage in your allocated blocks,"
"case-insensitive","case-sensitive","faster overall, char is quicker overall,more than just  overall,slower overall,","32202691,7164786,53178680,7652236,","btw a case-sensitive search done with removing i is much faster than a case-insensitive search,if you re looking for a single character and it is not case-sensitive use the overload that works with a char;searching for a single case-insensitive char is quicker than a sub-string,this method has the potential to be used for more than just case-insensitive pattern matching in regards to css the id selector #example is case-insensitive while the id attribute selector is case-sensitive unless you use the case-insensitive attribute selector,this tiny overhead on add is vastly outweighed by the savings on lookups since all programmers should know and understand that case-insensitive compares are vastly slower than case-sensitive especially with unicode - the cpu can t just do a block compare of data but must check each pair of characters specially even using a table look-up this is vastly slower,"
"fog","paperclip","more trouble overall,","11912590,","i have found though that since i started using the asset_sync gem which uses fog instead of aws-s3 gem i don t have any more trouble with paperclip and s3,"
"altitude","satellite","error much greater overall,more overall,","7618704,23617594,","quote from the third link - the altitude error is much greater because it is a satellite based system,vertical position altitude needs one more satellite in view at least 4 so is not always reliable if gps sees only 3 satelites,"
"innodb","tokudb","faster overall, has slower overall, is not at all overall,","5691542,54283381,36779703,","i use tokudb on tables of up to 18 billion rows and nothing else comes close it s at least 100 times faster than innodb for random inserts on big tables,in my experience tokudb has slower insert rate and struggle before hitting 100m of records so i went to innodb that time,engine innodb will create you an xtradb table;tokudb is not at all supported on homebrew mac os x in any way and likely will not even compile,"
"direct3d","gdi"," is not overall,faster overall,","3900409,33487104,","imho the right way to go is direct3d or opengl but given delphi is windows only maybe direct3d is better;delphi was introduced exactly because the gdi is not good at such tasks,so decide what you want to use direct3d is significaly faster than gdi,"
"qevent","qobject","faster overall,","10839789,","posting qevents to qobject is faster than using signal-slot invocations because there are no copy constructors called and there s no marshalling done except directly by you upon construction of a qevent,"
"fflush","stdout"," needs more overall, isn in chance buffer program,frequency lower overall, buffer isn in chance buffer program, doesn overall,more than  overall, doesn overall, doesn overall,","2605364,19206311,5903481,32577390,889246,52387073,31172218,36344612,","occasionally the stdout needs more than a write method fflush is another common one which stringio will handle,stdout can be line-buffered meaning that output won t appear until you ve printed an entire line;the fflush isn t always necessary but it s a good idea,i guess that these lines have shorter output so the fflush frequency is lower i used the stdout line to print a deliberate help message,when you interrupt the script there s a good chance the stdout buffer isn t fflush;in your program immediately after every printf add an fflush stdout to fflush the buffer,also relating to stdout in addition to flushing output per previous example stdout.flush you can ask ruby to automatically sync writes to an io buffer in this case stdout with associated device writes basically turns off internal buffering;also i find that sometimes fflush doesn t work for me and i use io.fsync instead,i m pretty sure you guys aren t experiencing any errors the adding of n does nothing more than fflush your printing buffer and make it in fact a visible indication within your stdout or a terminal for this matter,so when you tell the program to fflush stdout you re really just telling it to take what s in stdout and make sure it s all displayed on the screen.;in computer science the term fflush doesn t mean to erase what s already in a buffer,nothing is written to stdout which is what print does before it s fflush;this happens implicitly when print ends with the standard newline but when you provide a different end like this and the string is short the implicit fflush doesn t happen,"
"opus","speex","better overall,","7271348,","it will continue to be available but since opus is better than speex in all aspects users are encouraged to switch,"
"jlabel","jtextfield","always easier overall,more overall,","34516471,47476065,","because copying text result from jtextfield is always easier than retyping it from jlabel note that jlabel is not focusable,jtextfield is more than 1 line height and jlabel is not in left side,"
"openfeint","scoreloop","quicker overall,","8975862,","further to this i m still opting for scoreloop and i can confirm this behaves the same regarding one score per user but it offers a lot more functionality and does seem to respond much quicker than openfeint did for me,"
"invokeandwait","invokelater","better overall,","32543633,","you can use eventqueue.invokelater to switch to the swing thread and back greenfoot isn t designed to support this but it should work anyway -- invokelater is a better bet than invokeandwait,"
"silex","symfony","lower overhead overall, does not overall, doesn overall, gives us more overall,","14501349,31376551,42736898,33489771,","also take a look at silex as this has lower overhead than symfony and works with symfony forms,if you insist on going the yaml route you shouldn t if you need frameworkbundle you should use symfony instead of silex you can always create your own redirectcontroller with a urlredirectmethod be aware that resolving controller arguments won t work directly this is one of the reasons i suggest you to use symfony and don t reivent the wheel;afaik silex does not have such an option but you can just return a redirect response from your route without yaml,symfony has built-in http_basic_ldap authentication provider which perfectly suits your requirements;unfortunately silex doesn t have one so you need to do it on your own,now symfony gives us more control over the structure and architecture;good alternative to silex if you prefer the symfony 2 style,"
"smartgwt","uibinder","much more sophisticated overall,","6262121,","while smartgwt has much more sophisticated widgets than gwt you still can t use uibinder in conjunction with it which is why i rejected using smartgwt,"
"cocos2d-x","corona"," does not overall,better overall,easier overall,","6216870,10185108,11746599,","cocos2d-x has a much more managable learning curve than doing it all from scratch and there are a number of great resources available cocos2d-x forums this blog etc;a quick google search showed me that corona has costs associated with corona for distribution - cocos2d-x does not,i would recommend cocos2d-x as it is in my opinion equally or even better than corona and it is free,the consensus seems to be that corona is easier to use but that cocos2d-x has the advantages that come from being open source easy to customize merge with other code community etc,"
"subshell","zsh","more overall,","46202735,","simple function in zsh to parallelize jobs in not more than 4 subshell using lock files in tmp,"
"farsi","persian","better overall,","16129313,","in persian it s better to say farsi a decimal number like 32.98 is written 32 98 slash is used instead of point,"
"glsurfaceview","renderscript","more control overall,","21351418,","both a renderscript and opengl are used for getting high performance graphics and animation s.but still opengl is the best option to get high performance graphics because it is well documented and you will have more control over the glsurfaceview .but in renderscript some of the classes are depreciated in the current versions .its almost not possible to make a rssurfaceview to transparent,"
"gio","pygobject","nothing more overall,","5819202,","pygobject is nothing more than a thin layer of glue making gobject gio .,"
"bmp","pgm","more portable overall,","37988585,","i used ppm pgm files as they are simpler to write and more portable than bmp,"
"passenger","webrick"," that makes it better in likely popular specific, webserver; has more overall, mod_rails is far more overall,better overall, is not overall, handles more in likely popular specific,","3398838,25588609,1890342,4727430,10162011,31123831,","passenger is likely the most popular choice for production now due to its easy configuration speed and features;if there is a specific use case for webrick that makes it better than any of the other server choices i d love to know, realized that i had switched to using passenger as my web server from a singled threaded process webrick webserver;passenger has more than one process running on the same dyno and apparently the same dyno sends a callback url with a port number in the request,mongrel is still a valid option but passenger mod_rails is far more popular;you can try to use webrick which comes as a standard with rails,phusion passenger is the de facto apache mod and yes it s better than webrick which is really only good for tiny sites or testing,phusion passenger is a full fledged rails server;when it is used webrick is not involved,also i use standalone passenger as my integration test server rather than webrick since standalone passenger handles more realistic serving of static files,"
"azure","hdinsight","batch cheaper overall,","35640508,","running with azure batch is also an option as the .net will work well and azure batch is cheaper if the custom activity is the only reason for having a hdinsight cluster,"
"comm","grep","faster overall, will always overall,faster overall,","43449232,10803166,38694413,","i rather prefer grep since it s much faster than comm and also does not require the input to be sorted,the comm command should be faster than grep because the comm command will work with sorted data while grep will always do a linear search,if this is an issue you could try sd stream diff which doesn t require sorting like comm does nor process substitution like the above examples is orders or magnitude faster than grep -f and supports infinite streams,"
"httr","rcurl","nicer overall,","29130522,","the httr package is a bit nicer than rcurl for making http requests in my opinion and it sets a user-agent string by default,"
"beautifulsoup","lxml.html","faster overall,better overall,","8698357,4014590,","but if you are allowed to store whole tree into memory you can use lxml.html which is faster than beautifulsoup,lxml.html deals with badly formed html better than beautifulsoup is actively maintained beautifulsoup isn t and is a lot faster since it uses libxml2 internally,"
"plyr","sqldf","3x slower overall,","4329527,","in the first example sqldf is 3x slower than data.table and in the second its 200x faster than plyr and 100 times faster than data.table,"
"bazaar","svn"," is a little more overall, does not overall,much more overall,","5258530,16847428,1645672,","if you really can t decide pick the one with the nicer clients tortoisesvn vs the bazaar client - a better client will most likely make it easier to use;bear in mind that svn is a little more common and there are a lot of tools for svn,bazaar does not have a subrepository feature;correction apparently there s a bzr-externals plugin available at lp bzr-externals though it emulates the svn externals feature not git submodules or mercurial subrepos.,bazaar seems much more like svn with branches just being separate directories so maybe not,"
"tunnel","vpn"," is easier overall,nothing more overall,","52910964,9325951,","this appears to be all doable and maybe the reverse tunnel is easier than using a vpn for the same end,and between we are just looking to send and receive data over tunnel there is nothing more or need to control computer on the network all we need is to route data through corporate firewall and should support multiple vpn servers such as cisco microsoft etc,"
"gradle","ivy"," is lack overall, not overall, is a much more overall,longer overall, gives better dependency overall,","7060640,16841699,1534821,20470505,3182877,","although buildr is undoubtedly a great effort at the moment gradle is much more professionally made;the only problem i faced with gradle is lack of native support for reusing ivy.xml and ivysettings.xml despite the fact that gradle actually uses ivy as its dependency engine,they will be mapped to ivy configurations when gradle generates an ivy.xml;gradle s configurations are heavily inspired from ivy not ant but they aren t fully equivalent to ivy s configurations and we d have to discuss what you mean by equivalent,as you know ivy is a much more powerful and much less opinionated dependency management tool than say maven;gradle detects dependencies between projects and between projects and jars,today gradle comes with a full-fledged dependency management implementation that no longer builds upon ivy except for some traces that haven t been removed yet for backwards compatibility reasons,we use gradle and chose gradle over maven and ant;ant gave we total flexibility and ivy gives better dependency management than maven but there isn t great support for multi-project builds,"
"chart.js","d3.js","steeper learning overall,","41853317,","d3.js has a steeper learning curve than chart.js but once you get the hang of it you can do pretty much anything graph related with it.official site is here,"
"fgetc","getline","faster in iostreams faster release,version slower in iostreams faster release,","8854366,8854366,","the fact that using getline with iostreams is faster than fgetc at least in release mode runs counter to the reasoning that copying all that data must be slower than not copying it so i m not sure what all optimization is able to avoid and i didn t really look to find any explanation but it d be interesting to understand what s being optimized away,i confirmed that in debug mode the getline version is slower about 130 âµs vs 60 âµs for the fgetc version,"
"socks","ssl","more overall, or not overall,","41940245,46047667,","since socks proxy is in theory nothing more than a ssl tunnel i thought i could give that property another try,note that this is not the method proposed in draft-aft-socks-ssl-00 in 1997 which proposed layering tls over the top of socks framing imo an unnecessarily complicated way of doing it which only provides a single benefit - being able to negotiate ssl or not - with a lot of down-sides;wingate socks server supports accepting a connection tcp and immediately performing a tls handshake on it prior to socks protocol,"
"ecto","elixir","better overall,","41630254,","fortunately ecto 2.1 has a better alternative since it s supports the built-in calendrical types from elixir 1.3,"
"pbkdf2","salt"," which isn overall,no longer overall,","46987382,28428874,","because the password we want to use wizardsarecool is not 128 bits in length it s actually 14 utf8 characters long so 112 bits we use pbkdf2 which is a key derivation function to derive a key from our password that will be 128 bits long hence the keysize being 128;pbkdf2 takes a few parameters here including the password string the salt which isn t all that important when using pbkdf2 as a key derivation function but we still apply good practice and make it random as we should and an iteration count which determines how many times the underlying hash is applied to the password,it should be in my opinion more strongly noted that password hashing with salt is no longer secure and should not be used as others noted use pbkdf2 or bcrypt,"
"linefeed","spaces"," and only overall,more overall, is not overall,more than just  overall, not overall,more than single  overall,","52139086,33176095,51322396,23026821,23075702,34097833,","for example if i want to type this table character ascii tab 9 linefeed 10 carriage return 13 spaces 32 without using tabs i have to type spaces many times after the word tab not as many times after linefeed and only a few times after carriage return,this regex does replace by a single spaces all contiguous spaces 2 or more followed by a linefeed or individual tabs,syntax notation the following core rules are included by reference as defined in rfc5234 appendix b.1 alpha letters cr carriage return crlf cr linefeed ctl controls digit decimal 0-9 dquote double quote hexdig hexadecimal 0-9 a-f a-f htab horizontal tab linefeed linefeed octet any 8-bit sequence of data sp spaces and vchar any visible usascii character . so whitespace is not allowed in the name of a header;spaces is not a valid character in a header name,s will match more than just spaces because this also matches vertical whitespaces like linefeed carriage returns.,also notepad++ can show white spaces chars including new lines which can be handy in such cases;it is probably the application or system you are using to view your file ignoring the linefeed not c# ignoring it,it makes minimal change to the text in particular it never splits a word doesn t change wc -w and for text with no more than single spaces in a row and no cr it doesn t change wc -c because it replaces spaces with linefeed rather than inserting linefeed,"
"achartengine","mpandroidchart","more overall,","29864340,","achartengine is easy to use mpandroidchart has more option but is less easy to use than achartengine,"
"bstr","cstring","more overall, they re null-terminated too overall, won t overall,","12632385,56959567,15441462,","a cstring is more like a visual basic string or a bstr,about allocsysstring it creates a copy in bstr they re more complex than cstring they re null-terminated too but they also have length at negative offset,if the bstr doesn t have nul s embedded inside you can simply pass it to a cstring constructor that will make a deep-copy of it and you can locally work with your cstring;modifications to that cstring won t be visible on the original bstr,"
"mktime","strtotime"," is more overall, is more overall,dumber overall,","44305278,21206426,7263012,","you don t need to use mktime;strtotime is more human friendly,yeah entries are strings then mktime is more consistent to use the strtotime function,you d be better off using mktime for this as it s dumber than strtotime,"
"associativity","operator-precedence","higher precedence overall,","27779081,","taking java s operator-precedence notably that + has higher precedence than and associativity rules into account the expression is equivalent to,"
"modulus","primes","more than one  in several ways notation,more than one  overall, are relatively overall,more overall, that is larger overall,more than one  in several ways notation,","50081813,50081813,47957640,9195786,52270320,50081813,","if a higher power of 3 also divided more than one modulus we would need to check those higher powers as well. for each primes that we discovered and each modulus we find the highest power of that primes that divides the modulus,we also add the new equations we get for each power of the primes that divides more than one modulus,the linear congruential generator explores every value less than the modulus when the multiplier and modulus are relatively primes,it is perfectly possible to use rsa with a modulus n that is composed of more than two primes factors p and q but two things have to be noted,in particular original ntt required to find primes n as the working modulus that is larger than so that the result never overflows,there are several ways to do that. we find that 3 is the only primes that divides more than one modulus and the highest power of that primes that divides more than one modulus is i use the notation for exponentiation used in python and fortran for clarity since the caret also has other uses in computers. that could prevent your system of equations from having any solution at all,"
"tcpclient","tcplistener"," is just overall,nothing more overall,","46729979,9608101,","tcpclient doesn t actually use the tcplistener at all;the tcplistener is just the receptionist answering the phone and transferring calls,on the server side you re using tcplistener which is nothing more than a socket or tcpclient factory,"
"bmp","ppm","easier overall,","29444516,","the only thing i can suggest is that you use ppm format which is even easier than bmp for you to read from,"
"sata","ssd","faster overall,lower price overall,","38005710,7874956,","the ssd disks are from 4 to 8 times faster than a sata hdd depending on the model,as compilation is mostly reading small files i wonder if buying a fast usb key to work on can speed up compilation time compared to a standard sata drive and with a lower price than an ssd drive 16gb keys are 30,"
"undertow","wildfly","bigger overall,","45767806,","i m working on a wildfly swarm project using wildfly swarm version 2017.8.1 maven 3.5.0 openjdk 1.8.0_141 where users will often upload files way bigger than undertow s default 10485760 bytes 10mb max-post-size setting,"
"libsvm","weka","slower overall,","18696558,","i am using a sparse format but suggestions are welcome on other formats too i am able to use the data with weka in a dense format using the function names as variables and it works just muuch slower than with libsvm,"
"dkim","email"," but not overall, is more in measures easier api, are not configured properly in server gmail address, landed on junk in server gmail address,general overall,more overall, or not in server gmail address, is usually much in measures easier api,","16550403,42537948,51257522,51257522,55125749,33510409,36039440,44592240,","first is impossible to be sure that an email will not be marked as spam the only way is that the person who receive the email add the sender address in a white list;spf and dkim are only to guarantee that the email comes from that domain or email but not to guarantee that it is not spam,dentify all the origins of your email set all the origins of your email in your spf record which is a txt record in dns;dkim is more complex - your mail smtp server s must implement s,a lot of factors can lead to your email being delivered to junk;just to name a few - your server s ip address is blacklisted you can check it using tools like - you do not have a proper hostname defined for your server - you do not have a proper reverse dns for your server - spf and or dkim are not configured properly try sending an email from your server to a gmail address for example a gmail address that you own,just to name a few - your server s ip address is blacklisted you can check it using tools like - you do not have a proper hostname defined for your server - you do not have a proper reverse dns for your server - spf and or dkim are not configured properly try sending an email from your server to a gmail address for example a gmail address that you own;then go to gmail and even if the email landed on junk please check the email headers,then i d recommend implementing dkim signing with all email sources,4 some email providers might mark that email as spam however most spam checks are based on spf and dkim checks more than email existence checks,your script claims this email is sent from gmail which is not true;not clear whether your server signs emails with dkim or not does it have spf or not,you may have to take additional measures like implementing spf and or dkim;overall dealing with email is usually much easier if you just use an api service like mailgun,"
"jena","rdflib","more useful overall,","23196911,","since you tagged this with python you might find rdflib more useful than jena but the real question here should be about how to do the conversion not the library request since library requests are off topic for stack overflow,"
"keras","tensorboard","easier overall,","46996697,","you should check out losswise it has a plugin for keras that s easier to use than tensorboard and has some nice extra features,"
"jackson","xstream"," former is faster latter overall,faster overall,deserializable with  overall, is the easier overall,","640951,17566034,55400350,14232816,","i ll second suggestion to use jaxb or possibly xstream former is faster latter has more focus on object serialization part;plus i ll further suggest a decent json-based alternative jackson which can fully serializer deserialize beans to json text to store in the column,jackson xml module faster than xstream,what has been serialized with xstream is not necessarily deserializable with jackson,if you use jackson you can check this answer and this example;there are many other json apis jaxb etc but imo xstream is the easier to start with,"
"avassetwriter","avcapturesession","far more rudimentary overall,","38257010,","avcapturesession is far more rudimentary compared to avaudiorecorder - it does not provide any recording facilities by itself for example and so if you wanted to also record the audio you would need to use an avassetwriter to save the samples,"
"get","post","stricter semantics than  overall, allows bigger and more in bigger commandline runners,optimistic this with  overall, is much clearer overall,more elegant overall,choose  over  in important http functions, that delves further overall, verb is much more in secure data method, method using volley in secure data method,more overall,more secure in secure data method,","54218098,2586639,53767338,26644546,1030688,52726677,10106642,10277598,49485458,15089866,9330797,","what this observation by roy fielding helped me http does not attempt to require the results of a get to be safe does is require that the semantics of the operation be safe and therefore the operation is a fault of the implementation not the interface or the user of that interface if anything happens as a result that causes loss of property patch has stricter semantics than post,post allows bigger and more complex information to be passed;in terms of vocabulary the purists say that get should get information and post should post get should get information and post,the url you need to post to is i m optimistic this with get you in,owever since you re performing a query for which there isn t a single resource you could perform a get to it seems fine to use post as long as you re aware of the disadvantages and your documentation is clear about your documentation;frankly i think using a post is much clearer than encoding that payload as a json + base64 and sending that payload as a querystring merely for purism,i think using post is more elegant and has more options for further development than passing them via get,in the case of http one of the important reasons to choose get over post is caching,here s a basic example to get you started;here s a blog post that delves further into the concept of a work task queue,this means the amount of data you could pass by using post verb is much more than by using a query string and a get method,here is the best volley example i prefer always that gives you a better control with get and post method using volley,so it is considerably less then what facebook states in their documentation at least looking by the date however by count it seems you get more than 50 post,post is absolutely not more secure than get as to man in middle attacks,"
"get","post"," belongs s overall,more in secure data method, is more secure in secure data method,request not more in secure data method,more overall, gives you more overall, explains more overall, goes more in test details basic,more in blog access token, is more overall,more appropriate in secure data method,","49494283,10331965,50082991,8277796,14083346,52930237,55969318,55563556,8836906,53782234,45127067,","in other words if you don t own the page where the post belongs to the page where the post belongs s impossible to get user data,as powelljf3 said post is more secure then get though it can still be gotten to,i understand that post is more secure than get get makes the data visible in the http url however couldn t we just use post for everything then,standard post request is not more secure than get one,i had the idea to switch automatically between get and post for example if cookie set because get is more fast and cache able and post is more save,as the error you get indicates you should not use both pack and grid in the same parent. you should read this post gives you more insight into what you are doing when you use both pack and grid and also gives you an alternative conceptual dividing your interface into segments,the problem you re experiencing arises when you make a request api gateway and get back a cached authorizer response that doesn t match the requested arn of the current request;this post explains more about how lambda authorizers work including caching,this post goes more into details about why that s the case;having said all that here s a quick way to get your test to pass,this blog post provides more info to get started,try to send your request as post method you don t see get requests with the body very often and that s why post is more secure the body s never cached and most importantly there s no size limit maybe that s the reason cause your get request is not working,as suggested in the comments since you are saving data post is more appropriate than get,"
"get","post","more secure in secure data method,slightly less secure overall,general overall, is older in older current with,faster overall,better in secure data method,request marginally less in secure data method,better than  in better type since, request only overall,method not safer in secure data method,general overall,","1597598,3682493,330657,42314001,34187465,36831306,1744404,48945893,40004234,29245172,57101158,","for example post request are considered more secure than get request as creating a post request is relatively harder than creating a get request,after that is done you can go into security details where get is slightly less secure than post - see the comments for details.,security wise i prefer using post than get as the page gives at least some opaqueness as to what is being passed as a parameter and not anyone can just edit the url and play around,i know that this post is older but for this post may helpful for other user;if you want to get the current url...with the query string you can use this,i used get because is faster than post if you need it just to get a list of element for suggestions,so for creating a friendship post method suits better than get,the get request is marginally less secure than the post request,a post is semantically better than get because the concept of a job is to do some processing rather than getting something for you,nswering your question more directly either perform the get request synchronously and properly declare the variable in the scope you need to use it or restructure your code to use the information retrieved by the get request only inside the callback;read the post the post mentions for more information,get method is not safer than post data never trust datas coming from the client side,it looks like azure search now supports both a get and post method and recommends using post when the length of the url would exceed the max limit of 2048 characters 1024 for just the querystring,"
"get","post","presumably more level in blog access token, is more secure in secure data method, is larger overall, is much easier in better type since, anyone has a faster overall,more in secure data method,more specific in better type since,more safety overall, data is slightly more overall,more logical overall,more preferred in secure data method,","18308085,12266054,55470970,9132638,9408374,4466537,44642770,13484830,50423386,23850341,42979778,","but using post instead of get presumably adds one more level of protection for xsrf types of attack it does not you need some form of explicit xsrf protection token with post or with get,you can feel like post is more secure than get all you want but post s a false premise,on the one hand i suppose browser caching get requests could be an issue but on the other post is larger but encouraged for delivering sensitive information,my personal thought is that it is better to put it in post for a couple of reasons;get is much easier for users to manipulate,ut i couldn t get the conversion working so opted for this slower method in the rails console with more downtime;if anyone has a faster solution please post anyone has a faster solution,stick with post on any action that does more than get data avoid xss-issues,i ll get better with formatting and being more specific with the post,a post request would not provide any more safety than a get request any half-decent web debugging tool can forge post requests,while doing so i found there s a script on the page which is near identical to what you would get with the request i simply stripped the portion of the script as well as the trailing semi-colon parsed that like you would with the request and used the data the same way - the actual post data is slightly more deeply nested but it s all there,but a get seems more logical than a post anyway,is this why post method is more preferred than get or does it really matter in any case if it internally uses post or get,"
"get","post","better in better type since,safer in safer misleading safety,more secure in secure data method, seems more overall,more  after that  in blog access token,forgery much easier in second easier thats,safer overall, url s not more in secure data method,more overall,smaller in request requests body,wich more sense in note var wich,","26344599,6285401,8388711,46655150,53132224,3701546,26804763,24408656,42926672,18192221,38310005,","get is a better method in this case than post,regarding your question about safety -- the answer is post is absolutely no safer than get,post is not any more secure than get,this blog post seems more specific to rpi3 and probably does it too last jan 12 2017;there is a note on the wiki you are looking at of some things they did to get it to work on newer builds,i ll do something like get load_more last_post_id to posts#load_more as load_more and use it like load_more_path posts.last.id and on the controller you ll have access to so you know you need to load more post after that post id,from a hacker s point of view a get forgery is much easier than post forgery at the first you only post a link at the second you need to point to a malware website with hidden iframe and autosubmit forms but both of them fail if tokens are checked,use post its safer than get if you use rest,the only way to hide the get url but the get url s not more secure at all because the get url s easily found out is to use a post request with a form containing a hidden field with the id can be made in javascript,i need a query to get results from stackexchange dataset in such a way that it returns me 100 top reputation users with not more than 3 post body of post for each user,a get request will be slightly smaller than a post although using websockets would give a more accurate figure,note that doing this will get the values via post var instead get wich makes more sense since you are sending via post your form,"
"get","post","far easier in user easier safe,less than 9  overall,more appropriate overall, is a more overall,general in secure data method, requests is a better in secure data method,less secure in secure data method,more in instagram client last,more in performant yahoo idempotent,also more secure in secure data method,better in secure data method,","36313123,57114177,7205747,56775160,3063547,16725606,2080901,37363458,1850196,20042408,40896793,","while users can manipulate post it s far easier to manipulate get,when i use the second wp_query displays above i get 9 random post minus the possible 9 first post so i can have less than 9 post and the query stopped when the last page in reached,get is more appropriate than post for queries because of its safe semantics,on the other hand if republishing is expensive or should otherwise only be done on purpose then post is a more appropriate choice;that s ok then using get is fine -- and it even has some advantages when the requests are being sent across an unreliable network,if you re using an encryption library to encrypt the data then you can use get or post but this will be an added pain and you might not setup you re using an encryption library to encrypt the data correctly so i d still recommend using post over https rather than rolling your own encryption setup,get requests with url-encoded parameters are somewhat awkward with json-rpc;i think keeping everything in post requests is a better idea,i wouldn t call post more or less secure than get,how to get more than 20 post from instagram using their api,get is more performant as it is idempotent and does not contain a message body where as post does,post is also more secure than get because you aren t sticking,i think whoever said post is better than get when using passwords meant that the passwords would be part of the url in the get request which would obviously be bad,"
"get","post"," is easier in secure data method,more compact in requests compact apparent,less secure in secure data method,more overall,more useful overall,method faster in secure data method,prefer  over  overall,better overall,less safe in user easier safe, url is more overall, is also more overall,","872522,1494219,2977506,198565,20773871,39555144,179294,12100416,8566523,323219,8588362,","just change the method for the form to post if you want to do it via post;both are equally insecure although get is easier to hack,is post more compact than get since get requests have to be url-encoded,first off get is no less secure than post,get is more appropiate for idempotent queries while post is more appropiate for action queries but you can shoot yourself in the foot just as easily with any of them if you don t understand the security architecture for the application you re maintaining,when taking input we can use either get or post.the input is shown in the url when using get but not when post is used.is there a situation where get is more useful than post,i have seen other people asking the same question why get method is faster than post,further i recommend to use post wherever possible avoid using get for form submits - if some users want to simulate a form submit via get make sure your application supports the desired parameters either via post or via get but when you submit the page yourself via a button or js prefer post over get,this response thus assumes that a request using the post verb would not use the uri to transfer sensitive information or it would be no better than get,some people say get can be less safe than post -- but to the seasoned user this doesn t make any significant difference because they can intercept and modify headers of the request,also as pointed out in the link littlegeek posted a get url is more likely to be written to your server logs,post is also more secure but certainly not safe;although http fully supports crud html 4 only supports issuing get and post requests through a cleaner url various elements,"
"get","post","more in secure data method,more users in solution users standpoint, has more overall, is better in better type since,smaller limits overall,script more readable overall,more  as i  overall,more overall,less secure in body data encapsulation,more data than  in secure data method,more secure in secure data method,","6656594,25411516,31102141,7552520,25149172,17341132,50222787,39072833,24430127,8212449,42360118,","if you use post when get is more semantically correct it is less communicative of the intent of your request since post usually means you are sending a payload,here s the solution for post variables as well which is probably trickier for more users than the get solution,lternatvely you can use get_the_terms to get the terms attached to a post and use the term slug as value in your css class;this is a bit unreliable if a post has more than one term attached to a post,ecause it is a significant change of state the operation should not be done by get;while you could do it is a significant change of state by put technically post is better because of the non-idempotency assumptions associated with it is a significant change of state which in turn encourages browsers to pop-up a warning dialog,get requests have smaller limits than post in the specs,for eps is the same situation the post script is more readable but you should apply all necessery transformations on object before get its size,the site loads more post as i get to the bottom,i have been trying get data out of new reddit post but theres limitation where you cant get data from more than 100 post,get is basically for just getting retrieving data a get should not have a body so aside from cookies the only place to pass info is in the url and urls are limited in length get is less secure compared to post because data sent is part of the url,if not try to use the post method as the post method can hold more data than get method,please always try to use post method for submitting data as it is more secure than get method,"
"get","post","more details in blog access token,more robust in secure data method,not more in secure data method,less data in secure data method,worse in faster quicker performance,better in secure data method,safer than  overall,higher in user easier safe,more secure in secure data method,more data in secure data method,more than one  overall,","26336047,37863029,1008671,29409356,18455818,1008704,18954610,9322946,8956020,4565070,38070128,","these steps were taken from this blog post which contains more details about how to investigate and what output you ll get,characters post is more robust and secure than get,post is not more secure than get as itâ s also send unencrypted,supply more data upon post and receive less data upon get,post has worse performance than get,yes post is better than get because post data is not usualy logged by a proxy or server but it is not secure,one more thing some guys would misunderstand that post is safer than get as user can not see the sent data,post has a higher overhead marginally if we re honest but it all adds up but it doesn t become part of the url so can t be seen by say a casual observer over the user s shoulder - whereas get can,edit2 seems as if i m wrong re post being more secure than get,using post allows for more data to be transferred since get will append the values to the url itself,if you have more than one post with the same com_id in eve.com_id you will get multiple post in your result,"
"get","post","better in secure data method,more secure than  in secure data method, res mycontroller update overall,more details overall, more clearly in instagram client last,faster type overall, is better in better type since,less secure in secure data method,faster in faster quicker performance, and using a nonce in better type since,shorter in test details basic,","30721515,35128233,57676162,40746542,42085149,15757966,12873191,7292552,11008319,47893893,33863455,","as far as security goes post is much better than get as the data is sent behind-the-scenes rather than embedded in the url making it the typical protocol for login info,using post data is more secure than get because with get the details would be in the query string portion of the url,update2 i want to have something like get rest mycontroller info post res mycontroller update .,this post goes into more details how to get charles proxy work with android 7 nougat,first of all if your instagram client is in sandbox mode then you can your instagram client get more than 20 post more clearly last 20 post,is type get faster than type post cause i am curious how both work,you use post;since you are not changing anything get is better,data should be in post or get.if you dont put your data in your form then you will have to use get which is less secure than post,get is always faster than post,also post would be better than get and using a nonce would help prevent unwanted multiple submissions.,i have simplified the app so the post is shorter but the basic problem is that if i get test in workouthandler - it cannot recognize methods in wardenstrategies or loginhelper unless i also include those files in the handler they are already included in app.rb,"
"get","post"," is usually more in secure data method, gives more privacy in secure data method,i face with  overall,better overall, makes more sense in important http functions,apparent with  in requests compact apparent, request is more overall,better in better type since,also more secure in secure data method,more secure in secure data method,more secure in secure data method,","25825738,111340,53067266,28670988,6456747,50798117,51569166,17949020,7411370,27822450,38714426,","you load your makes through a post ajax request;for requests that only return data but do not change anything get is usually more appropriate,post gives more privacy to the data as it is not displayed anywhere,i have understood how to get data from server to application. now i face with post task. i have form and want post fields data to server by clicking on button,i am aware that a post is better for this kind of thing but my question is is this expected from a get request,post makes more sense in terms of http method definitions;get is supposed to be safe and make no changes to your system state database,this normally more apparent with post requests but is also applicable to get requests,in python post request is more complicated as compare to other requests.but here is the solution for post request in python;example#1 post request example#2 post request but in your scenario you re using the burp suite to get data and post data in python i don t know which specific version of python are you using but you should follow basic method of post request in python.if you re using the brup suite and must read the brup suite documentation here is the documentation link which ll explain you each step one by one and do keep an eye on the notes and todos while following the step in your scenario,and i think post is better than get,post is also more secure than get because you aren t sticking information into a url,post is no more secure than get over the http protocol,using post is more secure than get in the sense that data shown into the url in get request but not in post,"
"get","post"," it i ll help in error posts.id none,safer in request requests body,more safe in user easier safe,relevant with  in value api number,less subsequent overall,result earlier overall,easier overall,better in better type since,older in older current with,more overall,more help in information durbin-watson issue,","14405219,15663389,35813468,56208496,698048,20868786,9606511,2796836,33075819,10259488,22719995,","if you change libraries instead and hit an error post it i ll help you get it working,post request is a little safer than get because the parameters are not stored in browser history or in web server logs,however post is a bit more safe than get because get could be stored in the history,i tried to use format nested json but not relevant with post http because my api must send api key for get response value,an initial get request to a page usually costs less than subsequent post anyway so there s not much reason to avoid it,the problem i have noticed is that sometimes the jquery post result is retrieved earlier than the load of the .js file has been completed so i get an error and of course the jquery plugin doesn t work,another reason but rather a minor one is that get is easier to exploit that post as there are more ways to trigger get request than to trigger post request,using post over https is better than using get and http,curl commands seem to work as do some other post commands from older code but when i log out req.body all i get is,i get somehow category repeated if it has more than 1 post eg,to get more help with that post information about your table structures and content,"
"get","post"," is no more in secure data method,more overall, date is more overall,lower limit in idea good lower,safer overall,better in secure data method,more secure in secure data method,more tidy overall,more secure in secure data method,more secure in secure data method,here more information in information durbin-watson issue,","57021870,34493726,23088701,1678683,43934585,45940733,3735534,8297272,8985967,15523045,9378699,","post is no more secure than get it s quite easy to post anything to the page even without visiting the site by using something like postman etc,you can also chose to get more than 25 post at once,this piece of code with get the newest post and check if the newest post date is more than 3 days old,generally when passing lots of data to the server to be processed or stored using post is a good idea since get has a lower limit than that of post,i just want to know if i use post method for login api then is it safer than get method,but i believe sending data using curl with post method is better than get method,post isn t more secure than get,maybe i would mostly always choose post over get it s more tidy and refreshing and going back issues with post method are a thing from the past in any major browser they just ask you if you want to resend the data don t they,post method are generally consume more secure than get because when we use get method than it can display the data in url bar.if the data is more sensitive data like password then it can be inggeris,also note that using post is absolutely not more secure than get because the request parameters aren t immediately visible in url,following my comment on your original post here s some more information that will help you get through this,"
"get","post","more in secure data method,much smaller in limit size smaller,more info as i  overall,not better in better type since,safer in safer misleading safety,general in blog access token,more advantages overall,more secure in idea good lower,more appropriate in secure data method,better in secure data method,more information in blog access token,","27822450,1860743,33224211,6735520,1145134,57733929,17212144,18020715,4157893,12490278,17599439,","in effect post is no more secure than get,you ll hit problems with larger submissions and file-uploads as the size limit for a get is much smaller than a post,i ll post more info as i get more info,post is not better than get,and apart from common misleading information where post is safer than get it is not,have tried get post with authorization configured through postman i get unauthorized access is denied due to invalid credentials. error although i am told to have access,post has a lot more advantages than get,it is just an example that shows a scenario in which get is more secure than post but i don t think it would be a good idea to choose get over post from this attack reason,i can t help with the coding side of your question but if you re passing in a non-url input then the post method would be more appropriate than get,is get method is better than post method if you use a form,the post at has more information on how to get that access token,"
"get","post"," method is more overall,right track with  overall,more in secure data method,safe than  in secure data method, is more overall,more in limit size smaller, is more in secure data method,more secure in secure data method,more secure in secure data method, also has some more overall, is more in method request use,","19457177,30844848,16294056,50339001,20959465,12297727,504981,34822709,38099246,6234277,47945374,","the is very little difference to the programming functionality between sending the form as or but the post method is more secure and can transfer more information so most of us use that;finally on the other end there are three ways to get the variable values php example,i believe you re on the right track with post;get is much more simplier,if the post has more than 9 pictures related to it i only get data from the first 9,as far as i can tell get requests are supposed to be safer not less safe than post requests because get only retrieves data while post sends data to the server,in order words you should not use the complex body to include all the sub-resources in your post;get is more subtle,so is it enough only to use limit to get more than 50 post,get and post are part of http protocol;as mark noted post is more secure,on an unencrypted connection post is no more secure than get,hence post is more secure than get,once you add the configuration values you should be able to get a valid cachemanager from the cachefactory;the post also has some more information that might help you,in addition to the answer by don t panic dont forget to set the method attribute in your form tag to match your php code most likely it is pos or get or and you you can omit the method get since it is the default method. it worth mentioning that post is more secure and can carry more data since it uses the actual request body unlike get that appends the parameters to the url,"
"get","post","easier in user easier safe,method faster in secure data method, just seems safer overall,better in secure data method,request more different in request requests body, has the greater overall,more in secure data method,higher in default 2mb worse, allows more in user easier safe,neater overall, to page_id feed in page page_number search_phrase,","22266230,44409327,11460367,39251439,45697102,17501835,12582162,35679660,7810983,29165001,15603501,","using get is much easier than using post for most of developers,you should check this thread why get method is faster than post,update yes get requests are encrypted over https;but post just seems safer to me for some maybe unfounded reason,i don t recommend sending such parameters via url get it is better and safer if you use post form method,here i don t like that the body of the post request contains more different fields then the body returned from the get request - but maybe this is not a problem,give it a try works perfect for me what it is doing query get the all the post with a left jon with comments table so when a post has comment them n it also has the comment_date if no comments posted on a post then in result set it will be null so i have merged the comment_date with post_date so which post has the greater date for comment_date or post_date it will first and so on,i m searching for the best method to get result with database contain more than 100000 post and more than 100000 cat,there is a default limit of post method which is 2mb which is way higher than get,you would not want to use get for it because in that case a witty user may raise mayhem simply changing the id at the end of the url and deleting all random uses;post allows more data and can be hacked to send streams of files as well,the pinoyyd post is neater and get straight to the point how do i authorise a background web app without user intervention,to get only the page s own post and not the fans who like the page use the post endpoint;you execute an http get to page_id feed,"
"get","post","more in secure data method,more secure overall, method is better in secure data method,more in secure data method,similar between  overall,better in request requests body,more secure in secure data method,more secure in secure data method,request more overall,more overall,probably easier overall,","43934930,1311780,25739925,4218091,51128979,30069145,19343161,25757870,11287884,2816936,13225433,","post is more secure than get because you aren t showing information anywhere,there appears to be a common mis-conception that post is more secure than get simply because it seems more obvious how to get a browser to manipulate the post variable,you should not be using the post method for requests that contain no actual data get method is better for that,while the post method can be submitted with forms or ajax calls and it is pretty safe at least more than the get,as you can see above the dynamic mapping is very similar between post and get,speaking of get request types you should also know that get requests tend to perform better than post,although post is more secure as compared to get when ssl is present,last to address other answers while you should use post data when modifying something it is in no way more secure than get,but do also remember that in your case get request looks more appropriate and as balusc says its not good practice to use post for bookmarkable page-to-page navigation,if a post has a more recent reply i want to get the replies created_on value but also get the post post_id and subject,have a look at this post it is probably easier to get the selected radio button by changing your itemscontrol to use a listbox with the existing datatemplate,"
"get","post"," is not in difference data url,more secure in secure data method, is securer in security securer lax,more overall,secure as  in secure data method, it is much simpler overall,more than 50  in value api number,more suitable in secure data method,better in secure data method,more secure in secure data method,more overall,","52231950,2389112,30503351,6342651,56134329,5895463,6721054,33648257,17044607,45793760,6364373,","added on server side a route to accept also post to retrieve users data then added method post url new-url-for-post in table options;now it works but i still cannot understand why get is not working,why everybody here is saying post is more secure than get when used over https,neither post or get is securer than the other in that way and so if you need security you should use https,this post is more to get advises and ideas rather than solving a problem,hence with right logging practices get api is as secure as post api,if you want to use just get it is much simpler you only call from your php;depending to the response data format you can output you also want to get the response regarding to your post directly or parse your php first,however the number of post isn t clear how much the value can be for the standard tumblr api you can t get more than 50 post at a time,actually if you would like to retrieve specific task with all its details get request would be more suitable than post,also a post request would be better than a get if you re sending json,i am asked to apply to all the actions of one mvc project for security reason says post is more secure than get which i disagree with,usual way with tagging is to get objects by tag but if tags are used by few applications it may give more than post,"
"get","post"," and here overall,more secure in secure data method, is a bit in single able term,difficult than  in solution users standpoint, this is simpler then overall,more in important http functions, here is an example in error posts.id none,more overall,request earlier overall, has got more information in information durbin-watson issue,more overall,","8487166,43522161,20964388,8805725,1628230,19621961,50818082,47250254,20961057,48827644,24883267,","i was reading the yahoo best practices for speeding up your websites some day back and your websites have very well explained why we should prefer get over post and here is the post snippet for your reference,edit taking into account what was said in the comments though it looks more secure and is recommended by w3 see touheed khan s answer post isn t more secure than get and as riggsfolly said,you can either use post or get since i think post is a bit trickier to handle in wp,from an interface design standpoint you want user-agents to make post and put and delete more difficult than get or at least distinctly different so that users can rely on that difference to hint when users actions might cause a change in the resource state because users are responsible for those changes,if you have the ajax use get rather than post this is simpler then you can test the server s response by just viewing the page with the appropriate query string,however if you want to get more than one post rather use functions mentioned above i think the most important thing is to grab them in one db query and not in ineffective way one-by-one,and have more than 5 post here is an example that i found but it does not work i get error update posts.id to keyword same error,you can t get a patent any more because with this post i will claim prior art -,i am using custom delegate methods but when i run this the get request is performed earlier than the post so i cannot get the key before the get is done,to get durbin-watson for example you ll have to use . this post has got more information on the issue,on that base a get is more appropriate then a post,"
"get","post","more overall,request less in secure data method,simpler overall,better in secure data method,larger more overall, is more in detail blog message, bogged down;after review of earlier overall,more vulnerable overall,broader application overall,more in type things unsecure,better than  in better type since,","16195031,6219147,3521290,1500539,17974157,55844834,9726858,30753913,24353372,21773039,6777202,","post makes it more indirect to intercept and read the payload than get but not safer,however a post request is less efficient as a get request - bear that in mind and only use post if you really need to,as a pragmatist i m inclined to use get because implementing it is way simpler than post,since post is better suited to transfer large amounts of data or difficult formatted data it is the established standard for submitting forms but it is in no way safer unsafer or more less secure than get requests,for example on the blog page of my current project i have two widgets in the sidebar one for latest tweets and one for latest music news however as you can see on this page the content of the blog post all test posts... are much longer than the height of the two widgets and would of course get larger as more post are added,as the post is more than 2 years old by now the setting the article talks about to fix it is now on by default;meaning that you get a yellow message stating that your messages are end-to-end encrypted,and one learns from examples of real-world design concerns not some standards publications or top-down descriptions where you d have to skip first 150 pages or get bogged down;after review of earlier so post here s an extended older discussion,get is more vulnerable than post as it can be transmitted and stored,a post has broader application and is generally used to send persistent data to a server in fact prescriptively get should not be used for this purpose,get is more often used on unsecure type of datatransactions like for example a searchform and post is used when you want to make more secure things like a login form,i believe post is better than get,"
"get","post","more complex overall,more overall,more options overall,request longer in request requests body, should not in method request use, is the more in secure data method,slower in request requests body,more overall,more overall,simple as  overall,general in secure data method,","23586414,27907832,19724396,39968178,3477456,31096212,44409255,38592739,30662798,56118080,49300218,","post is a little more complex than get though,i m able to query my database below to get the result that i want but i don t want to have to iterate through all of the author objects just the ones that have more than one post,post gives you more options and dosn t have a limit on parameter query string length like get the only negative is post is slower by a couple of milliseconds to create the request,so if your get request is longer than 255 bytes it s advised to use post requests instead,a post unlike a get typically has relevant information in the body of the request;a get should not have a body so aside from cookies the only place to pass info is in the url. besides keeping the url relatively cleaner post also lets you send much more information as urls are limited in length for all practical purposes and lets you send just about any type of data file upload forms for example can t use get -- they have to use post plus a special content type encoding,the post method is meant to be used when you are changing data on the server;in this case you are only retrieving data so get is the more appropriate method to use here,but generally post is slower and would be bigger in size than a get request,however there s one thing we haven t considered yet - what if you never get more than 5 post,additional options are to show this via a timeout so it s only shown when the post takes more than a short time and to use some form of fadein not sure if possible with a dialog then you don t get the annoying flash on the times it s very quick,post response i receive a token but when i try to do something as simple as get limits get the response is the strange thing is that when i change all my credentials to a free developer account created with a different email address everything works fine,in general i would recommend using post as a form s method instead of get,"
"get","post","method faster in secure data method,safer in better type since, which is more in secure data method, contains more overall,more overall,more letter overall,more overall,same as  in secure data method, is more overall, is better instead in server textfiled turn,general in page page_number search_phrase,","16049028,2143377,49680528,6426570,19306690,1494266,43786621,47875125,6888070,55214765,57412047,","usually in ajax get method is faster as it doesn t involve proccessing post fields and as you are only getting information i would stick it,is post safer than get,i d recommend performing this with post method and not get which is more appropriate for altering data on the server,performance wise you might get a boost by using tcp bindings rather than http but connecting external applications is easier on http if some non wcf client is involved;perhaps this post contains more questions than answers but you are the only one who can decide given the brevity of your description,what i m trying to get is a page that displays 20 users per page will have pagination with the fallowing info it should show only users that have more than 2 post published,post has one more letter in it than get so it s less compact,when i want to get more than 54 post i receive this error,i am trying to post data using ajax and jquery but the problem arises when i am using post method i am getting result in url just same as get method why is it so,post is more secured cuz it s not gonna be cached or saved in history or bookmarked;u can clearly notice that post s dont display in the browsers address bar but get do,i am currently calling a function which in turn will call another function i am doing so as i want to use get instead of post because since there is no server side alteration hence get is better instead of post,there are some sensitive info in the page that needs to be submitted with post also there are some info like page_number and search_phrase that should to be submitted with get,"
"get","post"," is the more overall,more overall,choose between  overall,more overall,more in secure data method,larger overall,safer in secure data method,slightly faster in request requests body,more detailed in blog access token,choose between  overall,more details in test details basic,","2027294,7212715,50276921,45773707,31949863,5375742,42591598,25307589,12876876,14133601,22153922,","and after reading your question completely -d it seems that your request changes in this case adds data which means using post is the more correct way;but as you only send it via get to your proxy and then via post to the other app as long as it works it is fine,i normally use something like this tag.object.annotate num_post count post .filter num_post__gt 2 to get tags with more than 2 post,but in honesty for rpc remote procedure calls people choose between get and post by either get for operations that don t modify anything and post for other cases,anyone knows how to bypass this problem and get more than 100 post,also it seems that my view gets called more than once according to the log outputs and that is normal except it should be a post then redirect to a get but it seems to have more than one post request,the usual wait time of the post is not too long not too larger than get so i don t think that should be an issue and post always gets back with a 200,when you use post data is a alot more safer than get and you can send large no,get is slightly faster because the values are sent in the header unlike the post the values are sent in the request body in the format that the content type specifies,this blog post is more detailed but unfortunately does also not mention where to get the product key,to choose between get and post is simple,another post outlines some more details on this how to get master-master replication with subversion,"
"get","post","more data with  in secure data method, has no encapsulation in body data encapsulation, and has a bit overall, is more in secure data method,general in method request use,general overall, method has less in secure data method,more than one  overall, is probably more overall,possible with  overall, etc not in link browsers familiar,","3892977,33412031,6126448,44851402,57654405,49666709,40329077,49704114,53807563,51273612,56996691,","you can send more data with post than get but this doesn t mean t respond faster t s a separate issue,the get is usually faster because the get has no encapsulation sends data via url usually with a 255 character limit as post exists encapsulation by being sent by the body of the http request,you could try an ajax post;this will prompt the get and has a bit more flexibility than just using jquery,when you do get the parametres are written to url so you can access them easily on other hand the parametres can be rewritten by user so post is more secure way,some other notes on get requests get requests can be cached get requests remain in the browser history get requests can be bookmarked get requests should never be used when dealing with sensitive data get requests have length restrictions to prevent this from happening use post method,in fact finished would be fired when qnetworkreply finished would be fired qnetworkaccessmanager get just post request and returns said reply immediately,the get method is useful for passing a reasonable amount of non sensitive data between pages while the post method has less limitations and is best for more sensitive data,so far i have this home.html.erb post controller i get the general idea from looking at other answers but i still get an undefined method each for nil nilclass error even though i have more than one post in the database,although this question already has an answer this blog post is probably more complete;please visit the site and get their metrics up but for posterity here s a copy-paste,while searching on so i found out that it s not possible with get i have to take post,i am only familiar with post get etc not link and in a guzzle environment so trying to find more information on this and how to call link in guzzle,"
"get","post"," is more overall, it is more overall, request is how larger in request requests body,much more overall,better overall, has more detail overall,likely as   in secure data method,larger more overall,higher end overall,safer in safer misleading safety,worse in default 2mb worse,","55930751,43035993,17477570,1241338,1772728,52728098,4866556,24538442,37884205,7148371,5797770,","this iframe being modified clicking on stripe button doesn t get end-user to stripe checkout page of my product. so i wish to know how to modify this function adding a condition avoiding to modify sources that includes as all stripe s iframe have their sources starting with it. here is the code i need to modify thank you for your help i hope that this post is more consistent with the rules rafael pointed,however for a post it is more of a fire and forget it is not waiting for a reply,a post request is how larger forms would be submitted on a normal page parameters to a get request are passed as a body of data separate from the url and control headers,i m sure there s a work around to use get for emails but the alternative post is much more easier and doesn t require any hacks to get around,sometimes get is a better option in those read-only scenarios because it makes your url scheme richer and avoids these sort of post warnings,the 3rd step in that post is to try to however since i m unable to install packages without r-devel i get a non zero exit status this so post has more detail but it ultimatley points to the solution over here,you d basically need a set of procedures to act as controllers - parsing vars more likely as get post cookie performing any data lookup building + filling the model and then another set of procedures that at as views - taking what s in the model and rendering the model for the user,so i have a post system and i want the box to get larger as more post come in,gets have a size restrictions post do not or at least it s a configurable setting on the server and has a larger higher end than a get request would,however note that post is not much safer than get,one reason post is worse for security is that get is logged by default parameters and all data is almost universally logged by your webserver,"
"get","post","faster in faster quicker performance, is somehow more secure then in secure data method, requires more processing overall,simpler http overall,more data in secure data method,better in better type since,safer in secure data method,better overall, is more overall, the desired user in method request use,more secure in secure data method,","8515020,51899666,3893175,4594163,2539220,26117168,14039155,33811958,40370424,12165978,3544966,","get is faster than post,some people for some reason thing post is somehow more secure then get well it s not,on the client side the post requires more processing to prepare the message;you ll notice this if doing any ajax this s a lot easier to send a get request than post,this would make things considerably easier as the android library has native support for json serialization and making http get requests to restful urls is much simpler than http post requests it s not a huge deal but just less stuff you have to worry about,post get method doesn t have the same lenght limit you can use post to send more data than with get,i d question whether get is the right verb here considering you need to pass a complex type it sounds like it would be better served as a post,however post data are not much safer than get data anyway,if you are curious though rather than just trusting a book or a blog post it is better to make a simple example and take it apart - you get it straight from the horse s mouth and you learn a whole lot more,however if you must use the same route you could separate you must use the same route by get and post since post is more common with registration type scenarios anyway,this will give you the latest five postings assuming a higher id post is newer forcing each result to be from a different user;with this method if you need to load other user or post data it would be best to load it separately after you get the desired user ids and post ids,post is more secure then get and data transfer size limits are also there,"
"get","post","less insecure in secure data method,more secure in secure data method, parameters are as in note var wich,better in secure data method,anonymous function as  overall, is a better overall,higher overall,more overall,better in secure data method,more performant in performant yahoo idempotent, which has a bit in blog access token,","4938414,6735507,52998857,30907893,49702922,32935786,18140518,34934045,40474452,1849962,1350502,","get requests are no less insecure than post requests,using the post method isn t necessarily more secure than get,app config security.yml security encoders appbundle entity security algorithm bcrypt src appbundle controller securitycontroller.php form view javascript submit function side note should be obvious but you should use post and not get with your ajax query;first because get parameters are as clear as day.,because post method storing capacity of response is better than get response,edit 2 the error i get when i try to access data models 27 uncaught typeerror cannot read property success of undefined at success models 27 at e jquery.min.js 4 at xb jquery.min.js 4 at function.r.param jquery.min.js 4 at function.ajax jquery.min.js 4 at function.r. anonymous function as post at htmltablecellelement,the default form method is get so either change add or use;using post is a better option when sending user private user data,if you have both post and get params with same name and if you using then it will take post variable only.because post has the higher priority than get,i try it and i get all the categories and not just the ones with more than 1 post in them,yes generally post is a better way of submitting data than get,ajax calls using get are more performant than post according to yahoo,there s an interesting blog post which has a bit more information;if you re problem is just rubygems you might be able to get away with downloading rubygems and install downloading rubygems manually,"
"get","post"," is less suitable even in secure data method, is more overall,safer than using  in secure data method,larger overall,larger overall,more frequent overall,easier in user easier safe, is better overall,safer overall,more in single able term, is more general purpose;typically overall,","15339895,56094813,57737953,9745606,9422310,9923433,26404658,53951654,22791339,27456657,32507523,","if you want to retrieve a lot of data using a url advice rethink if there might be a better option then it s necessary to use post from wikipedia;there are times when http get is less suitable even for data retrieval,note even though it was deemed so by another user this question is not a duplicate of php pdo get the columns name of a table as that post is more about the general code rather than my specific issue,your form should be this you should use post method when you re creating a something new this is safer than using get method,post requests can be much larger than get requests as get requests are limited by the maximum length of a url,to answer part of your second question with .net if the post is larger than maxrequestlength part of the .net configuration but smaller than maxallowedcontentlength part of the iis configuration you can create a custom http module to get at the portion of the post that came through,get and head being much more frequent than post,get would obviously allow for a user to change the value a lot easier than post so suitable checks for existance and ownership of the record would be important,in the html file the fields aren t inside a form and the button isn t a submit...you need to do this or do a javascript that will get the values from the fields and send using post or get to the php file post is better,post is a little safer than get because the parameters are not stored in browser history or in web server logs. also data is not displayed in the url,not able to get more than 100 post in single rest api call for the linkedin company updates,if you want to follow the spec then put says that the body is a representation of the url you put to while post is more general purpose;typically you would post to a substring of the put uri so in other words put some json to posts my-post whereas you can post some json to posts and possibly get a new uri back,"
"get","post","better in better type since,better in better type since,better choice in better type since,more privileges in secure data method,slightly less overall,much more in security securer lax,better off with  in better type since, is more in method request use,more in blog access token, is ting bigger i in bigger commandline runners,better overall,","1678727,1678727,7381842,33307724,1744404,7852059,54135726,35232309,23905175,57206170,42527058,","when post is better than get,when get is better than post,instead i m using post which isn t exactly bullet proof but still a better choice than get in this case,param should be params and therefore your data is not being serialized which would explain why the server is rejecting your post since post requires more privileges than get presumably,the only scenario in which post is slightly less susceptible is that many websites that arenâ t under the attackerâ s control say a third-party forum allow embedding arbitrary images allowing the attacker to inject an arbitrary get request but prevent all ways of injecting an arbitary post request whether automatic or manual,the post get may fix the problem - get is much more lax with security than post,edit if you need to send back user input as the op explained in the comments then you are better off with get or post params,n the one of okhttp you send a put method with a json object serialized and in postman you send a put although i guess you do a get request with the parameters within the url i mean not in json body structure;anyway http 405 is telling you that your backend does not support the put method and probably your backend s expecting a post method with the x-http-method-override put http header since post is more standard method in rest than put,this blog post explains more on how to get an access token on android,so on you can configure your own decider your own job steps as you said above for two different configurations like below and use your own decider seperately in commandline runners since the post is getting bigger i am giving the details of just the job and command line runner these are the two jobs here are the command line runners i am making sure that the first job is completed before the second job is initialized but it is totally up to the user to execute the two jobs here in parallel following a different stratergy hope this gives a complete idea of how this can be done,i was taught from him that using get requests was better than post but after reading up i would like to switch to post my only problems is i am a little unsure how to modify my code to switch over from get,"
"get","post","more capacity in secure data method,less overall,more images overall, is slower in request ajax fb.getloginstatus, not in difference data url,better in better type since,harder simple in secure data method,more in blog access token, requests cannot in sensitive data points,more than one  overall,more secure in secure data method,","34988907,7652047,20499631,13661451,69989,11761592,41080455,11057732,52983065,49743797,27221476,","post has more capacity it can transfer more data than get,i also found a post that suggests a less brute force method to get the urlroutingmodule to catch the combres.axd route,i m trying to get cloudinary direct uploads working on a rails app using carrierwave and accepts_nested_attributes_for to submit one or more images with a post,verytime you make an ajax request you post the token you get from the fb.getloginstatus or fb.login and read it out in the php file and set it via facebook - setaccesstoken;not suitable in all circumstances you definately need to use post is slower and brings some security issues but still works,get is basically just like post ssl just has a limit on the amount of data you can send which is usually a lot smaller than post and a semantic difference which makes get not a good candidate from that point of view even if technically they both can do ssl,in this case post may be better than get anyways,although putting data with post method little harder than simple get,i need to create a menu based on that list but cannot simply list all post and get it because it is a busy blog and has more than 2000 post,found some limitations to get requests as follows get requests can be cached get requests remain in the browser history get requests can be bookmarked get requests should never be used when dealing with sensitive data get requests have length restrictions get requests should be used only to retrieve data some + points on post requests post requests are never cached post requests do not remain in the browser history post requests cannot be bookmarked post requests have no restrictions on data length reference,i know i can set a smaller wait time or i can get more than one post at a time and sort through the results but that doesn t really fix the problem it just makes it less likely,in my opinion post requests are not more secure than get requests and the data can be easily intercepted,"
"get","post","more than just  in link browsers familiar, has more overall,faster overall,more secure in secure data method,more clarification overall, request takes more overall, is easier in secure data method,more secure in secure data method, is more overall, is better suited;even in better type since, requires more than just in secure data method,","52983876,49780222,26923286,5287145,44779524,52790100,50226045,21832271,42199987,5296129,56411679,","obviously browsers support more than just get and post the link is not for an old version of spring dealing with archaic browsers and i would imagine a widely-used framework like spring has an accurate documentation,to get more expected output you would need to use delayed expansion now the first echo of var will display as expected this post has more info on delayed expansion,i run the ab test several times with same results post is faster but get is lighter,using post prevents this particular attack but is this actually any more secure than using get,hope you get the more clarification by this post,however immediately if i run the same script without any changes or closing simulator or any kind of reset following post request takes more than 5 min to get a response,change the method for the form to post if you want to do it via post;both are equally insecure although get is easier to hack,firstly post can seem more secure than get,the scenario in the post is more complicate which integrate the azure active directory with on-premise active directory;microsoft also provide lots of samples to help developers to get started to integrate with azure active directory you can refer samples from here,for this type of scenario post is better suited;even if you want to stuck with get you can use javascript and use window.open function for your purpose and build url dynamically reading fields,switching between get and post requires more than just changing the verb in the request,"
"get","post","more secure in secure data method, has a bit in detail blog message, that suggests a less overall, is more in secure data method, i found more overall,simpler in better type since,more than  in secure data method,quicker in faster quicker performance,more data overall, makes more sense overall, directly instead overall,","4294578,46174970,7652047,4635570,25656471,37863029,597858,11603297,3882214,51629512,48377760,","post data isn t inherently more secure than get data,this blog post has a bit more detail as well;another answer also has a point that if you are trying to request a blacklisted video from an app and not a web page you will get the error message below,one of the comments mentioned this update i haven t tested a post that suggests a less brute force method to get the urlroutingmodule to catch the combres.axd route yet though,and remember a get request should be used to retrieve and a post request should be used to create;in your case it seems a post is more appropriate reply,that way your clients would automatically get the option to call it synchronously or asynchronously via the generated service client;here s a post i found more useful than the msdn docs,get is simpler and faster than post and can be used in most cases,using get for critical data such as sending password over a get request will expose the password more than post because the password ll be stored in browser s history cache proxy caches server logs etc,a get is a bit quicker and than a post in an ajax scenario,i don t want to use the get request since it says in the guidelines of a restful api that it should use post for this purpose plus i might want to send more data than get can handle,there are some strange things here you make a post request but the view has no side-effects nor does the post transfers any data;in that case a get makes more sense,for this scenario i prefer using post get directly instead of using resources because i am not forced to use a specific function name such as create especially if the controller of your nested resource has only one endpoint function users admin with post,"
"get","post"," makes  easier to semantically in secure data method,more data than the  in secure data method,general in security securer lax, is older overall, is more in error posts.id none,safer in server textfiled turn, is more overall, has more overall, that goes into more in detail blog message,more overall, has more overall,","30744177,55998160,49970201,20643551,52892402,42603163,4410552,48071690,13993552,45773707,49603474,","post makes post easier to semantically separate getting data from the web server get and sending data to the webserver post,the post method allows sending far more data than the get method which is limited by the url length - about 2kb,you need to set your variables to not i also recommend using post instead of get just for security,to get more post between certain dates you can start by requesting a feed with the max_timestamp parameter and then move backwards in time by 20 post at a time using the next_url which is in the pagination object returned by your request;do this until a post is older than min_timestamp,i have seen many post about this but none of them answer the question they give examples that do not work all you get is more error messages or just sent off at other tangents,both get post user can put any thing in it and you must filter any input that user make it in your form example the value in textfiled sent to server by post method and that don t make it safer than get method the difference between them that the textfiled value doesn t visible in url,get is used when you are fetching data;post is more used when you are submitting a change to the application,this post has more the one question in this post;your base use case using powershell to get azure ad token jwt is a common one and there are several samples and pre-built examples to leverage,the simple solution of configureawait false will cause everyone to forget about it which means that once in a blue moon your continuation could just get lost;i have a blog post that goes into more detail on this subject,when i try to get more than 100 post it returns,i m trying to get the category of the current post in wordpress the problem i m having is i m trying to check if the wordpress post has more than 1 parent categories set,"
"get","post","parameter longer overall, the outdated data in request ajax fb.getloginstatus, is more dangerous and more overall,cleaner overall,request better overall, does a bit more in blog access token,more than 9  in single able term,method faster in secure data method,more in secure data method,easier in user easier safe,quicker than a  overall,","14532340,52351303,22452751,10292008,42391293,3901872,49642555,1211881,11211762,26405424,57486737,","i noticed once the length of a get parameter is longer than 400 bytes or 2000 bytes for a post parameter g-wan returns 400 error,dr you run two ajax calls in this case the get response is faster fewer data to precede probably or some other case than the post request so you ll get the outdated data at the first time,here might be a cross-domain request problem but i give a cross-domain request problem no so many chances when your wfs post request is working while wms get returns a problem;usually a cross-domain request problem is vice versa the post is more dangerous and more cross-domain sensitive,also you can determine if someone is submitting a form or getting the form by checking if _server request_method post which is cleaner than using a get url parameter though i guess there s nothing wrong with the other approach...,as it s widely known that an asynchronous post request is better since the user won t get a perception that the app has crashed while it s loading the long process,this blog post does a bit more that what you re asking for but this blog post s something that will get you started,below is the code i am using to get all post in wordpress but somehow i am not able to get more than 9 post,it is said that get method is faster than post but i don t know why is it,post is more secure than get this is just a quick example,get would obviously allow for a user to change the value a lot easier than post,as a tip googling the functions will often get you the description and clarification on functions quicker than a post here and will have a lot more information,"
"get","post","more info in better type since, someone has a better in solution users standpoint, s still overall, is better in single able term, do is a better overall, is easier overall,more in user easier safe,safer in secure data method,less secure in secure data method,more lightweight in requests compact apparent, method is more overall,","6636839,9847406,36722956,48273908,17415822,50226045,38993766,6368250,21579295,32572284,4410552,","however i ve searched for a couple days and am not getting a definitive answer and i need a better understand of this so i know no better way to get more info than to post here,o i get the amount of entries persons in this case;if someone has a better solution then please post someone has a better solution,2 as mentioned by mike brent get is wide open to easy and simple abuses but while mike brent get recommendation of post is better his recommendation of post s still open to abuse unless appropriate measures are taken see 3 below,you d really only want to use get if you were sending a single term or a short amount of data;post is better for larger amounts of data and marginally safer,to post something to a servlet using http post dopost is a better option;get doget is to get a resource,you can use get post method or session cookie for passing data from one page to another page example get and post php code change the method for the form to post if you want to do it via post. both are equally insecure although get is easier to hack. get method can handle less amount of data. for handling higher amount of data use post methode and also check the post_max_size in php.ini file,you can t get more than 20 post if you are a sandbox user,post requests are no safer than restful requests which are no safer than get requests,however in the comparative table i linked get is listed as less secure than post,more over it seems that get requests are more lightweight than post under high load,i can think of a few reasons to use get rather than post;if the operation simply fetches results semantically the get method is more appropriate,"
"get","post","more than 50k  overall,method faster in secure data method,less overall, is just more overall,more in type things unsecure,deeper overall,safer in secure data method,better in secure data method, looks more overall, which did surprisingly better in better type since, is marginally more overall,","52470259,9644621,5199763,40032690,724272,33120663,16505451,25863164,15565968,88684,17772874,","you get an even more efficient query this query adds a convenient hyperlink to the user id. note that just the top 10 users have more than 50k post,why get method is faster than post,other than the fact that get has one character less than post i doubt there is any performance difference,post is just more practical for transmittng xml objects in the body of a http request or response;the query string in a get request would be awkward and has limitations,sorry that this post is more of the and for further information... type than a direct answer which i think has now been given-- i just thought it was helpful not to get things lost inside comments,i m trying to get deeper with post and pre incrementors but am a bit stuck with the following expression,post request are safer than get but that does not mean are invulnerable,1 reading other questions about the rest method for this proposes i think that i should use get better than post,giving a quick guess here but post looks more like a w3c.org mime request for mail;you send headers on separate lines crlf add onto a line in that a string in condition similar to a get request by appearence of that string is url encoded unlike the headers just like taken from an html form and similar to json notation,john does a better job of covering some of the advanced techniques without getting into a bunch of the academic tripe you get elsewhere;i also wrote this post which did surprisingly better than i thought,if you f5 the page it ll update again if the action is get;post is marginally more difficult to tamper with but should not be relied upon as a security measure in itself.,"
"get","post","better in better type since,better in better type since,easier in second easier thats, object is more overall, is more overall,outer url as   overall, is more overall, i looked more detail overall,more in user easier safe, method makes it simpler overall, routes is it better in better type since,","5884242,19732711,15992008,49983162,5635618,16346269,33002536,8912494,36406549,10478057,51231114,","also post is generally considered better than get for ajax calls as it doesn t have the same string length limit slightly more secure too,get is better for some cases but it seems to me you should use post instead,in my opinion the second way to achieve this is better because using get is not thats ecure in my opinion.it s easier than using post,assuming that the shape of a post object is more or less the same between those two rest endpoints you could reasonably get away with making the assumption that the content from api post 13 would be included in the content from api blogs page 1,the first link blog post is more relevant to your situation;your only other option would be to use wcf discovery to discover your endpoint and configuration at runtime but that approach is certainly more work and can get complicated in managed mode,this handler can take outer url as get post parameter and download this image on server and writes to response as binary to allow your silverlight project to get your silverlight project from your silverlight project origiated url,if you are finding that you need to send a large number of query parameters it s probably not really a simple get;my rule of thumb is that if the result is inherently not cacheable it s possible that a post is more appropriate or at least not inappropriate,i read so many blogs and post but couldn t get the right solution from anywhere else;even in this post i looked more detail and tried each of solution,note that i am assuming a user can get invited to more than one post which seems logical,and querystring will be generated like textbox username1234 from extension in the packet s body unlike one goes in get with the headers along side the url;jquery s post method makes it simpler for you to format data you send using json and then internally it converts that to a querystring to send parameters,instead of defining custom get and post routes is it better to use resource,"
"get","post"," is faster in faster quicker performance,more overall, makes much more in sensitive data points,shipping longer overall,shorter in faster quicker performance,faster response overall, is trickier overall,more than 10  overall, method is more in difference data url, cannot otherwise overall,more in user easier safe,","56460945,43769189,37343158,15689066,16102098,34426880,20577562,49897620,36223830,27869157,9557460,","get is faster than post,i know there can be problems with posts_per_page but nothing is working i ve searched a lot to get the query to display more than one post,the reason to use post for sending sensitive data is that sending sensitive data prevents data leakage via the query string although another way would be to use get with custom headers set although post makes much more sense,the problem is that the post shipping takes longer than the call get alert then shows me the session variable not updated,generaly get is shorter than post,we provide a number of support resources which may get faster response than post on third party sites including the virtuoso users mailing list public support forums and confidential support cases,testing post is trickier see the client code in the upload section for idea of using post;in the server code the do_get function handles all the get requests and the do_post function handles all the post requests,that s 3 queries to get one post which quickly takes its toll when returning more than 10 post,the most obvious difference is that the get method works by appending variables values to the url as you displayed in your question;the post method is more hidden -- you need to use developer tools to see the data -- so the post method is preferred,really it s just that the other verbs are idempotent no side effects and that performing a duplicate request should produce a duplicate result therefore post is the only verb that could satisfy those needs but in general post is a catch-all for anything the other verbs don t do well;post can legitimately be used as a retrieval more resulting in redirection or processing verb when the limitations of get cannot otherwise be worked around,i want to get all the users that has more than one post but,"
"get","post","more light weight in request requests body,less secure overall, is easier overall, is better in better type since,","30861556,32367715,9648919,20950402,","other than that accepting a get request might be - depending on the server side rest framework - slightly more light weight than post requests,the plugin defaults to requiring a post request to logout but using the config setting you reference you can make your application more convenient to use but less secure by allowing get or post requests,at most simplest you can perform a get as follows;a post is easier to be performed with httpclient,get is better for things that should be able to be bookmarked and simple queries with few short parameters;post is better for sensitive fields that the user shouldn t see for large binary transfers and for transfers with many fields or very long fields,"
"innodb","myisam","often slower in faster slower inserts, not overall,significantly faster in faster slower inserts, implementation has 10x better in better search postgis,more overall,faster in faster slower inserts,effecient than  in data primary situations, is not robust enough in robust things cloud,better crash in better engine data, table spends a lot overall,safer in better engine data,","5896369,29324517,27496923,51192211,6314981,6174246,49546605,32788507,10308655,37805257,1042989,","innodb is often slower than myisam being a transactional db engine with acid properties,you are using innodb not myisam i hope;myisam uses table locks and you won t be able to keep up.,myisam is the perfect choice since the database is almost only used for reading and myisam is significantly faster that innodb,you can create spatial indexes in aurora using the default innodb engine;edit there is a good article discussion the aurora geospatial indexing at which claims that you absolutely have to have myisam implementation has 10x better performance than mysql,not sure why people think myisam is more performant than innodb - it s a conundrum wrapped in an enigma,in some benchmarks i see that myisam is faster than innodb but seems i have a little improvement,innodb -- more effecient than myisam because of the way the primary key is clustered with the data in innodb. index y_id x_id -- the primary key makes it efficient to go one direction,you should be using innodb rather than myisam as myisam is not robust enough for the cloud environment;innodb has built in caching as part of it s buffer pool,innodb has better crash recovery while myisam is poor at recovering data integrity at system crashes,a table scan in a myisam table spends a lot of time stepping over cow paddies;innodb is much faster if you don t touch the blob,innodb is a safer acid compliant engine with some integrity features that myisam lacks,"
"innodb","myisam"," is better in better engine data,faster in data primary situations,faster overall,better overall,better crash in better engine data, not in tables myisam bigger, has not in better search postgis,faster in faster slower inserts, is a better in better engine data, engine provides more concurrency in default engine tuning, is old technology; overall,","43767498,4246897,9948201,9947851,13247861,3939107,7776000,30776508,19316364,22677434,17635225,","in fact oracle is so sure that innodb is better that oracle removed myisam from 8.0,myisam is still widely used in web applications as it has traditionally been perceived as faster than innodb in situations where most db access is reads,myisam has historically been viewed as faster than innodb but for recent versions of innodb that is true for a much much smaller set of use cases,if your database has a large innodb buffer pool and a small key buffer then innodb performance is going to be better than myisam performance especially for large tables,innodb has better crash recovery while myisam is poor at recovering,since you select innodb when creating tables all of your tables are innodb not myisam;myisam is the default storage engine on your server,innodb has better crash recovery while myisam is poor at recovering data integrity at system crashes;myisam has full-text search index while innodb has not,innodb seems slightly faster than myisam but this is really marginal,i can t find the timeout setting for myisam right now - it s set in mysqld.conf for innodb;as others have said - innodb is a better engine if you need transaction support,the innodb engine provides more concurrency and can do row level locking rather than locking the entire table;there may be some cases where you want to take locks on multiple myisam tables if you want to maintain referential integrity for example and you want to disallow other sessions from making changes to any of the tables while your session does your session work,myisam is old technology;innodb is the more modern engine,"
"innodb","myisam","newer in newer older transaction,better in better engine data,better in better engine data,faster in faster slower inserts,more effecient in data primary situations,faster in faster slower inserts, requires more attention overall, table is much faster in faster slower inserts, is more in data primary situations, is better overall,more overall,","15558403,32689837,42305333,5676173,30134365,1774377,14846908,4087592,8092532,47681187,24906639,","innodb is newer while myisam is older,but in certain scenarios myisam works better than innodb,innodb uses row level locking while myisam can only use table level locking that is why innodb has crash revovery is better than myisam,myisam is faster than innodb for reads myth,innodb -- more effecient than myisam because of the way the primary key is clustered with the data in innodb,myisam inserts are going to be faster than innodb so if you re logging data and retrieving it later that will be a win,innodb requires more attention to tuning configuration variables since the defaults are not exactly suitable for use by a production website,myisam table is much faster than innodb but no rollback is possible,innodb is more suitable for data critical situations that require frequent inserts and updates;myisam on the other hand performs better with applications that don t quite depend on the data integrity and mostly just select and display the data,some table-scan queries and bulk inserts are faster in myisam;innodb is better at indexed searches,innodb is more resistant to table corruption than myisam,"
"innodb","myisam","much more overall,faster in faster slower inserts, has a limit overall,better in better engine data,stricter overall,slower overall, -- not in supports row-level table-level, was the default overall,faster than  in data primary situations,much slower 7x overall,faster in contention tests model,","31182295,20297324,40110286,23940847,8021206,37657534,2458944,24947269,8153417,25632749,3707592,","this is a huge performance boost look like myisam is much more quicker for fulltext in mysql than innodb,also note that some mysql engines are faster than others for example myisam may run faster than innodb at expense of the lack of real foreign keys,myisam has a limit of 1000 bytes for indexes;innodb has an even smaller limit 767 bytes unless you re on mysql 5.7.7+ in which case the limit is 3072 bytes by default,innodb when tuned well performs better than myisam,innodb is much much stricter than myisam,there are several q a for why is innodb much slower than myisam but i could not find any topic for the opposite,mysql fulltext indexes are only possibles for tables in myisam -- not for tables in innodb;and only innodb supports foreign keys transactions .,innodb was not a default storage engine until mysql 5.5;in my case myisam was the default storage engine and did not support the transactions,myisam is still widely used in web applications as myisam has traditionally been perceived as faster than innodb in situations where most db access is reads,first question is it normal that innodb is much slower 7x slower than myisam for such usage,i ve figure out that even though myisam has locking contention it s still faster than innodb in most scenarios because of the rapid lock acquisition scheme it uses,"
"innodb","myisam"," row becomes overall,better in better engine data,slower in faster slower inserts,faster in faster slower inserts,more overall,much better in better engine data,much faster in slower table stuff,significant difference between  overall,2x faster then overall,more overall,more strict in data primary situations,","43562730,5850324,16785740,30655594,23090582,30545353,2943437,1730263,13494410,8203841,10308655,","f so then there is a nasty problem in myisam not innodb that may lead to 500 additional fragmentation hits;when a myisam row is updated and a myisam row becomes longer then a myisam row will not longer fit where a myisam row is,it is said that if you have a mostly read based application you should use myisam as it is better than innodb,in innodb the count s when where group by or join is not used execute slower than in myisam because the row count is not stored internally,myisam is faster when the query is simple but it s much slower in a high concurrent environment as its table level lock comparing to innodb s row level lock,innodb is more scalable and myisam doesn t support any of the properties of acid,in many discussions and even in mysql documents it has been mentioned that in case of updates innodb performs much better than myisam,if you have so many records in the table then the first thing is to change the table engine to innodb if its not innodb because for large number of records innodb is much faster as it caches the table data while on the contrary myisam engine only caches the indexes so each time it has to do a full table scan from disk if the data required cannot be fetched from index,with only a few transactions per second tps you re not likely to tell the difference in the performance but innodb is more reliable and scales better;2 with only 2-3 tps you will not see any significant difference between innodb and myisam,select queries in myisam runs 2x faster then in innodb but the updates and insert queries are much slower in myisam,innodb is also being developed more than myisam,innodb is more strict in data integrity while myisam is not as,"
"innodb","myisam","better job overall,relatively newer in newer older transaction,better choice in better engine data,desirable than  in control mvcc multi-versioning, is slower in slower table stuff, there is also in better engine data,longer rows in longer keys foreign,really faster in faster slower inserts,quicker in faster slower inserts,better in better engine data,better choice in better engine data,","45256543,8213500,24803981,56878249,12813429,8021213,12584994,45542506,32987720,8444146,15329943,","in this area i think innodb s change buffer does a better job than myisam s do it now,innodb is relatively newer than myisam and is transaction safe,mostly innodb is better choice than a myisam,-- numa control -- myisam is deprecated and mostly less desirable than innodb -- local_infile on is a potential security issue -- size of qc -- too small not of much use,myisam does not have relations foreign keys if you want to keep integrity between tables you have to code some triggers whereas in innodb you can use delete on cascade and this stuff;but innodb is slower when in migrations because it has to check constraints,generally innodb is much better for preserving your data integrity so stick with it and do not go back to myisam there is also a matter of what each engine is best fore but unless you give us more details there is no point in dwelling on that,myisam supports longer rows than innodb does,you are right because myisam is really faster than innodb,there are other optimizations available but general rule is that innodb will be quicker than myisam is even with table growth,innodb scales better than myisam,so for such an application is it will innodb be a better choice over myisam,"
"innodb","myisam","more overall,better in better engine data,faster overall,faster in faster slower inserts,higher in higher resource consumption,infinitely more efficient in data primary situations, runs in auto-commit in default engine tuning,less suitable in tables myisam bigger, to manage doctrine in keys relationship contraints,less memory overall,more strict in data primary situations,","365834,17765637,12088160,24465773,579589,18088402,15070416,754723,10429195,7558745,7776021,","innodb has more overhead but uses row-level locking so that reads and writes can happen concurrently without the problems that myisam s table locking incurs,almost always innodb is a better choice than myisam,the engine is myisam i ve heard people recommend switching to innodb but many others said myisam is faster with large amounts of data in terms of counting and innodb better for safe transactions,in a thread i came to know that myisam is faster for reads innodb is faster for writes,their conclusion innodb has 30 higher performance than myisam on average,there are some situations when myisam is infinitely more efficient than innodb when manipulating large data dumps offline because of table lock,i m assuming you are using innodb so this answer is only valid for that engine or any other transaction-capable engine meaning myisam isn t included;by default innodb runs in auto-commit mode,plus you have to make your tables myisam which is generally less suitable than innodb,doctrine2 uses innodb which supports foreign keys used in doctrine associations;but as myisam does not support this yet you can not use myisam to manage doctrine entities,myisam uses less memory than innodb and the actual data files are often quite a bit larger for innodb,innodb is more strict in data integrity while myisam is loose,"
"innodb","myisam","slower in faster slower inserts,slower overall, is better in level row quote,indexes better in better search postgis, was improved a lot; overall,sometimes better in better engine data, has faster in faster slower inserts,slower in faster slower inserts,faster in faster slower inserts, enforces referential integrity; in referential myisam integriry,bigger in tables myisam bigger,","9744170,32780790,3293364,9776443,4460056,37900930,7403512,37088068,32692860,5173263,2114537,","innodb is slower than myisam but in which cases,to answer the real question why is myisam slower than innodb i can t give an authoritative answer,i ll just reiterate that in tvanfosson s quote from an article myisam is better suited for system with mostly reads;this is because this uses table level locking instead of row level like innodb so myisam can t handle high concurrency as well plus this s missing features that help with data integrity such as transactions and foreign keys again already mentioned by others,your table sounds fairly large so i doubt it s being held entirely in memory but it sounds like innodb handles indexes better than myisam anyway plus since you re being so specific it may be caching the result of the query,since then innodb was improved a lot;myisam was not,this is one way in which innodb is sometimes better than myisam,use innodb instead of myisam;innodb has faster indexes,after all innodb shouldn t be slower than myisam when using count + where but that s exactly what is happening here,speed does matter here myisam is still slightly faster than innodb especially for reads,innodb enforces referential integrity;myisam does not,innodb tables are about 4x bigger than their myisam counterparts,"
"innodb","myisam","better in better engine data,more susceptible in better engine data,better in better engine data,higher in higher resource consumption,better in better engine data, does much better overall,better due in better engine data,better in better engine data, has far better overall,better in level row quote,better in better engine data,","4548313,14464983,1318324,13888408,15729523,8603197,13628627,5675916,9524360,9913344,8132481,","i heard correct me if i am wrong that innodb is better for tables that will be updated a lot because of row locking opposed to myisam s table locking,myisam is more susceptible to data corruption than innodb and usually innodb performs better because it caches both data and indexes,3 mysqlhotcopy inly works on myisam tables and in most applications you are better off with innodb,innodb has a higher resource consumption rates then myisam so keep that in mind,innodb is a better choice as myisam is really old engine and would fade away in near future,innodb loves to be able to have it s indexes in memory it slows considerably if it can t;myisam does much better in low memory situations with large data sets,furthermore i do not have control over the engine â myisam or innodb innodb performs better due to row based locking instead of table as in case of myisam,1 in mysql engines myisam is better or innodb,myisam could crash on insert update intensive applications but it has automatic recovery;however keep in mind that in such application innodb has far better performance,innodb is usually much better than myisam at tables being available while insert update and delete are happening because innodb uses row level locking for updates whereas myisam uses table level locking,you insert in there but never select from there as far as i know today myisam is better than innodb in this case,"
"innodb","myisam","slower in faster slower inserts,general overall,more overall, you wouldn overall,much slower in faster slower inserts,better faster in faster slower inserts,generally faster in faster slower inserts,slower in faster slower inserts,better than  in better search postgis,slower in faster slower inserts,faster in faster slower inserts,","15678536,48388047,14232379,10803241,1730317,35190842,3536007,15213134,7887458,990830,12285797,","edit for the read-performance this link shows that innodb often is actually not slower than myisam,unless there s a feature of myisam that you can t get with innodb i would highly recommend using innodb and optimizing where necessary,myisam is more prone to locking problems than innodb i believe,and yes using innodb makes a difference;if you were using for instance myisam you wouldn t have transactions at all so any change will be permanent similarly to autocommit true,if you find that innodb is much slower for inserts updates bear in mind that it offers a much better level of durability - if you tune it for approximately the same durability as myisam then you ll see good performance hopefully,most of the literature that says myisam is better faster whatever then innodb is old literature,myisam generally performs faster because it lacks certain functions innodb has such as rollback... but it has only table locking,innodb shouldn t be much slower than myisam,myisam is actually better than innodb for spatial data btw because postgis also supports r-tree spatial indexes but not as powerful queries as postgis if you just need points though innodb or myisam b-trees are adequate,innodb is slower for read only databases because it has features acid compliant row level locking that myisam leaves out,myisam has proved to be faster than innodb for me,"
"innodb","myisam","heavier in faster slower inserts, maintains referential integriry; in referential myisam integriry,slower in faster slower inserts,often faster in faster slower inserts,usually more overall,faster in faster slower inserts,slower than  overall,faster in faster slower inserts,better overall,faster in faster slower inserts,higher throughput than  in higher harder knobs,","262798,1777759,12620218,19705475,29054457,389824,49600438,15620322,380074,26759803,405022,","i know innodb is heavier than myisam but just how much more,innodb maintains referential integriry;myisam does not,i have been told innodb is faster on executing writes but slower than myisam doing reads i cannot back this up and could not find any article that analyses this i do however have the guy that told me this in high regard feel free to ignore this point or do your own research,myisam is often faster than innodb in terms of raw performance mostly because it is not acid,for an oltp type system innodb is usually more sensible than myisam did you check the contention before trying a different engine,innodb is actually faster than myisam in quite a few cases so it depends on what your application s mix of selects updates concurrent queries indexes buffer configuration etc,the myisam storage engine s slower than innodb for most queries and myisam has design defects with respect to data integrity and atomicity,now the response i got from my boss is that i need to prove that innodb will run faster than myisam,innodb has better performance than myisam though innodb needs more attention to tuning the configuration innodb supports atomic changes transactions foreign keys and innodb is much more resistant to corrupting data in a crash,note however that for high traffic websites we do modify the joomla core and we also switch the tables from innodb to myisam regardless what others might think here myisam is much faster than innodb,innodb is harder to tune since innodb has more knobs but a properly tuned innodb system can often have higher throughput than myisam due to better locking and better i o patterns,"
"innodb","myisam"," has shown much better overall,more in keys relationship contraints,faster than  overall,more in better engine data,more optimization overall,faster in faster slower inserts, does not in supports row-level table-level,less in faster slower inserts,better in better engine data,faster in faster slower inserts,quicker in faster slower inserts,","10562872,9114324,12906984,15558403,12100342,26888417,15558403,16397869,6564281,20327983,38256015,","innodb has shown much better results in the beginning but has quickly deteriorated much before 1mil records until i have removed all indexes including primary;at that point innodb has become superior to myisam when executing inserts updates,innodb provides more complex keys structure than myisam foreign keys and regenerating keys is really slow in innodb,myisam has historically been viewed as faster than innodb but for recent versions of innodb that is true for a much much smaller set of use cases;myisam is typically faster for table scans of read-only tables,innodb is more complex while myisam is simpler,3 a first of all if you haven t done yet change the storage engine to innodb which has row level locking and in newer versions of mysql has more optimization than myisam has,myisam for reads may well be faster than innodb,innodb implements row-level lock for inserting and updating while myisam implements table-level lock;innodb has transactions while myisam does not,with innodb there is less time lost from table locking while myisam is faster in table readings,sometimes i got asked on some interviews what benefits does innodb have against myisam and when myisam is better than innodb,myisam is almost 4 time faster than innodb which is not acceptable in the environment we are working as i mentioned earlier that every second is worth many dollers for us,innodb handles inserts with composite primary keys quicker than myisam,"
"innodb","myisam"," tables; is more in robust things cloud,slower in faster slower inserts, does not in supports row-level table-level,better in better engine data, is a bit more in tables myisam bigger,less space in data primary situations, is better in better search postgis, is a better in level row quote,reliable than  in robust things cloud, doesn overall,faster in faster slower inserts,","55971083,4747099,277445,46680251,7765857,41418598,3941624,3533390,5676173,57182043,990780,","those are things that are sometimes needed for myisam tables;innodb is more robust,innodb table is a bit slower than myisam tables but i don t think it is a major problem as you told you are using drupal system is that a kind of mult-sites like a word-press system,innodb support transactions whilst myisam does not;innodb supports referential integrity whilst myisam does not,oracle has kept improving innodb by so much that they declare that it is better than myisam in virtually all situations,myisam tables will be reloaded automatically as soon as the table is next touched by mysql obviously you need to make sure the .myi .myd and .frm files are consistent;innodb is a bit more complex and depends on whether it s a traditional configuration with all tables in the same tablespace or if you re using innodb_file_per_table,myisam also tends to store data in less space than innodb,you have an acute limit on how much space you can use innodb tables take up a fair bit more space;you require full-text search capabilities myisam is better for this purpose,additionally innodb has row level locking which is far more performant under concurrent load than myisam table level locking;i could keep going but somone s already provided a really good summary of why innodb is a better choice for oltp,innodb is more reliable than myisam,innodb is a transactional storage engine of mysql whereas myisam is a non-transactional storage engine;in other words innodb follows the acid properties to maintain the integrity of data but myisam doesn t follow acid properties thus failing to maintain the integrity of the data,i heard myisam is faster but others say innodb can be fast also but it takes abit more to optimize it,"
"innodb","myisam"," and implements the fulltext in faster slower inserts, is faster overall, not ; overall, does not in keys relationship contraints,older overall,more in space disk 2x-3x,faster in faster slower inserts, it s a bit overall,slower in faster slower inserts,faster in faster slower inserts,faster in faster slower inserts,","4233878,4514872,34260915,7776000,39187518,38401922,23940847,4418648,277511,40690288,10558700,","myisam is slightly faster than innodb and implements the fulltext index which is quite useful for integrating search capabilities;myisam is not transacted and doesn t implement foreign key constraints which is a major drawback,innodb supports a ton of features which myisam doesn t the most important being transactions followed by referential integrity and bunch of other items;many people will try to argue that myisam is faster and so it still has a place but a that gap has closed considerably and speedups can be done outside of the database ie memcached etc and b a fast database with bad data isn t all that useful,make sure the engine is innodb not myisam;myisam silently ignores transaction statements start commit,innodb has transactions while myisam does not;innodb has foreign keys and relationship contraints while myisam does not,the lone exception is myisam which is an older mysql storage engine which does not support transactions is more prone to corruption and generally tends to perform worse than innodb,innodb consumes more disk space than myisam -- typically 2x-3x,even this blog from 2007 shows benchmark results that innodb is on par with or faster than myisam under most workloads,if this all seems like too much remember that you can just use a filesystem or lvm snapshot online with innodb and then just copy the files;with myisam it s a bit trickier but can still be done,innodb is slower than myisam for most uses but can perform faster in certain conditions due to a better locking mechanism,the trope about myisam being faster than innodb is a holdover from code that was current in the mid-2000 s,beststat is innodb so i have row-level locking and consindering i do a lot of inserts-updates it should be faster than myisam,"
"innodb","myisam","better in mysql versions original,faster in faster slower inserts, may still in default engine tuning, has crash recovery in storage recovery engine,slower than  in faster slower inserts, is more in data primary situations,more in data primary situations, has just gotten better faster in faster slower inserts, is not overall,better in better engine data,better in better engine data,","8388057,18971053,13503732,11869634,13442619,13950440,17195485,40690288,7218155,13482192,13388561,","newer versions of mysql 5.5+ have extended innodb to support all the features that were previously only available on myisam such as fulltext and geospatial indexing and innodb performance is usually considerably better than myisam when configured properly,use myisam usually much faster than innodb if your data base isnt transaction oriented,though innodb has many more tuning parameters and it s important to set some edge cases where myisam may still run faster well because the defaults are much too low for good performance,innodb has crash recovery built into the storage engine s initialization;myisam does not,this also affects how the data is stored which leads to myisam being slower than innodb on insert due to myisam requiring a full index re-write on every insertion,normally you should use innodb;however for read-only tasks data mining myisam is more efficient,innodb is affected by the primary key much more than myisam and it could make a noticeable difference,innodb has just gotten better faster and more reliable since then;myisam is not being developed,i think innodb implements a true acid and does a lot of fsync s to save the data;and myisam is not a true acid and does less fsync s,finally even if somehow you could manage myisam to perform better than innodb that doesn t mean innodb is inadecuate at all,i have heard that myisam performs better than innodb on read-oriented tables,"
"innodb","myisam","faster in faster slower inserts,slower in contention tests model,faster in faster slower inserts,better in better engine data, was also worse overall, is usually quicker if properly in faster slower inserts,slower in faster slower inserts,often faster in faster slower inserts, was much slower in faster slower inserts, does not perform well overall,better choice in better engine data,","3137522,46446800,8869636,19664261,77625,4552297,15213196,14568413,5868125,6047005,19725362,","but innodb has improved dramatically in the past few years and in most cases today innodb performs faster than myisam,during my tests of innodb v myisam i found that when i did resolve any contention issues the innodb model was 40 slower than myisam,if you are not using transactions while storing or updating tables switch table type to myisam its quite faster than innodb with much less overhead,myisam delivers better read performance compared to innodb at the cost of being less acid compliant,i found that the table-level locking in myisam caused serious performance problems for our workload which sounds similar to yours;unfortunately i also found that performance under innodb was also worse than i d hoped,while myisam can be faster for some queries innodb is usually quicker if properly tuned,innodb does support transactions and referential integrity but the trade-off is that it is a bit slower than myisam,myisam is often faster than innodb but isn t safe to use in a production environment for critical data,so if you know how to use it it can be screaming fast innodb was much slower due to concurrency issues;myisam is faster for small simple selects is total bull postgres will zap a small simple select in 50-100 microseconds,one of the main performance advantages of innodb is row-level locking as opposed to table-level locking of myisam which means that multiple insert update queries against the same table won t block each other unless really necessary;however innodb does not perform well at all out of the box you really need to fine-tune your mysql configuration for your specific site,innodb would be a better choice than myisam for a high-write table,"
"innodb","myisam","more space in space disk 2x-3x, works better in tables myisam bigger,not faster in faster slower inserts,faster in faster slower inserts, instead; is slower in slower table stuff,faster in higher harder knobs,better in better engine data,longer in longer keys foreign,faster in faster slower inserts,marginally better in default engine tuning, maintains the count in slower table stuff,","26765961,11830012,40690288,20412828,327972,11390363,1247801,15385253,10332345,7492861,54183894,","myisam is more space friendly than innodb you can start with that one,also restructure your tables to use innodb vs myisam;innodb works better with larger tables which are frequently read written,myisam is not faster than innodb anymore for most types of queries,have you considered changing to innodb - it has much better concurrency support and in some contexts will run faster than myisam,nother thing is that i think you use myisam table engine which locks the entire table on update.i suggest you use innodb instead;innodb is slower on select -queries but faster on insert and update because it only locks the row it s working on and not the entire table,out of experience i m involved to a project that uses huge amount of data using mysql and we mostly prefer myisam for data that can be generated it allows to achieve much higher performance losing transactions but generally speaking myisam is faster but innodb is more reliable,innodb supports better locking semantics so if there will be occasional or frequent writes or if you want better data integrity i d suggest starting there and then benchmarking myisam later if you can t hit your performance targets,one thing is that if your inserts are not in the order of increasing primary keys innodb can take a bit longer than myisam,2 - i have read about myisam vs innodb the conclusion for me was that myisam is faster when it comes to read-only whereas innodb is designed for tables that get updated or inserts more frequently,there may be some exotic corner cases where myisam performs marginally better for certain workloads table-scans or high-volume insert-only work but the default choice should be innodb unless you can prove you have a case that myisam does better,count with innodb is slower than count id because innodb doesn t cache the row count;on the other hand with myisam count query would be faster and return result in no time because myisam maintains the count of rows,"
"innodb","myisam","smaller footprints overall, supports referential integrity overall,slower in faster slower inserts,slower in faster slower inserts,more overall,longer in longer keys foreign,better engine in default engine tuning,slower in faster slower inserts,better in better engine data,larger in slower table stuff,better in better engine data,","5676173,277445,5676173,2369500,9675447,3075441,11977129,6348127,8615205,8004971,6564322,","myisam tables have smaller footprints than innodb ones myth,innodb supports referential integrity whilst myisam does not;innodb handles indexes a bit differently storing the primary key as part of every index making indexes take up more room on the disk but also making a covering index more likely,innodb is transactional so inserts will generally be slower than myisam,innodb tables are even slower than myisam tables for inserts and the delayed key write option is not available,innodb doesnt read number of rows from stored cached value like myisam does if column is not null cause innodb is more optimized for writing than reading opposing to myisam,so that means that in practice an innodb write will take 3 times longer than a myisam write,tuning innodb which is the better engine requires adjusting different values than myisam which is often the fast-but-unsafe default,generally speaking innodb is slower than myisam as innodb is atomic while myisam is not,since you don t do updates i think myisam would be better than innodb in this scenario,put another way let s say i start with a new innodb table and insert 20 gb of data assuming that 20 gb incorporates all the excess innodb stuff i realize data stored in innodb is larger than myisam then i delete all data then i insert 10 gb of data,myisam is better than innodb when you don t need those advanced features and storage speed is more important than other concerns,"
"innodb","myisam","faster in faster slower inserts, which uses a significantly in default engine tuning,faster in indexes pattern unrestricted,much better in better engine data, is definitely safer so overall,more complex in control mvcc multi-versioning,faster in faster slower inserts, can also overall,better in better engine data,faster in better engine data,smaller same in tables myisam bigger,","1647114,57489346,4548349,1970160,803031,9211731,6679187,745963,1729806,1959795,14454893,","ok there are some cases where myisam is faster than innodb but rarely enough that it s worth putting up with the lack of acid-compliance,if you cannot control the server configuration you could switch your table to innodb which uses a significantly smaller stopword list;some additional notes the fulltext index uses a minimum word length by default 4 for myisam and 3 for innodb,myisam is faster for certain queries and supports fulltext and spatial indexes while innodb is transactional and more concurrent,someone just told me that innodb is much better than myisam,innodb is definitely safer so i d stick with it anyhow,innodb implements mvcc multi-versioning concurrency control so locking is much more complex than with myisam,after testing it seems that myisam is faster than innodb when using when there is no where clause,innodb is not always so lucky;the myisam can also be backed-up out from under the database,am i right that innodb is better for frequent concurrent updates and inserts than myisam,anyone who thinks myisam is faster is either not tuning innodb correctly or has such small data that who cares,as you know myisam table sizes are about three or more times smaller than same innodb tables,"
"innodb","myisam","longer in better engine data,slower in faster slower inserts,faster in faster slower inserts,more in default engine tuning,faster in faster slower inserts,faster in faster slower inserts, doesn in supports row-level table-level,more overall, which handles indexes differently overall, works best overall,more concurrent overall,","11056538,5291159,1647078,24803579,9114209,8171849,18139586,20327983,51181862,38713277,8349825,","although 4m isn t that many rows with contention and depending on your structure and database engine it may take a while i think myisam alters take longer than innodb because myisam requires whole table locks and handles concurrency less well,oh and just incase you were thinking innodb is slower than myisam - the myisam implementation i tested was twice as slow in all counts,i am not sure if this is no longer true myisam is faster than innodb for reads,and now i have learned that innodb uses more memory at-least while reading than myisam engine so i am trying to change the default engine of mysql to use myisam,the script was tested using myisam and it indexes products relatively fast much much faster than innodb,in general it seems as though the concensus is to primarily use innodb but there are still some areas in which myisam is much faster than innodb,innodb supports row-level locking transactions and foreign key constraints;myisam doesn t,to our suprise innodb took hours more than myisam,furthermore the clustering of the pk in innodb may make your main selects run faster than with myisam which handles indexes differently,myisam works best on non-transactional purpose such as where you need search;innodb works better where you use transaction such as insert update delete,innodb is more concurrent than myisam since it provides row-level locking,"
"innodb","myisam","engine more overall,faster in faster slower inserts,much faster in faster slower inserts,faster in faster slower inserts, is much faster in faster slower inserts,efficient than  overall,faster in faster slower inserts,better performance in better engine data, may be more overall,less overall, has a better in mysql versions original,","9671726,14846908,10474759,29879444,1094085,52765705,43767498,5019157,6885549,19705475,45261525,","it is possible that the config of your innodb engine is more efficient for your searches than the way you have myisam set up,innodb not only has the advantages you list but it is also faster than myisam in many benchmarks,myisam is much faster for reads but since it locks the whole table for writes this is where overall throughput drops compared with innodb,also mysql version starting from 5.5 - innodb performs faster than myisam,just the table locking alone will affect speed such that innodb would be overall faster;also notice that myisam is much faster on inserts not so much on selects,this is an example of where innodb s design is inherently more efficient than myisam s. if your goal is to get a bunch of random rows from a table read my treatise,yes it is an old wives tale that myisam is faster than innodb,i currently have myisam and i would like to stay with it because it had far better performance than innodb in my case but i heard that innodb has acid transactions,innodb is the more modern storage engine and is considered more reliable supporting things such as atomic transactions and foreign key constraints;however myisam may be more performant and supports full text search,therefore accessing myisam consumes less resources than innodb,myisam was the original format of mysql but many years ago innodb has been preferred for many reasons;on high-level picture your app will better perform as innodb has a better lock management,"
"innodb","myisam"," not overall,slower in faster slower inserts,slower in faster slower inserts, does not; in keys relationship contraints,faster overall, performs much in storage recovery engine,better in better engine data,better in better engine data,safer in better engine data, does not yet in supports row-level table-level,better in better engine data,","53055534,31143008,1359270,7776021,405022,22774410,31744395,8893726,46067630,12613994,5248641,","i strongly recommend using innodb not myisam,i know innodb tends to be a bit slower than myisam on counting but this is far too long,however innodb tends to be slower as myisam,innodb has foreign keys and relationship contraints while myisam does not;innodb has better crash recovery while myisam is poor at recovering data integrity at system crashes,myisam is faster in data warehousing situations such as full table scan reporting etc.. but innodb can actually be faster in many cases with normal oltp queries,the myisam storage engine takes table level locks even for select statements and that kills concurrency;innodb performs much better with concurrent queries,you can also try using innodb since it s said is better than myisam handling bulk inserts with data already on it,myisam is better for high read volumes innodb for high update volumes due to table vs row locking,nothing is 100 safe but innodb properly used is a lot safer than myisam against data loss and corruption,since you posted your create table statements the problem is now totally obvious - only innodb supports foreign key constraints in mysql;myisam does not yet,you can also use compression on the tables to keep them smaller but innodb is vastly better than myisam at data integrity,"
"innodb","myisam"," it gets a little overall,higher throughput in higher resource consumption,slower in faster slower inserts, creates indices automatically in longer keys foreign, does not in longer keys foreign,slower in faster slower inserts, does not; in keys relationship contraints, is much more overall,better in better engine data, to ; is more in tables myisam bigger,faster in faster slower inserts,","3075441,9220735,21863823,11245313,7896584,45542267,10308655,13629530,3286802,4057456,4077800,","myisam performance is really bound by disk speed for this reason;with innodb it gets a little more complicated,innodb delivered 4.6x higher throughput than myisam while achieving,at the beginning the insert performance of innodb is almost 50 times slower than myisam and tokudb is 40 times slower than myisam,innodb creates indices automatically for foreign keys;myisam does not support foreign keys at all,innodb does not support hash keys only btree;myisam is infamous for being unreliable,i am trying to compare the myisam and innodb write read performance but i am suprised that the myisam s read is much more slower than innodb while its write is much more faster this is totally opposite compared to what i have learned,innodb has transactions while myisam does not;innodb has foreign keys and relationship contraints while myisam does,myisam is liable to explode in the worst possible way if the system crashes or loses power during heavy write activity it s not journaled and can t always be recovered;innodb is much more robust,therefore innodb can handle higher levels of concurrency better than myisam,change your tables from myisam to innodb;innodb is more reliable in periods of high traffic,myisam is faster but does not support the use of transactions like innodb does,"
"innodb","myisam","faster in faster slower inserts,faster in slower table stuff,primarily better in better search postgis,better in better engine data,slower than  in faster slower inserts,better in better engine data,less archived overall,faster in faster slower inserts,slower in indexes pattern unrestricted, is the better in better engine data,faster in faster slower inserts,","4459983,32931270,7335774,14063234,52138484,20900198,40198622,6689067,11505760,3359437,15620322,","some people have said that for reads myisam is faster but recent improvements in innodb have either alleviated or eradicated this difference,i am creating an commerce website and i am stuck in a database problem i am storing customer orders please tell me which is better myisam or innodb i have to use transaction like feature in customer order table and i personally prefer myisam because it is much faster than innodb and it also supports full-text searching is there any way to use transaction like rollback feature in myisam so that if anything goes wrong table will be rollback to its previous state how to do that without any external library or any other server side access and i have to use mysql,as you are no doubt aware from your searches innodb is primarily better than myisam for high volume inserts not counting non-comparable features like foreign keys or full text searches which are exclusive to innodb and myisam respectively while myisam is primarily useful for tables which are read more often than they are written to,also i can t see your table engine but myisam is better for full text searching rather than innodb,myisam is slower than innodb in most cases and fails to support any acid properties,since myisam is better on selecting while innodb is better on writing,you will need about six times less disk space to store and memory to process your dataset compared to innodb or about 2-3 times less than archived myisam,edited to add myisam is faster than innodb because it is simpler,if you use a where clause though it changes the execution pattern to use indexes so in general innodb will be slower than myisam on full unrestricted counts where as the performance matches up on restricted counts,since you re looking at a pretty even mix of read write traffic innodb is the better choice;myisam s full-table locks on every write would probably be murder,in general is myisam faster than innodb,"
"innodb","myisam"," doesn in longer keys foreign,faster in faster slower inserts,slower in faster slower inserts,faster in faster slower inserts, is pretty overall, supposedly keeps data safer in data primary situations,slower in faster slower inserts,faster in data primary situations,faster in faster slower inserts,faster in faster slower inserts, does not; in keys relationship contraints,","389824,3184908,228750,30776508,9776326,8030863,9114209,9671122,4546410,1730317,13247861,","innodb enforces foreign key constraints;myisam doesn t enforce or even store foreign key constraints,there is a difference between the different storage engines though myisam is faster for a lot of select innodb is faster for a lot of insert update because it uses row locking instead of table locking and the way it handles indexes,also innodb is slower than myisam unless myisam is blocking for a huge select,in terms of pure speed it is not always the case that myisam is faster than innodb but in my experience it tends to be faster for pure read working environments by a factor of about 2.0-2.5 times,innodb is pretty much newer and has more support from mysql team so using it for smaller and easily recreatable restorable tables is what i am practising right now;datas which are more frequently updated inserted still use myisam on my own server,innodb and myisam;3 innodb supposedly keeps data safer,why is innodb so much slower than myisam in my case,i did a search online comparing myisam and innodb but all of the articles i read judged myisam being faster than innodb on select queries,in my opinion myisam use to be faster than innodb now they are pretty much the same in speed,it is a massive over simplification in some cases and plain wrong in others to say we know that myisam is faster than innodb,innodb has transactions while myisam does not;innodb has foreign keys and relationship contraints while myisam,"
"innodb","myisam"," is better overall,slower in slower table stuff,faster in faster slower inserts,faster in faster slower inserts, regarding performance in better engine data, has table-level locking in supports row-level table-level,faster in faster slower inserts,","15609235,17641256,1729806,7558597,57690911,29048612,2757616,","he speed comes with a price though myisam does not support transactions or foreign keys;this means this is optimal for example for saving and analyzing log data and as a backend for data mining and data warehouse technologies while innodb is better for transaction processing and as the storage layer of information systems in general,generally you can have as good performance for reading as in myisam in innodb tables - you just can use count without where clause and you always should have a suitable index for where clauses as in innodb table scan will be slower than in myisam,we know that myisam is faster than innodb when we don t have many concurrent updates inserts,if there are many modifications of the data it s said that innodb works faster because it uses row locking instead of table locking like myisam,you should never change your innodb storage engine to myisam because innodb has more benefits over myisam regarding performance and locking,innodb supports transactions myisam does not;innodb has row-level locking myisam has table-level locking,database performance of the innodb is not necessary faster than the myisam engine,"
